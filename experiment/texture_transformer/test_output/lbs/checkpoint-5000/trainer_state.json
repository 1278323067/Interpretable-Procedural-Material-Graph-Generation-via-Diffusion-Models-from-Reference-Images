{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.30363757818667636,
  "eval_steps": 5001,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006072751563733527,
      "grad_norm": 7.186389446258545,
      "learning_rate": 9.999998988960184e-05,
      "loss": 28.5336,
      "step": 10
    },
    {
      "epoch": 0.0012145503127467055,
      "grad_norm": 6.450451374053955,
      "learning_rate": 9.999995955841146e-05,
      "loss": 5.8003,
      "step": 20
    },
    {
      "epoch": 0.0018218254691200583,
      "grad_norm": 19.285921096801758,
      "learning_rate": 9.99999090064411e-05,
      "loss": 4.1025,
      "step": 30
    },
    {
      "epoch": 0.002429100625493411,
      "grad_norm": 6.392780303955078,
      "learning_rate": 9.999983823371122e-05,
      "loss": 3.7077,
      "step": 40
    },
    {
      "epoch": 0.0030363757818667636,
      "grad_norm": 3.809990406036377,
      "learning_rate": 9.999974724025045e-05,
      "loss": 3.4147,
      "step": 50
    },
    {
      "epoch": 0.0036436509382401167,
      "grad_norm": 3.4429056644439697,
      "learning_rate": 9.999963602609556e-05,
      "loss": 3.434,
      "step": 60
    },
    {
      "epoch": 0.004250926094613469,
      "grad_norm": 4.669011116027832,
      "learning_rate": 9.999950459129158e-05,
      "loss": 3.4633,
      "step": 70
    },
    {
      "epoch": 0.004858201250986822,
      "grad_norm": 4.70878267288208,
      "learning_rate": 9.99993529358916e-05,
      "loss": 3.8761,
      "step": 80
    },
    {
      "epoch": 0.005465476407360175,
      "grad_norm": 5.6883344650268555,
      "learning_rate": 9.9999181059957e-05,
      "loss": 3.4797,
      "step": 90
    },
    {
      "epoch": 0.006072751563733527,
      "grad_norm": 4.667462348937988,
      "learning_rate": 9.999898896355726e-05,
      "loss": 3.2826,
      "step": 100
    },
    {
      "epoch": 0.006680026720106881,
      "grad_norm": 5.330298900604248,
      "learning_rate": 9.999877664677009e-05,
      "loss": 2.9715,
      "step": 110
    },
    {
      "epoch": 0.007287301876480233,
      "grad_norm": 6.202093601226807,
      "learning_rate": 9.999854410968134e-05,
      "loss": 2.9419,
      "step": 120
    },
    {
      "epoch": 0.007894577032853586,
      "grad_norm": 4.214776039123535,
      "learning_rate": 9.999829135238505e-05,
      "loss": 3.3057,
      "step": 130
    },
    {
      "epoch": 0.008501852189226939,
      "grad_norm": 3.475522994995117,
      "learning_rate": 9.999801837498346e-05,
      "loss": 3.3707,
      "step": 140
    },
    {
      "epoch": 0.009109127345600291,
      "grad_norm": 5.682331562042236,
      "learning_rate": 9.999772517758694e-05,
      "loss": 3.1139,
      "step": 150
    },
    {
      "epoch": 0.009716402501973644,
      "grad_norm": 4.191882133483887,
      "learning_rate": 9.999741176031408e-05,
      "loss": 3.0561,
      "step": 160
    },
    {
      "epoch": 0.010323677658346997,
      "grad_norm": 4.490618705749512,
      "learning_rate": 9.999707812329162e-05,
      "loss": 2.8682,
      "step": 170
    },
    {
      "epoch": 0.01093095281472035,
      "grad_norm": 3.096764326095581,
      "learning_rate": 9.99967242666545e-05,
      "loss": 3.1537,
      "step": 180
    },
    {
      "epoch": 0.011538227971093702,
      "grad_norm": 4.115010738372803,
      "learning_rate": 9.99963501905458e-05,
      "loss": 3.1016,
      "step": 190
    },
    {
      "epoch": 0.012145503127467054,
      "grad_norm": 2.6895015239715576,
      "learning_rate": 9.999595589511684e-05,
      "loss": 3.0048,
      "step": 200
    },
    {
      "epoch": 0.012752778283840409,
      "grad_norm": 5.5280046463012695,
      "learning_rate": 9.999554138052704e-05,
      "loss": 2.6903,
      "step": 210
    },
    {
      "epoch": 0.013360053440213761,
      "grad_norm": 4.60183572769165,
      "learning_rate": 9.999510664694408e-05,
      "loss": 2.6745,
      "step": 220
    },
    {
      "epoch": 0.013967328596587114,
      "grad_norm": 5.704331398010254,
      "learning_rate": 9.999465169454374e-05,
      "loss": 2.9479,
      "step": 230
    },
    {
      "epoch": 0.014574603752960467,
      "grad_norm": 2.7345144748687744,
      "learning_rate": 9.999417652351002e-05,
      "loss": 2.9361,
      "step": 240
    },
    {
      "epoch": 0.01518187890933382,
      "grad_norm": 3.391403913497925,
      "learning_rate": 9.999368113403508e-05,
      "loss": 3.0954,
      "step": 250
    },
    {
      "epoch": 0.015789154065707172,
      "grad_norm": 3.0357401371002197,
      "learning_rate": 9.999316552631928e-05,
      "loss": 3.1664,
      "step": 260
    },
    {
      "epoch": 0.016396429222080525,
      "grad_norm": 2.4493205547332764,
      "learning_rate": 9.999262970057113e-05,
      "loss": 2.8576,
      "step": 270
    },
    {
      "epoch": 0.017003704378453877,
      "grad_norm": 3.509894847869873,
      "learning_rate": 9.999207365700733e-05,
      "loss": 3.0379,
      "step": 280
    },
    {
      "epoch": 0.01761097953482723,
      "grad_norm": 2.7530264854431152,
      "learning_rate": 9.999149739585273e-05,
      "loss": 2.7517,
      "step": 290
    },
    {
      "epoch": 0.018218254691200583,
      "grad_norm": 4.6260576248168945,
      "learning_rate": 9.999090091734043e-05,
      "loss": 2.8676,
      "step": 300
    },
    {
      "epoch": 0.018825529847573935,
      "grad_norm": 5.038768768310547,
      "learning_rate": 9.999028422171159e-05,
      "loss": 3.1278,
      "step": 310
    },
    {
      "epoch": 0.019432805003947288,
      "grad_norm": 2.4521424770355225,
      "learning_rate": 9.998964730921568e-05,
      "loss": 2.8147,
      "step": 320
    },
    {
      "epoch": 0.02004008016032064,
      "grad_norm": 3.764702796936035,
      "learning_rate": 9.99889901801102e-05,
      "loss": 2.9978,
      "step": 330
    },
    {
      "epoch": 0.020647355316693993,
      "grad_norm": 3.569005250930786,
      "learning_rate": 9.998831283466099e-05,
      "loss": 2.8446,
      "step": 340
    },
    {
      "epoch": 0.021254630473067346,
      "grad_norm": 2.5747430324554443,
      "learning_rate": 9.998761527314191e-05,
      "loss": 2.6242,
      "step": 350
    },
    {
      "epoch": 0.0218619056294407,
      "grad_norm": 2.6841931343078613,
      "learning_rate": 9.99868974958351e-05,
      "loss": 2.7303,
      "step": 360
    },
    {
      "epoch": 0.02246918078581405,
      "grad_norm": 4.337249279022217,
      "learning_rate": 9.998615950303083e-05,
      "loss": 3.0008,
      "step": 370
    },
    {
      "epoch": 0.023076455942187404,
      "grad_norm": 5.8416595458984375,
      "learning_rate": 9.998540129502756e-05,
      "loss": 3.0236,
      "step": 380
    },
    {
      "epoch": 0.023683731098560756,
      "grad_norm": 3.5167787075042725,
      "learning_rate": 9.998462287213191e-05,
      "loss": 3.0139,
      "step": 390
    },
    {
      "epoch": 0.02429100625493411,
      "grad_norm": 3.1287996768951416,
      "learning_rate": 9.998382423465871e-05,
      "loss": 2.6025,
      "step": 400
    },
    {
      "epoch": 0.024898281411307465,
      "grad_norm": 2.7561633586883545,
      "learning_rate": 9.998300538293091e-05,
      "loss": 2.7249,
      "step": 410
    },
    {
      "epoch": 0.025505556567680818,
      "grad_norm": 2.219248056411743,
      "learning_rate": 9.99821663172797e-05,
      "loss": 2.5885,
      "step": 420
    },
    {
      "epoch": 0.02611283172405417,
      "grad_norm": 2.8309640884399414,
      "learning_rate": 9.998130703804438e-05,
      "loss": 2.6203,
      "step": 430
    },
    {
      "epoch": 0.026720106880427523,
      "grad_norm": 2.67535662651062,
      "learning_rate": 9.998042754557249e-05,
      "loss": 2.8381,
      "step": 440
    },
    {
      "epoch": 0.027327382036800876,
      "grad_norm": 3.424489736557007,
      "learning_rate": 9.997952784021967e-05,
      "loss": 2.8307,
      "step": 450
    },
    {
      "epoch": 0.027934657193174228,
      "grad_norm": 2.4972362518310547,
      "learning_rate": 9.997860792234981e-05,
      "loss": 2.727,
      "step": 460
    },
    {
      "epoch": 0.02854193234954758,
      "grad_norm": 2.3320651054382324,
      "learning_rate": 9.997766779233493e-05,
      "loss": 2.733,
      "step": 470
    },
    {
      "epoch": 0.029149207505920934,
      "grad_norm": 4.219310283660889,
      "learning_rate": 9.997670745055522e-05,
      "loss": 2.5761,
      "step": 480
    },
    {
      "epoch": 0.029756482662294286,
      "grad_norm": 2.828003168106079,
      "learning_rate": 9.997572689739907e-05,
      "loss": 2.8755,
      "step": 490
    },
    {
      "epoch": 0.03036375781866764,
      "grad_norm": 2.7965126037597656,
      "learning_rate": 9.997472613326304e-05,
      "loss": 2.8552,
      "step": 500
    },
    {
      "epoch": 0.03097103297504099,
      "grad_norm": 3.650280475616455,
      "learning_rate": 9.997370515855182e-05,
      "loss": 2.6381,
      "step": 510
    },
    {
      "epoch": 0.031578308131414344,
      "grad_norm": 3.817025661468506,
      "learning_rate": 9.997266397367836e-05,
      "loss": 2.4954,
      "step": 520
    },
    {
      "epoch": 0.03218558328778769,
      "grad_norm": 1.6731635332107544,
      "learning_rate": 9.99716025790637e-05,
      "loss": 2.4817,
      "step": 530
    },
    {
      "epoch": 0.03279285844416105,
      "grad_norm": 2.273855209350586,
      "learning_rate": 9.997052097513709e-05,
      "loss": 2.7781,
      "step": 540
    },
    {
      "epoch": 0.033400133600534405,
      "grad_norm": 2.9378833770751953,
      "learning_rate": 9.996941916233594e-05,
      "loss": 2.5581,
      "step": 550
    },
    {
      "epoch": 0.034007408756907755,
      "grad_norm": 3.8389534950256348,
      "learning_rate": 9.996829714110583e-05,
      "loss": 3.0879,
      "step": 560
    },
    {
      "epoch": 0.03461468391328111,
      "grad_norm": 3.4130256175994873,
      "learning_rate": 9.996715491190057e-05,
      "loss": 2.9079,
      "step": 570
    },
    {
      "epoch": 0.03522195906965446,
      "grad_norm": 2.044156551361084,
      "learning_rate": 9.996599247518206e-05,
      "loss": 2.6539,
      "step": 580
    },
    {
      "epoch": 0.035829234226027816,
      "grad_norm": 3.4560179710388184,
      "learning_rate": 9.996480983142041e-05,
      "loss": 2.9231,
      "step": 590
    },
    {
      "epoch": 0.036436509382401165,
      "grad_norm": 3.428556442260742,
      "learning_rate": 9.99636069810939e-05,
      "loss": 2.9796,
      "step": 600
    },
    {
      "epoch": 0.03704378453877452,
      "grad_norm": 2.4829540252685547,
      "learning_rate": 9.9962383924689e-05,
      "loss": 2.8804,
      "step": 610
    },
    {
      "epoch": 0.03765105969514787,
      "grad_norm": 3.457402229309082,
      "learning_rate": 9.99611406627003e-05,
      "loss": 3.1318,
      "step": 620
    },
    {
      "epoch": 0.03825833485152123,
      "grad_norm": 2.778350830078125,
      "learning_rate": 9.995987719563062e-05,
      "loss": 2.9028,
      "step": 630
    },
    {
      "epoch": 0.038865610007894576,
      "grad_norm": 3.100822687149048,
      "learning_rate": 9.995859352399094e-05,
      "loss": 2.8307,
      "step": 640
    },
    {
      "epoch": 0.03947288516426793,
      "grad_norm": 2.6477577686309814,
      "learning_rate": 9.995728964830036e-05,
      "loss": 2.8336,
      "step": 650
    },
    {
      "epoch": 0.04008016032064128,
      "grad_norm": 3.6645150184631348,
      "learning_rate": 9.995596556908622e-05,
      "loss": 2.6316,
      "step": 660
    },
    {
      "epoch": 0.04068743547701464,
      "grad_norm": 4.381188869476318,
      "learning_rate": 9.995462128688397e-05,
      "loss": 2.6818,
      "step": 670
    },
    {
      "epoch": 0.041294710633387986,
      "grad_norm": 3.122107982635498,
      "learning_rate": 9.995325680223728e-05,
      "loss": 2.9583,
      "step": 680
    },
    {
      "epoch": 0.04190198578976134,
      "grad_norm": 2.9715704917907715,
      "learning_rate": 9.995187211569797e-05,
      "loss": 2.8996,
      "step": 690
    },
    {
      "epoch": 0.04250926094613469,
      "grad_norm": 2.5021092891693115,
      "learning_rate": 9.995046722782601e-05,
      "loss": 2.6537,
      "step": 700
    },
    {
      "epoch": 0.04311653610250805,
      "grad_norm": 2.472275733947754,
      "learning_rate": 9.994904213918959e-05,
      "loss": 2.7742,
      "step": 710
    },
    {
      "epoch": 0.0437238112588814,
      "grad_norm": 2.840935707092285,
      "learning_rate": 9.994759685036501e-05,
      "loss": 2.7137,
      "step": 720
    },
    {
      "epoch": 0.04433108641525475,
      "grad_norm": 4.177767753601074,
      "learning_rate": 9.994613136193679e-05,
      "loss": 2.6396,
      "step": 730
    },
    {
      "epoch": 0.0449383615716281,
      "grad_norm": 2.6978330612182617,
      "learning_rate": 9.994464567449757e-05,
      "loss": 3.107,
      "step": 740
    },
    {
      "epoch": 0.04554563672800146,
      "grad_norm": 2.9850990772247314,
      "learning_rate": 9.99431397886482e-05,
      "loss": 3.0693,
      "step": 750
    },
    {
      "epoch": 0.04615291188437481,
      "grad_norm": 3.750215768814087,
      "learning_rate": 9.994161370499769e-05,
      "loss": 2.6602,
      "step": 760
    },
    {
      "epoch": 0.04676018704074816,
      "grad_norm": 2.1821258068084717,
      "learning_rate": 9.994006742416321e-05,
      "loss": 2.6184,
      "step": 770
    },
    {
      "epoch": 0.04736746219712151,
      "grad_norm": 2.588514804840088,
      "learning_rate": 9.99385009467701e-05,
      "loss": 2.6797,
      "step": 780
    },
    {
      "epoch": 0.04797473735349487,
      "grad_norm": 2.306520700454712,
      "learning_rate": 9.993691427345187e-05,
      "loss": 2.5766,
      "step": 790
    },
    {
      "epoch": 0.04858201250986822,
      "grad_norm": 2.2094011306762695,
      "learning_rate": 9.993530740485018e-05,
      "loss": 2.815,
      "step": 800
    },
    {
      "epoch": 0.049189287666241574,
      "grad_norm": 2.3844077587127686,
      "learning_rate": 9.993368034161489e-05,
      "loss": 2.5661,
      "step": 810
    },
    {
      "epoch": 0.04979656282261493,
      "grad_norm": 2.4530060291290283,
      "learning_rate": 9.9932033084404e-05,
      "loss": 2.9215,
      "step": 820
    },
    {
      "epoch": 0.05040383797898828,
      "grad_norm": 2.7166547775268555,
      "learning_rate": 9.99303656338837e-05,
      "loss": 2.6268,
      "step": 830
    },
    {
      "epoch": 0.051011113135361635,
      "grad_norm": 2.2288105487823486,
      "learning_rate": 9.992867799072833e-05,
      "loss": 2.6068,
      "step": 840
    },
    {
      "epoch": 0.051618388291734985,
      "grad_norm": 2.3088345527648926,
      "learning_rate": 9.99269701556204e-05,
      "loss": 2.4566,
      "step": 850
    },
    {
      "epoch": 0.05222566344810834,
      "grad_norm": 2.3499398231506348,
      "learning_rate": 9.992524212925056e-05,
      "loss": 2.7308,
      "step": 860
    },
    {
      "epoch": 0.05283293860448169,
      "grad_norm": 2.4770405292510986,
      "learning_rate": 9.99234939123177e-05,
      "loss": 2.7754,
      "step": 870
    },
    {
      "epoch": 0.053440213760855046,
      "grad_norm": 4.333104133605957,
      "learning_rate": 9.992172550552879e-05,
      "loss": 2.8169,
      "step": 880
    },
    {
      "epoch": 0.054047488917228395,
      "grad_norm": 3.036212205886841,
      "learning_rate": 9.9919936909599e-05,
      "loss": 2.8564,
      "step": 890
    },
    {
      "epoch": 0.05465476407360175,
      "grad_norm": 2.8859453201293945,
      "learning_rate": 9.99181281252517e-05,
      "loss": 3.1683,
      "step": 900
    },
    {
      "epoch": 0.0552620392299751,
      "grad_norm": 3.3369028568267822,
      "learning_rate": 9.991629915321836e-05,
      "loss": 2.7407,
      "step": 910
    },
    {
      "epoch": 0.055869314386348456,
      "grad_norm": 4.032517433166504,
      "learning_rate": 9.991444999423865e-05,
      "loss": 2.9512,
      "step": 920
    },
    {
      "epoch": 0.056476589542721806,
      "grad_norm": 2.22833251953125,
      "learning_rate": 9.991258064906041e-05,
      "loss": 2.9532,
      "step": 930
    },
    {
      "epoch": 0.05708386469909516,
      "grad_norm": 6.261169910430908,
      "learning_rate": 9.991069111843964e-05,
      "loss": 2.8817,
      "step": 940
    },
    {
      "epoch": 0.05769113985546851,
      "grad_norm": 3.274970054626465,
      "learning_rate": 9.990878140314047e-05,
      "loss": 2.8114,
      "step": 950
    },
    {
      "epoch": 0.05829841501184187,
      "grad_norm": 2.3626973628997803,
      "learning_rate": 9.990685150393523e-05,
      "loss": 2.455,
      "step": 960
    },
    {
      "epoch": 0.058905690168215216,
      "grad_norm": 3.046497106552124,
      "learning_rate": 9.990490142160442e-05,
      "loss": 2.5229,
      "step": 970
    },
    {
      "epoch": 0.05951296532458857,
      "grad_norm": 2.2845818996429443,
      "learning_rate": 9.990293115693667e-05,
      "loss": 2.8898,
      "step": 980
    },
    {
      "epoch": 0.06012024048096192,
      "grad_norm": 4.140326976776123,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.9451,
      "step": 990
    },
    {
      "epoch": 0.06072751563733528,
      "grad_norm": 5.539736747741699,
      "learning_rate": 9.989893008378572e-05,
      "loss": 3.0845,
      "step": 1000
    },
    {
      "epoch": 0.06133479079370863,
      "grad_norm": 4.5421013832092285,
      "learning_rate": 9.989689927692062e-05,
      "loss": 3.3652,
      "step": 1010
    },
    {
      "epoch": 0.06194206595008198,
      "grad_norm": 2.962928295135498,
      "learning_rate": 9.989484829095478e-05,
      "loss": 2.8889,
      "step": 1020
    },
    {
      "epoch": 0.06254934110645534,
      "grad_norm": 3.3312432765960693,
      "learning_rate": 9.989277712671766e-05,
      "loss": 2.6089,
      "step": 1030
    },
    {
      "epoch": 0.06315661626282869,
      "grad_norm": 2.9687602519989014,
      "learning_rate": 9.989068578504684e-05,
      "loss": 2.7753,
      "step": 1040
    },
    {
      "epoch": 0.06376389141920204,
      "grad_norm": 4.476959705352783,
      "learning_rate": 9.988857426678811e-05,
      "loss": 2.3883,
      "step": 1050
    },
    {
      "epoch": 0.06437116657557539,
      "grad_norm": 3.7683510780334473,
      "learning_rate": 9.98864425727954e-05,
      "loss": 2.6421,
      "step": 1060
    },
    {
      "epoch": 0.06497844173194875,
      "grad_norm": 2.254359722137451,
      "learning_rate": 9.98842907039308e-05,
      "loss": 2.5235,
      "step": 1070
    },
    {
      "epoch": 0.0655857168883221,
      "grad_norm": 2.0745208263397217,
      "learning_rate": 9.988211866106457e-05,
      "loss": 2.6447,
      "step": 1080
    },
    {
      "epoch": 0.06619299204469545,
      "grad_norm": 2.1825270652770996,
      "learning_rate": 9.987992644507511e-05,
      "loss": 2.6447,
      "step": 1090
    },
    {
      "epoch": 0.06680026720106881,
      "grad_norm": 2.115710973739624,
      "learning_rate": 9.987771405684897e-05,
      "loss": 2.3655,
      "step": 1100
    },
    {
      "epoch": 0.06740754235744216,
      "grad_norm": 1.6710492372512817,
      "learning_rate": 9.987548149728092e-05,
      "loss": 2.3085,
      "step": 1110
    },
    {
      "epoch": 0.06801481751381551,
      "grad_norm": 2.5440642833709717,
      "learning_rate": 9.987322876727381e-05,
      "loss": 2.8059,
      "step": 1120
    },
    {
      "epoch": 0.06862209267018886,
      "grad_norm": 2.6967358589172363,
      "learning_rate": 9.98709558677387e-05,
      "loss": 3.04,
      "step": 1130
    },
    {
      "epoch": 0.06922936782656222,
      "grad_norm": 3.9061031341552734,
      "learning_rate": 9.986866279959474e-05,
      "loss": 3.1271,
      "step": 1140
    },
    {
      "epoch": 0.06983664298293557,
      "grad_norm": 3.2761754989624023,
      "learning_rate": 9.986634956376932e-05,
      "loss": 2.6869,
      "step": 1150
    },
    {
      "epoch": 0.07044391813930892,
      "grad_norm": 2.55582857131958,
      "learning_rate": 9.986401616119795e-05,
      "loss": 2.9894,
      "step": 1160
    },
    {
      "epoch": 0.07105119329568227,
      "grad_norm": 2.079308032989502,
      "learning_rate": 9.98616625928243e-05,
      "loss": 2.6543,
      "step": 1170
    },
    {
      "epoch": 0.07165846845205563,
      "grad_norm": 2.0200679302215576,
      "learning_rate": 9.985928885960019e-05,
      "loss": 2.5643,
      "step": 1180
    },
    {
      "epoch": 0.07226574360842898,
      "grad_norm": 2.95072078704834,
      "learning_rate": 9.985689496248556e-05,
      "loss": 2.428,
      "step": 1190
    },
    {
      "epoch": 0.07287301876480233,
      "grad_norm": 2.0961287021636963,
      "learning_rate": 9.985448090244858e-05,
      "loss": 2.7229,
      "step": 1200
    },
    {
      "epoch": 0.07348029392117568,
      "grad_norm": 3.2801177501678467,
      "learning_rate": 9.985204668046553e-05,
      "loss": 2.8076,
      "step": 1210
    },
    {
      "epoch": 0.07408756907754904,
      "grad_norm": 6.785343170166016,
      "learning_rate": 9.984959229752082e-05,
      "loss": 2.756,
      "step": 1220
    },
    {
      "epoch": 0.07469484423392239,
      "grad_norm": 2.543053150177002,
      "learning_rate": 9.984711775460707e-05,
      "loss": 3.0355,
      "step": 1230
    },
    {
      "epoch": 0.07530211939029574,
      "grad_norm": 2.2619287967681885,
      "learning_rate": 9.9844623052725e-05,
      "loss": 2.9984,
      "step": 1240
    },
    {
      "epoch": 0.07590939454666909,
      "grad_norm": 6.4721198081970215,
      "learning_rate": 9.984210819288354e-05,
      "loss": 2.6957,
      "step": 1250
    },
    {
      "epoch": 0.07651666970304245,
      "grad_norm": 2.721078872680664,
      "learning_rate": 9.983957317609971e-05,
      "loss": 2.7023,
      "step": 1260
    },
    {
      "epoch": 0.0771239448594158,
      "grad_norm": 1.7425986528396606,
      "learning_rate": 9.983701800339873e-05,
      "loss": 2.7241,
      "step": 1270
    },
    {
      "epoch": 0.07773122001578915,
      "grad_norm": 2.0464706420898438,
      "learning_rate": 9.983444267581394e-05,
      "loss": 2.6473,
      "step": 1280
    },
    {
      "epoch": 0.0783384951721625,
      "grad_norm": 7.386662006378174,
      "learning_rate": 9.983184719438687e-05,
      "loss": 2.8031,
      "step": 1290
    },
    {
      "epoch": 0.07894577032853586,
      "grad_norm": 5.077014923095703,
      "learning_rate": 9.982923156016713e-05,
      "loss": 2.5077,
      "step": 1300
    },
    {
      "epoch": 0.07955304548490921,
      "grad_norm": 5.140445232391357,
      "learning_rate": 9.982659577421255e-05,
      "loss": 2.6643,
      "step": 1310
    },
    {
      "epoch": 0.08016032064128256,
      "grad_norm": 4.758424282073975,
      "learning_rate": 9.982393983758908e-05,
      "loss": 2.9352,
      "step": 1320
    },
    {
      "epoch": 0.08076759579765591,
      "grad_norm": 3.2941057682037354,
      "learning_rate": 9.982126375137083e-05,
      "loss": 2.9868,
      "step": 1330
    },
    {
      "epoch": 0.08137487095402927,
      "grad_norm": 3.253530979156494,
      "learning_rate": 9.981856751664004e-05,
      "loss": 3.0862,
      "step": 1340
    },
    {
      "epoch": 0.08198214611040262,
      "grad_norm": 7.478233814239502,
      "learning_rate": 9.981585113448713e-05,
      "loss": 2.6801,
      "step": 1350
    },
    {
      "epoch": 0.08258942126677597,
      "grad_norm": 2.1226282119750977,
      "learning_rate": 9.981311460601061e-05,
      "loss": 2.3369,
      "step": 1360
    },
    {
      "epoch": 0.08319669642314934,
      "grad_norm": 2.751624345779419,
      "learning_rate": 9.981035793231722e-05,
      "loss": 2.6989,
      "step": 1370
    },
    {
      "epoch": 0.08380397157952268,
      "grad_norm": 2.239170789718628,
      "learning_rate": 9.980758111452177e-05,
      "loss": 2.8433,
      "step": 1380
    },
    {
      "epoch": 0.08441124673589603,
      "grad_norm": 3.954367160797119,
      "learning_rate": 9.980478415374726e-05,
      "loss": 2.9201,
      "step": 1390
    },
    {
      "epoch": 0.08501852189226938,
      "grad_norm": 5.653280735015869,
      "learning_rate": 9.980196705112484e-05,
      "loss": 3.2969,
      "step": 1400
    },
    {
      "epoch": 0.08562579704864275,
      "grad_norm": 4.375555515289307,
      "learning_rate": 9.979912980779377e-05,
      "loss": 3.0847,
      "step": 1410
    },
    {
      "epoch": 0.0862330722050161,
      "grad_norm": 3.537280559539795,
      "learning_rate": 9.979627242490148e-05,
      "loss": 2.9035,
      "step": 1420
    },
    {
      "epoch": 0.08684034736138944,
      "grad_norm": 2.722881317138672,
      "learning_rate": 9.979339490360355e-05,
      "loss": 2.952,
      "step": 1430
    },
    {
      "epoch": 0.0874476225177628,
      "grad_norm": 2.460787057876587,
      "learning_rate": 9.979049724506369e-05,
      "loss": 2.8785,
      "step": 1440
    },
    {
      "epoch": 0.08805489767413616,
      "grad_norm": 2.098292827606201,
      "learning_rate": 9.978757945045378e-05,
      "loss": 2.8513,
      "step": 1450
    },
    {
      "epoch": 0.0886621728305095,
      "grad_norm": 2.7332565784454346,
      "learning_rate": 9.978464152095378e-05,
      "loss": 2.7216,
      "step": 1460
    },
    {
      "epoch": 0.08926944798688285,
      "grad_norm": 4.435109615325928,
      "learning_rate": 9.978168345775187e-05,
      "loss": 3.0856,
      "step": 1470
    },
    {
      "epoch": 0.0898767231432562,
      "grad_norm": 3.5636463165283203,
      "learning_rate": 9.977870526204431e-05,
      "loss": 2.8489,
      "step": 1480
    },
    {
      "epoch": 0.09048399829962957,
      "grad_norm": 3.468517780303955,
      "learning_rate": 9.977570693503557e-05,
      "loss": 2.7209,
      "step": 1490
    },
    {
      "epoch": 0.09109127345600292,
      "grad_norm": 3.9199130535125732,
      "learning_rate": 9.977268847793819e-05,
      "loss": 3.0091,
      "step": 1500
    },
    {
      "epoch": 0.09169854861237627,
      "grad_norm": 3.373481512069702,
      "learning_rate": 9.976964989197288e-05,
      "loss": 2.6523,
      "step": 1510
    },
    {
      "epoch": 0.09230582376874961,
      "grad_norm": 3.3667783737182617,
      "learning_rate": 9.976659117836851e-05,
      "loss": 2.668,
      "step": 1520
    },
    {
      "epoch": 0.09291309892512298,
      "grad_norm": 2.519352436065674,
      "learning_rate": 9.976351233836207e-05,
      "loss": 2.8741,
      "step": 1530
    },
    {
      "epoch": 0.09352037408149633,
      "grad_norm": 2.5113368034362793,
      "learning_rate": 9.976041337319867e-05,
      "loss": 2.7558,
      "step": 1540
    },
    {
      "epoch": 0.09412764923786968,
      "grad_norm": 2.948071002960205,
      "learning_rate": 9.975729428413162e-05,
      "loss": 2.933,
      "step": 1550
    },
    {
      "epoch": 0.09473492439424303,
      "grad_norm": 4.039847373962402,
      "learning_rate": 9.975415507242229e-05,
      "loss": 2.7249,
      "step": 1560
    },
    {
      "epoch": 0.09534219955061639,
      "grad_norm": 2.9555819034576416,
      "learning_rate": 9.975099573934026e-05,
      "loss": 2.8253,
      "step": 1570
    },
    {
      "epoch": 0.09594947470698974,
      "grad_norm": 7.770235061645508,
      "learning_rate": 9.974781628616318e-05,
      "loss": 2.7095,
      "step": 1580
    },
    {
      "epoch": 0.09655674986336309,
      "grad_norm": 2.452521324157715,
      "learning_rate": 9.974461671417689e-05,
      "loss": 2.9011,
      "step": 1590
    },
    {
      "epoch": 0.09716402501973644,
      "grad_norm": 3.272028923034668,
      "learning_rate": 9.974139702467538e-05,
      "loss": 2.6145,
      "step": 1600
    },
    {
      "epoch": 0.0977713001761098,
      "grad_norm": 2.645780086517334,
      "learning_rate": 9.973815721896068e-05,
      "loss": 2.4509,
      "step": 1610
    },
    {
      "epoch": 0.09837857533248315,
      "grad_norm": 1.6018285751342773,
      "learning_rate": 9.973489729834307e-05,
      "loss": 2.4907,
      "step": 1620
    },
    {
      "epoch": 0.0989858504888565,
      "grad_norm": 2.603994607925415,
      "learning_rate": 9.973161726414088e-05,
      "loss": 2.7411,
      "step": 1630
    },
    {
      "epoch": 0.09959312564522986,
      "grad_norm": 2.387714147567749,
      "learning_rate": 9.972831711768063e-05,
      "loss": 2.7527,
      "step": 1640
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 2.995133876800537,
      "learning_rate": 9.972499686029694e-05,
      "loss": 2.9751,
      "step": 1650
    },
    {
      "epoch": 0.10080767595797656,
      "grad_norm": 3.6726107597351074,
      "learning_rate": 9.972165649333259e-05,
      "loss": 2.7097,
      "step": 1660
    },
    {
      "epoch": 0.10141495111434991,
      "grad_norm": 2.033721685409546,
      "learning_rate": 9.971829601813845e-05,
      "loss": 2.6913,
      "step": 1670
    },
    {
      "epoch": 0.10202222627072327,
      "grad_norm": 2.304652214050293,
      "learning_rate": 9.971491543607356e-05,
      "loss": 3.0866,
      "step": 1680
    },
    {
      "epoch": 0.10262950142709662,
      "grad_norm": 4.654269695281982,
      "learning_rate": 9.971151474850511e-05,
      "loss": 2.8732,
      "step": 1690
    },
    {
      "epoch": 0.10323677658346997,
      "grad_norm": 3.589049816131592,
      "learning_rate": 9.970809395680837e-05,
      "loss": 2.6607,
      "step": 1700
    },
    {
      "epoch": 0.10384405173984332,
      "grad_norm": 3.154991865158081,
      "learning_rate": 9.970465306236676e-05,
      "loss": 2.4037,
      "step": 1710
    },
    {
      "epoch": 0.10445132689621668,
      "grad_norm": 2.4109930992126465,
      "learning_rate": 9.970119206657182e-05,
      "loss": 2.6353,
      "step": 1720
    },
    {
      "epoch": 0.10505860205259003,
      "grad_norm": 2.327291250228882,
      "learning_rate": 9.969771097082326e-05,
      "loss": 2.8521,
      "step": 1730
    },
    {
      "epoch": 0.10566587720896338,
      "grad_norm": 2.3901686668395996,
      "learning_rate": 9.969420977652888e-05,
      "loss": 2.6906,
      "step": 1740
    },
    {
      "epoch": 0.10627315236533673,
      "grad_norm": 3.146944999694824,
      "learning_rate": 9.969068848510461e-05,
      "loss": 2.6366,
      "step": 1750
    },
    {
      "epoch": 0.10688042752171009,
      "grad_norm": 5.862473011016846,
      "learning_rate": 9.968714709797453e-05,
      "loss": 2.6662,
      "step": 1760
    },
    {
      "epoch": 0.10748770267808344,
      "grad_norm": 4.54126501083374,
      "learning_rate": 9.968358561657083e-05,
      "loss": 2.7903,
      "step": 1770
    },
    {
      "epoch": 0.10809497783445679,
      "grad_norm": 2.4159276485443115,
      "learning_rate": 9.968000404233382e-05,
      "loss": 3.2408,
      "step": 1780
    },
    {
      "epoch": 0.10870225299083014,
      "grad_norm": 2.532350540161133,
      "learning_rate": 9.967640237671195e-05,
      "loss": 2.8161,
      "step": 1790
    },
    {
      "epoch": 0.1093095281472035,
      "grad_norm": 2.955338954925537,
      "learning_rate": 9.967278062116179e-05,
      "loss": 2.7066,
      "step": 1800
    },
    {
      "epoch": 0.10991680330357685,
      "grad_norm": 5.425250053405762,
      "learning_rate": 9.966913877714803e-05,
      "loss": 3.1352,
      "step": 1810
    },
    {
      "epoch": 0.1105240784599502,
      "grad_norm": 7.161690711975098,
      "learning_rate": 9.966547684614352e-05,
      "loss": 2.8076,
      "step": 1820
    },
    {
      "epoch": 0.11113135361632355,
      "grad_norm": 4.221362113952637,
      "learning_rate": 9.966179482962916e-05,
      "loss": 2.6411,
      "step": 1830
    },
    {
      "epoch": 0.11173862877269691,
      "grad_norm": 2.1997218132019043,
      "learning_rate": 9.965809272909406e-05,
      "loss": 2.6229,
      "step": 1840
    },
    {
      "epoch": 0.11234590392907026,
      "grad_norm": 3.221076726913452,
      "learning_rate": 9.965437054603538e-05,
      "loss": 2.4802,
      "step": 1850
    },
    {
      "epoch": 0.11295317908544361,
      "grad_norm": 2.018519878387451,
      "learning_rate": 9.965062828195841e-05,
      "loss": 2.7864,
      "step": 1860
    },
    {
      "epoch": 0.11356045424181696,
      "grad_norm": 3.077029228210449,
      "learning_rate": 9.964686593837663e-05,
      "loss": 2.8535,
      "step": 1870
    },
    {
      "epoch": 0.11416772939819032,
      "grad_norm": 2.1237363815307617,
      "learning_rate": 9.964308351681157e-05,
      "loss": 2.6879,
      "step": 1880
    },
    {
      "epoch": 0.11477500455456367,
      "grad_norm": 3.0007996559143066,
      "learning_rate": 9.96392810187929e-05,
      "loss": 2.4494,
      "step": 1890
    },
    {
      "epoch": 0.11538227971093702,
      "grad_norm": 1.815516710281372,
      "learning_rate": 9.96354584458584e-05,
      "loss": 2.7779,
      "step": 1900
    },
    {
      "epoch": 0.11598955486731038,
      "grad_norm": 2.029073715209961,
      "learning_rate": 9.9631615799554e-05,
      "loss": 2.6198,
      "step": 1910
    },
    {
      "epoch": 0.11659683002368373,
      "grad_norm": 2.1800644397735596,
      "learning_rate": 9.96277530814337e-05,
      "loss": 2.4975,
      "step": 1920
    },
    {
      "epoch": 0.11720410518005708,
      "grad_norm": 3.3478686809539795,
      "learning_rate": 9.962387029305968e-05,
      "loss": 2.8814,
      "step": 1930
    },
    {
      "epoch": 0.11781138033643043,
      "grad_norm": 2.567033529281616,
      "learning_rate": 9.96199674360022e-05,
      "loss": 2.7724,
      "step": 1940
    },
    {
      "epoch": 0.1184186554928038,
      "grad_norm": 3.720099449157715,
      "learning_rate": 9.961604451183958e-05,
      "loss": 2.6863,
      "step": 1950
    },
    {
      "epoch": 0.11902593064917714,
      "grad_norm": 2.103940725326538,
      "learning_rate": 9.961210152215839e-05,
      "loss": 2.6206,
      "step": 1960
    },
    {
      "epoch": 0.1196332058055505,
      "grad_norm": 3.92439866065979,
      "learning_rate": 9.960813846855318e-05,
      "loss": 2.7931,
      "step": 1970
    },
    {
      "epoch": 0.12024048096192384,
      "grad_norm": 4.141117572784424,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.7756,
      "step": 1980
    },
    {
      "epoch": 0.1208477561182972,
      "grad_norm": 2.5056381225585938,
      "learning_rate": 9.96001521759898e-05,
      "loss": 2.5133,
      "step": 1990
    },
    {
      "epoch": 0.12145503127467056,
      "grad_norm": 2.824169635772705,
      "learning_rate": 9.959612894026138e-05,
      "loss": 2.6841,
      "step": 2000
    },
    {
      "epoch": 0.1220623064310439,
      "grad_norm": 3.8654754161834717,
      "learning_rate": 9.959208564706854e-05,
      "loss": 2.9506,
      "step": 2010
    },
    {
      "epoch": 0.12266958158741725,
      "grad_norm": 3.2301290035247803,
      "learning_rate": 9.958802229804643e-05,
      "loss": 2.8329,
      "step": 2020
    },
    {
      "epoch": 0.12327685674379062,
      "grad_norm": 3.3621160984039307,
      "learning_rate": 9.958393889483837e-05,
      "loss": 3.0437,
      "step": 2030
    },
    {
      "epoch": 0.12388413190016397,
      "grad_norm": 3.243455648422241,
      "learning_rate": 9.95798354390957e-05,
      "loss": 2.5902,
      "step": 2040
    },
    {
      "epoch": 0.12449140705653731,
      "grad_norm": 2.0370466709136963,
      "learning_rate": 9.957571193247797e-05,
      "loss": 2.8775,
      "step": 2050
    },
    {
      "epoch": 0.12509868221291068,
      "grad_norm": 3.824655771255493,
      "learning_rate": 9.957156837665276e-05,
      "loss": 2.7865,
      "step": 2060
    },
    {
      "epoch": 0.12570595736928403,
      "grad_norm": 1.9630728960037231,
      "learning_rate": 9.95674047732958e-05,
      "loss": 2.4911,
      "step": 2070
    },
    {
      "epoch": 0.12631323252565738,
      "grad_norm": 3.390956401824951,
      "learning_rate": 9.956322112409093e-05,
      "loss": 2.7021,
      "step": 2080
    },
    {
      "epoch": 0.12692050768203073,
      "grad_norm": 2.7686266899108887,
      "learning_rate": 9.955901743073006e-05,
      "loss": 2.9021,
      "step": 2090
    },
    {
      "epoch": 0.12752778283840407,
      "grad_norm": 4.272260665893555,
      "learning_rate": 9.955479369491326e-05,
      "loss": 3.1487,
      "step": 2100
    },
    {
      "epoch": 0.12813505799477742,
      "grad_norm": 3.350961446762085,
      "learning_rate": 9.955054991834866e-05,
      "loss": 3.1143,
      "step": 2110
    },
    {
      "epoch": 0.12874233315115077,
      "grad_norm": 2.762260913848877,
      "learning_rate": 9.954628610275249e-05,
      "loss": 2.8706,
      "step": 2120
    },
    {
      "epoch": 0.12934960830752415,
      "grad_norm": 3.9122231006622314,
      "learning_rate": 9.954200224984915e-05,
      "loss": 2.6773,
      "step": 2130
    },
    {
      "epoch": 0.1299568834638975,
      "grad_norm": 3.533756971359253,
      "learning_rate": 9.953769836137106e-05,
      "loss": 3.2229,
      "step": 2140
    },
    {
      "epoch": 0.13056415862027085,
      "grad_norm": 5.022109031677246,
      "learning_rate": 9.95333744390588e-05,
      "loss": 3.1665,
      "step": 2150
    },
    {
      "epoch": 0.1311714337766442,
      "grad_norm": 4.488645076751709,
      "learning_rate": 9.952903048466104e-05,
      "loss": 2.7469,
      "step": 2160
    },
    {
      "epoch": 0.13177870893301755,
      "grad_norm": 2.999912738800049,
      "learning_rate": 9.952466649993451e-05,
      "loss": 3.1273,
      "step": 2170
    },
    {
      "epoch": 0.1323859840893909,
      "grad_norm": 7.09088134765625,
      "learning_rate": 9.952028248664411e-05,
      "loss": 3.1609,
      "step": 2180
    },
    {
      "epoch": 0.13299325924576424,
      "grad_norm": 4.756022930145264,
      "learning_rate": 9.95158784465628e-05,
      "loss": 2.9298,
      "step": 2190
    },
    {
      "epoch": 0.13360053440213762,
      "grad_norm": 6.609065532684326,
      "learning_rate": 9.951145438147162e-05,
      "loss": 2.8606,
      "step": 2200
    },
    {
      "epoch": 0.13420780955851097,
      "grad_norm": 2.513035774230957,
      "learning_rate": 9.950701029315977e-05,
      "loss": 3.0793,
      "step": 2210
    },
    {
      "epoch": 0.13481508471488432,
      "grad_norm": 3.002197027206421,
      "learning_rate": 9.950254618342447e-05,
      "loss": 2.7873,
      "step": 2220
    },
    {
      "epoch": 0.13542235987125767,
      "grad_norm": 2.292532444000244,
      "learning_rate": 9.949806205407111e-05,
      "loss": 2.8288,
      "step": 2230
    },
    {
      "epoch": 0.13602963502763102,
      "grad_norm": 3.5423099994659424,
      "learning_rate": 9.949355790691311e-05,
      "loss": 2.893,
      "step": 2240
    },
    {
      "epoch": 0.13663691018400437,
      "grad_norm": 2.4385581016540527,
      "learning_rate": 9.948903374377205e-05,
      "loss": 2.7611,
      "step": 2250
    },
    {
      "epoch": 0.13724418534037772,
      "grad_norm": 3.24420428276062,
      "learning_rate": 9.948448956647757e-05,
      "loss": 2.9368,
      "step": 2260
    },
    {
      "epoch": 0.13785146049675107,
      "grad_norm": 2.549713373184204,
      "learning_rate": 9.947992537686739e-05,
      "loss": 2.9616,
      "step": 2270
    },
    {
      "epoch": 0.13845873565312444,
      "grad_norm": 6.837342739105225,
      "learning_rate": 9.947534117678735e-05,
      "loss": 2.5777,
      "step": 2280
    },
    {
      "epoch": 0.1390660108094978,
      "grad_norm": 1.8564249277114868,
      "learning_rate": 9.947073696809137e-05,
      "loss": 2.8812,
      "step": 2290
    },
    {
      "epoch": 0.13967328596587114,
      "grad_norm": 3.8077194690704346,
      "learning_rate": 9.946611275264148e-05,
      "loss": 2.5728,
      "step": 2300
    },
    {
      "epoch": 0.1402805611222445,
      "grad_norm": 2.3424782752990723,
      "learning_rate": 9.946146853230777e-05,
      "loss": 2.5292,
      "step": 2310
    },
    {
      "epoch": 0.14088783627861784,
      "grad_norm": 3.4835190773010254,
      "learning_rate": 9.945680430896844e-05,
      "loss": 2.6803,
      "step": 2320
    },
    {
      "epoch": 0.1414951114349912,
      "grad_norm": 8.598039627075195,
      "learning_rate": 9.94521200845098e-05,
      "loss": 2.8117,
      "step": 2330
    },
    {
      "epoch": 0.14210238659136454,
      "grad_norm": 2.788564682006836,
      "learning_rate": 9.944741586082617e-05,
      "loss": 2.4584,
      "step": 2340
    },
    {
      "epoch": 0.1427096617477379,
      "grad_norm": 4.074861526489258,
      "learning_rate": 9.944269163982007e-05,
      "loss": 2.6019,
      "step": 2350
    },
    {
      "epoch": 0.14331693690411126,
      "grad_norm": 4.219268321990967,
      "learning_rate": 9.943794742340202e-05,
      "loss": 2.6492,
      "step": 2360
    },
    {
      "epoch": 0.1439242120604846,
      "grad_norm": 3.0509867668151855,
      "learning_rate": 9.943318321349067e-05,
      "loss": 3.151,
      "step": 2370
    },
    {
      "epoch": 0.14453148721685796,
      "grad_norm": 2.362668514251709,
      "learning_rate": 9.942839901201272e-05,
      "loss": 2.7968,
      "step": 2380
    },
    {
      "epoch": 0.1451387623732313,
      "grad_norm": 3.283731460571289,
      "learning_rate": 9.9423594820903e-05,
      "loss": 2.5412,
      "step": 2390
    },
    {
      "epoch": 0.14574603752960466,
      "grad_norm": 2.352623701095581,
      "learning_rate": 9.94187706421044e-05,
      "loss": 2.7589,
      "step": 2400
    },
    {
      "epoch": 0.146353312685978,
      "grad_norm": 2.5188491344451904,
      "learning_rate": 9.941392647756789e-05,
      "loss": 2.8871,
      "step": 2410
    },
    {
      "epoch": 0.14696058784235136,
      "grad_norm": 3.1891841888427734,
      "learning_rate": 9.940906232925251e-05,
      "loss": 2.7545,
      "step": 2420
    },
    {
      "epoch": 0.14756786299872474,
      "grad_norm": 2.551603078842163,
      "learning_rate": 9.940417819912544e-05,
      "loss": 2.6814,
      "step": 2430
    },
    {
      "epoch": 0.14817513815509809,
      "grad_norm": 1.3717840909957886,
      "learning_rate": 9.939927408916185e-05,
      "loss": 2.5997,
      "step": 2440
    },
    {
      "epoch": 0.14878241331147143,
      "grad_norm": 4.24821662902832,
      "learning_rate": 9.939435000134509e-05,
      "loss": 2.7488,
      "step": 2450
    },
    {
      "epoch": 0.14938968846784478,
      "grad_norm": 2.477276563644409,
      "learning_rate": 9.93894059376665e-05,
      "loss": 2.304,
      "step": 2460
    },
    {
      "epoch": 0.14999696362421813,
      "grad_norm": 1.2770167589187622,
      "learning_rate": 9.938444190012556e-05,
      "loss": 2.0069,
      "step": 2470
    },
    {
      "epoch": 0.15060423878059148,
      "grad_norm": 1.531536340713501,
      "learning_rate": 9.93794578907298e-05,
      "loss": 2.2887,
      "step": 2480
    },
    {
      "epoch": 0.15121151393696483,
      "grad_norm": 2.0877492427825928,
      "learning_rate": 9.937445391149483e-05,
      "loss": 2.9653,
      "step": 2490
    },
    {
      "epoch": 0.15181878909333818,
      "grad_norm": 6.722013473510742,
      "learning_rate": 9.936942996444434e-05,
      "loss": 2.4992,
      "step": 2500
    },
    {
      "epoch": 0.15242606424971156,
      "grad_norm": 2.083906412124634,
      "learning_rate": 9.936438605161009e-05,
      "loss": 2.9464,
      "step": 2510
    },
    {
      "epoch": 0.1530333394060849,
      "grad_norm": 2.181049346923828,
      "learning_rate": 9.935932217503193e-05,
      "loss": 2.9778,
      "step": 2520
    },
    {
      "epoch": 0.15364061456245826,
      "grad_norm": 2.2514467239379883,
      "learning_rate": 9.935423833675777e-05,
      "loss": 2.6186,
      "step": 2530
    },
    {
      "epoch": 0.1542478897188316,
      "grad_norm": 2.5771031379699707,
      "learning_rate": 9.934913453884358e-05,
      "loss": 2.4291,
      "step": 2540
    },
    {
      "epoch": 0.15485516487520495,
      "grad_norm": 1.4450653791427612,
      "learning_rate": 9.934401078335342e-05,
      "loss": 2.4076,
      "step": 2550
    },
    {
      "epoch": 0.1554624400315783,
      "grad_norm": 2.9123857021331787,
      "learning_rate": 9.933886707235946e-05,
      "loss": 2.4538,
      "step": 2560
    },
    {
      "epoch": 0.15606971518795165,
      "grad_norm": 2.603353500366211,
      "learning_rate": 9.933370340794183e-05,
      "loss": 2.8142,
      "step": 2570
    },
    {
      "epoch": 0.156676990344325,
      "grad_norm": 2.504354953765869,
      "learning_rate": 9.932851979218886e-05,
      "loss": 3.0566,
      "step": 2580
    },
    {
      "epoch": 0.15728426550069838,
      "grad_norm": 6.334001064300537,
      "learning_rate": 9.932331622719685e-05,
      "loss": 2.9577,
      "step": 2590
    },
    {
      "epoch": 0.15789154065707173,
      "grad_norm": 2.168508291244507,
      "learning_rate": 9.931809271507023e-05,
      "loss": 2.6132,
      "step": 2600
    },
    {
      "epoch": 0.15849881581344508,
      "grad_norm": 3.2646288871765137,
      "learning_rate": 9.931284925792142e-05,
      "loss": 2.7586,
      "step": 2610
    },
    {
      "epoch": 0.15910609096981843,
      "grad_norm": 5.00638484954834,
      "learning_rate": 9.930758585787102e-05,
      "loss": 2.9365,
      "step": 2620
    },
    {
      "epoch": 0.15971336612619177,
      "grad_norm": 3.4067678451538086,
      "learning_rate": 9.930230251704759e-05,
      "loss": 2.7673,
      "step": 2630
    },
    {
      "epoch": 0.16032064128256512,
      "grad_norm": 3.0509109497070312,
      "learning_rate": 9.929699923758783e-05,
      "loss": 2.4575,
      "step": 2640
    },
    {
      "epoch": 0.16092791643893847,
      "grad_norm": 3.062145233154297,
      "learning_rate": 9.929167602163647e-05,
      "loss": 2.3961,
      "step": 2650
    },
    {
      "epoch": 0.16153519159531182,
      "grad_norm": 2.161017417907715,
      "learning_rate": 9.928633287134626e-05,
      "loss": 2.9916,
      "step": 2660
    },
    {
      "epoch": 0.1621424667516852,
      "grad_norm": 5.448423385620117,
      "learning_rate": 9.928096978887809e-05,
      "loss": 2.9117,
      "step": 2670
    },
    {
      "epoch": 0.16274974190805855,
      "grad_norm": 4.232268810272217,
      "learning_rate": 9.927558677640088e-05,
      "loss": 2.7004,
      "step": 2680
    },
    {
      "epoch": 0.1633570170644319,
      "grad_norm": 5.422119140625,
      "learning_rate": 9.927018383609158e-05,
      "loss": 2.6605,
      "step": 2690
    },
    {
      "epoch": 0.16396429222080525,
      "grad_norm": 2.119215965270996,
      "learning_rate": 9.926476097013524e-05,
      "loss": 2.6782,
      "step": 2700
    },
    {
      "epoch": 0.1645715673771786,
      "grad_norm": 2.2161812782287598,
      "learning_rate": 9.925931818072496e-05,
      "loss": 2.6402,
      "step": 2710
    },
    {
      "epoch": 0.16517884253355195,
      "grad_norm": 2.8675131797790527,
      "learning_rate": 9.925385547006189e-05,
      "loss": 3.1298,
      "step": 2720
    },
    {
      "epoch": 0.1657861176899253,
      "grad_norm": 4.072360038757324,
      "learning_rate": 9.924837284035522e-05,
      "loss": 2.8118,
      "step": 2730
    },
    {
      "epoch": 0.16639339284629867,
      "grad_norm": 4.261361122131348,
      "learning_rate": 9.924287029382223e-05,
      "loss": 2.9002,
      "step": 2740
    },
    {
      "epoch": 0.16700066800267202,
      "grad_norm": 1.7519627809524536,
      "learning_rate": 9.923734783268823e-05,
      "loss": 2.7755,
      "step": 2750
    },
    {
      "epoch": 0.16760794315904537,
      "grad_norm": 3.7197766304016113,
      "learning_rate": 9.92318054591866e-05,
      "loss": 2.8496,
      "step": 2760
    },
    {
      "epoch": 0.16821521831541872,
      "grad_norm": 3.0197410583496094,
      "learning_rate": 9.922624317555874e-05,
      "loss": 2.6811,
      "step": 2770
    },
    {
      "epoch": 0.16882249347179207,
      "grad_norm": 2.217505693435669,
      "learning_rate": 9.922066098405415e-05,
      "loss": 2.4643,
      "step": 2780
    },
    {
      "epoch": 0.16942976862816542,
      "grad_norm": 4.421277046203613,
      "learning_rate": 9.921505888693036e-05,
      "loss": 2.3106,
      "step": 2790
    },
    {
      "epoch": 0.17003704378453877,
      "grad_norm": 1.5161346197128296,
      "learning_rate": 9.920943688645292e-05,
      "loss": 2.6155,
      "step": 2800
    },
    {
      "epoch": 0.17064431894091212,
      "grad_norm": 1.6676315069198608,
      "learning_rate": 9.920379498489549e-05,
      "loss": 2.6192,
      "step": 2810
    },
    {
      "epoch": 0.1712515940972855,
      "grad_norm": 2.112467050552368,
      "learning_rate": 9.919813318453973e-05,
      "loss": 2.7792,
      "step": 2820
    },
    {
      "epoch": 0.17185886925365884,
      "grad_norm": 3.6455938816070557,
      "learning_rate": 9.919245148767535e-05,
      "loss": 3.0264,
      "step": 2830
    },
    {
      "epoch": 0.1724661444100322,
      "grad_norm": 3.838238000869751,
      "learning_rate": 9.918674989660013e-05,
      "loss": 2.8347,
      "step": 2840
    },
    {
      "epoch": 0.17307341956640554,
      "grad_norm": 2.5430784225463867,
      "learning_rate": 9.91810284136199e-05,
      "loss": 3.1742,
      "step": 2850
    },
    {
      "epoch": 0.1736806947227789,
      "grad_norm": 2.5899415016174316,
      "learning_rate": 9.917528704104848e-05,
      "loss": 2.5676,
      "step": 2860
    },
    {
      "epoch": 0.17428796987915224,
      "grad_norm": 2.067662239074707,
      "learning_rate": 9.916952578120781e-05,
      "loss": 2.5503,
      "step": 2870
    },
    {
      "epoch": 0.1748952450355256,
      "grad_norm": 2.4310338497161865,
      "learning_rate": 9.916374463642781e-05,
      "loss": 2.5014,
      "step": 2880
    },
    {
      "epoch": 0.17550252019189894,
      "grad_norm": 2.253208637237549,
      "learning_rate": 9.915794360904649e-05,
      "loss": 2.5005,
      "step": 2890
    },
    {
      "epoch": 0.1761097953482723,
      "grad_norm": 2.0732288360595703,
      "learning_rate": 9.915212270140985e-05,
      "loss": 2.6896,
      "step": 2900
    },
    {
      "epoch": 0.17671707050464566,
      "grad_norm": 2.6870555877685547,
      "learning_rate": 9.914628191587198e-05,
      "loss": 2.5548,
      "step": 2910
    },
    {
      "epoch": 0.177324345661019,
      "grad_norm": 2.1309797763824463,
      "learning_rate": 9.914042125479499e-05,
      "loss": 2.5855,
      "step": 2920
    },
    {
      "epoch": 0.17793162081739236,
      "grad_norm": 2.771636724472046,
      "learning_rate": 9.913454072054901e-05,
      "loss": 2.4663,
      "step": 2930
    },
    {
      "epoch": 0.1785388959737657,
      "grad_norm": 2.417732000350952,
      "learning_rate": 9.912864031551222e-05,
      "loss": 2.4668,
      "step": 2940
    },
    {
      "epoch": 0.17914617113013906,
      "grad_norm": 2.665285348892212,
      "learning_rate": 9.912272004207085e-05,
      "loss": 2.6413,
      "step": 2950
    },
    {
      "epoch": 0.1797534462865124,
      "grad_norm": 2.796678304672241,
      "learning_rate": 9.911677990261913e-05,
      "loss": 3.0202,
      "step": 2960
    },
    {
      "epoch": 0.18036072144288579,
      "grad_norm": 2.566383123397827,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.6453,
      "step": 2970
    },
    {
      "epoch": 0.18096799659925913,
      "grad_norm": 1.996123194694519,
      "learning_rate": 9.910484003530192e-05,
      "loss": 2.4306,
      "step": 2980
    },
    {
      "epoch": 0.18157527175563248,
      "grad_norm": 1.706026554107666,
      "learning_rate": 9.909884031226506e-05,
      "loss": 2.2706,
      "step": 2990
    },
    {
      "epoch": 0.18218254691200583,
      "grad_norm": 1.729822039604187,
      "learning_rate": 9.909282073287522e-05,
      "loss": 2.457,
      "step": 3000
    },
    {
      "epoch": 0.18278982206837918,
      "grad_norm": 2.3749873638153076,
      "learning_rate": 9.908678129956681e-05,
      "loss": 2.6506,
      "step": 3010
    },
    {
      "epoch": 0.18339709722475253,
      "grad_norm": 2.04215407371521,
      "learning_rate": 9.908072201478225e-05,
      "loss": 2.7665,
      "step": 3020
    },
    {
      "epoch": 0.18400437238112588,
      "grad_norm": 3.055835723876953,
      "learning_rate": 9.907464288097203e-05,
      "loss": 2.7069,
      "step": 3030
    },
    {
      "epoch": 0.18461164753749923,
      "grad_norm": 1.7778457403182983,
      "learning_rate": 9.906854390059467e-05,
      "loss": 2.5494,
      "step": 3040
    },
    {
      "epoch": 0.1852189226938726,
      "grad_norm": 2.1889514923095703,
      "learning_rate": 9.906242507611665e-05,
      "loss": 2.882,
      "step": 3050
    },
    {
      "epoch": 0.18582619785024596,
      "grad_norm": 4.832243919372559,
      "learning_rate": 9.905628641001255e-05,
      "loss": 2.7401,
      "step": 3060
    },
    {
      "epoch": 0.1864334730066193,
      "grad_norm": 2.737074136734009,
      "learning_rate": 9.905012790476493e-05,
      "loss": 2.7053,
      "step": 3070
    },
    {
      "epoch": 0.18704074816299265,
      "grad_norm": 4.174485206604004,
      "learning_rate": 9.904394956286441e-05,
      "loss": 2.5649,
      "step": 3080
    },
    {
      "epoch": 0.187648023319366,
      "grad_norm": 2.3177812099456787,
      "learning_rate": 9.903775138680956e-05,
      "loss": 2.9224,
      "step": 3090
    },
    {
      "epoch": 0.18825529847573935,
      "grad_norm": 3.777872085571289,
      "learning_rate": 9.903153337910708e-05,
      "loss": 2.7775,
      "step": 3100
    },
    {
      "epoch": 0.1888625736321127,
      "grad_norm": 2.733771324157715,
      "learning_rate": 9.90252955422716e-05,
      "loss": 2.4246,
      "step": 3110
    },
    {
      "epoch": 0.18946984878848605,
      "grad_norm": 1.4292099475860596,
      "learning_rate": 9.90190378788258e-05,
      "loss": 2.4589,
      "step": 3120
    },
    {
      "epoch": 0.19007712394485943,
      "grad_norm": 3.246751546859741,
      "learning_rate": 9.901276039130039e-05,
      "loss": 2.9849,
      "step": 3130
    },
    {
      "epoch": 0.19068439910123278,
      "grad_norm": 2.8562397956848145,
      "learning_rate": 9.900646308223408e-05,
      "loss": 2.7477,
      "step": 3140
    },
    {
      "epoch": 0.19129167425760613,
      "grad_norm": 4.488560199737549,
      "learning_rate": 9.90001459541736e-05,
      "loss": 2.462,
      "step": 3150
    },
    {
      "epoch": 0.19189894941397947,
      "grad_norm": 2.5003440380096436,
      "learning_rate": 9.899380900967371e-05,
      "loss": 2.801,
      "step": 3160
    },
    {
      "epoch": 0.19250622457035282,
      "grad_norm": 3.0068466663360596,
      "learning_rate": 9.898745225129715e-05,
      "loss": 2.5794,
      "step": 3170
    },
    {
      "epoch": 0.19311349972672617,
      "grad_norm": 3.695321798324585,
      "learning_rate": 9.898107568161472e-05,
      "loss": 3.0487,
      "step": 3180
    },
    {
      "epoch": 0.19372077488309952,
      "grad_norm": 3.300095796585083,
      "learning_rate": 9.897467930320519e-05,
      "loss": 3.0191,
      "step": 3190
    },
    {
      "epoch": 0.19432805003947287,
      "grad_norm": 2.3612868785858154,
      "learning_rate": 9.896826311865534e-05,
      "loss": 2.9502,
      "step": 3200
    },
    {
      "epoch": 0.19493532519584625,
      "grad_norm": 1.7939715385437012,
      "learning_rate": 9.896182713056001e-05,
      "loss": 2.899,
      "step": 3210
    },
    {
      "epoch": 0.1955426003522196,
      "grad_norm": 2.2416467666625977,
      "learning_rate": 9.8955371341522e-05,
      "loss": 2.6435,
      "step": 3220
    },
    {
      "epoch": 0.19614987550859295,
      "grad_norm": 3.1400442123413086,
      "learning_rate": 9.894889575415214e-05,
      "loss": 2.7126,
      "step": 3230
    },
    {
      "epoch": 0.1967571506649663,
      "grad_norm": 5.574371337890625,
      "learning_rate": 9.894240037106927e-05,
      "loss": 2.9349,
      "step": 3240
    },
    {
      "epoch": 0.19736442582133965,
      "grad_norm": 3.5921504497528076,
      "learning_rate": 9.89358851949002e-05,
      "loss": 2.9763,
      "step": 3250
    },
    {
      "epoch": 0.197971700977713,
      "grad_norm": 4.121638298034668,
      "learning_rate": 9.892935022827978e-05,
      "loss": 2.6553,
      "step": 3260
    },
    {
      "epoch": 0.19857897613408634,
      "grad_norm": 2.0624380111694336,
      "learning_rate": 9.892279547385087e-05,
      "loss": 2.542,
      "step": 3270
    },
    {
      "epoch": 0.19918625129045972,
      "grad_norm": 1.2536828517913818,
      "learning_rate": 9.891622093426429e-05,
      "loss": 2.411,
      "step": 3280
    },
    {
      "epoch": 0.19979352644683307,
      "grad_norm": 1.965015172958374,
      "learning_rate": 9.890962661217892e-05,
      "loss": 2.6575,
      "step": 3290
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 2.6900694370269775,
      "learning_rate": 9.89030125102616e-05,
      "loss": 2.2768,
      "step": 3300
    },
    {
      "epoch": 0.20100807675957977,
      "grad_norm": 4.045104503631592,
      "learning_rate": 9.889637863118715e-05,
      "loss": 3.1668,
      "step": 3310
    },
    {
      "epoch": 0.20161535191595312,
      "grad_norm": 4.006162166595459,
      "learning_rate": 9.888972497763844e-05,
      "loss": 2.6059,
      "step": 3320
    },
    {
      "epoch": 0.20222262707232647,
      "grad_norm": 1.8307310342788696,
      "learning_rate": 9.888305155230632e-05,
      "loss": 2.6234,
      "step": 3330
    },
    {
      "epoch": 0.20282990222869982,
      "grad_norm": 3.2577028274536133,
      "learning_rate": 9.887635835788965e-05,
      "loss": 2.4421,
      "step": 3340
    },
    {
      "epoch": 0.20343717738507316,
      "grad_norm": 3.091216564178467,
      "learning_rate": 9.886964539709519e-05,
      "loss": 3.2579,
      "step": 3350
    },
    {
      "epoch": 0.20404445254144654,
      "grad_norm": 2.574180841445923,
      "learning_rate": 9.886291267263783e-05,
      "loss": 2.8493,
      "step": 3360
    },
    {
      "epoch": 0.2046517276978199,
      "grad_norm": 2.3522448539733887,
      "learning_rate": 9.885616018724037e-05,
      "loss": 3.0643,
      "step": 3370
    },
    {
      "epoch": 0.20525900285419324,
      "grad_norm": 3.863027811050415,
      "learning_rate": 9.884938794363365e-05,
      "loss": 2.718,
      "step": 3380
    },
    {
      "epoch": 0.2058662780105666,
      "grad_norm": 2.705446720123291,
      "learning_rate": 9.884259594455643e-05,
      "loss": 3.0853,
      "step": 3390
    },
    {
      "epoch": 0.20647355316693994,
      "grad_norm": 5.110780715942383,
      "learning_rate": 9.883578419275553e-05,
      "loss": 3.1031,
      "step": 3400
    },
    {
      "epoch": 0.2070808283233133,
      "grad_norm": 9.48508071899414,
      "learning_rate": 9.882895269098572e-05,
      "loss": 2.8142,
      "step": 3410
    },
    {
      "epoch": 0.20768810347968664,
      "grad_norm": 2.7131142616271973,
      "learning_rate": 9.882210144200979e-05,
      "loss": 2.9707,
      "step": 3420
    },
    {
      "epoch": 0.20829537863605999,
      "grad_norm": 6.04401159286499,
      "learning_rate": 9.881523044859844e-05,
      "loss": 2.9288,
      "step": 3430
    },
    {
      "epoch": 0.20890265379243336,
      "grad_norm": 2.5574951171875,
      "learning_rate": 9.880833971353048e-05,
      "loss": 2.8821,
      "step": 3440
    },
    {
      "epoch": 0.2095099289488067,
      "grad_norm": 3.428109645843506,
      "learning_rate": 9.880142923959258e-05,
      "loss": 2.4052,
      "step": 3450
    },
    {
      "epoch": 0.21011720410518006,
      "grad_norm": 3.915019989013672,
      "learning_rate": 9.879449902957946e-05,
      "loss": 2.4556,
      "step": 3460
    },
    {
      "epoch": 0.2107244792615534,
      "grad_norm": 4.7336249351501465,
      "learning_rate": 9.878754908629384e-05,
      "loss": 3.0088,
      "step": 3470
    },
    {
      "epoch": 0.21133175441792676,
      "grad_norm": 5.3492608070373535,
      "learning_rate": 9.878057941254634e-05,
      "loss": 2.9061,
      "step": 3480
    },
    {
      "epoch": 0.2119390295743001,
      "grad_norm": 5.061074733734131,
      "learning_rate": 9.877359001115563e-05,
      "loss": 2.977,
      "step": 3490
    },
    {
      "epoch": 0.21254630473067346,
      "grad_norm": 6.3792009353637695,
      "learning_rate": 9.876658088494832e-05,
      "loss": 2.6252,
      "step": 3500
    },
    {
      "epoch": 0.21315357988704683,
      "grad_norm": 2.6951260566711426,
      "learning_rate": 9.875955203675905e-05,
      "loss": 2.5433,
      "step": 3510
    },
    {
      "epoch": 0.21376085504342018,
      "grad_norm": 2.335087299346924,
      "learning_rate": 9.875250346943035e-05,
      "loss": 2.8298,
      "step": 3520
    },
    {
      "epoch": 0.21436813019979353,
      "grad_norm": 3.5351386070251465,
      "learning_rate": 9.874543518581279e-05,
      "loss": 2.7372,
      "step": 3530
    },
    {
      "epoch": 0.21497540535616688,
      "grad_norm": 3.641818046569824,
      "learning_rate": 9.873834718876491e-05,
      "loss": 2.7473,
      "step": 3540
    },
    {
      "epoch": 0.21558268051254023,
      "grad_norm": 3.574537515640259,
      "learning_rate": 9.873123948115321e-05,
      "loss": 2.402,
      "step": 3550
    },
    {
      "epoch": 0.21618995566891358,
      "grad_norm": 2.720914363861084,
      "learning_rate": 9.872411206585215e-05,
      "loss": 2.566,
      "step": 3560
    },
    {
      "epoch": 0.21679723082528693,
      "grad_norm": 2.3451085090637207,
      "learning_rate": 9.871696494574417e-05,
      "loss": 2.6943,
      "step": 3570
    },
    {
      "epoch": 0.21740450598166028,
      "grad_norm": 3.902515411376953,
      "learning_rate": 9.870979812371967e-05,
      "loss": 2.7848,
      "step": 3580
    },
    {
      "epoch": 0.21801178113803366,
      "grad_norm": 3.8705780506134033,
      "learning_rate": 9.870261160267704e-05,
      "loss": 2.6667,
      "step": 3590
    },
    {
      "epoch": 0.218619056294407,
      "grad_norm": 2.5219051837921143,
      "learning_rate": 9.869540538552263e-05,
      "loss": 2.8836,
      "step": 3600
    },
    {
      "epoch": 0.21922633145078035,
      "grad_norm": 3.556544065475464,
      "learning_rate": 9.868817947517073e-05,
      "loss": 2.7057,
      "step": 3610
    },
    {
      "epoch": 0.2198336066071537,
      "grad_norm": 2.2481508255004883,
      "learning_rate": 9.868093387454362e-05,
      "loss": 2.9842,
      "step": 3620
    },
    {
      "epoch": 0.22044088176352705,
      "grad_norm": 1.7546168565750122,
      "learning_rate": 9.867366858657155e-05,
      "loss": 2.6182,
      "step": 3630
    },
    {
      "epoch": 0.2210481569199004,
      "grad_norm": 3.4513635635375977,
      "learning_rate": 9.866638361419269e-05,
      "loss": 2.5793,
      "step": 3640
    },
    {
      "epoch": 0.22165543207627375,
      "grad_norm": 5.610491752624512,
      "learning_rate": 9.865907896035324e-05,
      "loss": 2.5074,
      "step": 3650
    },
    {
      "epoch": 0.2222627072326471,
      "grad_norm": 2.9097673892974854,
      "learning_rate": 9.865175462800727e-05,
      "loss": 3.0177,
      "step": 3660
    },
    {
      "epoch": 0.22286998238902048,
      "grad_norm": 2.3843166828155518,
      "learning_rate": 9.86444106201169e-05,
      "loss": 2.9034,
      "step": 3670
    },
    {
      "epoch": 0.22347725754539383,
      "grad_norm": 3.045614719390869,
      "learning_rate": 9.863704693965214e-05,
      "loss": 2.9245,
      "step": 3680
    },
    {
      "epoch": 0.22408453270176718,
      "grad_norm": 2.782386064529419,
      "learning_rate": 9.862966358959099e-05,
      "loss": 2.4482,
      "step": 3690
    },
    {
      "epoch": 0.22469180785814052,
      "grad_norm": 1.9459657669067383,
      "learning_rate": 9.862226057291937e-05,
      "loss": 2.3088,
      "step": 3700
    },
    {
      "epoch": 0.22529908301451387,
      "grad_norm": 1.696479082107544,
      "learning_rate": 9.861483789263122e-05,
      "loss": 2.5437,
      "step": 3710
    },
    {
      "epoch": 0.22590635817088722,
      "grad_norm": 2.0274083614349365,
      "learning_rate": 9.860739555172835e-05,
      "loss": 2.6204,
      "step": 3720
    },
    {
      "epoch": 0.22651363332726057,
      "grad_norm": 2.0878758430480957,
      "learning_rate": 9.859993355322058e-05,
      "loss": 2.8906,
      "step": 3730
    },
    {
      "epoch": 0.22712090848363392,
      "grad_norm": 1.5803723335266113,
      "learning_rate": 9.859245190012566e-05,
      "loss": 2.4404,
      "step": 3740
    },
    {
      "epoch": 0.2277281836400073,
      "grad_norm": 1.9750295877456665,
      "learning_rate": 9.85849505954693e-05,
      "loss": 2.7873,
      "step": 3750
    },
    {
      "epoch": 0.22833545879638065,
      "grad_norm": 2.813697576522827,
      "learning_rate": 9.857742964228512e-05,
      "loss": 2.5473,
      "step": 3760
    },
    {
      "epoch": 0.228942733952754,
      "grad_norm": 2.6191413402557373,
      "learning_rate": 9.856988904361474e-05,
      "loss": 2.5801,
      "step": 3770
    },
    {
      "epoch": 0.22955000910912735,
      "grad_norm": 1.8832244873046875,
      "learning_rate": 9.856232880250769e-05,
      "loss": 3.0785,
      "step": 3780
    },
    {
      "epoch": 0.2301572842655007,
      "grad_norm": 4.4343342781066895,
      "learning_rate": 9.855474892202144e-05,
      "loss": 2.8938,
      "step": 3790
    },
    {
      "epoch": 0.23076455942187404,
      "grad_norm": 2.286416530609131,
      "learning_rate": 9.854714940522142e-05,
      "loss": 2.7686,
      "step": 3800
    },
    {
      "epoch": 0.2313718345782474,
      "grad_norm": 3.0311994552612305,
      "learning_rate": 9.853953025518102e-05,
      "loss": 2.7424,
      "step": 3810
    },
    {
      "epoch": 0.23197910973462077,
      "grad_norm": 1.974241018295288,
      "learning_rate": 9.853189147498149e-05,
      "loss": 2.3339,
      "step": 3820
    },
    {
      "epoch": 0.23258638489099412,
      "grad_norm": 2.4821865558624268,
      "learning_rate": 9.852423306771214e-05,
      "loss": 2.4837,
      "step": 3830
    },
    {
      "epoch": 0.23319366004736747,
      "grad_norm": 3.675553560256958,
      "learning_rate": 9.85165550364701e-05,
      "loss": 2.3941,
      "step": 3840
    },
    {
      "epoch": 0.23380093520374082,
      "grad_norm": 2.205625295639038,
      "learning_rate": 9.850885738436053e-05,
      "loss": 2.4988,
      "step": 3850
    },
    {
      "epoch": 0.23440821036011417,
      "grad_norm": 3.900033473968506,
      "learning_rate": 9.850114011449645e-05,
      "loss": 2.7455,
      "step": 3860
    },
    {
      "epoch": 0.23501548551648752,
      "grad_norm": 3.8293356895446777,
      "learning_rate": 9.849340322999886e-05,
      "loss": 2.4713,
      "step": 3870
    },
    {
      "epoch": 0.23562276067286086,
      "grad_norm": 4.01187801361084,
      "learning_rate": 9.848564673399667e-05,
      "loss": 2.5779,
      "step": 3880
    },
    {
      "epoch": 0.23623003582923421,
      "grad_norm": 2.6362264156341553,
      "learning_rate": 9.847787062962675e-05,
      "loss": 2.679,
      "step": 3890
    },
    {
      "epoch": 0.2368373109856076,
      "grad_norm": 4.051872253417969,
      "learning_rate": 9.847007492003388e-05,
      "loss": 2.8158,
      "step": 3900
    },
    {
      "epoch": 0.23744458614198094,
      "grad_norm": 1.8156551122665405,
      "learning_rate": 9.846225960837075e-05,
      "loss": 2.5326,
      "step": 3910
    },
    {
      "epoch": 0.2380518612983543,
      "grad_norm": 3.5477824211120605,
      "learning_rate": 9.8454424697798e-05,
      "loss": 2.4435,
      "step": 3920
    },
    {
      "epoch": 0.23865913645472764,
      "grad_norm": 3.5380911827087402,
      "learning_rate": 9.844657019148418e-05,
      "loss": 2.6089,
      "step": 3930
    },
    {
      "epoch": 0.239266411611101,
      "grad_norm": 1.6853457689285278,
      "learning_rate": 9.843869609260583e-05,
      "loss": 2.392,
      "step": 3940
    },
    {
      "epoch": 0.23987368676747434,
      "grad_norm": 2.7005269527435303,
      "learning_rate": 9.84308024043473e-05,
      "loss": 2.5094,
      "step": 3950
    },
    {
      "epoch": 0.24048096192384769,
      "grad_norm": 2.7162938117980957,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.5644,
      "step": 3960
    },
    {
      "epoch": 0.24108823708022104,
      "grad_norm": 2.0047061443328857,
      "learning_rate": 9.841495627246707e-05,
      "loss": 2.3856,
      "step": 3970
    },
    {
      "epoch": 0.2416955122365944,
      "grad_norm": 2.2503981590270996,
      "learning_rate": 9.840700383525376e-05,
      "loss": 2.5599,
      "step": 3980
    },
    {
      "epoch": 0.24230278739296776,
      "grad_norm": 3.7740635871887207,
      "learning_rate": 9.839903182147717e-05,
      "loss": 2.7236,
      "step": 3990
    },
    {
      "epoch": 0.2429100625493411,
      "grad_norm": 3.18257474899292,
      "learning_rate": 9.839104023436128e-05,
      "loss": 2.6896,
      "step": 4000
    },
    {
      "epoch": 0.24351733770571446,
      "grad_norm": 2.369929313659668,
      "learning_rate": 9.8383029077138e-05,
      "loss": 3.0945,
      "step": 4010
    },
    {
      "epoch": 0.2441246128620878,
      "grad_norm": 4.257975101470947,
      "learning_rate": 9.837499835304724e-05,
      "loss": 3.031,
      "step": 4020
    },
    {
      "epoch": 0.24473188801846116,
      "grad_norm": 3.2274887561798096,
      "learning_rate": 9.836694806533669e-05,
      "loss": 2.4955,
      "step": 4030
    },
    {
      "epoch": 0.2453391631748345,
      "grad_norm": 2.2119297981262207,
      "learning_rate": 9.835887821726202e-05,
      "loss": 2.1706,
      "step": 4040
    },
    {
      "epoch": 0.24594643833120788,
      "grad_norm": 2.16085147857666,
      "learning_rate": 9.835078881208681e-05,
      "loss": 2.4878,
      "step": 4050
    },
    {
      "epoch": 0.24655371348758123,
      "grad_norm": 4.253424167633057,
      "learning_rate": 9.834267985308256e-05,
      "loss": 2.7579,
      "step": 4060
    },
    {
      "epoch": 0.24716098864395458,
      "grad_norm": 3.1904826164245605,
      "learning_rate": 9.833455134352866e-05,
      "loss": 2.6957,
      "step": 4070
    },
    {
      "epoch": 0.24776826380032793,
      "grad_norm": 2.0744168758392334,
      "learning_rate": 9.832640328671238e-05,
      "loss": 2.7579,
      "step": 4080
    },
    {
      "epoch": 0.24837553895670128,
      "grad_norm": 2.6864755153656006,
      "learning_rate": 9.831823568592897e-05,
      "loss": 2.5567,
      "step": 4090
    },
    {
      "epoch": 0.24898281411307463,
      "grad_norm": 3.30214262008667,
      "learning_rate": 9.831004854448152e-05,
      "loss": 2.6107,
      "step": 4100
    },
    {
      "epoch": 0.24959008926944798,
      "grad_norm": 2.6300244331359863,
      "learning_rate": 9.830184186568101e-05,
      "loss": 2.9667,
      "step": 4110
    },
    {
      "epoch": 0.25019736442582136,
      "grad_norm": 3.1646106243133545,
      "learning_rate": 9.829361565284639e-05,
      "loss": 2.7619,
      "step": 4120
    },
    {
      "epoch": 0.2508046395821947,
      "grad_norm": 4.114932537078857,
      "learning_rate": 9.828536990930444e-05,
      "loss": 2.8465,
      "step": 4130
    },
    {
      "epoch": 0.25141191473856805,
      "grad_norm": 2.61739444732666,
      "learning_rate": 9.82771046383899e-05,
      "loss": 2.7121,
      "step": 4140
    },
    {
      "epoch": 0.2520191898949414,
      "grad_norm": 2.7783448696136475,
      "learning_rate": 9.826881984344536e-05,
      "loss": 2.8407,
      "step": 4150
    },
    {
      "epoch": 0.25262646505131475,
      "grad_norm": 4.8291916847229,
      "learning_rate": 9.826051552782132e-05,
      "loss": 2.5552,
      "step": 4160
    },
    {
      "epoch": 0.25323374020768813,
      "grad_norm": 2.3700718879699707,
      "learning_rate": 9.825219169487618e-05,
      "loss": 2.6864,
      "step": 4170
    },
    {
      "epoch": 0.25384101536406145,
      "grad_norm": 2.8589563369750977,
      "learning_rate": 9.824384834797625e-05,
      "loss": 2.95,
      "step": 4180
    },
    {
      "epoch": 0.2544482905204348,
      "grad_norm": 2.6734585762023926,
      "learning_rate": 9.823548549049569e-05,
      "loss": 2.9177,
      "step": 4190
    },
    {
      "epoch": 0.25505556567680815,
      "grad_norm": 3.068936586380005,
      "learning_rate": 9.82271031258166e-05,
      "loss": 2.9135,
      "step": 4200
    },
    {
      "epoch": 0.2556628408331815,
      "grad_norm": 6.355357646942139,
      "learning_rate": 9.82187012573289e-05,
      "loss": 2.7002,
      "step": 4210
    },
    {
      "epoch": 0.25627011598955485,
      "grad_norm": 3.9745824337005615,
      "learning_rate": 9.821027988843046e-05,
      "loss": 3.2746,
      "step": 4220
    },
    {
      "epoch": 0.2568773911459282,
      "grad_norm": 2.342935800552368,
      "learning_rate": 9.820183902252702e-05,
      "loss": 2.5797,
      "step": 4230
    },
    {
      "epoch": 0.25748466630230155,
      "grad_norm": 2.6542980670928955,
      "learning_rate": 9.81933786630322e-05,
      "loss": 2.5502,
      "step": 4240
    },
    {
      "epoch": 0.2580919414586749,
      "grad_norm": 3.0595574378967285,
      "learning_rate": 9.81848988133675e-05,
      "loss": 2.6272,
      "step": 4250
    },
    {
      "epoch": 0.2586992166150483,
      "grad_norm": 4.398388385772705,
      "learning_rate": 9.817639947696231e-05,
      "loss": 2.5148,
      "step": 4260
    },
    {
      "epoch": 0.2593064917714216,
      "grad_norm": 3.045703887939453,
      "learning_rate": 9.816788065725389e-05,
      "loss": 3.2038,
      "step": 4270
    },
    {
      "epoch": 0.259913766927795,
      "grad_norm": 3.259399652481079,
      "learning_rate": 9.81593423576874e-05,
      "loss": 3.2504,
      "step": 4280
    },
    {
      "epoch": 0.2605210420841683,
      "grad_norm": 6.774500370025635,
      "learning_rate": 9.815078458171585e-05,
      "loss": 3.0742,
      "step": 4290
    },
    {
      "epoch": 0.2611283172405417,
      "grad_norm": 3.3682825565338135,
      "learning_rate": 9.814220733280015e-05,
      "loss": 2.6362,
      "step": 4300
    },
    {
      "epoch": 0.261735592396915,
      "grad_norm": 2.7376015186309814,
      "learning_rate": 9.813361061440907e-05,
      "loss": 2.6324,
      "step": 4310
    },
    {
      "epoch": 0.2623428675532884,
      "grad_norm": 3.0718369483947754,
      "learning_rate": 9.812499443001926e-05,
      "loss": 2.506,
      "step": 4320
    },
    {
      "epoch": 0.26295014270966177,
      "grad_norm": 3.2094228267669678,
      "learning_rate": 9.811635878311524e-05,
      "loss": 2.7084,
      "step": 4330
    },
    {
      "epoch": 0.2635574178660351,
      "grad_norm": 2.701871156692505,
      "learning_rate": 9.810770367718942e-05,
      "loss": 2.384,
      "step": 4340
    },
    {
      "epoch": 0.26416469302240847,
      "grad_norm": 3.7065112590789795,
      "learning_rate": 9.809902911574204e-05,
      "loss": 2.4978,
      "step": 4350
    },
    {
      "epoch": 0.2647719681787818,
      "grad_norm": 3.8052141666412354,
      "learning_rate": 9.809033510228126e-05,
      "loss": 3.0138,
      "step": 4360
    },
    {
      "epoch": 0.26537924333515517,
      "grad_norm": 5.367794036865234,
      "learning_rate": 9.808162164032304e-05,
      "loss": 2.7716,
      "step": 4370
    },
    {
      "epoch": 0.2659865184915285,
      "grad_norm": 1.9300705194473267,
      "learning_rate": 9.807288873339126e-05,
      "loss": 2.5247,
      "step": 4380
    },
    {
      "epoch": 0.26659379364790187,
      "grad_norm": 3.477221965789795,
      "learning_rate": 9.806413638501766e-05,
      "loss": 2.6192,
      "step": 4390
    },
    {
      "epoch": 0.26720106880427524,
      "grad_norm": 3.5737721920013428,
      "learning_rate": 9.805536459874182e-05,
      "loss": 2.5503,
      "step": 4400
    },
    {
      "epoch": 0.26780834396064856,
      "grad_norm": 3.9153947830200195,
      "learning_rate": 9.804657337811119e-05,
      "loss": 2.3362,
      "step": 4410
    },
    {
      "epoch": 0.26841561911702194,
      "grad_norm": 2.249321937561035,
      "learning_rate": 9.803776272668106e-05,
      "loss": 2.8203,
      "step": 4420
    },
    {
      "epoch": 0.26902289427339526,
      "grad_norm": 2.5892300605773926,
      "learning_rate": 9.802893264801462e-05,
      "loss": 2.3298,
      "step": 4430
    },
    {
      "epoch": 0.26963016942976864,
      "grad_norm": 2.309278964996338,
      "learning_rate": 9.80200831456829e-05,
      "loss": 2.6912,
      "step": 4440
    },
    {
      "epoch": 0.27023744458614196,
      "grad_norm": 3.7015762329101562,
      "learning_rate": 9.801121422326475e-05,
      "loss": 2.8123,
      "step": 4450
    },
    {
      "epoch": 0.27084471974251534,
      "grad_norm": 3.5909526348114014,
      "learning_rate": 9.800232588434692e-05,
      "loss": 2.8582,
      "step": 4460
    },
    {
      "epoch": 0.27145199489888866,
      "grad_norm": 2.810145854949951,
      "learning_rate": 9.799341813252402e-05,
      "loss": 2.8055,
      "step": 4470
    },
    {
      "epoch": 0.27205927005526204,
      "grad_norm": 4.735893726348877,
      "learning_rate": 9.798449097139845e-05,
      "loss": 3.0835,
      "step": 4480
    },
    {
      "epoch": 0.2726665452116354,
      "grad_norm": 2.36032772064209,
      "learning_rate": 9.797554440458052e-05,
      "loss": 2.7253,
      "step": 4490
    },
    {
      "epoch": 0.27327382036800874,
      "grad_norm": 2.2880494594573975,
      "learning_rate": 9.796657843568835e-05,
      "loss": 2.5313,
      "step": 4500
    },
    {
      "epoch": 0.2738810955243821,
      "grad_norm": 5.525994300842285,
      "learning_rate": 9.795759306834793e-05,
      "loss": 2.6936,
      "step": 4510
    },
    {
      "epoch": 0.27448837068075543,
      "grad_norm": 3.9558563232421875,
      "learning_rate": 9.794858830619307e-05,
      "loss": 2.4912,
      "step": 4520
    },
    {
      "epoch": 0.2750956458371288,
      "grad_norm": 2.2502200603485107,
      "learning_rate": 9.793956415286545e-05,
      "loss": 2.7239,
      "step": 4530
    },
    {
      "epoch": 0.27570292099350213,
      "grad_norm": 4.233541011810303,
      "learning_rate": 9.79305206120146e-05,
      "loss": 2.7652,
      "step": 4540
    },
    {
      "epoch": 0.2763101961498755,
      "grad_norm": 3.0988237857818604,
      "learning_rate": 9.792145768729785e-05,
      "loss": 2.5716,
      "step": 4550
    },
    {
      "epoch": 0.2769174713062489,
      "grad_norm": 2.9098269939422607,
      "learning_rate": 9.791237538238038e-05,
      "loss": 2.4281,
      "step": 4560
    },
    {
      "epoch": 0.2775247464626222,
      "grad_norm": 2.3917245864868164,
      "learning_rate": 9.790327370093525e-05,
      "loss": 2.3824,
      "step": 4570
    },
    {
      "epoch": 0.2781320216189956,
      "grad_norm": 3.6895751953125,
      "learning_rate": 9.78941526466433e-05,
      "loss": 2.3797,
      "step": 4580
    },
    {
      "epoch": 0.2787392967753689,
      "grad_norm": 2.311485528945923,
      "learning_rate": 9.788501222319324e-05,
      "loss": 2.4595,
      "step": 4590
    },
    {
      "epoch": 0.2793465719317423,
      "grad_norm": 2.711461067199707,
      "learning_rate": 9.78758524342816e-05,
      "loss": 2.7317,
      "step": 4600
    },
    {
      "epoch": 0.2799538470881156,
      "grad_norm": 4.229835033416748,
      "learning_rate": 9.786667328361274e-05,
      "loss": 2.6339,
      "step": 4610
    },
    {
      "epoch": 0.280561122244489,
      "grad_norm": 2.738090991973877,
      "learning_rate": 9.785747477489887e-05,
      "loss": 2.5725,
      "step": 4620
    },
    {
      "epoch": 0.28116839740086236,
      "grad_norm": 3.264017105102539,
      "learning_rate": 9.784825691186e-05,
      "loss": 2.6348,
      "step": 4630
    },
    {
      "epoch": 0.2817756725572357,
      "grad_norm": 2.1726365089416504,
      "learning_rate": 9.783901969822399e-05,
      "loss": 2.6744,
      "step": 4640
    },
    {
      "epoch": 0.28238294771360906,
      "grad_norm": 2.5314559936523438,
      "learning_rate": 9.78297631377265e-05,
      "loss": 2.5572,
      "step": 4650
    },
    {
      "epoch": 0.2829902228699824,
      "grad_norm": 2.3580260276794434,
      "learning_rate": 9.782048723411106e-05,
      "loss": 2.5524,
      "step": 4660
    },
    {
      "epoch": 0.28359749802635575,
      "grad_norm": 2.5427303314208984,
      "learning_rate": 9.781119199112896e-05,
      "loss": 2.2684,
      "step": 4670
    },
    {
      "epoch": 0.2842047731827291,
      "grad_norm": 1.748712420463562,
      "learning_rate": 9.780187741253935e-05,
      "loss": 2.5007,
      "step": 4680
    },
    {
      "epoch": 0.28481204833910245,
      "grad_norm": 3.388526439666748,
      "learning_rate": 9.779254350210922e-05,
      "loss": 2.5284,
      "step": 4690
    },
    {
      "epoch": 0.2854193234954758,
      "grad_norm": 2.6103951930999756,
      "learning_rate": 9.778319026361332e-05,
      "loss": 2.5922,
      "step": 4700
    },
    {
      "epoch": 0.28602659865184915,
      "grad_norm": 3.6787071228027344,
      "learning_rate": 9.777381770083426e-05,
      "loss": 2.8014,
      "step": 4710
    },
    {
      "epoch": 0.28663387380822253,
      "grad_norm": 2.8082218170166016,
      "learning_rate": 9.776442581756248e-05,
      "loss": 2.8619,
      "step": 4720
    },
    {
      "epoch": 0.28724114896459585,
      "grad_norm": 2.760582447052002,
      "learning_rate": 9.775501461759617e-05,
      "loss": 3.0972,
      "step": 4730
    },
    {
      "epoch": 0.2878484241209692,
      "grad_norm": 3.080962657928467,
      "learning_rate": 9.774558410474139e-05,
      "loss": 2.728,
      "step": 4740
    },
    {
      "epoch": 0.28845569927734255,
      "grad_norm": 3.8554110527038574,
      "learning_rate": 9.773613428281196e-05,
      "loss": 2.9752,
      "step": 4750
    },
    {
      "epoch": 0.2890629744337159,
      "grad_norm": 4.114931583404541,
      "learning_rate": 9.772666515562958e-05,
      "loss": 2.8765,
      "step": 4760
    },
    {
      "epoch": 0.28967024959008925,
      "grad_norm": 4.620038032531738,
      "learning_rate": 9.77171767270237e-05,
      "loss": 2.8728,
      "step": 4770
    },
    {
      "epoch": 0.2902775247464626,
      "grad_norm": 1.9616671800613403,
      "learning_rate": 9.77076690008316e-05,
      "loss": 2.9237,
      "step": 4780
    },
    {
      "epoch": 0.290884799902836,
      "grad_norm": 3.4501352310180664,
      "learning_rate": 9.769814198089832e-05,
      "loss": 2.871,
      "step": 4790
    },
    {
      "epoch": 0.2914920750592093,
      "grad_norm": 2.927307367324829,
      "learning_rate": 9.768859567107677e-05,
      "loss": 2.8622,
      "step": 4800
    },
    {
      "epoch": 0.2920993502155827,
      "grad_norm": 2.5114638805389404,
      "learning_rate": 9.767903007522763e-05,
      "loss": 2.6475,
      "step": 4810
    },
    {
      "epoch": 0.292706625371956,
      "grad_norm": 3.0498149394989014,
      "learning_rate": 9.766944519721937e-05,
      "loss": 2.5377,
      "step": 4820
    },
    {
      "epoch": 0.2933139005283294,
      "grad_norm": 3.047322988510132,
      "learning_rate": 9.765984104092826e-05,
      "loss": 2.6764,
      "step": 4830
    },
    {
      "epoch": 0.2939211756847027,
      "grad_norm": 2.788280487060547,
      "learning_rate": 9.765021761023838e-05,
      "loss": 2.8318,
      "step": 4840
    },
    {
      "epoch": 0.2945284508410761,
      "grad_norm": 5.985361099243164,
      "learning_rate": 9.764057490904162e-05,
      "loss": 2.8889,
      "step": 4850
    },
    {
      "epoch": 0.29513572599744947,
      "grad_norm": 5.169188976287842,
      "learning_rate": 9.763091294123762e-05,
      "loss": 2.9491,
      "step": 4860
    },
    {
      "epoch": 0.2957430011538228,
      "grad_norm": 3.305724620819092,
      "learning_rate": 9.762123171073383e-05,
      "loss": 2.9127,
      "step": 4870
    },
    {
      "epoch": 0.29635027631019617,
      "grad_norm": 3.5985920429229736,
      "learning_rate": 9.76115312214455e-05,
      "loss": 2.5104,
      "step": 4880
    },
    {
      "epoch": 0.2969575514665695,
      "grad_norm": 2.1707334518432617,
      "learning_rate": 9.760181147729568e-05,
      "loss": 2.7028,
      "step": 4890
    },
    {
      "epoch": 0.29756482662294287,
      "grad_norm": 2.2230172157287598,
      "learning_rate": 9.759207248221516e-05,
      "loss": 2.9218,
      "step": 4900
    },
    {
      "epoch": 0.2981721017793162,
      "grad_norm": 2.9418625831604004,
      "learning_rate": 9.758231424014256e-05,
      "loss": 2.5685,
      "step": 4910
    },
    {
      "epoch": 0.29877937693568957,
      "grad_norm": 3.084195137023926,
      "learning_rate": 9.757253675502426e-05,
      "loss": 2.7473,
      "step": 4920
    },
    {
      "epoch": 0.2993866520920629,
      "grad_norm": 2.7403950691223145,
      "learning_rate": 9.756274003081444e-05,
      "loss": 2.412,
      "step": 4930
    },
    {
      "epoch": 0.29999392724843627,
      "grad_norm": 1.7992706298828125,
      "learning_rate": 9.755292407147505e-05,
      "loss": 2.7033,
      "step": 4940
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 2.1876397132873535,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.6382,
      "step": 4950
    },
    {
      "epoch": 0.30120847756118296,
      "grad_norm": 3.588412284851074,
      "learning_rate": 9.753323446329427e-05,
      "loss": 2.5126,
      "step": 4960
    },
    {
      "epoch": 0.30181575271755634,
      "grad_norm": 2.112494945526123,
      "learning_rate": 9.752336082241564e-05,
      "loss": 2.7804,
      "step": 4970
    },
    {
      "epoch": 0.30242302787392966,
      "grad_norm": 2.2478232383728027,
      "learning_rate": 9.751346796233305e-05,
      "loss": 2.7227,
      "step": 4980
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 2.3947324752807617,
      "learning_rate": 9.750355588704727e-05,
      "loss": 2.5894,
      "step": 4990
    },
    {
      "epoch": 0.30363757818667636,
      "grad_norm": 2.0972561836242676,
      "learning_rate": 9.749362460056694e-05,
      "loss": 2.6187,
      "step": 5000
    }
  ],
  "logging_steps": 10,
  "max_steps": 49401,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "total_flos": 1.045168128e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
