{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3036983057023137,
  "eval_steps": 5000,
  "global_step": 5001,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006072751563733527,
      "grad_norm": 44.58989715576172,
      "learning_rate": 9.999998988960184e-05,
      "loss": 32.676,
      "step": 10
    },
    {
      "epoch": 0.0012145503127467055,
      "grad_norm": 4.839274883270264,
      "learning_rate": 9.999995955841146e-05,
      "loss": 10.068,
      "step": 20
    },
    {
      "epoch": 0.0018218254691200583,
      "grad_norm": 4.185300827026367,
      "learning_rate": 9.99999090064411e-05,
      "loss": 6.3583,
      "step": 30
    },
    {
      "epoch": 0.002429100625493411,
      "grad_norm": 5.0572028160095215,
      "learning_rate": 9.999983823371122e-05,
      "loss": 5.9704,
      "step": 40
    },
    {
      "epoch": 0.0030363757818667636,
      "grad_norm": 7.083920955657959,
      "learning_rate": 9.999974724025045e-05,
      "loss": 5.3705,
      "step": 50
    },
    {
      "epoch": 0.0036436509382401167,
      "grad_norm": 17.105676651000977,
      "learning_rate": 9.999963602609556e-05,
      "loss": 5.2614,
      "step": 60
    },
    {
      "epoch": 0.004250926094613469,
      "grad_norm": 8.324318885803223,
      "learning_rate": 9.999950459129158e-05,
      "loss": 5.0721,
      "step": 70
    },
    {
      "epoch": 0.004858201250986822,
      "grad_norm": 15.436798095703125,
      "learning_rate": 9.99993529358916e-05,
      "loss": 5.254,
      "step": 80
    },
    {
      "epoch": 0.005465476407360175,
      "grad_norm": 10.366856575012207,
      "learning_rate": 9.9999181059957e-05,
      "loss": 5.0635,
      "step": 90
    },
    {
      "epoch": 0.006072751563733527,
      "grad_norm": 25.154645919799805,
      "learning_rate": 9.999898896355726e-05,
      "loss": 4.9426,
      "step": 100
    },
    {
      "epoch": 0.006680026720106881,
      "grad_norm": 2.2850334644317627,
      "learning_rate": 9.999877664677009e-05,
      "loss": 4.7192,
      "step": 110
    },
    {
      "epoch": 0.007287301876480233,
      "grad_norm": 3.613799810409546,
      "learning_rate": 9.999854410968134e-05,
      "loss": 4.6513,
      "step": 120
    },
    {
      "epoch": 0.007894577032853586,
      "grad_norm": 9.10619068145752,
      "learning_rate": 9.999829135238505e-05,
      "loss": 5.0277,
      "step": 130
    },
    {
      "epoch": 0.008501852189226939,
      "grad_norm": 4.0168046951293945,
      "learning_rate": 9.999801837498346e-05,
      "loss": 5.0613,
      "step": 140
    },
    {
      "epoch": 0.009109127345600291,
      "grad_norm": 5.578673839569092,
      "learning_rate": 9.999772517758694e-05,
      "loss": 4.8303,
      "step": 150
    },
    {
      "epoch": 0.009716402501973644,
      "grad_norm": 7.578082084655762,
      "learning_rate": 9.999741176031408e-05,
      "loss": 4.8722,
      "step": 160
    },
    {
      "epoch": 0.010323677658346997,
      "grad_norm": 3.6842041015625,
      "learning_rate": 9.999707812329162e-05,
      "loss": 4.7186,
      "step": 170
    },
    {
      "epoch": 0.01093095281472035,
      "grad_norm": 2.513396978378296,
      "learning_rate": 9.99967242666545e-05,
      "loss": 4.6967,
      "step": 180
    },
    {
      "epoch": 0.011538227971093702,
      "grad_norm": 19.180917739868164,
      "learning_rate": 9.99963501905458e-05,
      "loss": 4.6914,
      "step": 190
    },
    {
      "epoch": 0.012145503127467054,
      "grad_norm": 12.742111206054688,
      "learning_rate": 9.999595589511684e-05,
      "loss": 4.685,
      "step": 200
    },
    {
      "epoch": 0.012752778283840409,
      "grad_norm": 3.468679428100586,
      "learning_rate": 9.999554138052704e-05,
      "loss": 4.3118,
      "step": 210
    },
    {
      "epoch": 0.013360053440213761,
      "grad_norm": 3.2972075939178467,
      "learning_rate": 9.999510664694408e-05,
      "loss": 4.2704,
      "step": 220
    },
    {
      "epoch": 0.013967328596587114,
      "grad_norm": 4.380640506744385,
      "learning_rate": 9.999465169454374e-05,
      "loss": 4.3906,
      "step": 230
    },
    {
      "epoch": 0.014574603752960467,
      "grad_norm": 7.445106506347656,
      "learning_rate": 9.999417652351002e-05,
      "loss": 4.3736,
      "step": 240
    },
    {
      "epoch": 0.01518187890933382,
      "grad_norm": 5.385582447052002,
      "learning_rate": 9.999368113403508e-05,
      "loss": 4.349,
      "step": 250
    },
    {
      "epoch": 0.015789154065707172,
      "grad_norm": 2.83884596824646,
      "learning_rate": 9.999316552631928e-05,
      "loss": 4.5075,
      "step": 260
    },
    {
      "epoch": 0.016396429222080525,
      "grad_norm": 5.685907363891602,
      "learning_rate": 9.999262970057113e-05,
      "loss": 4.1651,
      "step": 270
    },
    {
      "epoch": 0.017003704378453877,
      "grad_norm": 7.791661739349365,
      "learning_rate": 9.999207365700733e-05,
      "loss": 4.2608,
      "step": 280
    },
    {
      "epoch": 0.01761097953482723,
      "grad_norm": 6.540879249572754,
      "learning_rate": 9.999149739585273e-05,
      "loss": 4.0514,
      "step": 290
    },
    {
      "epoch": 0.018218254691200583,
      "grad_norm": 4.365360260009766,
      "learning_rate": 9.999090091734043e-05,
      "loss": 4.2361,
      "step": 300
    },
    {
      "epoch": 0.018825529847573935,
      "grad_norm": 8.125962257385254,
      "learning_rate": 9.999028422171159e-05,
      "loss": 4.3536,
      "step": 310
    },
    {
      "epoch": 0.019432805003947288,
      "grad_norm": 7.2782158851623535,
      "learning_rate": 9.998964730921568e-05,
      "loss": 4.1832,
      "step": 320
    },
    {
      "epoch": 0.02004008016032064,
      "grad_norm": 3.2552385330200195,
      "learning_rate": 9.99889901801102e-05,
      "loss": 4.1653,
      "step": 330
    },
    {
      "epoch": 0.020647355316693993,
      "grad_norm": 11.414443969726562,
      "learning_rate": 9.998831283466099e-05,
      "loss": 4.0131,
      "step": 340
    },
    {
      "epoch": 0.021254630473067346,
      "grad_norm": 2.905280590057373,
      "learning_rate": 9.998761527314191e-05,
      "loss": 3.7341,
      "step": 350
    },
    {
      "epoch": 0.0218619056294407,
      "grad_norm": 3.630624771118164,
      "learning_rate": 9.99868974958351e-05,
      "loss": 3.7894,
      "step": 360
    },
    {
      "epoch": 0.02246918078581405,
      "grad_norm": 3.606355667114258,
      "learning_rate": 9.998615950303083e-05,
      "loss": 4.1184,
      "step": 370
    },
    {
      "epoch": 0.023076455942187404,
      "grad_norm": 4.188241958618164,
      "learning_rate": 9.998540129502756e-05,
      "loss": 4.1417,
      "step": 380
    },
    {
      "epoch": 0.023683731098560756,
      "grad_norm": 2.5141687393188477,
      "learning_rate": 9.998462287213191e-05,
      "loss": 4.1592,
      "step": 390
    },
    {
      "epoch": 0.02429100625493411,
      "grad_norm": 2.63543701171875,
      "learning_rate": 9.998382423465871e-05,
      "loss": 3.9305,
      "step": 400
    },
    {
      "epoch": 0.024898281411307465,
      "grad_norm": 5.720279216766357,
      "learning_rate": 9.998300538293091e-05,
      "loss": 3.7782,
      "step": 410
    },
    {
      "epoch": 0.025505556567680818,
      "grad_norm": 3.3065123558044434,
      "learning_rate": 9.99821663172797e-05,
      "loss": 3.637,
      "step": 420
    },
    {
      "epoch": 0.02611283172405417,
      "grad_norm": 9.346588134765625,
      "learning_rate": 9.998130703804438e-05,
      "loss": 3.5291,
      "step": 430
    },
    {
      "epoch": 0.026720106880427523,
      "grad_norm": 2.4772918224334717,
      "learning_rate": 9.998042754557249e-05,
      "loss": 3.6874,
      "step": 440
    },
    {
      "epoch": 0.027327382036800876,
      "grad_norm": 2.295374870300293,
      "learning_rate": 9.997952784021967e-05,
      "loss": 3.7639,
      "step": 450
    },
    {
      "epoch": 0.027934657193174228,
      "grad_norm": 2.9437098503112793,
      "learning_rate": 9.997860792234981e-05,
      "loss": 3.6958,
      "step": 460
    },
    {
      "epoch": 0.02854193234954758,
      "grad_norm": 7.253818035125732,
      "learning_rate": 9.997766779233493e-05,
      "loss": 3.6954,
      "step": 470
    },
    {
      "epoch": 0.029149207505920934,
      "grad_norm": 3.395220994949341,
      "learning_rate": 9.997670745055522e-05,
      "loss": 3.5597,
      "step": 480
    },
    {
      "epoch": 0.029756482662294286,
      "grad_norm": 4.023594856262207,
      "learning_rate": 9.997572689739907e-05,
      "loss": 3.7995,
      "step": 490
    },
    {
      "epoch": 0.03036375781866764,
      "grad_norm": 5.856431007385254,
      "learning_rate": 9.997472613326304e-05,
      "loss": 3.9596,
      "step": 500
    },
    {
      "epoch": 0.03097103297504099,
      "grad_norm": 4.918809413909912,
      "learning_rate": 9.997370515855182e-05,
      "loss": 3.6553,
      "step": 510
    },
    {
      "epoch": 0.031578308131414344,
      "grad_norm": 6.928577423095703,
      "learning_rate": 9.997266397367836e-05,
      "loss": 3.5024,
      "step": 520
    },
    {
      "epoch": 0.03218558328778769,
      "grad_norm": 1.7731853723526,
      "learning_rate": 9.99716025790637e-05,
      "loss": 3.4951,
      "step": 530
    },
    {
      "epoch": 0.03279285844416105,
      "grad_norm": 5.018946647644043,
      "learning_rate": 9.997052097513709e-05,
      "loss": 3.602,
      "step": 540
    },
    {
      "epoch": 0.033400133600534405,
      "grad_norm": 9.073817253112793,
      "learning_rate": 9.996941916233594e-05,
      "loss": 3.5595,
      "step": 550
    },
    {
      "epoch": 0.034007408756907755,
      "grad_norm": 3.559922933578491,
      "learning_rate": 9.996829714110583e-05,
      "loss": 3.9014,
      "step": 560
    },
    {
      "epoch": 0.03461468391328111,
      "grad_norm": 2.851785898208618,
      "learning_rate": 9.996715491190057e-05,
      "loss": 3.6663,
      "step": 570
    },
    {
      "epoch": 0.03522195906965446,
      "grad_norm": 4.44431734085083,
      "learning_rate": 9.996599247518206e-05,
      "loss": 3.3449,
      "step": 580
    },
    {
      "epoch": 0.035829234226027816,
      "grad_norm": 11.87641429901123,
      "learning_rate": 9.996480983142041e-05,
      "loss": 3.7258,
      "step": 590
    },
    {
      "epoch": 0.036436509382401165,
      "grad_norm": 5.302826404571533,
      "learning_rate": 9.99636069810939e-05,
      "loss": 3.7431,
      "step": 600
    },
    {
      "epoch": 0.03704378453877452,
      "grad_norm": 5.828855514526367,
      "learning_rate": 9.9962383924689e-05,
      "loss": 3.525,
      "step": 610
    },
    {
      "epoch": 0.03765105969514787,
      "grad_norm": 3.5189740657806396,
      "learning_rate": 9.99611406627003e-05,
      "loss": 3.8314,
      "step": 620
    },
    {
      "epoch": 0.03825833485152123,
      "grad_norm": 4.719743251800537,
      "learning_rate": 9.995987719563062e-05,
      "loss": 3.709,
      "step": 630
    },
    {
      "epoch": 0.038865610007894576,
      "grad_norm": 6.268887042999268,
      "learning_rate": 9.995859352399094e-05,
      "loss": 3.4962,
      "step": 640
    },
    {
      "epoch": 0.03947288516426793,
      "grad_norm": 3.0720744132995605,
      "learning_rate": 9.995728964830036e-05,
      "loss": 3.6415,
      "step": 650
    },
    {
      "epoch": 0.04008016032064128,
      "grad_norm": 4.220585346221924,
      "learning_rate": 9.995596556908622e-05,
      "loss": 3.4433,
      "step": 660
    },
    {
      "epoch": 0.04068743547701464,
      "grad_norm": 14.61599349975586,
      "learning_rate": 9.995462128688397e-05,
      "loss": 3.3592,
      "step": 670
    },
    {
      "epoch": 0.041294710633387986,
      "grad_norm": 9.54877758026123,
      "learning_rate": 9.995325680223728e-05,
      "loss": 3.5712,
      "step": 680
    },
    {
      "epoch": 0.04190198578976134,
      "grad_norm": 3.4485435485839844,
      "learning_rate": 9.995187211569797e-05,
      "loss": 3.6142,
      "step": 690
    },
    {
      "epoch": 0.04250926094613469,
      "grad_norm": 2.693326711654663,
      "learning_rate": 9.995046722782601e-05,
      "loss": 3.3277,
      "step": 700
    },
    {
      "epoch": 0.04311653610250805,
      "grad_norm": 4.016327857971191,
      "learning_rate": 9.994904213918959e-05,
      "loss": 3.32,
      "step": 710
    },
    {
      "epoch": 0.0437238112588814,
      "grad_norm": 2.823516607284546,
      "learning_rate": 9.994759685036501e-05,
      "loss": 3.2107,
      "step": 720
    },
    {
      "epoch": 0.04433108641525475,
      "grad_norm": 2.6761887073516846,
      "learning_rate": 9.994613136193679e-05,
      "loss": 3.1468,
      "step": 730
    },
    {
      "epoch": 0.0449383615716281,
      "grad_norm": 2.4636666774749756,
      "learning_rate": 9.994464567449757e-05,
      "loss": 3.6992,
      "step": 740
    },
    {
      "epoch": 0.04554563672800146,
      "grad_norm": 8.047789573669434,
      "learning_rate": 9.99431397886482e-05,
      "loss": 3.5355,
      "step": 750
    },
    {
      "epoch": 0.04615291188437481,
      "grad_norm": 3.4024500846862793,
      "learning_rate": 9.994161370499769e-05,
      "loss": 3.1944,
      "step": 760
    },
    {
      "epoch": 0.04676018704074816,
      "grad_norm": 3.4887309074401855,
      "learning_rate": 9.994006742416321e-05,
      "loss": 3.1069,
      "step": 770
    },
    {
      "epoch": 0.04736746219712151,
      "grad_norm": 3.892395496368408,
      "learning_rate": 9.99385009467701e-05,
      "loss": 3.1356,
      "step": 780
    },
    {
      "epoch": 0.04797473735349487,
      "grad_norm": 2.279851198196411,
      "learning_rate": 9.993691427345187e-05,
      "loss": 2.998,
      "step": 790
    },
    {
      "epoch": 0.04858201250986822,
      "grad_norm": 4.58197546005249,
      "learning_rate": 9.993530740485018e-05,
      "loss": 3.0858,
      "step": 800
    },
    {
      "epoch": 0.049189287666241574,
      "grad_norm": 3.205735445022583,
      "learning_rate": 9.993368034161489e-05,
      "loss": 2.9556,
      "step": 810
    },
    {
      "epoch": 0.04979656282261493,
      "grad_norm": 2.890172243118286,
      "learning_rate": 9.9932033084404e-05,
      "loss": 3.1863,
      "step": 820
    },
    {
      "epoch": 0.05040383797898828,
      "grad_norm": 2.745922565460205,
      "learning_rate": 9.99303656338837e-05,
      "loss": 2.9293,
      "step": 830
    },
    {
      "epoch": 0.051011113135361635,
      "grad_norm": 3.1544532775878906,
      "learning_rate": 9.992867799072833e-05,
      "loss": 2.8766,
      "step": 840
    },
    {
      "epoch": 0.051618388291734985,
      "grad_norm": 4.327705383300781,
      "learning_rate": 9.99269701556204e-05,
      "loss": 2.7607,
      "step": 850
    },
    {
      "epoch": 0.05222566344810834,
      "grad_norm": 3.2554094791412354,
      "learning_rate": 9.992524212925056e-05,
      "loss": 2.8812,
      "step": 860
    },
    {
      "epoch": 0.05283293860448169,
      "grad_norm": 7.686600208282471,
      "learning_rate": 9.99234939123177e-05,
      "loss": 2.9385,
      "step": 870
    },
    {
      "epoch": 0.053440213760855046,
      "grad_norm": 11.637928009033203,
      "learning_rate": 9.992172550552879e-05,
      "loss": 2.9127,
      "step": 880
    },
    {
      "epoch": 0.054047488917228395,
      "grad_norm": 7.6015095710754395,
      "learning_rate": 9.9919936909599e-05,
      "loss": 2.8672,
      "step": 890
    },
    {
      "epoch": 0.05465476407360175,
      "grad_norm": 4.072060585021973,
      "learning_rate": 9.99181281252517e-05,
      "loss": 2.915,
      "step": 900
    },
    {
      "epoch": 0.0552620392299751,
      "grad_norm": 4.688607215881348,
      "learning_rate": 9.991629915321836e-05,
      "loss": 2.6457,
      "step": 910
    },
    {
      "epoch": 0.055869314386348456,
      "grad_norm": 6.268974304199219,
      "learning_rate": 9.991444999423865e-05,
      "loss": 2.7439,
      "step": 920
    },
    {
      "epoch": 0.056476589542721806,
      "grad_norm": 3.0477046966552734,
      "learning_rate": 9.991258064906041e-05,
      "loss": 2.6361,
      "step": 930
    },
    {
      "epoch": 0.05708386469909516,
      "grad_norm": 7.941761493682861,
      "learning_rate": 9.991069111843964e-05,
      "loss": 2.7091,
      "step": 940
    },
    {
      "epoch": 0.05769113985546851,
      "grad_norm": 2.905221700668335,
      "learning_rate": 9.990878140314047e-05,
      "loss": 2.4843,
      "step": 950
    },
    {
      "epoch": 0.05829841501184187,
      "grad_norm": 3.220487356185913,
      "learning_rate": 9.990685150393523e-05,
      "loss": 2.3568,
      "step": 960
    },
    {
      "epoch": 0.058905690168215216,
      "grad_norm": 2.7990384101867676,
      "learning_rate": 9.990490142160442e-05,
      "loss": 2.3204,
      "step": 970
    },
    {
      "epoch": 0.05951296532458857,
      "grad_norm": 3.4303619861602783,
      "learning_rate": 9.990293115693667e-05,
      "loss": 2.3734,
      "step": 980
    },
    {
      "epoch": 0.06012024048096192,
      "grad_norm": 3.9998552799224854,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.3281,
      "step": 990
    },
    {
      "epoch": 0.06072751563733528,
      "grad_norm": 4.2996697425842285,
      "learning_rate": 9.989893008378572e-05,
      "loss": 2.4217,
      "step": 1000
    },
    {
      "epoch": 0.06133479079370863,
      "grad_norm": 5.526533603668213,
      "learning_rate": 9.989689927692062e-05,
      "loss": 2.4254,
      "step": 1010
    },
    {
      "epoch": 0.06194206595008198,
      "grad_norm": 3.6924266815185547,
      "learning_rate": 9.989484829095478e-05,
      "loss": 2.3352,
      "step": 1020
    },
    {
      "epoch": 0.06254934110645534,
      "grad_norm": 2.6935300827026367,
      "learning_rate": 9.989277712671766e-05,
      "loss": 2.2147,
      "step": 1030
    },
    {
      "epoch": 0.06315661626282869,
      "grad_norm": 10.323328018188477,
      "learning_rate": 9.989068578504684e-05,
      "loss": 2.1944,
      "step": 1040
    },
    {
      "epoch": 0.06376389141920204,
      "grad_norm": 4.4990949630737305,
      "learning_rate": 9.988857426678811e-05,
      "loss": 2.1228,
      "step": 1050
    },
    {
      "epoch": 0.06437116657557539,
      "grad_norm": 2.7184057235717773,
      "learning_rate": 9.98864425727954e-05,
      "loss": 2.1053,
      "step": 1060
    },
    {
      "epoch": 0.06497844173194875,
      "grad_norm": 2.328578472137451,
      "learning_rate": 9.98842907039308e-05,
      "loss": 2.0511,
      "step": 1070
    },
    {
      "epoch": 0.0655857168883221,
      "grad_norm": 2.5375561714172363,
      "learning_rate": 9.988211866106457e-05,
      "loss": 2.0544,
      "step": 1080
    },
    {
      "epoch": 0.06619299204469545,
      "grad_norm": 2.879633665084839,
      "learning_rate": 9.987992644507511e-05,
      "loss": 2.0081,
      "step": 1090
    },
    {
      "epoch": 0.06680026720106881,
      "grad_norm": 8.820672988891602,
      "learning_rate": 9.987771405684897e-05,
      "loss": 1.9809,
      "step": 1100
    },
    {
      "epoch": 0.06740754235744216,
      "grad_norm": 1.840754508972168,
      "learning_rate": 9.987548149728092e-05,
      "loss": 1.9069,
      "step": 1110
    },
    {
      "epoch": 0.06801481751381551,
      "grad_norm": 3.3017938137054443,
      "learning_rate": 9.987322876727381e-05,
      "loss": 1.973,
      "step": 1120
    },
    {
      "epoch": 0.06862209267018886,
      "grad_norm": 2.486572265625,
      "learning_rate": 9.98709558677387e-05,
      "loss": 1.9659,
      "step": 1130
    },
    {
      "epoch": 0.06922936782656222,
      "grad_norm": 4.514452934265137,
      "learning_rate": 9.986866279959474e-05,
      "loss": 1.944,
      "step": 1140
    },
    {
      "epoch": 0.06983664298293557,
      "grad_norm": 2.884150743484497,
      "learning_rate": 9.986634956376932e-05,
      "loss": 1.9032,
      "step": 1150
    },
    {
      "epoch": 0.07044391813930892,
      "grad_norm": 2.6356775760650635,
      "learning_rate": 9.986401616119795e-05,
      "loss": 1.9257,
      "step": 1160
    },
    {
      "epoch": 0.07105119329568227,
      "grad_norm": 2.6357204914093018,
      "learning_rate": 9.98616625928243e-05,
      "loss": 1.8749,
      "step": 1170
    },
    {
      "epoch": 0.07165846845205563,
      "grad_norm": 1.4228825569152832,
      "learning_rate": 9.985928885960019e-05,
      "loss": 1.8166,
      "step": 1180
    },
    {
      "epoch": 0.07226574360842898,
      "grad_norm": 1.8244305849075317,
      "learning_rate": 9.985689496248556e-05,
      "loss": 1.7903,
      "step": 1190
    },
    {
      "epoch": 0.07287301876480233,
      "grad_norm": 2.345052719116211,
      "learning_rate": 9.985448090244858e-05,
      "loss": 1.8263,
      "step": 1200
    },
    {
      "epoch": 0.07348029392117568,
      "grad_norm": 4.026148319244385,
      "learning_rate": 9.985204668046553e-05,
      "loss": 1.8052,
      "step": 1210
    },
    {
      "epoch": 0.07408756907754904,
      "grad_norm": 3.625269889831543,
      "learning_rate": 9.984959229752082e-05,
      "loss": 1.7843,
      "step": 1220
    },
    {
      "epoch": 0.07469484423392239,
      "grad_norm": 2.9221153259277344,
      "learning_rate": 9.984711775460707e-05,
      "loss": 1.8104,
      "step": 1230
    },
    {
      "epoch": 0.07530211939029574,
      "grad_norm": 3.255139112472534,
      "learning_rate": 9.9844623052725e-05,
      "loss": 1.8217,
      "step": 1240
    },
    {
      "epoch": 0.07590939454666909,
      "grad_norm": 4.969590663909912,
      "learning_rate": 9.984210819288354e-05,
      "loss": 1.8132,
      "step": 1250
    },
    {
      "epoch": 0.07651666970304245,
      "grad_norm": 3.2164132595062256,
      "learning_rate": 9.983957317609971e-05,
      "loss": 1.7951,
      "step": 1260
    },
    {
      "epoch": 0.0771239448594158,
      "grad_norm": 2.240711212158203,
      "learning_rate": 9.983701800339873e-05,
      "loss": 1.7965,
      "step": 1270
    },
    {
      "epoch": 0.07773122001578915,
      "grad_norm": 1.9440107345581055,
      "learning_rate": 9.983444267581394e-05,
      "loss": 1.7567,
      "step": 1280
    },
    {
      "epoch": 0.0783384951721625,
      "grad_norm": 2.198976755142212,
      "learning_rate": 9.983184719438687e-05,
      "loss": 1.7463,
      "step": 1290
    },
    {
      "epoch": 0.07894577032853586,
      "grad_norm": 1.5337246656417847,
      "learning_rate": 9.982923156016713e-05,
      "loss": 1.7112,
      "step": 1300
    },
    {
      "epoch": 0.07955304548490921,
      "grad_norm": 2.130181074142456,
      "learning_rate": 9.982659577421255e-05,
      "loss": 1.7032,
      "step": 1310
    },
    {
      "epoch": 0.08016032064128256,
      "grad_norm": 1.2442429065704346,
      "learning_rate": 9.982393983758908e-05,
      "loss": 1.7546,
      "step": 1320
    },
    {
      "epoch": 0.08076759579765591,
      "grad_norm": 3.46728253364563,
      "learning_rate": 9.982126375137083e-05,
      "loss": 1.7226,
      "step": 1330
    },
    {
      "epoch": 0.08137487095402927,
      "grad_norm": 1.113364338874817,
      "learning_rate": 9.981856751664004e-05,
      "loss": 1.7197,
      "step": 1340
    },
    {
      "epoch": 0.08198214611040262,
      "grad_norm": 2.5879998207092285,
      "learning_rate": 9.981585113448713e-05,
      "loss": 1.6905,
      "step": 1350
    },
    {
      "epoch": 0.08258942126677597,
      "grad_norm": 1.7046889066696167,
      "learning_rate": 9.981311460601061e-05,
      "loss": 1.6616,
      "step": 1360
    },
    {
      "epoch": 0.08319669642314934,
      "grad_norm": 1.0623019933700562,
      "learning_rate": 9.981035793231722e-05,
      "loss": 1.6918,
      "step": 1370
    },
    {
      "epoch": 0.08380397157952268,
      "grad_norm": 9.209843635559082,
      "learning_rate": 9.980758111452177e-05,
      "loss": 1.7075,
      "step": 1380
    },
    {
      "epoch": 0.08441124673589603,
      "grad_norm": 5.1784210205078125,
      "learning_rate": 9.980478415374726e-05,
      "loss": 1.6972,
      "step": 1390
    },
    {
      "epoch": 0.08501852189226938,
      "grad_norm": 2.4732816219329834,
      "learning_rate": 9.980196705112484e-05,
      "loss": 1.7107,
      "step": 1400
    },
    {
      "epoch": 0.08562579704864275,
      "grad_norm": 2.0202836990356445,
      "learning_rate": 9.979912980779377e-05,
      "loss": 1.6877,
      "step": 1410
    },
    {
      "epoch": 0.0862330722050161,
      "grad_norm": 6.325260162353516,
      "learning_rate": 9.979627242490148e-05,
      "loss": 1.6776,
      "step": 1420
    },
    {
      "epoch": 0.08684034736138944,
      "grad_norm": 2.600013256072998,
      "learning_rate": 9.979339490360355e-05,
      "loss": 1.695,
      "step": 1430
    },
    {
      "epoch": 0.0874476225177628,
      "grad_norm": 1.0246304273605347,
      "learning_rate": 9.979049724506369e-05,
      "loss": 1.6736,
      "step": 1440
    },
    {
      "epoch": 0.08805489767413616,
      "grad_norm": 2.000196695327759,
      "learning_rate": 9.978757945045378e-05,
      "loss": 1.6632,
      "step": 1450
    },
    {
      "epoch": 0.0886621728305095,
      "grad_norm": 6.405223369598389,
      "learning_rate": 9.978464152095378e-05,
      "loss": 1.6653,
      "step": 1460
    },
    {
      "epoch": 0.08926944798688285,
      "grad_norm": 1.4600715637207031,
      "learning_rate": 9.978168345775187e-05,
      "loss": 1.6729,
      "step": 1470
    },
    {
      "epoch": 0.0898767231432562,
      "grad_norm": 1.5724115371704102,
      "learning_rate": 9.977870526204431e-05,
      "loss": 1.6515,
      "step": 1480
    },
    {
      "epoch": 0.09048399829962957,
      "grad_norm": 0.8256332278251648,
      "learning_rate": 9.977570693503557e-05,
      "loss": 1.6437,
      "step": 1490
    },
    {
      "epoch": 0.09109127345600292,
      "grad_norm": 0.9536433815956116,
      "learning_rate": 9.977268847793819e-05,
      "loss": 1.6565,
      "step": 1500
    },
    {
      "epoch": 0.09169854861237627,
      "grad_norm": 2.189920663833618,
      "learning_rate": 9.976964989197288e-05,
      "loss": 1.6406,
      "step": 1510
    },
    {
      "epoch": 0.09230582376874961,
      "grad_norm": 1.157241702079773,
      "learning_rate": 9.976659117836851e-05,
      "loss": 1.6242,
      "step": 1520
    },
    {
      "epoch": 0.09291309892512298,
      "grad_norm": 4.677481174468994,
      "learning_rate": 9.976351233836207e-05,
      "loss": 1.6467,
      "step": 1530
    },
    {
      "epoch": 0.09352037408149633,
      "grad_norm": 2.1429669857025146,
      "learning_rate": 9.976041337319867e-05,
      "loss": 1.6368,
      "step": 1540
    },
    {
      "epoch": 0.09412764923786968,
      "grad_norm": 1.3913663625717163,
      "learning_rate": 9.975729428413162e-05,
      "loss": 1.6263,
      "step": 1550
    },
    {
      "epoch": 0.09473492439424303,
      "grad_norm": 1.4719600677490234,
      "learning_rate": 9.975415507242229e-05,
      "loss": 1.6125,
      "step": 1560
    },
    {
      "epoch": 0.09534219955061639,
      "grad_norm": 1.1484360694885254,
      "learning_rate": 9.975099573934026e-05,
      "loss": 1.6254,
      "step": 1570
    },
    {
      "epoch": 0.09594947470698974,
      "grad_norm": 2.2875919342041016,
      "learning_rate": 9.974781628616318e-05,
      "loss": 1.6239,
      "step": 1580
    },
    {
      "epoch": 0.09655674986336309,
      "grad_norm": 0.7777429819107056,
      "learning_rate": 9.974461671417689e-05,
      "loss": 1.6257,
      "step": 1590
    },
    {
      "epoch": 0.09716402501973644,
      "grad_norm": 0.5533778667449951,
      "learning_rate": 9.974139702467538e-05,
      "loss": 1.6047,
      "step": 1600
    },
    {
      "epoch": 0.0977713001761098,
      "grad_norm": 1.1158292293548584,
      "learning_rate": 9.973815721896068e-05,
      "loss": 1.5953,
      "step": 1610
    },
    {
      "epoch": 0.09837857533248315,
      "grad_norm": 1.7562638521194458,
      "learning_rate": 9.973489729834307e-05,
      "loss": 1.5895,
      "step": 1620
    },
    {
      "epoch": 0.0989858504888565,
      "grad_norm": 0.6662419438362122,
      "learning_rate": 9.973161726414088e-05,
      "loss": 1.6026,
      "step": 1630
    },
    {
      "epoch": 0.09959312564522986,
      "grad_norm": 1.9023475646972656,
      "learning_rate": 9.972831711768063e-05,
      "loss": 1.6053,
      "step": 1640
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 2.9369125366210938,
      "learning_rate": 9.972499686029694e-05,
      "loss": 1.6175,
      "step": 1650
    },
    {
      "epoch": 0.10080767595797656,
      "grad_norm": 0.9626007080078125,
      "learning_rate": 9.972165649333259e-05,
      "loss": 1.5928,
      "step": 1660
    },
    {
      "epoch": 0.10141495111434991,
      "grad_norm": 1.1160480976104736,
      "learning_rate": 9.971829601813845e-05,
      "loss": 1.5956,
      "step": 1670
    },
    {
      "epoch": 0.10202222627072327,
      "grad_norm": 1.5677565336227417,
      "learning_rate": 9.971491543607356e-05,
      "loss": 1.6054,
      "step": 1680
    },
    {
      "epoch": 0.10262950142709662,
      "grad_norm": 1.3582483530044556,
      "learning_rate": 9.971151474850511e-05,
      "loss": 1.5949,
      "step": 1690
    },
    {
      "epoch": 0.10323677658346997,
      "grad_norm": 0.9674955606460571,
      "learning_rate": 9.970809395680837e-05,
      "loss": 1.5763,
      "step": 1700
    },
    {
      "epoch": 0.10384405173984332,
      "grad_norm": 0.7426239252090454,
      "learning_rate": 9.970465306236676e-05,
      "loss": 1.5711,
      "step": 1710
    },
    {
      "epoch": 0.10445132689621668,
      "grad_norm": 1.5571309328079224,
      "learning_rate": 9.970119206657182e-05,
      "loss": 1.5816,
      "step": 1720
    },
    {
      "epoch": 0.10505860205259003,
      "grad_norm": 0.6432847380638123,
      "learning_rate": 9.969771097082326e-05,
      "loss": 1.5887,
      "step": 1730
    },
    {
      "epoch": 0.10566587720896338,
      "grad_norm": 1.9309096336364746,
      "learning_rate": 9.969420977652888e-05,
      "loss": 1.5802,
      "step": 1740
    },
    {
      "epoch": 0.10627315236533673,
      "grad_norm": 0.5031464099884033,
      "learning_rate": 9.969068848510461e-05,
      "loss": 1.5793,
      "step": 1750
    },
    {
      "epoch": 0.10688042752171009,
      "grad_norm": 1.9073442220687866,
      "learning_rate": 9.968714709797453e-05,
      "loss": 1.5784,
      "step": 1760
    },
    {
      "epoch": 0.10748770267808344,
      "grad_norm": 5.623291492462158,
      "learning_rate": 9.968358561657083e-05,
      "loss": 1.5884,
      "step": 1770
    },
    {
      "epoch": 0.10809497783445679,
      "grad_norm": 1.1498005390167236,
      "learning_rate": 9.968000404233382e-05,
      "loss": 1.6071,
      "step": 1780
    },
    {
      "epoch": 0.10870225299083014,
      "grad_norm": 0.9673342108726501,
      "learning_rate": 9.967640237671195e-05,
      "loss": 1.5921,
      "step": 1790
    },
    {
      "epoch": 0.1093095281472035,
      "grad_norm": 2.7265498638153076,
      "learning_rate": 9.967278062116179e-05,
      "loss": 1.5748,
      "step": 1800
    },
    {
      "epoch": 0.10991680330357685,
      "grad_norm": 2.9655444622039795,
      "learning_rate": 9.966913877714803e-05,
      "loss": 1.5836,
      "step": 1810
    },
    {
      "epoch": 0.1105240784599502,
      "grad_norm": 1.5925918817520142,
      "learning_rate": 9.966547684614352e-05,
      "loss": 1.5726,
      "step": 1820
    },
    {
      "epoch": 0.11113135361632355,
      "grad_norm": 1.0414015054702759,
      "learning_rate": 9.966179482962916e-05,
      "loss": 1.5645,
      "step": 1830
    },
    {
      "epoch": 0.11173862877269691,
      "grad_norm": 0.9214867353439331,
      "learning_rate": 9.965809272909406e-05,
      "loss": 1.5618,
      "step": 1840
    },
    {
      "epoch": 0.11234590392907026,
      "grad_norm": 1.2022026777267456,
      "learning_rate": 9.965437054603538e-05,
      "loss": 1.5603,
      "step": 1850
    },
    {
      "epoch": 0.11295317908544361,
      "grad_norm": 1.7635812759399414,
      "learning_rate": 9.965062828195841e-05,
      "loss": 1.5703,
      "step": 1860
    },
    {
      "epoch": 0.11356045424181696,
      "grad_norm": 0.5576514601707458,
      "learning_rate": 9.964686593837663e-05,
      "loss": 1.5849,
      "step": 1870
    },
    {
      "epoch": 0.11416772939819032,
      "grad_norm": 1.4585694074630737,
      "learning_rate": 9.964308351681157e-05,
      "loss": 1.5795,
      "step": 1880
    },
    {
      "epoch": 0.11477500455456367,
      "grad_norm": 2.4499082565307617,
      "learning_rate": 9.96392810187929e-05,
      "loss": 1.5611,
      "step": 1890
    },
    {
      "epoch": 0.11538227971093702,
      "grad_norm": 0.5180209875106812,
      "learning_rate": 9.96354584458584e-05,
      "loss": 1.5565,
      "step": 1900
    },
    {
      "epoch": 0.11598955486731038,
      "grad_norm": 0.7009605169296265,
      "learning_rate": 9.9631615799554e-05,
      "loss": 1.5513,
      "step": 1910
    },
    {
      "epoch": 0.11659683002368373,
      "grad_norm": 0.952308714389801,
      "learning_rate": 9.96277530814337e-05,
      "loss": 1.5484,
      "step": 1920
    },
    {
      "epoch": 0.11720410518005708,
      "grad_norm": 0.6812533140182495,
      "learning_rate": 9.962387029305968e-05,
      "loss": 1.5599,
      "step": 1930
    },
    {
      "epoch": 0.11781138033643043,
      "grad_norm": 2.4961624145507812,
      "learning_rate": 9.96199674360022e-05,
      "loss": 1.5597,
      "step": 1940
    },
    {
      "epoch": 0.1184186554928038,
      "grad_norm": 0.996629536151886,
      "learning_rate": 9.961604451183958e-05,
      "loss": 1.5564,
      "step": 1950
    },
    {
      "epoch": 0.11902593064917714,
      "grad_norm": 2.2444164752960205,
      "learning_rate": 9.961210152215839e-05,
      "loss": 1.5523,
      "step": 1960
    },
    {
      "epoch": 0.1196332058055505,
      "grad_norm": 3.332181692123413,
      "learning_rate": 9.960813846855318e-05,
      "loss": 1.5583,
      "step": 1970
    },
    {
      "epoch": 0.12024048096192384,
      "grad_norm": 0.45020031929016113,
      "learning_rate": 9.960415535262671e-05,
      "loss": 1.5622,
      "step": 1980
    },
    {
      "epoch": 0.1208477561182972,
      "grad_norm": 0.6216002106666565,
      "learning_rate": 9.96001521759898e-05,
      "loss": 1.5466,
      "step": 1990
    },
    {
      "epoch": 0.12145503127467056,
      "grad_norm": 0.8379803895950317,
      "learning_rate": 9.959612894026138e-05,
      "loss": 1.5444,
      "step": 2000
    },
    {
      "epoch": 0.1220623064310439,
      "grad_norm": 0.9769300222396851,
      "learning_rate": 9.959208564706854e-05,
      "loss": 1.5512,
      "step": 2010
    },
    {
      "epoch": 0.12266958158741725,
      "grad_norm": 0.4828249216079712,
      "learning_rate": 9.958802229804643e-05,
      "loss": 1.5511,
      "step": 2020
    },
    {
      "epoch": 0.12327685674379062,
      "grad_norm": 0.8729428648948669,
      "learning_rate": 9.958393889483837e-05,
      "loss": 1.5549,
      "step": 2030
    },
    {
      "epoch": 0.12388413190016397,
      "grad_norm": 0.3249347507953644,
      "learning_rate": 9.95798354390957e-05,
      "loss": 1.5396,
      "step": 2040
    },
    {
      "epoch": 0.12449140705653731,
      "grad_norm": 0.7837151885032654,
      "learning_rate": 9.957571193247797e-05,
      "loss": 1.5455,
      "step": 2050
    },
    {
      "epoch": 0.12509868221291068,
      "grad_norm": 0.6502939462661743,
      "learning_rate": 9.957156837665276e-05,
      "loss": 1.5429,
      "step": 2060
    },
    {
      "epoch": 0.12570595736928403,
      "grad_norm": 0.3820272982120514,
      "learning_rate": 9.95674047732958e-05,
      "loss": 1.5347,
      "step": 2070
    },
    {
      "epoch": 0.12631323252565738,
      "grad_norm": 0.7360146641731262,
      "learning_rate": 9.956322112409093e-05,
      "loss": 1.5379,
      "step": 2080
    },
    {
      "epoch": 0.12692050768203073,
      "grad_norm": 0.6144881248474121,
      "learning_rate": 9.955901743073006e-05,
      "loss": 1.5406,
      "step": 2090
    },
    {
      "epoch": 0.12752778283840407,
      "grad_norm": 1.2162202596664429,
      "learning_rate": 9.955479369491326e-05,
      "loss": 1.5455,
      "step": 2100
    },
    {
      "epoch": 0.12813505799477742,
      "grad_norm": 1.0104188919067383,
      "learning_rate": 9.955054991834866e-05,
      "loss": 1.5486,
      "step": 2110
    },
    {
      "epoch": 0.12874233315115077,
      "grad_norm": 0.8018045425415039,
      "learning_rate": 9.954628610275249e-05,
      "loss": 1.5415,
      "step": 2120
    },
    {
      "epoch": 0.12934960830752415,
      "grad_norm": 0.464362233877182,
      "learning_rate": 9.954200224984915e-05,
      "loss": 1.5368,
      "step": 2130
    },
    {
      "epoch": 0.1299568834638975,
      "grad_norm": 0.8029056787490845,
      "learning_rate": 9.953769836137106e-05,
      "loss": 1.5472,
      "step": 2140
    },
    {
      "epoch": 0.13056415862027085,
      "grad_norm": 1.5799574851989746,
      "learning_rate": 9.95333744390588e-05,
      "loss": 1.5564,
      "step": 2150
    },
    {
      "epoch": 0.1311714337766442,
      "grad_norm": 0.507415771484375,
      "learning_rate": 9.952903048466104e-05,
      "loss": 1.5473,
      "step": 2160
    },
    {
      "epoch": 0.13177870893301755,
      "grad_norm": 1.2989106178283691,
      "learning_rate": 9.952466649993451e-05,
      "loss": 1.5478,
      "step": 2170
    },
    {
      "epoch": 0.1323859840893909,
      "grad_norm": 0.814220666885376,
      "learning_rate": 9.952028248664411e-05,
      "loss": 1.5506,
      "step": 2180
    },
    {
      "epoch": 0.13299325924576424,
      "grad_norm": 0.4612579941749573,
      "learning_rate": 9.95158784465628e-05,
      "loss": 1.5393,
      "step": 2190
    },
    {
      "epoch": 0.13360053440213762,
      "grad_norm": 0.6511384844779968,
      "learning_rate": 9.951145438147162e-05,
      "loss": 1.5416,
      "step": 2200
    },
    {
      "epoch": 0.13420780955851097,
      "grad_norm": 0.40340229868888855,
      "learning_rate": 9.950701029315977e-05,
      "loss": 1.5419,
      "step": 2210
    },
    {
      "epoch": 0.13481508471488432,
      "grad_norm": 0.3209295868873596,
      "learning_rate": 9.950254618342447e-05,
      "loss": 1.5317,
      "step": 2220
    },
    {
      "epoch": 0.13542235987125767,
      "grad_norm": 0.5256797075271606,
      "learning_rate": 9.949806205407111e-05,
      "loss": 1.532,
      "step": 2230
    },
    {
      "epoch": 0.13602963502763102,
      "grad_norm": 0.29455143213272095,
      "learning_rate": 9.949355790691311e-05,
      "loss": 1.5321,
      "step": 2240
    },
    {
      "epoch": 0.13663691018400437,
      "grad_norm": 0.6841529011726379,
      "learning_rate": 9.948903374377205e-05,
      "loss": 1.5308,
      "step": 2250
    },
    {
      "epoch": 0.13724418534037772,
      "grad_norm": 1.6318036317825317,
      "learning_rate": 9.948448956647757e-05,
      "loss": 1.5322,
      "step": 2260
    },
    {
      "epoch": 0.13785146049675107,
      "grad_norm": 0.32057538628578186,
      "learning_rate": 9.947992537686739e-05,
      "loss": 1.5312,
      "step": 2270
    },
    {
      "epoch": 0.13845873565312444,
      "grad_norm": 0.3316732347011566,
      "learning_rate": 9.947534117678735e-05,
      "loss": 1.522,
      "step": 2280
    },
    {
      "epoch": 0.1390660108094978,
      "grad_norm": 0.5923681259155273,
      "learning_rate": 9.947073696809137e-05,
      "loss": 1.5272,
      "step": 2290
    },
    {
      "epoch": 0.13967328596587114,
      "grad_norm": 0.26682209968566895,
      "learning_rate": 9.946611275264148e-05,
      "loss": 1.5186,
      "step": 2300
    },
    {
      "epoch": 0.1402805611222445,
      "grad_norm": 0.5413498282432556,
      "learning_rate": 9.946146853230777e-05,
      "loss": 1.5175,
      "step": 2310
    },
    {
      "epoch": 0.14088783627861784,
      "grad_norm": 0.45962652564048767,
      "learning_rate": 9.945680430896844e-05,
      "loss": 1.5276,
      "step": 2320
    },
    {
      "epoch": 0.1414951114349912,
      "grad_norm": 1.200325608253479,
      "learning_rate": 9.94521200845098e-05,
      "loss": 1.53,
      "step": 2330
    },
    {
      "epoch": 0.14210238659136454,
      "grad_norm": 0.3627772927284241,
      "learning_rate": 9.944741586082617e-05,
      "loss": 1.5252,
      "step": 2340
    },
    {
      "epoch": 0.1427096617477379,
      "grad_norm": 0.7417786717414856,
      "learning_rate": 9.944269163982007e-05,
      "loss": 1.5262,
      "step": 2350
    },
    {
      "epoch": 0.14331693690411126,
      "grad_norm": 0.380707323551178,
      "learning_rate": 9.943794742340202e-05,
      "loss": 1.5218,
      "step": 2360
    },
    {
      "epoch": 0.1439242120604846,
      "grad_norm": 0.9357976317405701,
      "learning_rate": 9.943318321349067e-05,
      "loss": 1.5319,
      "step": 2370
    },
    {
      "epoch": 0.14453148721685796,
      "grad_norm": 1.8207910060882568,
      "learning_rate": 9.942839901201272e-05,
      "loss": 1.5249,
      "step": 2380
    },
    {
      "epoch": 0.1451387623732313,
      "grad_norm": 1.627368450164795,
      "learning_rate": 9.9423594820903e-05,
      "loss": 1.5245,
      "step": 2390
    },
    {
      "epoch": 0.14574603752960466,
      "grad_norm": 0.43618181347846985,
      "learning_rate": 9.94187706421044e-05,
      "loss": 1.5236,
      "step": 2400
    },
    {
      "epoch": 0.146353312685978,
      "grad_norm": 0.6252896785736084,
      "learning_rate": 9.941392647756789e-05,
      "loss": 1.5221,
      "step": 2410
    },
    {
      "epoch": 0.14696058784235136,
      "grad_norm": 0.9731622934341431,
      "learning_rate": 9.940906232925251e-05,
      "loss": 1.5249,
      "step": 2420
    },
    {
      "epoch": 0.14756786299872474,
      "grad_norm": 0.43956342339515686,
      "learning_rate": 9.940417819912544e-05,
      "loss": 1.5269,
      "step": 2430
    },
    {
      "epoch": 0.14817513815509809,
      "grad_norm": 0.6231722831726074,
      "learning_rate": 9.939927408916185e-05,
      "loss": 1.5147,
      "step": 2440
    },
    {
      "epoch": 0.14878241331147143,
      "grad_norm": 0.37150782346725464,
      "learning_rate": 9.939435000134509e-05,
      "loss": 1.5147,
      "step": 2450
    },
    {
      "epoch": 0.14938968846784478,
      "grad_norm": 0.23954340815544128,
      "learning_rate": 9.93894059376665e-05,
      "loss": 1.5086,
      "step": 2460
    },
    {
      "epoch": 0.14999696362421813,
      "grad_norm": 0.5037174820899963,
      "learning_rate": 9.938444190012556e-05,
      "loss": 1.4997,
      "step": 2470
    },
    {
      "epoch": 0.15060423878059148,
      "grad_norm": 1.3787816762924194,
      "learning_rate": 9.93794578907298e-05,
      "loss": 1.5063,
      "step": 2480
    },
    {
      "epoch": 0.15121151393696483,
      "grad_norm": 1.1705409288406372,
      "learning_rate": 9.937445391149483e-05,
      "loss": 1.5239,
      "step": 2490
    },
    {
      "epoch": 0.15181878909333818,
      "grad_norm": 0.8342539668083191,
      "learning_rate": 9.936942996444434e-05,
      "loss": 1.5113,
      "step": 2500
    },
    {
      "epoch": 0.15242606424971156,
      "grad_norm": 1.370552897453308,
      "learning_rate": 9.936438605161009e-05,
      "loss": 1.5145,
      "step": 2510
    },
    {
      "epoch": 0.1530333394060849,
      "grad_norm": 0.26253047585487366,
      "learning_rate": 9.935932217503193e-05,
      "loss": 1.5168,
      "step": 2520
    },
    {
      "epoch": 0.15364061456245826,
      "grad_norm": 0.283877432346344,
      "learning_rate": 9.935423833675777e-05,
      "loss": 1.5107,
      "step": 2530
    },
    {
      "epoch": 0.1542478897188316,
      "grad_norm": 0.44528377056121826,
      "learning_rate": 9.934913453884358e-05,
      "loss": 1.5073,
      "step": 2540
    },
    {
      "epoch": 0.15485516487520495,
      "grad_norm": 0.3187698721885681,
      "learning_rate": 9.934401078335342e-05,
      "loss": 1.5018,
      "step": 2550
    },
    {
      "epoch": 0.1554624400315783,
      "grad_norm": 1.235939621925354,
      "learning_rate": 9.933886707235946e-05,
      "loss": 1.5022,
      "step": 2560
    },
    {
      "epoch": 0.15606971518795165,
      "grad_norm": 0.4086579382419586,
      "learning_rate": 9.933370340794183e-05,
      "loss": 1.511,
      "step": 2570
    },
    {
      "epoch": 0.156676990344325,
      "grad_norm": 0.21619322896003723,
      "learning_rate": 9.932851979218886e-05,
      "loss": 1.5114,
      "step": 2580
    },
    {
      "epoch": 0.15728426550069838,
      "grad_norm": 0.3778277635574341,
      "learning_rate": 9.932331622719685e-05,
      "loss": 1.511,
      "step": 2590
    },
    {
      "epoch": 0.15789154065707173,
      "grad_norm": 0.6113198399543762,
      "learning_rate": 9.931809271507023e-05,
      "loss": 1.5082,
      "step": 2600
    },
    {
      "epoch": 0.15849881581344508,
      "grad_norm": 0.19352899491786957,
      "learning_rate": 9.931284925792142e-05,
      "loss": 1.5058,
      "step": 2610
    },
    {
      "epoch": 0.15910609096981843,
      "grad_norm": 0.41881686449050903,
      "learning_rate": 9.930758585787102e-05,
      "loss": 1.5089,
      "step": 2620
    },
    {
      "epoch": 0.15971336612619177,
      "grad_norm": 0.7099284529685974,
      "learning_rate": 9.930230251704759e-05,
      "loss": 1.5066,
      "step": 2630
    },
    {
      "epoch": 0.16032064128256512,
      "grad_norm": 0.15529599785804749,
      "learning_rate": 9.929699923758783e-05,
      "loss": 1.5015,
      "step": 2640
    },
    {
      "epoch": 0.16092791643893847,
      "grad_norm": 0.6764611005783081,
      "learning_rate": 9.929167602163647e-05,
      "loss": 1.5005,
      "step": 2650
    },
    {
      "epoch": 0.16153519159531182,
      "grad_norm": 0.6013004779815674,
      "learning_rate": 9.928633287134626e-05,
      "loss": 1.5126,
      "step": 2660
    },
    {
      "epoch": 0.1621424667516852,
      "grad_norm": 0.7767142653465271,
      "learning_rate": 9.928096978887809e-05,
      "loss": 1.5109,
      "step": 2670
    },
    {
      "epoch": 0.16274974190805855,
      "grad_norm": 0.35201215744018555,
      "learning_rate": 9.927558677640088e-05,
      "loss": 1.5091,
      "step": 2680
    },
    {
      "epoch": 0.1633570170644319,
      "grad_norm": 0.3277357220649719,
      "learning_rate": 9.927018383609158e-05,
      "loss": 1.5048,
      "step": 2690
    },
    {
      "epoch": 0.16396429222080525,
      "grad_norm": 0.5561208128929138,
      "learning_rate": 9.926476097013524e-05,
      "loss": 1.5089,
      "step": 2700
    },
    {
      "epoch": 0.1645715673771786,
      "grad_norm": 0.3512042760848999,
      "learning_rate": 9.925931818072496e-05,
      "loss": 1.5029,
      "step": 2710
    },
    {
      "epoch": 0.16517884253355195,
      "grad_norm": 0.3351866900920868,
      "learning_rate": 9.925385547006189e-05,
      "loss": 1.5119,
      "step": 2720
    },
    {
      "epoch": 0.1657861176899253,
      "grad_norm": 0.41767722368240356,
      "learning_rate": 9.924837284035522e-05,
      "loss": 1.5071,
      "step": 2730
    },
    {
      "epoch": 0.16639339284629867,
      "grad_norm": 0.28144004940986633,
      "learning_rate": 9.924287029382223e-05,
      "loss": 1.5079,
      "step": 2740
    },
    {
      "epoch": 0.16700066800267202,
      "grad_norm": 0.627920389175415,
      "learning_rate": 9.923734783268823e-05,
      "loss": 1.5016,
      "step": 2750
    },
    {
      "epoch": 0.16760794315904537,
      "grad_norm": 1.5077235698699951,
      "learning_rate": 9.92318054591866e-05,
      "loss": 1.5048,
      "step": 2760
    },
    {
      "epoch": 0.16821521831541872,
      "grad_norm": 0.5815137028694153,
      "learning_rate": 9.922624317555874e-05,
      "loss": 1.4998,
      "step": 2770
    },
    {
      "epoch": 0.16882249347179207,
      "grad_norm": 0.38842812180519104,
      "learning_rate": 9.922066098405415e-05,
      "loss": 1.4981,
      "step": 2780
    },
    {
      "epoch": 0.16942976862816542,
      "grad_norm": 0.2217872142791748,
      "learning_rate": 9.921505888693036e-05,
      "loss": 1.4934,
      "step": 2790
    },
    {
      "epoch": 0.17003704378453877,
      "grad_norm": 0.23278041183948517,
      "learning_rate": 9.920943688645292e-05,
      "loss": 1.4961,
      "step": 2800
    },
    {
      "epoch": 0.17064431894091212,
      "grad_norm": 0.22068563103675842,
      "learning_rate": 9.920379498489549e-05,
      "loss": 1.4963,
      "step": 2810
    },
    {
      "epoch": 0.1712515940972855,
      "grad_norm": 0.23424416780471802,
      "learning_rate": 9.919813318453973e-05,
      "loss": 1.4982,
      "step": 2820
    },
    {
      "epoch": 0.17185886925365884,
      "grad_norm": 0.7808988094329834,
      "learning_rate": 9.919245148767535e-05,
      "loss": 1.5056,
      "step": 2830
    },
    {
      "epoch": 0.1724661444100322,
      "grad_norm": 0.395950585603714,
      "learning_rate": 9.918674989660013e-05,
      "loss": 1.5015,
      "step": 2840
    },
    {
      "epoch": 0.17307341956640554,
      "grad_norm": 0.2592198848724365,
      "learning_rate": 9.91810284136199e-05,
      "loss": 1.5103,
      "step": 2850
    },
    {
      "epoch": 0.1736806947227789,
      "grad_norm": 0.40802955627441406,
      "learning_rate": 9.917528704104848e-05,
      "loss": 1.4984,
      "step": 2860
    },
    {
      "epoch": 0.17428796987915224,
      "grad_norm": 0.8036662340164185,
      "learning_rate": 9.916952578120781e-05,
      "loss": 1.4979,
      "step": 2870
    },
    {
      "epoch": 0.1748952450355256,
      "grad_norm": 0.37362226843833923,
      "learning_rate": 9.916374463642781e-05,
      "loss": 1.4936,
      "step": 2880
    },
    {
      "epoch": 0.17550252019189894,
      "grad_norm": 0.27717021107673645,
      "learning_rate": 9.915794360904649e-05,
      "loss": 1.4927,
      "step": 2890
    },
    {
      "epoch": 0.1761097953482723,
      "grad_norm": 0.39600786566734314,
      "learning_rate": 9.915212270140985e-05,
      "loss": 1.4937,
      "step": 2900
    },
    {
      "epoch": 0.17671707050464566,
      "grad_norm": 0.300689160823822,
      "learning_rate": 9.914628191587198e-05,
      "loss": 1.4927,
      "step": 2910
    },
    {
      "epoch": 0.177324345661019,
      "grad_norm": 0.17525920271873474,
      "learning_rate": 9.914042125479499e-05,
      "loss": 1.4895,
      "step": 2920
    },
    {
      "epoch": 0.17793162081739236,
      "grad_norm": 0.18415267765522003,
      "learning_rate": 9.913454072054901e-05,
      "loss": 1.4885,
      "step": 2930
    },
    {
      "epoch": 0.1785388959737657,
      "grad_norm": 0.6952281594276428,
      "learning_rate": 9.912864031551222e-05,
      "loss": 1.4902,
      "step": 2940
    },
    {
      "epoch": 0.17914617113013906,
      "grad_norm": 0.960731565952301,
      "learning_rate": 9.912272004207085e-05,
      "loss": 1.4934,
      "step": 2950
    },
    {
      "epoch": 0.1797534462865124,
      "grad_norm": 0.4714548885822296,
      "learning_rate": 9.911677990261913e-05,
      "loss": 1.5005,
      "step": 2960
    },
    {
      "epoch": 0.18036072144288579,
      "grad_norm": 0.31599169969558716,
      "learning_rate": 9.91108198995594e-05,
      "loss": 1.494,
      "step": 2970
    },
    {
      "epoch": 0.18096799659925913,
      "grad_norm": 0.2875349819660187,
      "learning_rate": 9.910484003530192e-05,
      "loss": 1.4897,
      "step": 2980
    },
    {
      "epoch": 0.18157527175563248,
      "grad_norm": 0.25551894307136536,
      "learning_rate": 9.909884031226506e-05,
      "loss": 1.4857,
      "step": 2990
    },
    {
      "epoch": 0.18218254691200583,
      "grad_norm": 0.14835983514785767,
      "learning_rate": 9.909282073287522e-05,
      "loss": 1.4895,
      "step": 3000
    },
    {
      "epoch": 0.18278982206837918,
      "grad_norm": 0.20497334003448486,
      "learning_rate": 9.908678129956681e-05,
      "loss": 1.4899,
      "step": 3010
    },
    {
      "epoch": 0.18339709722475253,
      "grad_norm": 0.14935827255249023,
      "learning_rate": 9.908072201478225e-05,
      "loss": 1.4922,
      "step": 3020
    },
    {
      "epoch": 0.18400437238112588,
      "grad_norm": 0.9599311947822571,
      "learning_rate": 9.907464288097203e-05,
      "loss": 1.4955,
      "step": 3030
    },
    {
      "epoch": 0.18461164753749923,
      "grad_norm": 0.42684656381607056,
      "learning_rate": 9.906854390059467e-05,
      "loss": 1.4919,
      "step": 3040
    },
    {
      "epoch": 0.1852189226938726,
      "grad_norm": 0.2169746309518814,
      "learning_rate": 9.906242507611665e-05,
      "loss": 1.4929,
      "step": 3050
    },
    {
      "epoch": 0.18582619785024596,
      "grad_norm": 0.5408350229263306,
      "learning_rate": 9.905628641001255e-05,
      "loss": 1.4916,
      "step": 3060
    },
    {
      "epoch": 0.1864334730066193,
      "grad_norm": 1.322006344795227,
      "learning_rate": 9.905012790476493e-05,
      "loss": 1.4934,
      "step": 3070
    },
    {
      "epoch": 0.18704074816299265,
      "grad_norm": 0.20567694306373596,
      "learning_rate": 9.904394956286441e-05,
      "loss": 1.492,
      "step": 3080
    },
    {
      "epoch": 0.187648023319366,
      "grad_norm": 0.502598226070404,
      "learning_rate": 9.903775138680956e-05,
      "loss": 1.4963,
      "step": 3090
    },
    {
      "epoch": 0.18825529847573935,
      "grad_norm": 0.22949816286563873,
      "learning_rate": 9.903153337910708e-05,
      "loss": 1.4943,
      "step": 3100
    },
    {
      "epoch": 0.1888625736321127,
      "grad_norm": 0.7970278859138489,
      "learning_rate": 9.90252955422716e-05,
      "loss": 1.493,
      "step": 3110
    },
    {
      "epoch": 0.18946984878848605,
      "grad_norm": 0.7937749624252319,
      "learning_rate": 9.90190378788258e-05,
      "loss": 1.4914,
      "step": 3120
    },
    {
      "epoch": 0.19007712394485943,
      "grad_norm": 0.9797595739364624,
      "learning_rate": 9.901276039130039e-05,
      "loss": 1.506,
      "step": 3130
    },
    {
      "epoch": 0.19068439910123278,
      "grad_norm": 0.6101270318031311,
      "learning_rate": 9.900646308223408e-05,
      "loss": 1.4951,
      "step": 3140
    },
    {
      "epoch": 0.19129167425760613,
      "grad_norm": 0.1716468334197998,
      "learning_rate": 9.90001459541736e-05,
      "loss": 1.4909,
      "step": 3150
    },
    {
      "epoch": 0.19189894941397947,
      "grad_norm": 0.2998487949371338,
      "learning_rate": 9.899380900967371e-05,
      "loss": 1.4927,
      "step": 3160
    },
    {
      "epoch": 0.19250622457035282,
      "grad_norm": 0.4134252071380615,
      "learning_rate": 9.898745225129715e-05,
      "loss": 1.4892,
      "step": 3170
    },
    {
      "epoch": 0.19311349972672617,
      "grad_norm": 0.6891342997550964,
      "learning_rate": 9.898107568161472e-05,
      "loss": 1.4998,
      "step": 3180
    },
    {
      "epoch": 0.19372077488309952,
      "grad_norm": 2.7527339458465576,
      "learning_rate": 9.897467930320519e-05,
      "loss": 1.5021,
      "step": 3190
    },
    {
      "epoch": 0.19432805003947287,
      "grad_norm": 0.44325828552246094,
      "learning_rate": 9.896826311865534e-05,
      "loss": 1.5013,
      "step": 3200
    },
    {
      "epoch": 0.19493532519584625,
      "grad_norm": 0.33685916662216187,
      "learning_rate": 9.896182713056001e-05,
      "loss": 1.4962,
      "step": 3210
    },
    {
      "epoch": 0.1955426003522196,
      "grad_norm": 0.17095725238323212,
      "learning_rate": 9.8955371341522e-05,
      "loss": 1.4898,
      "step": 3220
    },
    {
      "epoch": 0.19614987550859295,
      "grad_norm": 0.2127601057291031,
      "learning_rate": 9.894889575415214e-05,
      "loss": 1.4893,
      "step": 3230
    },
    {
      "epoch": 0.1967571506649663,
      "grad_norm": 0.38042157888412476,
      "learning_rate": 9.894240037106927e-05,
      "loss": 1.493,
      "step": 3240
    },
    {
      "epoch": 0.19736442582133965,
      "grad_norm": 0.21604736149311066,
      "learning_rate": 9.89358851949002e-05,
      "loss": 1.4958,
      "step": 3250
    },
    {
      "epoch": 0.197971700977713,
      "grad_norm": 0.37819549441337585,
      "learning_rate": 9.892935022827978e-05,
      "loss": 1.4896,
      "step": 3260
    },
    {
      "epoch": 0.19857897613408634,
      "grad_norm": 0.4886377453804016,
      "learning_rate": 9.892279547385087e-05,
      "loss": 1.4894,
      "step": 3270
    },
    {
      "epoch": 0.19918625129045972,
      "grad_norm": 0.6503462791442871,
      "learning_rate": 9.891622093426429e-05,
      "loss": 1.4848,
      "step": 3280
    },
    {
      "epoch": 0.19979352644683307,
      "grad_norm": 0.264244019985199,
      "learning_rate": 9.890962661217892e-05,
      "loss": 1.4861,
      "step": 3290
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 0.19342026114463806,
      "learning_rate": 9.89030125102616e-05,
      "loss": 1.479,
      "step": 3300
    },
    {
      "epoch": 0.20100807675957977,
      "grad_norm": 0.24959543347358704,
      "learning_rate": 9.889637863118715e-05,
      "loss": 1.4896,
      "step": 3310
    },
    {
      "epoch": 0.20161535191595312,
      "grad_norm": 0.8692912459373474,
      "learning_rate": 9.888972497763844e-05,
      "loss": 1.4846,
      "step": 3320
    },
    {
      "epoch": 0.20222262707232647,
      "grad_norm": 0.124610036611557,
      "learning_rate": 9.888305155230632e-05,
      "loss": 1.4825,
      "step": 3330
    },
    {
      "epoch": 0.20282990222869982,
      "grad_norm": 0.33885836601257324,
      "learning_rate": 9.887635835788965e-05,
      "loss": 1.4824,
      "step": 3340
    },
    {
      "epoch": 0.20343717738507316,
      "grad_norm": 0.2192951738834381,
      "learning_rate": 9.886964539709519e-05,
      "loss": 1.4943,
      "step": 3350
    },
    {
      "epoch": 0.20404445254144654,
      "grad_norm": 0.13215480744838715,
      "learning_rate": 9.886291267263783e-05,
      "loss": 1.4876,
      "step": 3360
    },
    {
      "epoch": 0.2046517276978199,
      "grad_norm": 0.20339474081993103,
      "learning_rate": 9.885616018724037e-05,
      "loss": 1.4884,
      "step": 3370
    },
    {
      "epoch": 0.20525900285419324,
      "grad_norm": 0.2408524751663208,
      "learning_rate": 9.884938794363365e-05,
      "loss": 1.4872,
      "step": 3380
    },
    {
      "epoch": 0.2058662780105666,
      "grad_norm": 0.40413859486579895,
      "learning_rate": 9.884259594455643e-05,
      "loss": 1.4927,
      "step": 3390
    },
    {
      "epoch": 0.20647355316693994,
      "grad_norm": 0.7467237114906311,
      "learning_rate": 9.883578419275553e-05,
      "loss": 1.4909,
      "step": 3400
    },
    {
      "epoch": 0.2070808283233133,
      "grad_norm": 0.32259631156921387,
      "learning_rate": 9.882895269098572e-05,
      "loss": 1.4905,
      "step": 3410
    },
    {
      "epoch": 0.20768810347968664,
      "grad_norm": 0.5758771300315857,
      "learning_rate": 9.882210144200979e-05,
      "loss": 1.4871,
      "step": 3420
    },
    {
      "epoch": 0.20829537863605999,
      "grad_norm": 0.6275844573974609,
      "learning_rate": 9.881523044859844e-05,
      "loss": 1.4875,
      "step": 3430
    },
    {
      "epoch": 0.20890265379243336,
      "grad_norm": 0.18164034187793732,
      "learning_rate": 9.880833971353048e-05,
      "loss": 1.4862,
      "step": 3440
    },
    {
      "epoch": 0.2095099289488067,
      "grad_norm": 0.21054258942604065,
      "learning_rate": 9.880142923959258e-05,
      "loss": 1.4796,
      "step": 3450
    },
    {
      "epoch": 0.21011720410518006,
      "grad_norm": 0.3602127134799957,
      "learning_rate": 9.879449902957946e-05,
      "loss": 1.4801,
      "step": 3460
    },
    {
      "epoch": 0.2107244792615534,
      "grad_norm": 0.3125581443309784,
      "learning_rate": 9.878754908629384e-05,
      "loss": 1.4876,
      "step": 3470
    },
    {
      "epoch": 0.21133175441792676,
      "grad_norm": 0.39210858941078186,
      "learning_rate": 9.878057941254634e-05,
      "loss": 1.4895,
      "step": 3480
    },
    {
      "epoch": 0.2119390295743001,
      "grad_norm": 0.35661184787750244,
      "learning_rate": 9.877359001115563e-05,
      "loss": 1.4879,
      "step": 3490
    },
    {
      "epoch": 0.21254630473067346,
      "grad_norm": 0.38334518671035767,
      "learning_rate": 9.876658088494832e-05,
      "loss": 1.486,
      "step": 3500
    },
    {
      "epoch": 0.21315357988704683,
      "grad_norm": 0.4869404733181,
      "learning_rate": 9.875955203675905e-05,
      "loss": 1.4832,
      "step": 3510
    },
    {
      "epoch": 0.21376085504342018,
      "grad_norm": 0.1704762578010559,
      "learning_rate": 9.875250346943035e-05,
      "loss": 1.4851,
      "step": 3520
    },
    {
      "epoch": 0.21436813019979353,
      "grad_norm": 0.43974336981773376,
      "learning_rate": 9.874543518581279e-05,
      "loss": 1.4844,
      "step": 3530
    },
    {
      "epoch": 0.21497540535616688,
      "grad_norm": 0.4685124456882477,
      "learning_rate": 9.873834718876491e-05,
      "loss": 1.4879,
      "step": 3540
    },
    {
      "epoch": 0.21558268051254023,
      "grad_norm": 0.3651885688304901,
      "learning_rate": 9.873123948115321e-05,
      "loss": 1.4822,
      "step": 3550
    },
    {
      "epoch": 0.21618995566891358,
      "grad_norm": 0.19292744994163513,
      "learning_rate": 9.872411206585215e-05,
      "loss": 1.4809,
      "step": 3560
    },
    {
      "epoch": 0.21679723082528693,
      "grad_norm": 1.3711438179016113,
      "learning_rate": 9.871696494574417e-05,
      "loss": 1.4829,
      "step": 3570
    },
    {
      "epoch": 0.21740450598166028,
      "grad_norm": 0.3871600925922394,
      "learning_rate": 9.870979812371967e-05,
      "loss": 1.4854,
      "step": 3580
    },
    {
      "epoch": 0.21801178113803366,
      "grad_norm": 0.2094951719045639,
      "learning_rate": 9.870261160267704e-05,
      "loss": 1.4847,
      "step": 3590
    },
    {
      "epoch": 0.218619056294407,
      "grad_norm": 1.1221791505813599,
      "learning_rate": 9.869540538552263e-05,
      "loss": 1.5004,
      "step": 3600
    },
    {
      "epoch": 0.21922633145078035,
      "grad_norm": 0.8725524544715881,
      "learning_rate": 9.868817947517073e-05,
      "loss": 1.496,
      "step": 3610
    },
    {
      "epoch": 0.2198336066071537,
      "grad_norm": 0.21911974251270294,
      "learning_rate": 9.868093387454362e-05,
      "loss": 1.4892,
      "step": 3620
    },
    {
      "epoch": 0.22044088176352705,
      "grad_norm": 0.23615364730358124,
      "learning_rate": 9.867366858657155e-05,
      "loss": 1.4834,
      "step": 3630
    },
    {
      "epoch": 0.2210481569199004,
      "grad_norm": 0.33390048146247864,
      "learning_rate": 9.866638361419269e-05,
      "loss": 1.4837,
      "step": 3640
    },
    {
      "epoch": 0.22165543207627375,
      "grad_norm": 0.6263498067855835,
      "learning_rate": 9.865907896035324e-05,
      "loss": 1.4825,
      "step": 3650
    },
    {
      "epoch": 0.2222627072326471,
      "grad_norm": 0.2172243744134903,
      "learning_rate": 9.865175462800727e-05,
      "loss": 1.486,
      "step": 3660
    },
    {
      "epoch": 0.22286998238902048,
      "grad_norm": 0.13698874413967133,
      "learning_rate": 9.86444106201169e-05,
      "loss": 1.4822,
      "step": 3670
    },
    {
      "epoch": 0.22347725754539383,
      "grad_norm": 0.2987975478172302,
      "learning_rate": 9.863704693965214e-05,
      "loss": 1.4822,
      "step": 3680
    },
    {
      "epoch": 0.22408453270176718,
      "grad_norm": 0.36181411147117615,
      "learning_rate": 9.862966358959099e-05,
      "loss": 1.4782,
      "step": 3690
    },
    {
      "epoch": 0.22469180785814052,
      "grad_norm": 0.18219079077243805,
      "learning_rate": 9.862226057291937e-05,
      "loss": 1.4771,
      "step": 3700
    },
    {
      "epoch": 0.22529908301451387,
      "grad_norm": 0.2449469119310379,
      "learning_rate": 9.861483789263122e-05,
      "loss": 1.4789,
      "step": 3710
    },
    {
      "epoch": 0.22590635817088722,
      "grad_norm": 0.5723773837089539,
      "learning_rate": 9.860739555172835e-05,
      "loss": 1.4772,
      "step": 3720
    },
    {
      "epoch": 0.22651363332726057,
      "grad_norm": 0.12639056146144867,
      "learning_rate": 9.859993355322058e-05,
      "loss": 1.4803,
      "step": 3730
    },
    {
      "epoch": 0.22712090848363392,
      "grad_norm": 0.12730658054351807,
      "learning_rate": 9.859245190012566e-05,
      "loss": 1.4761,
      "step": 3740
    },
    {
      "epoch": 0.2277281836400073,
      "grad_norm": 0.1786966323852539,
      "learning_rate": 9.85849505954693e-05,
      "loss": 1.4779,
      "step": 3750
    },
    {
      "epoch": 0.22833545879638065,
      "grad_norm": 0.16773751378059387,
      "learning_rate": 9.857742964228512e-05,
      "loss": 1.4756,
      "step": 3760
    },
    {
      "epoch": 0.228942733952754,
      "grad_norm": 0.6510085463523865,
      "learning_rate": 9.856988904361474e-05,
      "loss": 1.4775,
      "step": 3770
    },
    {
      "epoch": 0.22955000910912735,
      "grad_norm": 0.34743934869766235,
      "learning_rate": 9.856232880250769e-05,
      "loss": 1.4824,
      "step": 3780
    },
    {
      "epoch": 0.2301572842655007,
      "grad_norm": 0.35699108242988586,
      "learning_rate": 9.855474892202144e-05,
      "loss": 1.4845,
      "step": 3790
    },
    {
      "epoch": 0.23076455942187404,
      "grad_norm": 0.8985147476196289,
      "learning_rate": 9.854714940522142e-05,
      "loss": 1.4816,
      "step": 3800
    },
    {
      "epoch": 0.2313718345782474,
      "grad_norm": 0.3393024504184723,
      "learning_rate": 9.853953025518102e-05,
      "loss": 1.4812,
      "step": 3810
    },
    {
      "epoch": 0.23197910973462077,
      "grad_norm": 0.1071815937757492,
      "learning_rate": 9.853189147498149e-05,
      "loss": 1.4788,
      "step": 3820
    },
    {
      "epoch": 0.23258638489099412,
      "grad_norm": 0.34408482909202576,
      "learning_rate": 9.852423306771214e-05,
      "loss": 1.4781,
      "step": 3830
    },
    {
      "epoch": 0.23319366004736747,
      "grad_norm": 0.4034941792488098,
      "learning_rate": 9.85165550364701e-05,
      "loss": 1.4764,
      "step": 3840
    },
    {
      "epoch": 0.23380093520374082,
      "grad_norm": 0.1753886342048645,
      "learning_rate": 9.850885738436053e-05,
      "loss": 1.4758,
      "step": 3850
    },
    {
      "epoch": 0.23440821036011417,
      "grad_norm": 0.1064094677567482,
      "learning_rate": 9.850114011449645e-05,
      "loss": 1.4784,
      "step": 3860
    },
    {
      "epoch": 0.23501548551648752,
      "grad_norm": 0.36612674593925476,
      "learning_rate": 9.849340322999886e-05,
      "loss": 1.474,
      "step": 3870
    },
    {
      "epoch": 0.23562276067286086,
      "grad_norm": 0.20045270025730133,
      "learning_rate": 9.848564673399667e-05,
      "loss": 1.475,
      "step": 3880
    },
    {
      "epoch": 0.23623003582923421,
      "grad_norm": 0.44198381900787354,
      "learning_rate": 9.847787062962675e-05,
      "loss": 1.4772,
      "step": 3890
    },
    {
      "epoch": 0.2368373109856076,
      "grad_norm": 0.6386992931365967,
      "learning_rate": 9.847007492003388e-05,
      "loss": 1.4814,
      "step": 3900
    },
    {
      "epoch": 0.23744458614198094,
      "grad_norm": 0.6091061234474182,
      "learning_rate": 9.846225960837075e-05,
      "loss": 1.4769,
      "step": 3910
    },
    {
      "epoch": 0.2380518612983543,
      "grad_norm": 0.14628371596336365,
      "learning_rate": 9.8454424697798e-05,
      "loss": 1.4743,
      "step": 3920
    },
    {
      "epoch": 0.23865913645472764,
      "grad_norm": 0.36147040128707886,
      "learning_rate": 9.844657019148418e-05,
      "loss": 1.4778,
      "step": 3930
    },
    {
      "epoch": 0.239266411611101,
      "grad_norm": 0.20877858996391296,
      "learning_rate": 9.843869609260583e-05,
      "loss": 1.4773,
      "step": 3940
    },
    {
      "epoch": 0.23987368676747434,
      "grad_norm": 0.12168101966381073,
      "learning_rate": 9.84308024043473e-05,
      "loss": 1.475,
      "step": 3950
    },
    {
      "epoch": 0.24048096192384769,
      "grad_norm": 0.16011252999305725,
      "learning_rate": 9.842288912990096e-05,
      "loss": 1.4764,
      "step": 3960
    },
    {
      "epoch": 0.24108823708022104,
      "grad_norm": 0.45867112278938293,
      "learning_rate": 9.841495627246707e-05,
      "loss": 1.4727,
      "step": 3970
    },
    {
      "epoch": 0.2416955122365944,
      "grad_norm": 0.6034095287322998,
      "learning_rate": 9.840700383525376e-05,
      "loss": 1.4749,
      "step": 3980
    },
    {
      "epoch": 0.24230278739296776,
      "grad_norm": 0.14721664786338806,
      "learning_rate": 9.839903182147717e-05,
      "loss": 1.4763,
      "step": 3990
    },
    {
      "epoch": 0.2429100625493411,
      "grad_norm": 0.35898557305336,
      "learning_rate": 9.839104023436128e-05,
      "loss": 1.4736,
      "step": 4000
    },
    {
      "epoch": 0.24351733770571446,
      "grad_norm": 0.3768307864665985,
      "learning_rate": 9.8383029077138e-05,
      "loss": 1.4788,
      "step": 4010
    },
    {
      "epoch": 0.2441246128620878,
      "grad_norm": 0.9319431781768799,
      "learning_rate": 9.837499835304724e-05,
      "loss": 1.4803,
      "step": 4020
    },
    {
      "epoch": 0.24473188801846116,
      "grad_norm": 0.1216370165348053,
      "learning_rate": 9.836694806533669e-05,
      "loss": 1.4754,
      "step": 4030
    },
    {
      "epoch": 0.2453391631748345,
      "grad_norm": 0.46724292635917664,
      "learning_rate": 9.835887821726202e-05,
      "loss": 1.4717,
      "step": 4040
    },
    {
      "epoch": 0.24594643833120788,
      "grad_norm": 0.1448553502559662,
      "learning_rate": 9.835078881208681e-05,
      "loss": 1.4741,
      "step": 4050
    },
    {
      "epoch": 0.24655371348758123,
      "grad_norm": 0.20433714985847473,
      "learning_rate": 9.834267985308256e-05,
      "loss": 1.4764,
      "step": 4060
    },
    {
      "epoch": 0.24716098864395458,
      "grad_norm": 0.1736728698015213,
      "learning_rate": 9.833455134352866e-05,
      "loss": 1.4746,
      "step": 4070
    },
    {
      "epoch": 0.24776826380032793,
      "grad_norm": 0.19943077862262726,
      "learning_rate": 9.832640328671238e-05,
      "loss": 1.4738,
      "step": 4080
    },
    {
      "epoch": 0.24837553895670128,
      "grad_norm": 0.41766974329948425,
      "learning_rate": 9.831823568592897e-05,
      "loss": 1.4733,
      "step": 4090
    },
    {
      "epoch": 0.24898281411307463,
      "grad_norm": 0.14672663807868958,
      "learning_rate": 9.831004854448152e-05,
      "loss": 1.4739,
      "step": 4100
    },
    {
      "epoch": 0.24959008926944798,
      "grad_norm": 0.2923754155635834,
      "learning_rate": 9.830184186568101e-05,
      "loss": 1.4784,
      "step": 4110
    },
    {
      "epoch": 0.25019736442582136,
      "grad_norm": 0.7980467677116394,
      "learning_rate": 9.829361565284639e-05,
      "loss": 1.477,
      "step": 4120
    },
    {
      "epoch": 0.2508046395821947,
      "grad_norm": 0.8107094764709473,
      "learning_rate": 9.828536990930444e-05,
      "loss": 1.477,
      "step": 4130
    },
    {
      "epoch": 0.25141191473856805,
      "grad_norm": 1.0386661291122437,
      "learning_rate": 9.82771046383899e-05,
      "loss": 1.4783,
      "step": 4140
    },
    {
      "epoch": 0.2520191898949414,
      "grad_norm": 0.16204795241355896,
      "learning_rate": 9.826881984344536e-05,
      "loss": 1.479,
      "step": 4150
    },
    {
      "epoch": 0.25262646505131475,
      "grad_norm": 0.19675131142139435,
      "learning_rate": 9.826051552782132e-05,
      "loss": 1.4769,
      "step": 4160
    },
    {
      "epoch": 0.25323374020768813,
      "grad_norm": 0.4863335192203522,
      "learning_rate": 9.825219169487618e-05,
      "loss": 1.4738,
      "step": 4170
    },
    {
      "epoch": 0.25384101536406145,
      "grad_norm": 0.357919305562973,
      "learning_rate": 9.824384834797625e-05,
      "loss": 1.477,
      "step": 4180
    },
    {
      "epoch": 0.2544482905204348,
      "grad_norm": 1.043136715888977,
      "learning_rate": 9.823548549049569e-05,
      "loss": 1.4776,
      "step": 4190
    },
    {
      "epoch": 0.25505556567680815,
      "grad_norm": 0.6630415320396423,
      "learning_rate": 9.82271031258166e-05,
      "loss": 1.4787,
      "step": 4200
    },
    {
      "epoch": 0.2556628408331815,
      "grad_norm": 0.1833849549293518,
      "learning_rate": 9.82187012573289e-05,
      "loss": 1.4758,
      "step": 4210
    },
    {
      "epoch": 0.25627011598955485,
      "grad_norm": 0.2767007350921631,
      "learning_rate": 9.821027988843046e-05,
      "loss": 1.479,
      "step": 4220
    },
    {
      "epoch": 0.2568773911459282,
      "grad_norm": 0.15324276685714722,
      "learning_rate": 9.820183902252702e-05,
      "loss": 1.4718,
      "step": 4230
    },
    {
      "epoch": 0.25748466630230155,
      "grad_norm": 0.15861764550209045,
      "learning_rate": 9.81933786630322e-05,
      "loss": 1.4721,
      "step": 4240
    },
    {
      "epoch": 0.2580919414586749,
      "grad_norm": 0.14615561068058014,
      "learning_rate": 9.81848988133675e-05,
      "loss": 1.4727,
      "step": 4250
    },
    {
      "epoch": 0.2586992166150483,
      "grad_norm": 0.7122476100921631,
      "learning_rate": 9.817639947696231e-05,
      "loss": 1.472,
      "step": 4260
    },
    {
      "epoch": 0.2593064917714216,
      "grad_norm": 0.19409002363681793,
      "learning_rate": 9.816788065725389e-05,
      "loss": 1.4765,
      "step": 4270
    },
    {
      "epoch": 0.259913766927795,
      "grad_norm": 0.4614657461643219,
      "learning_rate": 9.81593423576874e-05,
      "loss": 1.477,
      "step": 4280
    },
    {
      "epoch": 0.2605210420841683,
      "grad_norm": 0.2477182298898697,
      "learning_rate": 9.815078458171585e-05,
      "loss": 1.4773,
      "step": 4290
    },
    {
      "epoch": 0.2611283172405417,
      "grad_norm": 0.11216845363378525,
      "learning_rate": 9.814220733280015e-05,
      "loss": 1.4708,
      "step": 4300
    },
    {
      "epoch": 0.261735592396915,
      "grad_norm": 0.15539830923080444,
      "learning_rate": 9.813361061440907e-05,
      "loss": 1.4732,
      "step": 4310
    },
    {
      "epoch": 0.2623428675532884,
      "grad_norm": 0.20518863201141357,
      "learning_rate": 9.812499443001926e-05,
      "loss": 1.471,
      "step": 4320
    },
    {
      "epoch": 0.26295014270966177,
      "grad_norm": 0.46541017293930054,
      "learning_rate": 9.811635878311524e-05,
      "loss": 1.4727,
      "step": 4330
    },
    {
      "epoch": 0.2635574178660351,
      "grad_norm": 0.25717630982398987,
      "learning_rate": 9.810770367718942e-05,
      "loss": 1.4739,
      "step": 4340
    },
    {
      "epoch": 0.26416469302240847,
      "grad_norm": 0.4192458689212799,
      "learning_rate": 9.809902911574204e-05,
      "loss": 1.4716,
      "step": 4350
    },
    {
      "epoch": 0.2647719681787818,
      "grad_norm": 0.6545537114143372,
      "learning_rate": 9.809033510228126e-05,
      "loss": 1.4766,
      "step": 4360
    },
    {
      "epoch": 0.26537924333515517,
      "grad_norm": 0.1778479814529419,
      "learning_rate": 9.808162164032304e-05,
      "loss": 1.4787,
      "step": 4370
    },
    {
      "epoch": 0.2659865184915285,
      "grad_norm": 0.17698591947555542,
      "learning_rate": 9.807288873339126e-05,
      "loss": 1.4719,
      "step": 4380
    },
    {
      "epoch": 0.26659379364790187,
      "grad_norm": 0.21979044377803802,
      "learning_rate": 9.806413638501766e-05,
      "loss": 1.4712,
      "step": 4390
    },
    {
      "epoch": 0.26720106880427524,
      "grad_norm": 0.4853202700614929,
      "learning_rate": 9.805536459874182e-05,
      "loss": 1.4715,
      "step": 4400
    },
    {
      "epoch": 0.26780834396064856,
      "grad_norm": 0.09129375219345093,
      "learning_rate": 9.804657337811119e-05,
      "loss": 1.4684,
      "step": 4410
    },
    {
      "epoch": 0.26841561911702194,
      "grad_norm": 0.07913380861282349,
      "learning_rate": 9.803776272668106e-05,
      "loss": 1.4713,
      "step": 4420
    },
    {
      "epoch": 0.26902289427339526,
      "grad_norm": 0.12933886051177979,
      "learning_rate": 9.802893264801462e-05,
      "loss": 1.4679,
      "step": 4430
    },
    {
      "epoch": 0.26963016942976864,
      "grad_norm": 0.4591981768608093,
      "learning_rate": 9.80200831456829e-05,
      "loss": 1.4696,
      "step": 4440
    },
    {
      "epoch": 0.27023744458614196,
      "grad_norm": 0.2045873999595642,
      "learning_rate": 9.801121422326475e-05,
      "loss": 1.4734,
      "step": 4450
    },
    {
      "epoch": 0.27084471974251534,
      "grad_norm": 0.4337940812110901,
      "learning_rate": 9.800232588434692e-05,
      "loss": 1.4779,
      "step": 4460
    },
    {
      "epoch": 0.27145199489888866,
      "grad_norm": 0.2012259066104889,
      "learning_rate": 9.799341813252402e-05,
      "loss": 1.4753,
      "step": 4470
    },
    {
      "epoch": 0.27205927005526204,
      "grad_norm": 0.2445773482322693,
      "learning_rate": 9.798449097139845e-05,
      "loss": 1.477,
      "step": 4480
    },
    {
      "epoch": 0.2726665452116354,
      "grad_norm": 0.6886539459228516,
      "learning_rate": 9.797554440458052e-05,
      "loss": 1.4747,
      "step": 4490
    },
    {
      "epoch": 0.27327382036800874,
      "grad_norm": 0.1874409168958664,
      "learning_rate": 9.796657843568835e-05,
      "loss": 1.4714,
      "step": 4500
    },
    {
      "epoch": 0.2738810955243821,
      "grad_norm": 0.3703071177005768,
      "learning_rate": 9.795759306834793e-05,
      "loss": 1.473,
      "step": 4510
    },
    {
      "epoch": 0.27448837068075543,
      "grad_norm": 0.7026022672653198,
      "learning_rate": 9.794858830619307e-05,
      "loss": 1.4716,
      "step": 4520
    },
    {
      "epoch": 0.2750956458371288,
      "grad_norm": 0.5306034684181213,
      "learning_rate": 9.793956415286545e-05,
      "loss": 1.472,
      "step": 4530
    },
    {
      "epoch": 0.27570292099350213,
      "grad_norm": 0.3780011534690857,
      "learning_rate": 9.79305206120146e-05,
      "loss": 1.4736,
      "step": 4540
    },
    {
      "epoch": 0.2763101961498755,
      "grad_norm": 0.09543360769748688,
      "learning_rate": 9.792145768729785e-05,
      "loss": 1.4709,
      "step": 4550
    },
    {
      "epoch": 0.2769174713062489,
      "grad_norm": 0.4021846652030945,
      "learning_rate": 9.791237538238038e-05,
      "loss": 1.4704,
      "step": 4560
    },
    {
      "epoch": 0.2775247464626222,
      "grad_norm": 0.6078127026557922,
      "learning_rate": 9.790327370093525e-05,
      "loss": 1.4706,
      "step": 4570
    },
    {
      "epoch": 0.2781320216189956,
      "grad_norm": 0.3112201690673828,
      "learning_rate": 9.78941526466433e-05,
      "loss": 1.4692,
      "step": 4580
    },
    {
      "epoch": 0.2787392967753689,
      "grad_norm": 0.23154406249523163,
      "learning_rate": 9.788501222319324e-05,
      "loss": 1.4683,
      "step": 4590
    },
    {
      "epoch": 0.2793465719317423,
      "grad_norm": 0.6422960758209229,
      "learning_rate": 9.78758524342816e-05,
      "loss": 1.472,
      "step": 4600
    },
    {
      "epoch": 0.2799538470881156,
      "grad_norm": 0.1669650673866272,
      "learning_rate": 9.786667328361274e-05,
      "loss": 1.4721,
      "step": 4610
    },
    {
      "epoch": 0.280561122244489,
      "grad_norm": 0.5995563864707947,
      "learning_rate": 9.785747477489887e-05,
      "loss": 1.4714,
      "step": 4620
    },
    {
      "epoch": 0.28116839740086236,
      "grad_norm": 0.3238060176372528,
      "learning_rate": 9.784825691186e-05,
      "loss": 1.4704,
      "step": 4630
    },
    {
      "epoch": 0.2817756725572357,
      "grad_norm": 0.19492845237255096,
      "learning_rate": 9.783901969822399e-05,
      "loss": 1.4696,
      "step": 4640
    },
    {
      "epoch": 0.28238294771360906,
      "grad_norm": 0.21959063410758972,
      "learning_rate": 9.78297631377265e-05,
      "loss": 1.4682,
      "step": 4650
    },
    {
      "epoch": 0.2829902228699824,
      "grad_norm": 0.3861495852470398,
      "learning_rate": 9.782048723411106e-05,
      "loss": 1.4686,
      "step": 4660
    },
    {
      "epoch": 0.28359749802635575,
      "grad_norm": 0.2855699062347412,
      "learning_rate": 9.781119199112896e-05,
      "loss": 1.4698,
      "step": 4670
    },
    {
      "epoch": 0.2842047731827291,
      "grad_norm": 0.26596105098724365,
      "learning_rate": 9.780187741253935e-05,
      "loss": 1.4679,
      "step": 4680
    },
    {
      "epoch": 0.28481204833910245,
      "grad_norm": 0.3322386145591736,
      "learning_rate": 9.779254350210922e-05,
      "loss": 1.4677,
      "step": 4690
    },
    {
      "epoch": 0.2854193234954758,
      "grad_norm": 1.0565403699874878,
      "learning_rate": 9.778319026361332e-05,
      "loss": 1.4718,
      "step": 4700
    },
    {
      "epoch": 0.28602659865184915,
      "grad_norm": 0.7230435609817505,
      "learning_rate": 9.777381770083426e-05,
      "loss": 1.4725,
      "step": 4710
    },
    {
      "epoch": 0.28663387380822253,
      "grad_norm": 0.2733442783355713,
      "learning_rate": 9.776442581756248e-05,
      "loss": 1.4701,
      "step": 4720
    },
    {
      "epoch": 0.28724114896459585,
      "grad_norm": 0.6896551251411438,
      "learning_rate": 9.775501461759617e-05,
      "loss": 1.4716,
      "step": 4730
    },
    {
      "epoch": 0.2878484241209692,
      "grad_norm": 0.22592943906784058,
      "learning_rate": 9.774558410474139e-05,
      "loss": 1.4703,
      "step": 4740
    },
    {
      "epoch": 0.28845569927734255,
      "grad_norm": 0.2749931514263153,
      "learning_rate": 9.773613428281196e-05,
      "loss": 1.4738,
      "step": 4750
    },
    {
      "epoch": 0.2890629744337159,
      "grad_norm": 0.12627564370632172,
      "learning_rate": 9.772666515562958e-05,
      "loss": 1.4705,
      "step": 4760
    },
    {
      "epoch": 0.28967024959008925,
      "grad_norm": 0.23479557037353516,
      "learning_rate": 9.77171767270237e-05,
      "loss": 1.4698,
      "step": 4770
    },
    {
      "epoch": 0.2902775247464626,
      "grad_norm": 0.06809554994106293,
      "learning_rate": 9.77076690008316e-05,
      "loss": 1.4685,
      "step": 4780
    },
    {
      "epoch": 0.290884799902836,
      "grad_norm": 0.08136152476072311,
      "learning_rate": 9.769814198089832e-05,
      "loss": 1.4699,
      "step": 4790
    },
    {
      "epoch": 0.2914920750592093,
      "grad_norm": 0.17595534026622772,
      "learning_rate": 9.768859567107677e-05,
      "loss": 1.4683,
      "step": 4800
    },
    {
      "epoch": 0.2920993502155827,
      "grad_norm": 0.1331147998571396,
      "learning_rate": 9.767903007522763e-05,
      "loss": 1.4676,
      "step": 4810
    },
    {
      "epoch": 0.292706625371956,
      "grad_norm": 0.1369035392999649,
      "learning_rate": 9.766944519721937e-05,
      "loss": 1.4672,
      "step": 4820
    },
    {
      "epoch": 0.2933139005283294,
      "grad_norm": 0.1334962397813797,
      "learning_rate": 9.765984104092826e-05,
      "loss": 1.467,
      "step": 4830
    },
    {
      "epoch": 0.2939211756847027,
      "grad_norm": 0.19374661147594452,
      "learning_rate": 9.765021761023838e-05,
      "loss": 1.4682,
      "step": 4840
    },
    {
      "epoch": 0.2945284508410761,
      "grad_norm": 0.38825109601020813,
      "learning_rate": 9.764057490904162e-05,
      "loss": 1.4701,
      "step": 4850
    },
    {
      "epoch": 0.29513572599744947,
      "grad_norm": 0.1348358392715454,
      "learning_rate": 9.763091294123762e-05,
      "loss": 1.4691,
      "step": 4860
    },
    {
      "epoch": 0.2957430011538228,
      "grad_norm": 0.6390857100486755,
      "learning_rate": 9.762123171073383e-05,
      "loss": 1.4733,
      "step": 4870
    },
    {
      "epoch": 0.29635027631019617,
      "grad_norm": 0.3327884376049042,
      "learning_rate": 9.76115312214455e-05,
      "loss": 1.4689,
      "step": 4880
    },
    {
      "epoch": 0.2969575514665695,
      "grad_norm": 0.6578207015991211,
      "learning_rate": 9.760181147729568e-05,
      "loss": 1.4705,
      "step": 4890
    },
    {
      "epoch": 0.29756482662294287,
      "grad_norm": 0.4865994453430176,
      "learning_rate": 9.759207248221516e-05,
      "loss": 1.471,
      "step": 4900
    },
    {
      "epoch": 0.2981721017793162,
      "grad_norm": 0.1395849585533142,
      "learning_rate": 9.758231424014256e-05,
      "loss": 1.4686,
      "step": 4910
    },
    {
      "epoch": 0.29877937693568957,
      "grad_norm": 0.4766610860824585,
      "learning_rate": 9.757253675502426e-05,
      "loss": 1.4674,
      "step": 4920
    },
    {
      "epoch": 0.2993866520920629,
      "grad_norm": 0.20688915252685547,
      "learning_rate": 9.756274003081444e-05,
      "loss": 1.4647,
      "step": 4930
    },
    {
      "epoch": 0.29999392724843627,
      "grad_norm": 0.17840464413166046,
      "learning_rate": 9.755292407147505e-05,
      "loss": 1.4669,
      "step": 4940
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 0.08848084509372711,
      "learning_rate": 9.754308888097583e-05,
      "loss": 1.466,
      "step": 4950
    },
    {
      "epoch": 0.30120847756118296,
      "grad_norm": 0.1276450902223587,
      "learning_rate": 9.753323446329427e-05,
      "loss": 1.465,
      "step": 4960
    },
    {
      "epoch": 0.30181575271755634,
      "grad_norm": 0.08486519753932953,
      "learning_rate": 9.752336082241564e-05,
      "loss": 1.4656,
      "step": 4970
    },
    {
      "epoch": 0.30242302787392966,
      "grad_norm": 0.11868297308683395,
      "learning_rate": 9.751346796233305e-05,
      "loss": 1.4655,
      "step": 4980
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.29449063539505005,
      "learning_rate": 9.750355588704727e-05,
      "loss": 1.4655,
      "step": 4990
    },
    {
      "epoch": 0.30363757818667636,
      "grad_norm": 0.2484273612499237,
      "learning_rate": 9.749362460056694e-05,
      "loss": 1.4664,
      "step": 5000
    },
    {
      "epoch": 0.30363757818667636,
      "eval_loss": 1.4714359045028687,
      "eval_runtime": 2115.6338,
      "eval_samples_per_second": 7.783,
      "eval_steps_per_second": 1.946,
      "step": 5000
    }
  ],
  "logging_steps": 10,
  "max_steps": 49401,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5001,
  "total_flos": 1.0453771616256e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
