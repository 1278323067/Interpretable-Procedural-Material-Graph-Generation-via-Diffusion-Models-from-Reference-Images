{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9109127345600292,
  "eval_steps": 5001,
  "global_step": 15000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006072751563733527,
      "grad_norm": 7.186389446258545,
      "learning_rate": 9.999998988960184e-05,
      "loss": 28.5336,
      "step": 10
    },
    {
      "epoch": 0.0012145503127467055,
      "grad_norm": 6.450451374053955,
      "learning_rate": 9.999995955841146e-05,
      "loss": 5.8003,
      "step": 20
    },
    {
      "epoch": 0.0018218254691200583,
      "grad_norm": 19.285921096801758,
      "learning_rate": 9.99999090064411e-05,
      "loss": 4.1025,
      "step": 30
    },
    {
      "epoch": 0.002429100625493411,
      "grad_norm": 6.392780303955078,
      "learning_rate": 9.999983823371122e-05,
      "loss": 3.7077,
      "step": 40
    },
    {
      "epoch": 0.0030363757818667636,
      "grad_norm": 3.809990406036377,
      "learning_rate": 9.999974724025045e-05,
      "loss": 3.4147,
      "step": 50
    },
    {
      "epoch": 0.0036436509382401167,
      "grad_norm": 3.4429056644439697,
      "learning_rate": 9.999963602609556e-05,
      "loss": 3.434,
      "step": 60
    },
    {
      "epoch": 0.004250926094613469,
      "grad_norm": 4.669011116027832,
      "learning_rate": 9.999950459129158e-05,
      "loss": 3.4633,
      "step": 70
    },
    {
      "epoch": 0.004858201250986822,
      "grad_norm": 4.70878267288208,
      "learning_rate": 9.99993529358916e-05,
      "loss": 3.8761,
      "step": 80
    },
    {
      "epoch": 0.005465476407360175,
      "grad_norm": 5.6883344650268555,
      "learning_rate": 9.9999181059957e-05,
      "loss": 3.4797,
      "step": 90
    },
    {
      "epoch": 0.006072751563733527,
      "grad_norm": 4.667462348937988,
      "learning_rate": 9.999898896355726e-05,
      "loss": 3.2826,
      "step": 100
    },
    {
      "epoch": 0.006680026720106881,
      "grad_norm": 5.330298900604248,
      "learning_rate": 9.999877664677009e-05,
      "loss": 2.9715,
      "step": 110
    },
    {
      "epoch": 0.007287301876480233,
      "grad_norm": 6.202093601226807,
      "learning_rate": 9.999854410968134e-05,
      "loss": 2.9419,
      "step": 120
    },
    {
      "epoch": 0.007894577032853586,
      "grad_norm": 4.214776039123535,
      "learning_rate": 9.999829135238505e-05,
      "loss": 3.3057,
      "step": 130
    },
    {
      "epoch": 0.008501852189226939,
      "grad_norm": 3.475522994995117,
      "learning_rate": 9.999801837498346e-05,
      "loss": 3.3707,
      "step": 140
    },
    {
      "epoch": 0.009109127345600291,
      "grad_norm": 5.682331562042236,
      "learning_rate": 9.999772517758694e-05,
      "loss": 3.1139,
      "step": 150
    },
    {
      "epoch": 0.009716402501973644,
      "grad_norm": 4.191882133483887,
      "learning_rate": 9.999741176031408e-05,
      "loss": 3.0561,
      "step": 160
    },
    {
      "epoch": 0.010323677658346997,
      "grad_norm": 4.490618705749512,
      "learning_rate": 9.999707812329162e-05,
      "loss": 2.8682,
      "step": 170
    },
    {
      "epoch": 0.01093095281472035,
      "grad_norm": 3.096764326095581,
      "learning_rate": 9.99967242666545e-05,
      "loss": 3.1537,
      "step": 180
    },
    {
      "epoch": 0.011538227971093702,
      "grad_norm": 4.115010738372803,
      "learning_rate": 9.99963501905458e-05,
      "loss": 3.1016,
      "step": 190
    },
    {
      "epoch": 0.012145503127467054,
      "grad_norm": 2.6895015239715576,
      "learning_rate": 9.999595589511684e-05,
      "loss": 3.0048,
      "step": 200
    },
    {
      "epoch": 0.012752778283840409,
      "grad_norm": 5.5280046463012695,
      "learning_rate": 9.999554138052704e-05,
      "loss": 2.6903,
      "step": 210
    },
    {
      "epoch": 0.013360053440213761,
      "grad_norm": 4.60183572769165,
      "learning_rate": 9.999510664694408e-05,
      "loss": 2.6745,
      "step": 220
    },
    {
      "epoch": 0.013967328596587114,
      "grad_norm": 5.704331398010254,
      "learning_rate": 9.999465169454374e-05,
      "loss": 2.9479,
      "step": 230
    },
    {
      "epoch": 0.014574603752960467,
      "grad_norm": 2.7345144748687744,
      "learning_rate": 9.999417652351002e-05,
      "loss": 2.9361,
      "step": 240
    },
    {
      "epoch": 0.01518187890933382,
      "grad_norm": 3.391403913497925,
      "learning_rate": 9.999368113403508e-05,
      "loss": 3.0954,
      "step": 250
    },
    {
      "epoch": 0.015789154065707172,
      "grad_norm": 3.0357401371002197,
      "learning_rate": 9.999316552631928e-05,
      "loss": 3.1664,
      "step": 260
    },
    {
      "epoch": 0.016396429222080525,
      "grad_norm": 2.4493205547332764,
      "learning_rate": 9.999262970057113e-05,
      "loss": 2.8576,
      "step": 270
    },
    {
      "epoch": 0.017003704378453877,
      "grad_norm": 3.509894847869873,
      "learning_rate": 9.999207365700733e-05,
      "loss": 3.0379,
      "step": 280
    },
    {
      "epoch": 0.01761097953482723,
      "grad_norm": 2.7530264854431152,
      "learning_rate": 9.999149739585273e-05,
      "loss": 2.7517,
      "step": 290
    },
    {
      "epoch": 0.018218254691200583,
      "grad_norm": 4.6260576248168945,
      "learning_rate": 9.999090091734043e-05,
      "loss": 2.8676,
      "step": 300
    },
    {
      "epoch": 0.018825529847573935,
      "grad_norm": 5.038768768310547,
      "learning_rate": 9.999028422171159e-05,
      "loss": 3.1278,
      "step": 310
    },
    {
      "epoch": 0.019432805003947288,
      "grad_norm": 2.4521424770355225,
      "learning_rate": 9.998964730921568e-05,
      "loss": 2.8147,
      "step": 320
    },
    {
      "epoch": 0.02004008016032064,
      "grad_norm": 3.764702796936035,
      "learning_rate": 9.99889901801102e-05,
      "loss": 2.9978,
      "step": 330
    },
    {
      "epoch": 0.020647355316693993,
      "grad_norm": 3.569005250930786,
      "learning_rate": 9.998831283466099e-05,
      "loss": 2.8446,
      "step": 340
    },
    {
      "epoch": 0.021254630473067346,
      "grad_norm": 2.5747430324554443,
      "learning_rate": 9.998761527314191e-05,
      "loss": 2.6242,
      "step": 350
    },
    {
      "epoch": 0.0218619056294407,
      "grad_norm": 2.6841931343078613,
      "learning_rate": 9.99868974958351e-05,
      "loss": 2.7303,
      "step": 360
    },
    {
      "epoch": 0.02246918078581405,
      "grad_norm": 4.337249279022217,
      "learning_rate": 9.998615950303083e-05,
      "loss": 3.0008,
      "step": 370
    },
    {
      "epoch": 0.023076455942187404,
      "grad_norm": 5.8416595458984375,
      "learning_rate": 9.998540129502756e-05,
      "loss": 3.0236,
      "step": 380
    },
    {
      "epoch": 0.023683731098560756,
      "grad_norm": 3.5167787075042725,
      "learning_rate": 9.998462287213191e-05,
      "loss": 3.0139,
      "step": 390
    },
    {
      "epoch": 0.02429100625493411,
      "grad_norm": 3.1287996768951416,
      "learning_rate": 9.998382423465871e-05,
      "loss": 2.6025,
      "step": 400
    },
    {
      "epoch": 0.024898281411307465,
      "grad_norm": 2.7561633586883545,
      "learning_rate": 9.998300538293091e-05,
      "loss": 2.7249,
      "step": 410
    },
    {
      "epoch": 0.025505556567680818,
      "grad_norm": 2.219248056411743,
      "learning_rate": 9.99821663172797e-05,
      "loss": 2.5885,
      "step": 420
    },
    {
      "epoch": 0.02611283172405417,
      "grad_norm": 2.8309640884399414,
      "learning_rate": 9.998130703804438e-05,
      "loss": 2.6203,
      "step": 430
    },
    {
      "epoch": 0.026720106880427523,
      "grad_norm": 2.67535662651062,
      "learning_rate": 9.998042754557249e-05,
      "loss": 2.8381,
      "step": 440
    },
    {
      "epoch": 0.027327382036800876,
      "grad_norm": 3.424489736557007,
      "learning_rate": 9.997952784021967e-05,
      "loss": 2.8307,
      "step": 450
    },
    {
      "epoch": 0.027934657193174228,
      "grad_norm": 2.4972362518310547,
      "learning_rate": 9.997860792234981e-05,
      "loss": 2.727,
      "step": 460
    },
    {
      "epoch": 0.02854193234954758,
      "grad_norm": 2.3320651054382324,
      "learning_rate": 9.997766779233493e-05,
      "loss": 2.733,
      "step": 470
    },
    {
      "epoch": 0.029149207505920934,
      "grad_norm": 4.219310283660889,
      "learning_rate": 9.997670745055522e-05,
      "loss": 2.5761,
      "step": 480
    },
    {
      "epoch": 0.029756482662294286,
      "grad_norm": 2.828003168106079,
      "learning_rate": 9.997572689739907e-05,
      "loss": 2.8755,
      "step": 490
    },
    {
      "epoch": 0.03036375781866764,
      "grad_norm": 2.7965126037597656,
      "learning_rate": 9.997472613326304e-05,
      "loss": 2.8552,
      "step": 500
    },
    {
      "epoch": 0.03097103297504099,
      "grad_norm": 3.650280475616455,
      "learning_rate": 9.997370515855182e-05,
      "loss": 2.6381,
      "step": 510
    },
    {
      "epoch": 0.031578308131414344,
      "grad_norm": 3.817025661468506,
      "learning_rate": 9.997266397367836e-05,
      "loss": 2.4954,
      "step": 520
    },
    {
      "epoch": 0.03218558328778769,
      "grad_norm": 1.6731635332107544,
      "learning_rate": 9.99716025790637e-05,
      "loss": 2.4817,
      "step": 530
    },
    {
      "epoch": 0.03279285844416105,
      "grad_norm": 2.273855209350586,
      "learning_rate": 9.997052097513709e-05,
      "loss": 2.7781,
      "step": 540
    },
    {
      "epoch": 0.033400133600534405,
      "grad_norm": 2.9378833770751953,
      "learning_rate": 9.996941916233594e-05,
      "loss": 2.5581,
      "step": 550
    },
    {
      "epoch": 0.034007408756907755,
      "grad_norm": 3.8389534950256348,
      "learning_rate": 9.996829714110583e-05,
      "loss": 3.0879,
      "step": 560
    },
    {
      "epoch": 0.03461468391328111,
      "grad_norm": 3.4130256175994873,
      "learning_rate": 9.996715491190057e-05,
      "loss": 2.9079,
      "step": 570
    },
    {
      "epoch": 0.03522195906965446,
      "grad_norm": 2.044156551361084,
      "learning_rate": 9.996599247518206e-05,
      "loss": 2.6539,
      "step": 580
    },
    {
      "epoch": 0.035829234226027816,
      "grad_norm": 3.4560179710388184,
      "learning_rate": 9.996480983142041e-05,
      "loss": 2.9231,
      "step": 590
    },
    {
      "epoch": 0.036436509382401165,
      "grad_norm": 3.428556442260742,
      "learning_rate": 9.99636069810939e-05,
      "loss": 2.9796,
      "step": 600
    },
    {
      "epoch": 0.03704378453877452,
      "grad_norm": 2.4829540252685547,
      "learning_rate": 9.9962383924689e-05,
      "loss": 2.8804,
      "step": 610
    },
    {
      "epoch": 0.03765105969514787,
      "grad_norm": 3.457402229309082,
      "learning_rate": 9.99611406627003e-05,
      "loss": 3.1318,
      "step": 620
    },
    {
      "epoch": 0.03825833485152123,
      "grad_norm": 2.778350830078125,
      "learning_rate": 9.995987719563062e-05,
      "loss": 2.9028,
      "step": 630
    },
    {
      "epoch": 0.038865610007894576,
      "grad_norm": 3.100822687149048,
      "learning_rate": 9.995859352399094e-05,
      "loss": 2.8307,
      "step": 640
    },
    {
      "epoch": 0.03947288516426793,
      "grad_norm": 2.6477577686309814,
      "learning_rate": 9.995728964830036e-05,
      "loss": 2.8336,
      "step": 650
    },
    {
      "epoch": 0.04008016032064128,
      "grad_norm": 3.6645150184631348,
      "learning_rate": 9.995596556908622e-05,
      "loss": 2.6316,
      "step": 660
    },
    {
      "epoch": 0.04068743547701464,
      "grad_norm": 4.381188869476318,
      "learning_rate": 9.995462128688397e-05,
      "loss": 2.6818,
      "step": 670
    },
    {
      "epoch": 0.041294710633387986,
      "grad_norm": 3.122107982635498,
      "learning_rate": 9.995325680223728e-05,
      "loss": 2.9583,
      "step": 680
    },
    {
      "epoch": 0.04190198578976134,
      "grad_norm": 2.9715704917907715,
      "learning_rate": 9.995187211569797e-05,
      "loss": 2.8996,
      "step": 690
    },
    {
      "epoch": 0.04250926094613469,
      "grad_norm": 2.5021092891693115,
      "learning_rate": 9.995046722782601e-05,
      "loss": 2.6537,
      "step": 700
    },
    {
      "epoch": 0.04311653610250805,
      "grad_norm": 2.472275733947754,
      "learning_rate": 9.994904213918959e-05,
      "loss": 2.7742,
      "step": 710
    },
    {
      "epoch": 0.0437238112588814,
      "grad_norm": 2.840935707092285,
      "learning_rate": 9.994759685036501e-05,
      "loss": 2.7137,
      "step": 720
    },
    {
      "epoch": 0.04433108641525475,
      "grad_norm": 4.177767753601074,
      "learning_rate": 9.994613136193679e-05,
      "loss": 2.6396,
      "step": 730
    },
    {
      "epoch": 0.0449383615716281,
      "grad_norm": 2.6978330612182617,
      "learning_rate": 9.994464567449757e-05,
      "loss": 3.107,
      "step": 740
    },
    {
      "epoch": 0.04554563672800146,
      "grad_norm": 2.9850990772247314,
      "learning_rate": 9.99431397886482e-05,
      "loss": 3.0693,
      "step": 750
    },
    {
      "epoch": 0.04615291188437481,
      "grad_norm": 3.750215768814087,
      "learning_rate": 9.994161370499769e-05,
      "loss": 2.6602,
      "step": 760
    },
    {
      "epoch": 0.04676018704074816,
      "grad_norm": 2.1821258068084717,
      "learning_rate": 9.994006742416321e-05,
      "loss": 2.6184,
      "step": 770
    },
    {
      "epoch": 0.04736746219712151,
      "grad_norm": 2.588514804840088,
      "learning_rate": 9.99385009467701e-05,
      "loss": 2.6797,
      "step": 780
    },
    {
      "epoch": 0.04797473735349487,
      "grad_norm": 2.306520700454712,
      "learning_rate": 9.993691427345187e-05,
      "loss": 2.5766,
      "step": 790
    },
    {
      "epoch": 0.04858201250986822,
      "grad_norm": 2.2094011306762695,
      "learning_rate": 9.993530740485018e-05,
      "loss": 2.815,
      "step": 800
    },
    {
      "epoch": 0.049189287666241574,
      "grad_norm": 2.3844077587127686,
      "learning_rate": 9.993368034161489e-05,
      "loss": 2.5661,
      "step": 810
    },
    {
      "epoch": 0.04979656282261493,
      "grad_norm": 2.4530060291290283,
      "learning_rate": 9.9932033084404e-05,
      "loss": 2.9215,
      "step": 820
    },
    {
      "epoch": 0.05040383797898828,
      "grad_norm": 2.7166547775268555,
      "learning_rate": 9.99303656338837e-05,
      "loss": 2.6268,
      "step": 830
    },
    {
      "epoch": 0.051011113135361635,
      "grad_norm": 2.2288105487823486,
      "learning_rate": 9.992867799072833e-05,
      "loss": 2.6068,
      "step": 840
    },
    {
      "epoch": 0.051618388291734985,
      "grad_norm": 2.3088345527648926,
      "learning_rate": 9.99269701556204e-05,
      "loss": 2.4566,
      "step": 850
    },
    {
      "epoch": 0.05222566344810834,
      "grad_norm": 2.3499398231506348,
      "learning_rate": 9.992524212925056e-05,
      "loss": 2.7308,
      "step": 860
    },
    {
      "epoch": 0.05283293860448169,
      "grad_norm": 2.4770405292510986,
      "learning_rate": 9.99234939123177e-05,
      "loss": 2.7754,
      "step": 870
    },
    {
      "epoch": 0.053440213760855046,
      "grad_norm": 4.333104133605957,
      "learning_rate": 9.992172550552879e-05,
      "loss": 2.8169,
      "step": 880
    },
    {
      "epoch": 0.054047488917228395,
      "grad_norm": 3.036212205886841,
      "learning_rate": 9.9919936909599e-05,
      "loss": 2.8564,
      "step": 890
    },
    {
      "epoch": 0.05465476407360175,
      "grad_norm": 2.8859453201293945,
      "learning_rate": 9.99181281252517e-05,
      "loss": 3.1683,
      "step": 900
    },
    {
      "epoch": 0.0552620392299751,
      "grad_norm": 3.3369028568267822,
      "learning_rate": 9.991629915321836e-05,
      "loss": 2.7407,
      "step": 910
    },
    {
      "epoch": 0.055869314386348456,
      "grad_norm": 4.032517433166504,
      "learning_rate": 9.991444999423865e-05,
      "loss": 2.9512,
      "step": 920
    },
    {
      "epoch": 0.056476589542721806,
      "grad_norm": 2.22833251953125,
      "learning_rate": 9.991258064906041e-05,
      "loss": 2.9532,
      "step": 930
    },
    {
      "epoch": 0.05708386469909516,
      "grad_norm": 6.261169910430908,
      "learning_rate": 9.991069111843964e-05,
      "loss": 2.8817,
      "step": 940
    },
    {
      "epoch": 0.05769113985546851,
      "grad_norm": 3.274970054626465,
      "learning_rate": 9.990878140314047e-05,
      "loss": 2.8114,
      "step": 950
    },
    {
      "epoch": 0.05829841501184187,
      "grad_norm": 2.3626973628997803,
      "learning_rate": 9.990685150393523e-05,
      "loss": 2.455,
      "step": 960
    },
    {
      "epoch": 0.058905690168215216,
      "grad_norm": 3.046497106552124,
      "learning_rate": 9.990490142160442e-05,
      "loss": 2.5229,
      "step": 970
    },
    {
      "epoch": 0.05951296532458857,
      "grad_norm": 2.2845818996429443,
      "learning_rate": 9.990293115693667e-05,
      "loss": 2.8898,
      "step": 980
    },
    {
      "epoch": 0.06012024048096192,
      "grad_norm": 4.140326976776123,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.9451,
      "step": 990
    },
    {
      "epoch": 0.06072751563733528,
      "grad_norm": 5.539736747741699,
      "learning_rate": 9.989893008378572e-05,
      "loss": 3.0845,
      "step": 1000
    },
    {
      "epoch": 0.06133479079370863,
      "grad_norm": 4.5421013832092285,
      "learning_rate": 9.989689927692062e-05,
      "loss": 3.3652,
      "step": 1010
    },
    {
      "epoch": 0.06194206595008198,
      "grad_norm": 2.962928295135498,
      "learning_rate": 9.989484829095478e-05,
      "loss": 2.8889,
      "step": 1020
    },
    {
      "epoch": 0.06254934110645534,
      "grad_norm": 3.3312432765960693,
      "learning_rate": 9.989277712671766e-05,
      "loss": 2.6089,
      "step": 1030
    },
    {
      "epoch": 0.06315661626282869,
      "grad_norm": 2.9687602519989014,
      "learning_rate": 9.989068578504684e-05,
      "loss": 2.7753,
      "step": 1040
    },
    {
      "epoch": 0.06376389141920204,
      "grad_norm": 4.476959705352783,
      "learning_rate": 9.988857426678811e-05,
      "loss": 2.3883,
      "step": 1050
    },
    {
      "epoch": 0.06437116657557539,
      "grad_norm": 3.7683510780334473,
      "learning_rate": 9.98864425727954e-05,
      "loss": 2.6421,
      "step": 1060
    },
    {
      "epoch": 0.06497844173194875,
      "grad_norm": 2.254359722137451,
      "learning_rate": 9.98842907039308e-05,
      "loss": 2.5235,
      "step": 1070
    },
    {
      "epoch": 0.0655857168883221,
      "grad_norm": 2.0745208263397217,
      "learning_rate": 9.988211866106457e-05,
      "loss": 2.6447,
      "step": 1080
    },
    {
      "epoch": 0.06619299204469545,
      "grad_norm": 2.1825270652770996,
      "learning_rate": 9.987992644507511e-05,
      "loss": 2.6447,
      "step": 1090
    },
    {
      "epoch": 0.06680026720106881,
      "grad_norm": 2.115710973739624,
      "learning_rate": 9.987771405684897e-05,
      "loss": 2.3655,
      "step": 1100
    },
    {
      "epoch": 0.06740754235744216,
      "grad_norm": 1.6710492372512817,
      "learning_rate": 9.987548149728092e-05,
      "loss": 2.3085,
      "step": 1110
    },
    {
      "epoch": 0.06801481751381551,
      "grad_norm": 2.5440642833709717,
      "learning_rate": 9.987322876727381e-05,
      "loss": 2.8059,
      "step": 1120
    },
    {
      "epoch": 0.06862209267018886,
      "grad_norm": 2.6967358589172363,
      "learning_rate": 9.98709558677387e-05,
      "loss": 3.04,
      "step": 1130
    },
    {
      "epoch": 0.06922936782656222,
      "grad_norm": 3.9061031341552734,
      "learning_rate": 9.986866279959474e-05,
      "loss": 3.1271,
      "step": 1140
    },
    {
      "epoch": 0.06983664298293557,
      "grad_norm": 3.2761754989624023,
      "learning_rate": 9.986634956376932e-05,
      "loss": 2.6869,
      "step": 1150
    },
    {
      "epoch": 0.07044391813930892,
      "grad_norm": 2.55582857131958,
      "learning_rate": 9.986401616119795e-05,
      "loss": 2.9894,
      "step": 1160
    },
    {
      "epoch": 0.07105119329568227,
      "grad_norm": 2.079308032989502,
      "learning_rate": 9.98616625928243e-05,
      "loss": 2.6543,
      "step": 1170
    },
    {
      "epoch": 0.07165846845205563,
      "grad_norm": 2.0200679302215576,
      "learning_rate": 9.985928885960019e-05,
      "loss": 2.5643,
      "step": 1180
    },
    {
      "epoch": 0.07226574360842898,
      "grad_norm": 2.95072078704834,
      "learning_rate": 9.985689496248556e-05,
      "loss": 2.428,
      "step": 1190
    },
    {
      "epoch": 0.07287301876480233,
      "grad_norm": 2.0961287021636963,
      "learning_rate": 9.985448090244858e-05,
      "loss": 2.7229,
      "step": 1200
    },
    {
      "epoch": 0.07348029392117568,
      "grad_norm": 3.2801177501678467,
      "learning_rate": 9.985204668046553e-05,
      "loss": 2.8076,
      "step": 1210
    },
    {
      "epoch": 0.07408756907754904,
      "grad_norm": 6.785343170166016,
      "learning_rate": 9.984959229752082e-05,
      "loss": 2.756,
      "step": 1220
    },
    {
      "epoch": 0.07469484423392239,
      "grad_norm": 2.543053150177002,
      "learning_rate": 9.984711775460707e-05,
      "loss": 3.0355,
      "step": 1230
    },
    {
      "epoch": 0.07530211939029574,
      "grad_norm": 2.2619287967681885,
      "learning_rate": 9.9844623052725e-05,
      "loss": 2.9984,
      "step": 1240
    },
    {
      "epoch": 0.07590939454666909,
      "grad_norm": 6.4721198081970215,
      "learning_rate": 9.984210819288354e-05,
      "loss": 2.6957,
      "step": 1250
    },
    {
      "epoch": 0.07651666970304245,
      "grad_norm": 2.721078872680664,
      "learning_rate": 9.983957317609971e-05,
      "loss": 2.7023,
      "step": 1260
    },
    {
      "epoch": 0.0771239448594158,
      "grad_norm": 1.7425986528396606,
      "learning_rate": 9.983701800339873e-05,
      "loss": 2.7241,
      "step": 1270
    },
    {
      "epoch": 0.07773122001578915,
      "grad_norm": 2.0464706420898438,
      "learning_rate": 9.983444267581394e-05,
      "loss": 2.6473,
      "step": 1280
    },
    {
      "epoch": 0.0783384951721625,
      "grad_norm": 7.386662006378174,
      "learning_rate": 9.983184719438687e-05,
      "loss": 2.8031,
      "step": 1290
    },
    {
      "epoch": 0.07894577032853586,
      "grad_norm": 5.077014923095703,
      "learning_rate": 9.982923156016713e-05,
      "loss": 2.5077,
      "step": 1300
    },
    {
      "epoch": 0.07955304548490921,
      "grad_norm": 5.140445232391357,
      "learning_rate": 9.982659577421255e-05,
      "loss": 2.6643,
      "step": 1310
    },
    {
      "epoch": 0.08016032064128256,
      "grad_norm": 4.758424282073975,
      "learning_rate": 9.982393983758908e-05,
      "loss": 2.9352,
      "step": 1320
    },
    {
      "epoch": 0.08076759579765591,
      "grad_norm": 3.2941057682037354,
      "learning_rate": 9.982126375137083e-05,
      "loss": 2.9868,
      "step": 1330
    },
    {
      "epoch": 0.08137487095402927,
      "grad_norm": 3.253530979156494,
      "learning_rate": 9.981856751664004e-05,
      "loss": 3.0862,
      "step": 1340
    },
    {
      "epoch": 0.08198214611040262,
      "grad_norm": 7.478233814239502,
      "learning_rate": 9.981585113448713e-05,
      "loss": 2.6801,
      "step": 1350
    },
    {
      "epoch": 0.08258942126677597,
      "grad_norm": 2.1226282119750977,
      "learning_rate": 9.981311460601061e-05,
      "loss": 2.3369,
      "step": 1360
    },
    {
      "epoch": 0.08319669642314934,
      "grad_norm": 2.751624345779419,
      "learning_rate": 9.981035793231722e-05,
      "loss": 2.6989,
      "step": 1370
    },
    {
      "epoch": 0.08380397157952268,
      "grad_norm": 2.239170789718628,
      "learning_rate": 9.980758111452177e-05,
      "loss": 2.8433,
      "step": 1380
    },
    {
      "epoch": 0.08441124673589603,
      "grad_norm": 3.954367160797119,
      "learning_rate": 9.980478415374726e-05,
      "loss": 2.9201,
      "step": 1390
    },
    {
      "epoch": 0.08501852189226938,
      "grad_norm": 5.653280735015869,
      "learning_rate": 9.980196705112484e-05,
      "loss": 3.2969,
      "step": 1400
    },
    {
      "epoch": 0.08562579704864275,
      "grad_norm": 4.375555515289307,
      "learning_rate": 9.979912980779377e-05,
      "loss": 3.0847,
      "step": 1410
    },
    {
      "epoch": 0.0862330722050161,
      "grad_norm": 3.537280559539795,
      "learning_rate": 9.979627242490148e-05,
      "loss": 2.9035,
      "step": 1420
    },
    {
      "epoch": 0.08684034736138944,
      "grad_norm": 2.722881317138672,
      "learning_rate": 9.979339490360355e-05,
      "loss": 2.952,
      "step": 1430
    },
    {
      "epoch": 0.0874476225177628,
      "grad_norm": 2.460787057876587,
      "learning_rate": 9.979049724506369e-05,
      "loss": 2.8785,
      "step": 1440
    },
    {
      "epoch": 0.08805489767413616,
      "grad_norm": 2.098292827606201,
      "learning_rate": 9.978757945045378e-05,
      "loss": 2.8513,
      "step": 1450
    },
    {
      "epoch": 0.0886621728305095,
      "grad_norm": 2.7332565784454346,
      "learning_rate": 9.978464152095378e-05,
      "loss": 2.7216,
      "step": 1460
    },
    {
      "epoch": 0.08926944798688285,
      "grad_norm": 4.435109615325928,
      "learning_rate": 9.978168345775187e-05,
      "loss": 3.0856,
      "step": 1470
    },
    {
      "epoch": 0.0898767231432562,
      "grad_norm": 3.5636463165283203,
      "learning_rate": 9.977870526204431e-05,
      "loss": 2.8489,
      "step": 1480
    },
    {
      "epoch": 0.09048399829962957,
      "grad_norm": 3.468517780303955,
      "learning_rate": 9.977570693503557e-05,
      "loss": 2.7209,
      "step": 1490
    },
    {
      "epoch": 0.09109127345600292,
      "grad_norm": 3.9199130535125732,
      "learning_rate": 9.977268847793819e-05,
      "loss": 3.0091,
      "step": 1500
    },
    {
      "epoch": 0.09169854861237627,
      "grad_norm": 3.373481512069702,
      "learning_rate": 9.976964989197288e-05,
      "loss": 2.6523,
      "step": 1510
    },
    {
      "epoch": 0.09230582376874961,
      "grad_norm": 3.3667783737182617,
      "learning_rate": 9.976659117836851e-05,
      "loss": 2.668,
      "step": 1520
    },
    {
      "epoch": 0.09291309892512298,
      "grad_norm": 2.519352436065674,
      "learning_rate": 9.976351233836207e-05,
      "loss": 2.8741,
      "step": 1530
    },
    {
      "epoch": 0.09352037408149633,
      "grad_norm": 2.5113368034362793,
      "learning_rate": 9.976041337319867e-05,
      "loss": 2.7558,
      "step": 1540
    },
    {
      "epoch": 0.09412764923786968,
      "grad_norm": 2.948071002960205,
      "learning_rate": 9.975729428413162e-05,
      "loss": 2.933,
      "step": 1550
    },
    {
      "epoch": 0.09473492439424303,
      "grad_norm": 4.039847373962402,
      "learning_rate": 9.975415507242229e-05,
      "loss": 2.7249,
      "step": 1560
    },
    {
      "epoch": 0.09534219955061639,
      "grad_norm": 2.9555819034576416,
      "learning_rate": 9.975099573934026e-05,
      "loss": 2.8253,
      "step": 1570
    },
    {
      "epoch": 0.09594947470698974,
      "grad_norm": 7.770235061645508,
      "learning_rate": 9.974781628616318e-05,
      "loss": 2.7095,
      "step": 1580
    },
    {
      "epoch": 0.09655674986336309,
      "grad_norm": 2.452521324157715,
      "learning_rate": 9.974461671417689e-05,
      "loss": 2.9011,
      "step": 1590
    },
    {
      "epoch": 0.09716402501973644,
      "grad_norm": 3.272028923034668,
      "learning_rate": 9.974139702467538e-05,
      "loss": 2.6145,
      "step": 1600
    },
    {
      "epoch": 0.0977713001761098,
      "grad_norm": 2.645780086517334,
      "learning_rate": 9.973815721896068e-05,
      "loss": 2.4509,
      "step": 1610
    },
    {
      "epoch": 0.09837857533248315,
      "grad_norm": 1.6018285751342773,
      "learning_rate": 9.973489729834307e-05,
      "loss": 2.4907,
      "step": 1620
    },
    {
      "epoch": 0.0989858504888565,
      "grad_norm": 2.603994607925415,
      "learning_rate": 9.973161726414088e-05,
      "loss": 2.7411,
      "step": 1630
    },
    {
      "epoch": 0.09959312564522986,
      "grad_norm": 2.387714147567749,
      "learning_rate": 9.972831711768063e-05,
      "loss": 2.7527,
      "step": 1640
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 2.995133876800537,
      "learning_rate": 9.972499686029694e-05,
      "loss": 2.9751,
      "step": 1650
    },
    {
      "epoch": 0.10080767595797656,
      "grad_norm": 3.6726107597351074,
      "learning_rate": 9.972165649333259e-05,
      "loss": 2.7097,
      "step": 1660
    },
    {
      "epoch": 0.10141495111434991,
      "grad_norm": 2.033721685409546,
      "learning_rate": 9.971829601813845e-05,
      "loss": 2.6913,
      "step": 1670
    },
    {
      "epoch": 0.10202222627072327,
      "grad_norm": 2.304652214050293,
      "learning_rate": 9.971491543607356e-05,
      "loss": 3.0866,
      "step": 1680
    },
    {
      "epoch": 0.10262950142709662,
      "grad_norm": 4.654269695281982,
      "learning_rate": 9.971151474850511e-05,
      "loss": 2.8732,
      "step": 1690
    },
    {
      "epoch": 0.10323677658346997,
      "grad_norm": 3.589049816131592,
      "learning_rate": 9.970809395680837e-05,
      "loss": 2.6607,
      "step": 1700
    },
    {
      "epoch": 0.10384405173984332,
      "grad_norm": 3.154991865158081,
      "learning_rate": 9.970465306236676e-05,
      "loss": 2.4037,
      "step": 1710
    },
    {
      "epoch": 0.10445132689621668,
      "grad_norm": 2.4109930992126465,
      "learning_rate": 9.970119206657182e-05,
      "loss": 2.6353,
      "step": 1720
    },
    {
      "epoch": 0.10505860205259003,
      "grad_norm": 2.327291250228882,
      "learning_rate": 9.969771097082326e-05,
      "loss": 2.8521,
      "step": 1730
    },
    {
      "epoch": 0.10566587720896338,
      "grad_norm": 2.3901686668395996,
      "learning_rate": 9.969420977652888e-05,
      "loss": 2.6906,
      "step": 1740
    },
    {
      "epoch": 0.10627315236533673,
      "grad_norm": 3.146944999694824,
      "learning_rate": 9.969068848510461e-05,
      "loss": 2.6366,
      "step": 1750
    },
    {
      "epoch": 0.10688042752171009,
      "grad_norm": 5.862473011016846,
      "learning_rate": 9.968714709797453e-05,
      "loss": 2.6662,
      "step": 1760
    },
    {
      "epoch": 0.10748770267808344,
      "grad_norm": 4.54126501083374,
      "learning_rate": 9.968358561657083e-05,
      "loss": 2.7903,
      "step": 1770
    },
    {
      "epoch": 0.10809497783445679,
      "grad_norm": 2.4159276485443115,
      "learning_rate": 9.968000404233382e-05,
      "loss": 3.2408,
      "step": 1780
    },
    {
      "epoch": 0.10870225299083014,
      "grad_norm": 2.532350540161133,
      "learning_rate": 9.967640237671195e-05,
      "loss": 2.8161,
      "step": 1790
    },
    {
      "epoch": 0.1093095281472035,
      "grad_norm": 2.955338954925537,
      "learning_rate": 9.967278062116179e-05,
      "loss": 2.7066,
      "step": 1800
    },
    {
      "epoch": 0.10991680330357685,
      "grad_norm": 5.425250053405762,
      "learning_rate": 9.966913877714803e-05,
      "loss": 3.1352,
      "step": 1810
    },
    {
      "epoch": 0.1105240784599502,
      "grad_norm": 7.161690711975098,
      "learning_rate": 9.966547684614352e-05,
      "loss": 2.8076,
      "step": 1820
    },
    {
      "epoch": 0.11113135361632355,
      "grad_norm": 4.221362113952637,
      "learning_rate": 9.966179482962916e-05,
      "loss": 2.6411,
      "step": 1830
    },
    {
      "epoch": 0.11173862877269691,
      "grad_norm": 2.1997218132019043,
      "learning_rate": 9.965809272909406e-05,
      "loss": 2.6229,
      "step": 1840
    },
    {
      "epoch": 0.11234590392907026,
      "grad_norm": 3.221076726913452,
      "learning_rate": 9.965437054603538e-05,
      "loss": 2.4802,
      "step": 1850
    },
    {
      "epoch": 0.11295317908544361,
      "grad_norm": 2.018519878387451,
      "learning_rate": 9.965062828195841e-05,
      "loss": 2.7864,
      "step": 1860
    },
    {
      "epoch": 0.11356045424181696,
      "grad_norm": 3.077029228210449,
      "learning_rate": 9.964686593837663e-05,
      "loss": 2.8535,
      "step": 1870
    },
    {
      "epoch": 0.11416772939819032,
      "grad_norm": 2.1237363815307617,
      "learning_rate": 9.964308351681157e-05,
      "loss": 2.6879,
      "step": 1880
    },
    {
      "epoch": 0.11477500455456367,
      "grad_norm": 3.0007996559143066,
      "learning_rate": 9.96392810187929e-05,
      "loss": 2.4494,
      "step": 1890
    },
    {
      "epoch": 0.11538227971093702,
      "grad_norm": 1.815516710281372,
      "learning_rate": 9.96354584458584e-05,
      "loss": 2.7779,
      "step": 1900
    },
    {
      "epoch": 0.11598955486731038,
      "grad_norm": 2.029073715209961,
      "learning_rate": 9.9631615799554e-05,
      "loss": 2.6198,
      "step": 1910
    },
    {
      "epoch": 0.11659683002368373,
      "grad_norm": 2.1800644397735596,
      "learning_rate": 9.96277530814337e-05,
      "loss": 2.4975,
      "step": 1920
    },
    {
      "epoch": 0.11720410518005708,
      "grad_norm": 3.3478686809539795,
      "learning_rate": 9.962387029305968e-05,
      "loss": 2.8814,
      "step": 1930
    },
    {
      "epoch": 0.11781138033643043,
      "grad_norm": 2.567033529281616,
      "learning_rate": 9.96199674360022e-05,
      "loss": 2.7724,
      "step": 1940
    },
    {
      "epoch": 0.1184186554928038,
      "grad_norm": 3.720099449157715,
      "learning_rate": 9.961604451183958e-05,
      "loss": 2.6863,
      "step": 1950
    },
    {
      "epoch": 0.11902593064917714,
      "grad_norm": 2.103940725326538,
      "learning_rate": 9.961210152215839e-05,
      "loss": 2.6206,
      "step": 1960
    },
    {
      "epoch": 0.1196332058055505,
      "grad_norm": 3.92439866065979,
      "learning_rate": 9.960813846855318e-05,
      "loss": 2.7931,
      "step": 1970
    },
    {
      "epoch": 0.12024048096192384,
      "grad_norm": 4.141117572784424,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.7756,
      "step": 1980
    },
    {
      "epoch": 0.1208477561182972,
      "grad_norm": 2.5056381225585938,
      "learning_rate": 9.96001521759898e-05,
      "loss": 2.5133,
      "step": 1990
    },
    {
      "epoch": 0.12145503127467056,
      "grad_norm": 2.824169635772705,
      "learning_rate": 9.959612894026138e-05,
      "loss": 2.6841,
      "step": 2000
    },
    {
      "epoch": 0.1220623064310439,
      "grad_norm": 3.8654754161834717,
      "learning_rate": 9.959208564706854e-05,
      "loss": 2.9506,
      "step": 2010
    },
    {
      "epoch": 0.12266958158741725,
      "grad_norm": 3.2301290035247803,
      "learning_rate": 9.958802229804643e-05,
      "loss": 2.8329,
      "step": 2020
    },
    {
      "epoch": 0.12327685674379062,
      "grad_norm": 3.3621160984039307,
      "learning_rate": 9.958393889483837e-05,
      "loss": 3.0437,
      "step": 2030
    },
    {
      "epoch": 0.12388413190016397,
      "grad_norm": 3.243455648422241,
      "learning_rate": 9.95798354390957e-05,
      "loss": 2.5902,
      "step": 2040
    },
    {
      "epoch": 0.12449140705653731,
      "grad_norm": 2.0370466709136963,
      "learning_rate": 9.957571193247797e-05,
      "loss": 2.8775,
      "step": 2050
    },
    {
      "epoch": 0.12509868221291068,
      "grad_norm": 3.824655771255493,
      "learning_rate": 9.957156837665276e-05,
      "loss": 2.7865,
      "step": 2060
    },
    {
      "epoch": 0.12570595736928403,
      "grad_norm": 1.9630728960037231,
      "learning_rate": 9.95674047732958e-05,
      "loss": 2.4911,
      "step": 2070
    },
    {
      "epoch": 0.12631323252565738,
      "grad_norm": 3.390956401824951,
      "learning_rate": 9.956322112409093e-05,
      "loss": 2.7021,
      "step": 2080
    },
    {
      "epoch": 0.12692050768203073,
      "grad_norm": 2.7686266899108887,
      "learning_rate": 9.955901743073006e-05,
      "loss": 2.9021,
      "step": 2090
    },
    {
      "epoch": 0.12752778283840407,
      "grad_norm": 4.272260665893555,
      "learning_rate": 9.955479369491326e-05,
      "loss": 3.1487,
      "step": 2100
    },
    {
      "epoch": 0.12813505799477742,
      "grad_norm": 3.350961446762085,
      "learning_rate": 9.955054991834866e-05,
      "loss": 3.1143,
      "step": 2110
    },
    {
      "epoch": 0.12874233315115077,
      "grad_norm": 2.762260913848877,
      "learning_rate": 9.954628610275249e-05,
      "loss": 2.8706,
      "step": 2120
    },
    {
      "epoch": 0.12934960830752415,
      "grad_norm": 3.9122231006622314,
      "learning_rate": 9.954200224984915e-05,
      "loss": 2.6773,
      "step": 2130
    },
    {
      "epoch": 0.1299568834638975,
      "grad_norm": 3.533756971359253,
      "learning_rate": 9.953769836137106e-05,
      "loss": 3.2229,
      "step": 2140
    },
    {
      "epoch": 0.13056415862027085,
      "grad_norm": 5.022109031677246,
      "learning_rate": 9.95333744390588e-05,
      "loss": 3.1665,
      "step": 2150
    },
    {
      "epoch": 0.1311714337766442,
      "grad_norm": 4.488645076751709,
      "learning_rate": 9.952903048466104e-05,
      "loss": 2.7469,
      "step": 2160
    },
    {
      "epoch": 0.13177870893301755,
      "grad_norm": 2.999912738800049,
      "learning_rate": 9.952466649993451e-05,
      "loss": 3.1273,
      "step": 2170
    },
    {
      "epoch": 0.1323859840893909,
      "grad_norm": 7.09088134765625,
      "learning_rate": 9.952028248664411e-05,
      "loss": 3.1609,
      "step": 2180
    },
    {
      "epoch": 0.13299325924576424,
      "grad_norm": 4.756022930145264,
      "learning_rate": 9.95158784465628e-05,
      "loss": 2.9298,
      "step": 2190
    },
    {
      "epoch": 0.13360053440213762,
      "grad_norm": 6.609065532684326,
      "learning_rate": 9.951145438147162e-05,
      "loss": 2.8606,
      "step": 2200
    },
    {
      "epoch": 0.13420780955851097,
      "grad_norm": 2.513035774230957,
      "learning_rate": 9.950701029315977e-05,
      "loss": 3.0793,
      "step": 2210
    },
    {
      "epoch": 0.13481508471488432,
      "grad_norm": 3.002197027206421,
      "learning_rate": 9.950254618342447e-05,
      "loss": 2.7873,
      "step": 2220
    },
    {
      "epoch": 0.13542235987125767,
      "grad_norm": 2.292532444000244,
      "learning_rate": 9.949806205407111e-05,
      "loss": 2.8288,
      "step": 2230
    },
    {
      "epoch": 0.13602963502763102,
      "grad_norm": 3.5423099994659424,
      "learning_rate": 9.949355790691311e-05,
      "loss": 2.893,
      "step": 2240
    },
    {
      "epoch": 0.13663691018400437,
      "grad_norm": 2.4385581016540527,
      "learning_rate": 9.948903374377205e-05,
      "loss": 2.7611,
      "step": 2250
    },
    {
      "epoch": 0.13724418534037772,
      "grad_norm": 3.24420428276062,
      "learning_rate": 9.948448956647757e-05,
      "loss": 2.9368,
      "step": 2260
    },
    {
      "epoch": 0.13785146049675107,
      "grad_norm": 2.549713373184204,
      "learning_rate": 9.947992537686739e-05,
      "loss": 2.9616,
      "step": 2270
    },
    {
      "epoch": 0.13845873565312444,
      "grad_norm": 6.837342739105225,
      "learning_rate": 9.947534117678735e-05,
      "loss": 2.5777,
      "step": 2280
    },
    {
      "epoch": 0.1390660108094978,
      "grad_norm": 1.8564249277114868,
      "learning_rate": 9.947073696809137e-05,
      "loss": 2.8812,
      "step": 2290
    },
    {
      "epoch": 0.13967328596587114,
      "grad_norm": 3.8077194690704346,
      "learning_rate": 9.946611275264148e-05,
      "loss": 2.5728,
      "step": 2300
    },
    {
      "epoch": 0.1402805611222445,
      "grad_norm": 2.3424782752990723,
      "learning_rate": 9.946146853230777e-05,
      "loss": 2.5292,
      "step": 2310
    },
    {
      "epoch": 0.14088783627861784,
      "grad_norm": 3.4835190773010254,
      "learning_rate": 9.945680430896844e-05,
      "loss": 2.6803,
      "step": 2320
    },
    {
      "epoch": 0.1414951114349912,
      "grad_norm": 8.598039627075195,
      "learning_rate": 9.94521200845098e-05,
      "loss": 2.8117,
      "step": 2330
    },
    {
      "epoch": 0.14210238659136454,
      "grad_norm": 2.788564682006836,
      "learning_rate": 9.944741586082617e-05,
      "loss": 2.4584,
      "step": 2340
    },
    {
      "epoch": 0.1427096617477379,
      "grad_norm": 4.074861526489258,
      "learning_rate": 9.944269163982007e-05,
      "loss": 2.6019,
      "step": 2350
    },
    {
      "epoch": 0.14331693690411126,
      "grad_norm": 4.219268321990967,
      "learning_rate": 9.943794742340202e-05,
      "loss": 2.6492,
      "step": 2360
    },
    {
      "epoch": 0.1439242120604846,
      "grad_norm": 3.0509867668151855,
      "learning_rate": 9.943318321349067e-05,
      "loss": 3.151,
      "step": 2370
    },
    {
      "epoch": 0.14453148721685796,
      "grad_norm": 2.362668514251709,
      "learning_rate": 9.942839901201272e-05,
      "loss": 2.7968,
      "step": 2380
    },
    {
      "epoch": 0.1451387623732313,
      "grad_norm": 3.283731460571289,
      "learning_rate": 9.9423594820903e-05,
      "loss": 2.5412,
      "step": 2390
    },
    {
      "epoch": 0.14574603752960466,
      "grad_norm": 2.352623701095581,
      "learning_rate": 9.94187706421044e-05,
      "loss": 2.7589,
      "step": 2400
    },
    {
      "epoch": 0.146353312685978,
      "grad_norm": 2.5188491344451904,
      "learning_rate": 9.941392647756789e-05,
      "loss": 2.8871,
      "step": 2410
    },
    {
      "epoch": 0.14696058784235136,
      "grad_norm": 3.1891841888427734,
      "learning_rate": 9.940906232925251e-05,
      "loss": 2.7545,
      "step": 2420
    },
    {
      "epoch": 0.14756786299872474,
      "grad_norm": 2.551603078842163,
      "learning_rate": 9.940417819912544e-05,
      "loss": 2.6814,
      "step": 2430
    },
    {
      "epoch": 0.14817513815509809,
      "grad_norm": 1.3717840909957886,
      "learning_rate": 9.939927408916185e-05,
      "loss": 2.5997,
      "step": 2440
    },
    {
      "epoch": 0.14878241331147143,
      "grad_norm": 4.24821662902832,
      "learning_rate": 9.939435000134509e-05,
      "loss": 2.7488,
      "step": 2450
    },
    {
      "epoch": 0.14938968846784478,
      "grad_norm": 2.477276563644409,
      "learning_rate": 9.93894059376665e-05,
      "loss": 2.304,
      "step": 2460
    },
    {
      "epoch": 0.14999696362421813,
      "grad_norm": 1.2770167589187622,
      "learning_rate": 9.938444190012556e-05,
      "loss": 2.0069,
      "step": 2470
    },
    {
      "epoch": 0.15060423878059148,
      "grad_norm": 1.531536340713501,
      "learning_rate": 9.93794578907298e-05,
      "loss": 2.2887,
      "step": 2480
    },
    {
      "epoch": 0.15121151393696483,
      "grad_norm": 2.0877492427825928,
      "learning_rate": 9.937445391149483e-05,
      "loss": 2.9653,
      "step": 2490
    },
    {
      "epoch": 0.15181878909333818,
      "grad_norm": 6.722013473510742,
      "learning_rate": 9.936942996444434e-05,
      "loss": 2.4992,
      "step": 2500
    },
    {
      "epoch": 0.15242606424971156,
      "grad_norm": 2.083906412124634,
      "learning_rate": 9.936438605161009e-05,
      "loss": 2.9464,
      "step": 2510
    },
    {
      "epoch": 0.1530333394060849,
      "grad_norm": 2.181049346923828,
      "learning_rate": 9.935932217503193e-05,
      "loss": 2.9778,
      "step": 2520
    },
    {
      "epoch": 0.15364061456245826,
      "grad_norm": 2.2514467239379883,
      "learning_rate": 9.935423833675777e-05,
      "loss": 2.6186,
      "step": 2530
    },
    {
      "epoch": 0.1542478897188316,
      "grad_norm": 2.5771031379699707,
      "learning_rate": 9.934913453884358e-05,
      "loss": 2.4291,
      "step": 2540
    },
    {
      "epoch": 0.15485516487520495,
      "grad_norm": 1.4450653791427612,
      "learning_rate": 9.934401078335342e-05,
      "loss": 2.4076,
      "step": 2550
    },
    {
      "epoch": 0.1554624400315783,
      "grad_norm": 2.9123857021331787,
      "learning_rate": 9.933886707235946e-05,
      "loss": 2.4538,
      "step": 2560
    },
    {
      "epoch": 0.15606971518795165,
      "grad_norm": 2.603353500366211,
      "learning_rate": 9.933370340794183e-05,
      "loss": 2.8142,
      "step": 2570
    },
    {
      "epoch": 0.156676990344325,
      "grad_norm": 2.504354953765869,
      "learning_rate": 9.932851979218886e-05,
      "loss": 3.0566,
      "step": 2580
    },
    {
      "epoch": 0.15728426550069838,
      "grad_norm": 6.334001064300537,
      "learning_rate": 9.932331622719685e-05,
      "loss": 2.9577,
      "step": 2590
    },
    {
      "epoch": 0.15789154065707173,
      "grad_norm": 2.168508291244507,
      "learning_rate": 9.931809271507023e-05,
      "loss": 2.6132,
      "step": 2600
    },
    {
      "epoch": 0.15849881581344508,
      "grad_norm": 3.2646288871765137,
      "learning_rate": 9.931284925792142e-05,
      "loss": 2.7586,
      "step": 2610
    },
    {
      "epoch": 0.15910609096981843,
      "grad_norm": 5.00638484954834,
      "learning_rate": 9.930758585787102e-05,
      "loss": 2.9365,
      "step": 2620
    },
    {
      "epoch": 0.15971336612619177,
      "grad_norm": 3.4067678451538086,
      "learning_rate": 9.930230251704759e-05,
      "loss": 2.7673,
      "step": 2630
    },
    {
      "epoch": 0.16032064128256512,
      "grad_norm": 3.0509109497070312,
      "learning_rate": 9.929699923758783e-05,
      "loss": 2.4575,
      "step": 2640
    },
    {
      "epoch": 0.16092791643893847,
      "grad_norm": 3.062145233154297,
      "learning_rate": 9.929167602163647e-05,
      "loss": 2.3961,
      "step": 2650
    },
    {
      "epoch": 0.16153519159531182,
      "grad_norm": 2.161017417907715,
      "learning_rate": 9.928633287134626e-05,
      "loss": 2.9916,
      "step": 2660
    },
    {
      "epoch": 0.1621424667516852,
      "grad_norm": 5.448423385620117,
      "learning_rate": 9.928096978887809e-05,
      "loss": 2.9117,
      "step": 2670
    },
    {
      "epoch": 0.16274974190805855,
      "grad_norm": 4.232268810272217,
      "learning_rate": 9.927558677640088e-05,
      "loss": 2.7004,
      "step": 2680
    },
    {
      "epoch": 0.1633570170644319,
      "grad_norm": 5.422119140625,
      "learning_rate": 9.927018383609158e-05,
      "loss": 2.6605,
      "step": 2690
    },
    {
      "epoch": 0.16396429222080525,
      "grad_norm": 2.119215965270996,
      "learning_rate": 9.926476097013524e-05,
      "loss": 2.6782,
      "step": 2700
    },
    {
      "epoch": 0.1645715673771786,
      "grad_norm": 2.2161812782287598,
      "learning_rate": 9.925931818072496e-05,
      "loss": 2.6402,
      "step": 2710
    },
    {
      "epoch": 0.16517884253355195,
      "grad_norm": 2.8675131797790527,
      "learning_rate": 9.925385547006189e-05,
      "loss": 3.1298,
      "step": 2720
    },
    {
      "epoch": 0.1657861176899253,
      "grad_norm": 4.072360038757324,
      "learning_rate": 9.924837284035522e-05,
      "loss": 2.8118,
      "step": 2730
    },
    {
      "epoch": 0.16639339284629867,
      "grad_norm": 4.261361122131348,
      "learning_rate": 9.924287029382223e-05,
      "loss": 2.9002,
      "step": 2740
    },
    {
      "epoch": 0.16700066800267202,
      "grad_norm": 1.7519627809524536,
      "learning_rate": 9.923734783268823e-05,
      "loss": 2.7755,
      "step": 2750
    },
    {
      "epoch": 0.16760794315904537,
      "grad_norm": 3.7197766304016113,
      "learning_rate": 9.92318054591866e-05,
      "loss": 2.8496,
      "step": 2760
    },
    {
      "epoch": 0.16821521831541872,
      "grad_norm": 3.0197410583496094,
      "learning_rate": 9.922624317555874e-05,
      "loss": 2.6811,
      "step": 2770
    },
    {
      "epoch": 0.16882249347179207,
      "grad_norm": 2.217505693435669,
      "learning_rate": 9.922066098405415e-05,
      "loss": 2.4643,
      "step": 2780
    },
    {
      "epoch": 0.16942976862816542,
      "grad_norm": 4.421277046203613,
      "learning_rate": 9.921505888693036e-05,
      "loss": 2.3106,
      "step": 2790
    },
    {
      "epoch": 0.17003704378453877,
      "grad_norm": 1.5161346197128296,
      "learning_rate": 9.920943688645292e-05,
      "loss": 2.6155,
      "step": 2800
    },
    {
      "epoch": 0.17064431894091212,
      "grad_norm": 1.6676315069198608,
      "learning_rate": 9.920379498489549e-05,
      "loss": 2.6192,
      "step": 2810
    },
    {
      "epoch": 0.1712515940972855,
      "grad_norm": 2.112467050552368,
      "learning_rate": 9.919813318453973e-05,
      "loss": 2.7792,
      "step": 2820
    },
    {
      "epoch": 0.17185886925365884,
      "grad_norm": 3.6455938816070557,
      "learning_rate": 9.919245148767535e-05,
      "loss": 3.0264,
      "step": 2830
    },
    {
      "epoch": 0.1724661444100322,
      "grad_norm": 3.838238000869751,
      "learning_rate": 9.918674989660013e-05,
      "loss": 2.8347,
      "step": 2840
    },
    {
      "epoch": 0.17307341956640554,
      "grad_norm": 2.5430784225463867,
      "learning_rate": 9.91810284136199e-05,
      "loss": 3.1742,
      "step": 2850
    },
    {
      "epoch": 0.1736806947227789,
      "grad_norm": 2.5899415016174316,
      "learning_rate": 9.917528704104848e-05,
      "loss": 2.5676,
      "step": 2860
    },
    {
      "epoch": 0.17428796987915224,
      "grad_norm": 2.067662239074707,
      "learning_rate": 9.916952578120781e-05,
      "loss": 2.5503,
      "step": 2870
    },
    {
      "epoch": 0.1748952450355256,
      "grad_norm": 2.4310338497161865,
      "learning_rate": 9.916374463642781e-05,
      "loss": 2.5014,
      "step": 2880
    },
    {
      "epoch": 0.17550252019189894,
      "grad_norm": 2.253208637237549,
      "learning_rate": 9.915794360904649e-05,
      "loss": 2.5005,
      "step": 2890
    },
    {
      "epoch": 0.1761097953482723,
      "grad_norm": 2.0732288360595703,
      "learning_rate": 9.915212270140985e-05,
      "loss": 2.6896,
      "step": 2900
    },
    {
      "epoch": 0.17671707050464566,
      "grad_norm": 2.6870555877685547,
      "learning_rate": 9.914628191587198e-05,
      "loss": 2.5548,
      "step": 2910
    },
    {
      "epoch": 0.177324345661019,
      "grad_norm": 2.1309797763824463,
      "learning_rate": 9.914042125479499e-05,
      "loss": 2.5855,
      "step": 2920
    },
    {
      "epoch": 0.17793162081739236,
      "grad_norm": 2.771636724472046,
      "learning_rate": 9.913454072054901e-05,
      "loss": 2.4663,
      "step": 2930
    },
    {
      "epoch": 0.1785388959737657,
      "grad_norm": 2.417732000350952,
      "learning_rate": 9.912864031551222e-05,
      "loss": 2.4668,
      "step": 2940
    },
    {
      "epoch": 0.17914617113013906,
      "grad_norm": 2.665285348892212,
      "learning_rate": 9.912272004207085e-05,
      "loss": 2.6413,
      "step": 2950
    },
    {
      "epoch": 0.1797534462865124,
      "grad_norm": 2.796678304672241,
      "learning_rate": 9.911677990261913e-05,
      "loss": 3.0202,
      "step": 2960
    },
    {
      "epoch": 0.18036072144288579,
      "grad_norm": 2.566383123397827,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.6453,
      "step": 2970
    },
    {
      "epoch": 0.18096799659925913,
      "grad_norm": 1.996123194694519,
      "learning_rate": 9.910484003530192e-05,
      "loss": 2.4306,
      "step": 2980
    },
    {
      "epoch": 0.18157527175563248,
      "grad_norm": 1.706026554107666,
      "learning_rate": 9.909884031226506e-05,
      "loss": 2.2706,
      "step": 2990
    },
    {
      "epoch": 0.18218254691200583,
      "grad_norm": 1.729822039604187,
      "learning_rate": 9.909282073287522e-05,
      "loss": 2.457,
      "step": 3000
    },
    {
      "epoch": 0.18278982206837918,
      "grad_norm": 2.3749873638153076,
      "learning_rate": 9.908678129956681e-05,
      "loss": 2.6506,
      "step": 3010
    },
    {
      "epoch": 0.18339709722475253,
      "grad_norm": 2.04215407371521,
      "learning_rate": 9.908072201478225e-05,
      "loss": 2.7665,
      "step": 3020
    },
    {
      "epoch": 0.18400437238112588,
      "grad_norm": 3.055835723876953,
      "learning_rate": 9.907464288097203e-05,
      "loss": 2.7069,
      "step": 3030
    },
    {
      "epoch": 0.18461164753749923,
      "grad_norm": 1.7778457403182983,
      "learning_rate": 9.906854390059467e-05,
      "loss": 2.5494,
      "step": 3040
    },
    {
      "epoch": 0.1852189226938726,
      "grad_norm": 2.1889514923095703,
      "learning_rate": 9.906242507611665e-05,
      "loss": 2.882,
      "step": 3050
    },
    {
      "epoch": 0.18582619785024596,
      "grad_norm": 4.832243919372559,
      "learning_rate": 9.905628641001255e-05,
      "loss": 2.7401,
      "step": 3060
    },
    {
      "epoch": 0.1864334730066193,
      "grad_norm": 2.737074136734009,
      "learning_rate": 9.905012790476493e-05,
      "loss": 2.7053,
      "step": 3070
    },
    {
      "epoch": 0.18704074816299265,
      "grad_norm": 4.174485206604004,
      "learning_rate": 9.904394956286441e-05,
      "loss": 2.5649,
      "step": 3080
    },
    {
      "epoch": 0.187648023319366,
      "grad_norm": 2.3177812099456787,
      "learning_rate": 9.903775138680956e-05,
      "loss": 2.9224,
      "step": 3090
    },
    {
      "epoch": 0.18825529847573935,
      "grad_norm": 3.777872085571289,
      "learning_rate": 9.903153337910708e-05,
      "loss": 2.7775,
      "step": 3100
    },
    {
      "epoch": 0.1888625736321127,
      "grad_norm": 2.733771324157715,
      "learning_rate": 9.90252955422716e-05,
      "loss": 2.4246,
      "step": 3110
    },
    {
      "epoch": 0.18946984878848605,
      "grad_norm": 1.4292099475860596,
      "learning_rate": 9.90190378788258e-05,
      "loss": 2.4589,
      "step": 3120
    },
    {
      "epoch": 0.19007712394485943,
      "grad_norm": 3.246751546859741,
      "learning_rate": 9.901276039130039e-05,
      "loss": 2.9849,
      "step": 3130
    },
    {
      "epoch": 0.19068439910123278,
      "grad_norm": 2.8562397956848145,
      "learning_rate": 9.900646308223408e-05,
      "loss": 2.7477,
      "step": 3140
    },
    {
      "epoch": 0.19129167425760613,
      "grad_norm": 4.488560199737549,
      "learning_rate": 9.90001459541736e-05,
      "loss": 2.462,
      "step": 3150
    },
    {
      "epoch": 0.19189894941397947,
      "grad_norm": 2.5003440380096436,
      "learning_rate": 9.899380900967371e-05,
      "loss": 2.801,
      "step": 3160
    },
    {
      "epoch": 0.19250622457035282,
      "grad_norm": 3.0068466663360596,
      "learning_rate": 9.898745225129715e-05,
      "loss": 2.5794,
      "step": 3170
    },
    {
      "epoch": 0.19311349972672617,
      "grad_norm": 3.695321798324585,
      "learning_rate": 9.898107568161472e-05,
      "loss": 3.0487,
      "step": 3180
    },
    {
      "epoch": 0.19372077488309952,
      "grad_norm": 3.300095796585083,
      "learning_rate": 9.897467930320519e-05,
      "loss": 3.0191,
      "step": 3190
    },
    {
      "epoch": 0.19432805003947287,
      "grad_norm": 2.3612868785858154,
      "learning_rate": 9.896826311865534e-05,
      "loss": 2.9502,
      "step": 3200
    },
    {
      "epoch": 0.19493532519584625,
      "grad_norm": 1.7939715385437012,
      "learning_rate": 9.896182713056001e-05,
      "loss": 2.899,
      "step": 3210
    },
    {
      "epoch": 0.1955426003522196,
      "grad_norm": 2.2416467666625977,
      "learning_rate": 9.8955371341522e-05,
      "loss": 2.6435,
      "step": 3220
    },
    {
      "epoch": 0.19614987550859295,
      "grad_norm": 3.1400442123413086,
      "learning_rate": 9.894889575415214e-05,
      "loss": 2.7126,
      "step": 3230
    },
    {
      "epoch": 0.1967571506649663,
      "grad_norm": 5.574371337890625,
      "learning_rate": 9.894240037106927e-05,
      "loss": 2.9349,
      "step": 3240
    },
    {
      "epoch": 0.19736442582133965,
      "grad_norm": 3.5921504497528076,
      "learning_rate": 9.89358851949002e-05,
      "loss": 2.9763,
      "step": 3250
    },
    {
      "epoch": 0.197971700977713,
      "grad_norm": 4.121638298034668,
      "learning_rate": 9.892935022827978e-05,
      "loss": 2.6553,
      "step": 3260
    },
    {
      "epoch": 0.19857897613408634,
      "grad_norm": 2.0624380111694336,
      "learning_rate": 9.892279547385087e-05,
      "loss": 2.542,
      "step": 3270
    },
    {
      "epoch": 0.19918625129045972,
      "grad_norm": 1.2536828517913818,
      "learning_rate": 9.891622093426429e-05,
      "loss": 2.411,
      "step": 3280
    },
    {
      "epoch": 0.19979352644683307,
      "grad_norm": 1.965015172958374,
      "learning_rate": 9.890962661217892e-05,
      "loss": 2.6575,
      "step": 3290
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 2.6900694370269775,
      "learning_rate": 9.89030125102616e-05,
      "loss": 2.2768,
      "step": 3300
    },
    {
      "epoch": 0.20100807675957977,
      "grad_norm": 4.045104503631592,
      "learning_rate": 9.889637863118715e-05,
      "loss": 3.1668,
      "step": 3310
    },
    {
      "epoch": 0.20161535191595312,
      "grad_norm": 4.006162166595459,
      "learning_rate": 9.888972497763844e-05,
      "loss": 2.6059,
      "step": 3320
    },
    {
      "epoch": 0.20222262707232647,
      "grad_norm": 1.8307310342788696,
      "learning_rate": 9.888305155230632e-05,
      "loss": 2.6234,
      "step": 3330
    },
    {
      "epoch": 0.20282990222869982,
      "grad_norm": 3.2577028274536133,
      "learning_rate": 9.887635835788965e-05,
      "loss": 2.4421,
      "step": 3340
    },
    {
      "epoch": 0.20343717738507316,
      "grad_norm": 3.091216564178467,
      "learning_rate": 9.886964539709519e-05,
      "loss": 3.2579,
      "step": 3350
    },
    {
      "epoch": 0.20404445254144654,
      "grad_norm": 2.574180841445923,
      "learning_rate": 9.886291267263783e-05,
      "loss": 2.8493,
      "step": 3360
    },
    {
      "epoch": 0.2046517276978199,
      "grad_norm": 2.3522448539733887,
      "learning_rate": 9.885616018724037e-05,
      "loss": 3.0643,
      "step": 3370
    },
    {
      "epoch": 0.20525900285419324,
      "grad_norm": 3.863027811050415,
      "learning_rate": 9.884938794363365e-05,
      "loss": 2.718,
      "step": 3380
    },
    {
      "epoch": 0.2058662780105666,
      "grad_norm": 2.705446720123291,
      "learning_rate": 9.884259594455643e-05,
      "loss": 3.0853,
      "step": 3390
    },
    {
      "epoch": 0.20647355316693994,
      "grad_norm": 5.110780715942383,
      "learning_rate": 9.883578419275553e-05,
      "loss": 3.1031,
      "step": 3400
    },
    {
      "epoch": 0.2070808283233133,
      "grad_norm": 9.48508071899414,
      "learning_rate": 9.882895269098572e-05,
      "loss": 2.8142,
      "step": 3410
    },
    {
      "epoch": 0.20768810347968664,
      "grad_norm": 2.7131142616271973,
      "learning_rate": 9.882210144200979e-05,
      "loss": 2.9707,
      "step": 3420
    },
    {
      "epoch": 0.20829537863605999,
      "grad_norm": 6.04401159286499,
      "learning_rate": 9.881523044859844e-05,
      "loss": 2.9288,
      "step": 3430
    },
    {
      "epoch": 0.20890265379243336,
      "grad_norm": 2.5574951171875,
      "learning_rate": 9.880833971353048e-05,
      "loss": 2.8821,
      "step": 3440
    },
    {
      "epoch": 0.2095099289488067,
      "grad_norm": 3.428109645843506,
      "learning_rate": 9.880142923959258e-05,
      "loss": 2.4052,
      "step": 3450
    },
    {
      "epoch": 0.21011720410518006,
      "grad_norm": 3.915019989013672,
      "learning_rate": 9.879449902957946e-05,
      "loss": 2.4556,
      "step": 3460
    },
    {
      "epoch": 0.2107244792615534,
      "grad_norm": 4.7336249351501465,
      "learning_rate": 9.878754908629384e-05,
      "loss": 3.0088,
      "step": 3470
    },
    {
      "epoch": 0.21133175441792676,
      "grad_norm": 5.3492608070373535,
      "learning_rate": 9.878057941254634e-05,
      "loss": 2.9061,
      "step": 3480
    },
    {
      "epoch": 0.2119390295743001,
      "grad_norm": 5.061074733734131,
      "learning_rate": 9.877359001115563e-05,
      "loss": 2.977,
      "step": 3490
    },
    {
      "epoch": 0.21254630473067346,
      "grad_norm": 6.3792009353637695,
      "learning_rate": 9.876658088494832e-05,
      "loss": 2.6252,
      "step": 3500
    },
    {
      "epoch": 0.21315357988704683,
      "grad_norm": 2.6951260566711426,
      "learning_rate": 9.875955203675905e-05,
      "loss": 2.5433,
      "step": 3510
    },
    {
      "epoch": 0.21376085504342018,
      "grad_norm": 2.335087299346924,
      "learning_rate": 9.875250346943035e-05,
      "loss": 2.8298,
      "step": 3520
    },
    {
      "epoch": 0.21436813019979353,
      "grad_norm": 3.5351386070251465,
      "learning_rate": 9.874543518581279e-05,
      "loss": 2.7372,
      "step": 3530
    },
    {
      "epoch": 0.21497540535616688,
      "grad_norm": 3.641818046569824,
      "learning_rate": 9.873834718876491e-05,
      "loss": 2.7473,
      "step": 3540
    },
    {
      "epoch": 0.21558268051254023,
      "grad_norm": 3.574537515640259,
      "learning_rate": 9.873123948115321e-05,
      "loss": 2.402,
      "step": 3550
    },
    {
      "epoch": 0.21618995566891358,
      "grad_norm": 2.720914363861084,
      "learning_rate": 9.872411206585215e-05,
      "loss": 2.566,
      "step": 3560
    },
    {
      "epoch": 0.21679723082528693,
      "grad_norm": 2.3451085090637207,
      "learning_rate": 9.871696494574417e-05,
      "loss": 2.6943,
      "step": 3570
    },
    {
      "epoch": 0.21740450598166028,
      "grad_norm": 3.902515411376953,
      "learning_rate": 9.870979812371967e-05,
      "loss": 2.7848,
      "step": 3580
    },
    {
      "epoch": 0.21801178113803366,
      "grad_norm": 3.8705780506134033,
      "learning_rate": 9.870261160267704e-05,
      "loss": 2.6667,
      "step": 3590
    },
    {
      "epoch": 0.218619056294407,
      "grad_norm": 2.5219051837921143,
      "learning_rate": 9.869540538552263e-05,
      "loss": 2.8836,
      "step": 3600
    },
    {
      "epoch": 0.21922633145078035,
      "grad_norm": 3.556544065475464,
      "learning_rate": 9.868817947517073e-05,
      "loss": 2.7057,
      "step": 3610
    },
    {
      "epoch": 0.2198336066071537,
      "grad_norm": 2.2481508255004883,
      "learning_rate": 9.868093387454362e-05,
      "loss": 2.9842,
      "step": 3620
    },
    {
      "epoch": 0.22044088176352705,
      "grad_norm": 1.7546168565750122,
      "learning_rate": 9.867366858657155e-05,
      "loss": 2.6182,
      "step": 3630
    },
    {
      "epoch": 0.2210481569199004,
      "grad_norm": 3.4513635635375977,
      "learning_rate": 9.866638361419269e-05,
      "loss": 2.5793,
      "step": 3640
    },
    {
      "epoch": 0.22165543207627375,
      "grad_norm": 5.610491752624512,
      "learning_rate": 9.865907896035324e-05,
      "loss": 2.5074,
      "step": 3650
    },
    {
      "epoch": 0.2222627072326471,
      "grad_norm": 2.9097673892974854,
      "learning_rate": 9.865175462800727e-05,
      "loss": 3.0177,
      "step": 3660
    },
    {
      "epoch": 0.22286998238902048,
      "grad_norm": 2.3843166828155518,
      "learning_rate": 9.86444106201169e-05,
      "loss": 2.9034,
      "step": 3670
    },
    {
      "epoch": 0.22347725754539383,
      "grad_norm": 3.045614719390869,
      "learning_rate": 9.863704693965214e-05,
      "loss": 2.9245,
      "step": 3680
    },
    {
      "epoch": 0.22408453270176718,
      "grad_norm": 2.782386064529419,
      "learning_rate": 9.862966358959099e-05,
      "loss": 2.4482,
      "step": 3690
    },
    {
      "epoch": 0.22469180785814052,
      "grad_norm": 1.9459657669067383,
      "learning_rate": 9.862226057291937e-05,
      "loss": 2.3088,
      "step": 3700
    },
    {
      "epoch": 0.22529908301451387,
      "grad_norm": 1.696479082107544,
      "learning_rate": 9.861483789263122e-05,
      "loss": 2.5437,
      "step": 3710
    },
    {
      "epoch": 0.22590635817088722,
      "grad_norm": 2.0274083614349365,
      "learning_rate": 9.860739555172835e-05,
      "loss": 2.6204,
      "step": 3720
    },
    {
      "epoch": 0.22651363332726057,
      "grad_norm": 2.0878758430480957,
      "learning_rate": 9.859993355322058e-05,
      "loss": 2.8906,
      "step": 3730
    },
    {
      "epoch": 0.22712090848363392,
      "grad_norm": 1.5803723335266113,
      "learning_rate": 9.859245190012566e-05,
      "loss": 2.4404,
      "step": 3740
    },
    {
      "epoch": 0.2277281836400073,
      "grad_norm": 1.9750295877456665,
      "learning_rate": 9.85849505954693e-05,
      "loss": 2.7873,
      "step": 3750
    },
    {
      "epoch": 0.22833545879638065,
      "grad_norm": 2.813697576522827,
      "learning_rate": 9.857742964228512e-05,
      "loss": 2.5473,
      "step": 3760
    },
    {
      "epoch": 0.228942733952754,
      "grad_norm": 2.6191413402557373,
      "learning_rate": 9.856988904361474e-05,
      "loss": 2.5801,
      "step": 3770
    },
    {
      "epoch": 0.22955000910912735,
      "grad_norm": 1.8832244873046875,
      "learning_rate": 9.856232880250769e-05,
      "loss": 3.0785,
      "step": 3780
    },
    {
      "epoch": 0.2301572842655007,
      "grad_norm": 4.4343342781066895,
      "learning_rate": 9.855474892202144e-05,
      "loss": 2.8938,
      "step": 3790
    },
    {
      "epoch": 0.23076455942187404,
      "grad_norm": 2.286416530609131,
      "learning_rate": 9.854714940522142e-05,
      "loss": 2.7686,
      "step": 3800
    },
    {
      "epoch": 0.2313718345782474,
      "grad_norm": 3.0311994552612305,
      "learning_rate": 9.853953025518102e-05,
      "loss": 2.7424,
      "step": 3810
    },
    {
      "epoch": 0.23197910973462077,
      "grad_norm": 1.974241018295288,
      "learning_rate": 9.853189147498149e-05,
      "loss": 2.3339,
      "step": 3820
    },
    {
      "epoch": 0.23258638489099412,
      "grad_norm": 2.4821865558624268,
      "learning_rate": 9.852423306771214e-05,
      "loss": 2.4837,
      "step": 3830
    },
    {
      "epoch": 0.23319366004736747,
      "grad_norm": 3.675553560256958,
      "learning_rate": 9.85165550364701e-05,
      "loss": 2.3941,
      "step": 3840
    },
    {
      "epoch": 0.23380093520374082,
      "grad_norm": 2.205625295639038,
      "learning_rate": 9.850885738436053e-05,
      "loss": 2.4988,
      "step": 3850
    },
    {
      "epoch": 0.23440821036011417,
      "grad_norm": 3.900033473968506,
      "learning_rate": 9.850114011449645e-05,
      "loss": 2.7455,
      "step": 3860
    },
    {
      "epoch": 0.23501548551648752,
      "grad_norm": 3.8293356895446777,
      "learning_rate": 9.849340322999886e-05,
      "loss": 2.4713,
      "step": 3870
    },
    {
      "epoch": 0.23562276067286086,
      "grad_norm": 4.01187801361084,
      "learning_rate": 9.848564673399667e-05,
      "loss": 2.5779,
      "step": 3880
    },
    {
      "epoch": 0.23623003582923421,
      "grad_norm": 2.6362264156341553,
      "learning_rate": 9.847787062962675e-05,
      "loss": 2.679,
      "step": 3890
    },
    {
      "epoch": 0.2368373109856076,
      "grad_norm": 4.051872253417969,
      "learning_rate": 9.847007492003388e-05,
      "loss": 2.8158,
      "step": 3900
    },
    {
      "epoch": 0.23744458614198094,
      "grad_norm": 1.8156551122665405,
      "learning_rate": 9.846225960837075e-05,
      "loss": 2.5326,
      "step": 3910
    },
    {
      "epoch": 0.2380518612983543,
      "grad_norm": 3.5477824211120605,
      "learning_rate": 9.8454424697798e-05,
      "loss": 2.4435,
      "step": 3920
    },
    {
      "epoch": 0.23865913645472764,
      "grad_norm": 3.5380911827087402,
      "learning_rate": 9.844657019148418e-05,
      "loss": 2.6089,
      "step": 3930
    },
    {
      "epoch": 0.239266411611101,
      "grad_norm": 1.6853457689285278,
      "learning_rate": 9.843869609260583e-05,
      "loss": 2.392,
      "step": 3940
    },
    {
      "epoch": 0.23987368676747434,
      "grad_norm": 2.7005269527435303,
      "learning_rate": 9.84308024043473e-05,
      "loss": 2.5094,
      "step": 3950
    },
    {
      "epoch": 0.24048096192384769,
      "grad_norm": 2.7162938117980957,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.5644,
      "step": 3960
    },
    {
      "epoch": 0.24108823708022104,
      "grad_norm": 2.0047061443328857,
      "learning_rate": 9.841495627246707e-05,
      "loss": 2.3856,
      "step": 3970
    },
    {
      "epoch": 0.2416955122365944,
      "grad_norm": 2.2503981590270996,
      "learning_rate": 9.840700383525376e-05,
      "loss": 2.5599,
      "step": 3980
    },
    {
      "epoch": 0.24230278739296776,
      "grad_norm": 3.7740635871887207,
      "learning_rate": 9.839903182147717e-05,
      "loss": 2.7236,
      "step": 3990
    },
    {
      "epoch": 0.2429100625493411,
      "grad_norm": 3.18257474899292,
      "learning_rate": 9.839104023436128e-05,
      "loss": 2.6896,
      "step": 4000
    },
    {
      "epoch": 0.24351733770571446,
      "grad_norm": 2.369929313659668,
      "learning_rate": 9.8383029077138e-05,
      "loss": 3.0945,
      "step": 4010
    },
    {
      "epoch": 0.2441246128620878,
      "grad_norm": 4.257975101470947,
      "learning_rate": 9.837499835304724e-05,
      "loss": 3.031,
      "step": 4020
    },
    {
      "epoch": 0.24473188801846116,
      "grad_norm": 3.2274887561798096,
      "learning_rate": 9.836694806533669e-05,
      "loss": 2.4955,
      "step": 4030
    },
    {
      "epoch": 0.2453391631748345,
      "grad_norm": 2.2119297981262207,
      "learning_rate": 9.835887821726202e-05,
      "loss": 2.1706,
      "step": 4040
    },
    {
      "epoch": 0.24594643833120788,
      "grad_norm": 2.16085147857666,
      "learning_rate": 9.835078881208681e-05,
      "loss": 2.4878,
      "step": 4050
    },
    {
      "epoch": 0.24655371348758123,
      "grad_norm": 4.253424167633057,
      "learning_rate": 9.834267985308256e-05,
      "loss": 2.7579,
      "step": 4060
    },
    {
      "epoch": 0.24716098864395458,
      "grad_norm": 3.1904826164245605,
      "learning_rate": 9.833455134352866e-05,
      "loss": 2.6957,
      "step": 4070
    },
    {
      "epoch": 0.24776826380032793,
      "grad_norm": 2.0744168758392334,
      "learning_rate": 9.832640328671238e-05,
      "loss": 2.7579,
      "step": 4080
    },
    {
      "epoch": 0.24837553895670128,
      "grad_norm": 2.6864755153656006,
      "learning_rate": 9.831823568592897e-05,
      "loss": 2.5567,
      "step": 4090
    },
    {
      "epoch": 0.24898281411307463,
      "grad_norm": 3.30214262008667,
      "learning_rate": 9.831004854448152e-05,
      "loss": 2.6107,
      "step": 4100
    },
    {
      "epoch": 0.24959008926944798,
      "grad_norm": 2.6300244331359863,
      "learning_rate": 9.830184186568101e-05,
      "loss": 2.9667,
      "step": 4110
    },
    {
      "epoch": 0.25019736442582136,
      "grad_norm": 3.1646106243133545,
      "learning_rate": 9.829361565284639e-05,
      "loss": 2.7619,
      "step": 4120
    },
    {
      "epoch": 0.2508046395821947,
      "grad_norm": 4.114932537078857,
      "learning_rate": 9.828536990930444e-05,
      "loss": 2.8465,
      "step": 4130
    },
    {
      "epoch": 0.25141191473856805,
      "grad_norm": 2.61739444732666,
      "learning_rate": 9.82771046383899e-05,
      "loss": 2.7121,
      "step": 4140
    },
    {
      "epoch": 0.2520191898949414,
      "grad_norm": 2.7783448696136475,
      "learning_rate": 9.826881984344536e-05,
      "loss": 2.8407,
      "step": 4150
    },
    {
      "epoch": 0.25262646505131475,
      "grad_norm": 4.8291916847229,
      "learning_rate": 9.826051552782132e-05,
      "loss": 2.5552,
      "step": 4160
    },
    {
      "epoch": 0.25323374020768813,
      "grad_norm": 2.3700718879699707,
      "learning_rate": 9.825219169487618e-05,
      "loss": 2.6864,
      "step": 4170
    },
    {
      "epoch": 0.25384101536406145,
      "grad_norm": 2.8589563369750977,
      "learning_rate": 9.824384834797625e-05,
      "loss": 2.95,
      "step": 4180
    },
    {
      "epoch": 0.2544482905204348,
      "grad_norm": 2.6734585762023926,
      "learning_rate": 9.823548549049569e-05,
      "loss": 2.9177,
      "step": 4190
    },
    {
      "epoch": 0.25505556567680815,
      "grad_norm": 3.068936586380005,
      "learning_rate": 9.82271031258166e-05,
      "loss": 2.9135,
      "step": 4200
    },
    {
      "epoch": 0.2556628408331815,
      "grad_norm": 6.355357646942139,
      "learning_rate": 9.82187012573289e-05,
      "loss": 2.7002,
      "step": 4210
    },
    {
      "epoch": 0.25627011598955485,
      "grad_norm": 3.9745824337005615,
      "learning_rate": 9.821027988843046e-05,
      "loss": 3.2746,
      "step": 4220
    },
    {
      "epoch": 0.2568773911459282,
      "grad_norm": 2.342935800552368,
      "learning_rate": 9.820183902252702e-05,
      "loss": 2.5797,
      "step": 4230
    },
    {
      "epoch": 0.25748466630230155,
      "grad_norm": 2.6542980670928955,
      "learning_rate": 9.81933786630322e-05,
      "loss": 2.5502,
      "step": 4240
    },
    {
      "epoch": 0.2580919414586749,
      "grad_norm": 3.0595574378967285,
      "learning_rate": 9.81848988133675e-05,
      "loss": 2.6272,
      "step": 4250
    },
    {
      "epoch": 0.2586992166150483,
      "grad_norm": 4.398388385772705,
      "learning_rate": 9.817639947696231e-05,
      "loss": 2.5148,
      "step": 4260
    },
    {
      "epoch": 0.2593064917714216,
      "grad_norm": 3.045703887939453,
      "learning_rate": 9.816788065725389e-05,
      "loss": 3.2038,
      "step": 4270
    },
    {
      "epoch": 0.259913766927795,
      "grad_norm": 3.259399652481079,
      "learning_rate": 9.81593423576874e-05,
      "loss": 3.2504,
      "step": 4280
    },
    {
      "epoch": 0.2605210420841683,
      "grad_norm": 6.774500370025635,
      "learning_rate": 9.815078458171585e-05,
      "loss": 3.0742,
      "step": 4290
    },
    {
      "epoch": 0.2611283172405417,
      "grad_norm": 3.3682825565338135,
      "learning_rate": 9.814220733280015e-05,
      "loss": 2.6362,
      "step": 4300
    },
    {
      "epoch": 0.261735592396915,
      "grad_norm": 2.7376015186309814,
      "learning_rate": 9.813361061440907e-05,
      "loss": 2.6324,
      "step": 4310
    },
    {
      "epoch": 0.2623428675532884,
      "grad_norm": 3.0718369483947754,
      "learning_rate": 9.812499443001926e-05,
      "loss": 2.506,
      "step": 4320
    },
    {
      "epoch": 0.26295014270966177,
      "grad_norm": 3.2094228267669678,
      "learning_rate": 9.811635878311524e-05,
      "loss": 2.7084,
      "step": 4330
    },
    {
      "epoch": 0.2635574178660351,
      "grad_norm": 2.701871156692505,
      "learning_rate": 9.810770367718942e-05,
      "loss": 2.384,
      "step": 4340
    },
    {
      "epoch": 0.26416469302240847,
      "grad_norm": 3.7065112590789795,
      "learning_rate": 9.809902911574204e-05,
      "loss": 2.4978,
      "step": 4350
    },
    {
      "epoch": 0.2647719681787818,
      "grad_norm": 3.8052141666412354,
      "learning_rate": 9.809033510228126e-05,
      "loss": 3.0138,
      "step": 4360
    },
    {
      "epoch": 0.26537924333515517,
      "grad_norm": 5.367794036865234,
      "learning_rate": 9.808162164032304e-05,
      "loss": 2.7716,
      "step": 4370
    },
    {
      "epoch": 0.2659865184915285,
      "grad_norm": 1.9300705194473267,
      "learning_rate": 9.807288873339126e-05,
      "loss": 2.5247,
      "step": 4380
    },
    {
      "epoch": 0.26659379364790187,
      "grad_norm": 3.477221965789795,
      "learning_rate": 9.806413638501766e-05,
      "loss": 2.6192,
      "step": 4390
    },
    {
      "epoch": 0.26720106880427524,
      "grad_norm": 3.5737721920013428,
      "learning_rate": 9.805536459874182e-05,
      "loss": 2.5503,
      "step": 4400
    },
    {
      "epoch": 0.26780834396064856,
      "grad_norm": 3.9153947830200195,
      "learning_rate": 9.804657337811119e-05,
      "loss": 2.3362,
      "step": 4410
    },
    {
      "epoch": 0.26841561911702194,
      "grad_norm": 2.249321937561035,
      "learning_rate": 9.803776272668106e-05,
      "loss": 2.8203,
      "step": 4420
    },
    {
      "epoch": 0.26902289427339526,
      "grad_norm": 2.5892300605773926,
      "learning_rate": 9.802893264801462e-05,
      "loss": 2.3298,
      "step": 4430
    },
    {
      "epoch": 0.26963016942976864,
      "grad_norm": 2.309278964996338,
      "learning_rate": 9.80200831456829e-05,
      "loss": 2.6912,
      "step": 4440
    },
    {
      "epoch": 0.27023744458614196,
      "grad_norm": 3.7015762329101562,
      "learning_rate": 9.801121422326475e-05,
      "loss": 2.8123,
      "step": 4450
    },
    {
      "epoch": 0.27084471974251534,
      "grad_norm": 3.5909526348114014,
      "learning_rate": 9.800232588434692e-05,
      "loss": 2.8582,
      "step": 4460
    },
    {
      "epoch": 0.27145199489888866,
      "grad_norm": 2.810145854949951,
      "learning_rate": 9.799341813252402e-05,
      "loss": 2.8055,
      "step": 4470
    },
    {
      "epoch": 0.27205927005526204,
      "grad_norm": 4.735893726348877,
      "learning_rate": 9.798449097139845e-05,
      "loss": 3.0835,
      "step": 4480
    },
    {
      "epoch": 0.2726665452116354,
      "grad_norm": 2.36032772064209,
      "learning_rate": 9.797554440458052e-05,
      "loss": 2.7253,
      "step": 4490
    },
    {
      "epoch": 0.27327382036800874,
      "grad_norm": 2.2880494594573975,
      "learning_rate": 9.796657843568835e-05,
      "loss": 2.5313,
      "step": 4500
    },
    {
      "epoch": 0.2738810955243821,
      "grad_norm": 5.525994300842285,
      "learning_rate": 9.795759306834793e-05,
      "loss": 2.6936,
      "step": 4510
    },
    {
      "epoch": 0.27448837068075543,
      "grad_norm": 3.9558563232421875,
      "learning_rate": 9.794858830619307e-05,
      "loss": 2.4912,
      "step": 4520
    },
    {
      "epoch": 0.2750956458371288,
      "grad_norm": 2.2502200603485107,
      "learning_rate": 9.793956415286545e-05,
      "loss": 2.7239,
      "step": 4530
    },
    {
      "epoch": 0.27570292099350213,
      "grad_norm": 4.233541011810303,
      "learning_rate": 9.79305206120146e-05,
      "loss": 2.7652,
      "step": 4540
    },
    {
      "epoch": 0.2763101961498755,
      "grad_norm": 3.0988237857818604,
      "learning_rate": 9.792145768729785e-05,
      "loss": 2.5716,
      "step": 4550
    },
    {
      "epoch": 0.2769174713062489,
      "grad_norm": 2.9098269939422607,
      "learning_rate": 9.791237538238038e-05,
      "loss": 2.4281,
      "step": 4560
    },
    {
      "epoch": 0.2775247464626222,
      "grad_norm": 2.3917245864868164,
      "learning_rate": 9.790327370093525e-05,
      "loss": 2.3824,
      "step": 4570
    },
    {
      "epoch": 0.2781320216189956,
      "grad_norm": 3.6895751953125,
      "learning_rate": 9.78941526466433e-05,
      "loss": 2.3797,
      "step": 4580
    },
    {
      "epoch": 0.2787392967753689,
      "grad_norm": 2.311485528945923,
      "learning_rate": 9.788501222319324e-05,
      "loss": 2.4595,
      "step": 4590
    },
    {
      "epoch": 0.2793465719317423,
      "grad_norm": 2.711461067199707,
      "learning_rate": 9.78758524342816e-05,
      "loss": 2.7317,
      "step": 4600
    },
    {
      "epoch": 0.2799538470881156,
      "grad_norm": 4.229835033416748,
      "learning_rate": 9.786667328361274e-05,
      "loss": 2.6339,
      "step": 4610
    },
    {
      "epoch": 0.280561122244489,
      "grad_norm": 2.738090991973877,
      "learning_rate": 9.785747477489887e-05,
      "loss": 2.5725,
      "step": 4620
    },
    {
      "epoch": 0.28116839740086236,
      "grad_norm": 3.264017105102539,
      "learning_rate": 9.784825691186e-05,
      "loss": 2.6348,
      "step": 4630
    },
    {
      "epoch": 0.2817756725572357,
      "grad_norm": 2.1726365089416504,
      "learning_rate": 9.783901969822399e-05,
      "loss": 2.6744,
      "step": 4640
    },
    {
      "epoch": 0.28238294771360906,
      "grad_norm": 2.5314559936523438,
      "learning_rate": 9.78297631377265e-05,
      "loss": 2.5572,
      "step": 4650
    },
    {
      "epoch": 0.2829902228699824,
      "grad_norm": 2.3580260276794434,
      "learning_rate": 9.782048723411106e-05,
      "loss": 2.5524,
      "step": 4660
    },
    {
      "epoch": 0.28359749802635575,
      "grad_norm": 2.5427303314208984,
      "learning_rate": 9.781119199112896e-05,
      "loss": 2.2684,
      "step": 4670
    },
    {
      "epoch": 0.2842047731827291,
      "grad_norm": 1.748712420463562,
      "learning_rate": 9.780187741253935e-05,
      "loss": 2.5007,
      "step": 4680
    },
    {
      "epoch": 0.28481204833910245,
      "grad_norm": 3.388526439666748,
      "learning_rate": 9.779254350210922e-05,
      "loss": 2.5284,
      "step": 4690
    },
    {
      "epoch": 0.2854193234954758,
      "grad_norm": 2.6103951930999756,
      "learning_rate": 9.778319026361332e-05,
      "loss": 2.5922,
      "step": 4700
    },
    {
      "epoch": 0.28602659865184915,
      "grad_norm": 3.6787071228027344,
      "learning_rate": 9.777381770083426e-05,
      "loss": 2.8014,
      "step": 4710
    },
    {
      "epoch": 0.28663387380822253,
      "grad_norm": 2.8082218170166016,
      "learning_rate": 9.776442581756248e-05,
      "loss": 2.8619,
      "step": 4720
    },
    {
      "epoch": 0.28724114896459585,
      "grad_norm": 2.760582447052002,
      "learning_rate": 9.775501461759617e-05,
      "loss": 3.0972,
      "step": 4730
    },
    {
      "epoch": 0.2878484241209692,
      "grad_norm": 3.080962657928467,
      "learning_rate": 9.774558410474139e-05,
      "loss": 2.728,
      "step": 4740
    },
    {
      "epoch": 0.28845569927734255,
      "grad_norm": 3.8554110527038574,
      "learning_rate": 9.773613428281196e-05,
      "loss": 2.9752,
      "step": 4750
    },
    {
      "epoch": 0.2890629744337159,
      "grad_norm": 4.114931583404541,
      "learning_rate": 9.772666515562958e-05,
      "loss": 2.8765,
      "step": 4760
    },
    {
      "epoch": 0.28967024959008925,
      "grad_norm": 4.620038032531738,
      "learning_rate": 9.77171767270237e-05,
      "loss": 2.8728,
      "step": 4770
    },
    {
      "epoch": 0.2902775247464626,
      "grad_norm": 1.9616671800613403,
      "learning_rate": 9.77076690008316e-05,
      "loss": 2.9237,
      "step": 4780
    },
    {
      "epoch": 0.290884799902836,
      "grad_norm": 3.4501352310180664,
      "learning_rate": 9.769814198089832e-05,
      "loss": 2.871,
      "step": 4790
    },
    {
      "epoch": 0.2914920750592093,
      "grad_norm": 2.927307367324829,
      "learning_rate": 9.768859567107677e-05,
      "loss": 2.8622,
      "step": 4800
    },
    {
      "epoch": 0.2920993502155827,
      "grad_norm": 2.5114638805389404,
      "learning_rate": 9.767903007522763e-05,
      "loss": 2.6475,
      "step": 4810
    },
    {
      "epoch": 0.292706625371956,
      "grad_norm": 3.0498149394989014,
      "learning_rate": 9.766944519721937e-05,
      "loss": 2.5377,
      "step": 4820
    },
    {
      "epoch": 0.2933139005283294,
      "grad_norm": 3.047322988510132,
      "learning_rate": 9.765984104092826e-05,
      "loss": 2.6764,
      "step": 4830
    },
    {
      "epoch": 0.2939211756847027,
      "grad_norm": 2.788280487060547,
      "learning_rate": 9.765021761023838e-05,
      "loss": 2.8318,
      "step": 4840
    },
    {
      "epoch": 0.2945284508410761,
      "grad_norm": 5.985361099243164,
      "learning_rate": 9.764057490904162e-05,
      "loss": 2.8889,
      "step": 4850
    },
    {
      "epoch": 0.29513572599744947,
      "grad_norm": 5.169188976287842,
      "learning_rate": 9.763091294123762e-05,
      "loss": 2.9491,
      "step": 4860
    },
    {
      "epoch": 0.2957430011538228,
      "grad_norm": 3.305724620819092,
      "learning_rate": 9.762123171073383e-05,
      "loss": 2.9127,
      "step": 4870
    },
    {
      "epoch": 0.29635027631019617,
      "grad_norm": 3.5985920429229736,
      "learning_rate": 9.76115312214455e-05,
      "loss": 2.5104,
      "step": 4880
    },
    {
      "epoch": 0.2969575514665695,
      "grad_norm": 2.1707334518432617,
      "learning_rate": 9.760181147729568e-05,
      "loss": 2.7028,
      "step": 4890
    },
    {
      "epoch": 0.29756482662294287,
      "grad_norm": 2.2230172157287598,
      "learning_rate": 9.759207248221516e-05,
      "loss": 2.9218,
      "step": 4900
    },
    {
      "epoch": 0.2981721017793162,
      "grad_norm": 2.9418625831604004,
      "learning_rate": 9.758231424014256e-05,
      "loss": 2.5685,
      "step": 4910
    },
    {
      "epoch": 0.29877937693568957,
      "grad_norm": 3.084195137023926,
      "learning_rate": 9.757253675502426e-05,
      "loss": 2.7473,
      "step": 4920
    },
    {
      "epoch": 0.2993866520920629,
      "grad_norm": 2.7403950691223145,
      "learning_rate": 9.756274003081444e-05,
      "loss": 2.412,
      "step": 4930
    },
    {
      "epoch": 0.29999392724843627,
      "grad_norm": 1.7992706298828125,
      "learning_rate": 9.755292407147505e-05,
      "loss": 2.7033,
      "step": 4940
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 2.1876397132873535,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.6382,
      "step": 4950
    },
    {
      "epoch": 0.30120847756118296,
      "grad_norm": 3.588412284851074,
      "learning_rate": 9.753323446329427e-05,
      "loss": 2.5126,
      "step": 4960
    },
    {
      "epoch": 0.30181575271755634,
      "grad_norm": 2.112494945526123,
      "learning_rate": 9.752336082241564e-05,
      "loss": 2.7804,
      "step": 4970
    },
    {
      "epoch": 0.30242302787392966,
      "grad_norm": 2.2478232383728027,
      "learning_rate": 9.751346796233305e-05,
      "loss": 2.7227,
      "step": 4980
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 2.3947324752807617,
      "learning_rate": 9.750355588704727e-05,
      "loss": 2.5894,
      "step": 4990
    },
    {
      "epoch": 0.30363757818667636,
      "grad_norm": 2.0972561836242676,
      "learning_rate": 9.749362460056694e-05,
      "loss": 2.6187,
      "step": 5000
    },
    {
      "epoch": 0.3036983057023137,
      "eval_loss": 4.65754508972168,
      "eval_runtime": 2394.3048,
      "eval_samples_per_second": 6.878,
      "eval_steps_per_second": 1.719,
      "step": 5001
    },
    {
      "epoch": 0.30424485334304974,
      "grad_norm": 1.952795386314392,
      "learning_rate": 9.748367410690842e-05,
      "loss": 4.0428,
      "step": 5010
    },
    {
      "epoch": 0.3048521284994231,
      "grad_norm": 3.9655306339263916,
      "learning_rate": 9.747370441009584e-05,
      "loss": 3.4581,
      "step": 5020
    },
    {
      "epoch": 0.30545940365579644,
      "grad_norm": 4.936667442321777,
      "learning_rate": 9.746371551416113e-05,
      "loss": 2.8847,
      "step": 5030
    },
    {
      "epoch": 0.3060666788121698,
      "grad_norm": 2.3399593830108643,
      "learning_rate": 9.745370742314393e-05,
      "loss": 2.7816,
      "step": 5040
    },
    {
      "epoch": 0.30667395396854313,
      "grad_norm": 2.794912576675415,
      "learning_rate": 9.744368014109166e-05,
      "loss": 2.6891,
      "step": 5050
    },
    {
      "epoch": 0.3072812291249165,
      "grad_norm": 2.542238712310791,
      "learning_rate": 9.743363367205957e-05,
      "loss": 2.5726,
      "step": 5060
    },
    {
      "epoch": 0.30788850428128983,
      "grad_norm": 2.089010000228882,
      "learning_rate": 9.742356802011054e-05,
      "loss": 2.632,
      "step": 5070
    },
    {
      "epoch": 0.3084957794376632,
      "grad_norm": 3.0076348781585693,
      "learning_rate": 9.741348318931535e-05,
      "loss": 2.4753,
      "step": 5080
    },
    {
      "epoch": 0.30910305459403653,
      "grad_norm": 2.243069648742676,
      "learning_rate": 9.740337918375242e-05,
      "loss": 2.221,
      "step": 5090
    },
    {
      "epoch": 0.3097103297504099,
      "grad_norm": 2.6892781257629395,
      "learning_rate": 9.739325600750797e-05,
      "loss": 3.0242,
      "step": 5100
    },
    {
      "epoch": 0.3103176049067833,
      "grad_norm": 5.9112043380737305,
      "learning_rate": 9.7383113664676e-05,
      "loss": 3.0292,
      "step": 5110
    },
    {
      "epoch": 0.3109248800631566,
      "grad_norm": 2.819478750228882,
      "learning_rate": 9.737295215935822e-05,
      "loss": 2.6084,
      "step": 5120
    },
    {
      "epoch": 0.31153215521953,
      "grad_norm": 2.179046869277954,
      "learning_rate": 9.73627714956641e-05,
      "loss": 2.4918,
      "step": 5130
    },
    {
      "epoch": 0.3121394303759033,
      "grad_norm": 1.5824692249298096,
      "learning_rate": 9.735257167771088e-05,
      "loss": 2.3744,
      "step": 5140
    },
    {
      "epoch": 0.3127467055322767,
      "grad_norm": 1.7063863277435303,
      "learning_rate": 9.73423527096235e-05,
      "loss": 2.4673,
      "step": 5150
    },
    {
      "epoch": 0.31335398068865,
      "grad_norm": 2.7843480110168457,
      "learning_rate": 9.73321145955347e-05,
      "loss": 2.7704,
      "step": 5160
    },
    {
      "epoch": 0.3139612558450234,
      "grad_norm": 2.136676788330078,
      "learning_rate": 9.732185733958493e-05,
      "loss": 2.6841,
      "step": 5170
    },
    {
      "epoch": 0.31456853100139676,
      "grad_norm": 3.3537065982818604,
      "learning_rate": 9.731158094592238e-05,
      "loss": 2.563,
      "step": 5180
    },
    {
      "epoch": 0.3151758061577701,
      "grad_norm": 2.8524134159088135,
      "learning_rate": 9.7301285418703e-05,
      "loss": 2.5413,
      "step": 5190
    },
    {
      "epoch": 0.31578308131414345,
      "grad_norm": 2.492830991744995,
      "learning_rate": 9.729097076209046e-05,
      "loss": 2.5664,
      "step": 5200
    },
    {
      "epoch": 0.3163903564705168,
      "grad_norm": 2.9123501777648926,
      "learning_rate": 9.728063698025616e-05,
      "loss": 2.3671,
      "step": 5210
    },
    {
      "epoch": 0.31699763162689015,
      "grad_norm": 2.315617084503174,
      "learning_rate": 9.727028407737926e-05,
      "loss": 2.4119,
      "step": 5220
    },
    {
      "epoch": 0.3176049067832635,
      "grad_norm": 2.063382625579834,
      "learning_rate": 9.725991205764662e-05,
      "loss": 2.2221,
      "step": 5230
    },
    {
      "epoch": 0.31821218193963685,
      "grad_norm": 2.1030588150024414,
      "learning_rate": 9.724952092525287e-05,
      "loss": 2.5389,
      "step": 5240
    },
    {
      "epoch": 0.31881945709601023,
      "grad_norm": 2.068145275115967,
      "learning_rate": 9.723911068440036e-05,
      "loss": 2.8679,
      "step": 5250
    },
    {
      "epoch": 0.31942673225238355,
      "grad_norm": 2.002915859222412,
      "learning_rate": 9.722868133929912e-05,
      "loss": 2.3099,
      "step": 5260
    },
    {
      "epoch": 0.3200340074087569,
      "grad_norm": 1.9298429489135742,
      "learning_rate": 9.721823289416698e-05,
      "loss": 2.5772,
      "step": 5270
    },
    {
      "epoch": 0.32064128256513025,
      "grad_norm": 2.576030731201172,
      "learning_rate": 9.720776535322943e-05,
      "loss": 2.8001,
      "step": 5280
    },
    {
      "epoch": 0.3212485577215036,
      "grad_norm": 3.1845993995666504,
      "learning_rate": 9.719727872071973e-05,
      "loss": 2.8178,
      "step": 5290
    },
    {
      "epoch": 0.32185583287787695,
      "grad_norm": 2.2578365802764893,
      "learning_rate": 9.718677300087882e-05,
      "loss": 2.3876,
      "step": 5300
    },
    {
      "epoch": 0.3224631080342503,
      "grad_norm": 3.0310754776000977,
      "learning_rate": 9.71762481979554e-05,
      "loss": 2.3081,
      "step": 5310
    },
    {
      "epoch": 0.32307038319062364,
      "grad_norm": 3.4083526134490967,
      "learning_rate": 9.716570431620586e-05,
      "loss": 2.6982,
      "step": 5320
    },
    {
      "epoch": 0.323677658346997,
      "grad_norm": 2.6855759620666504,
      "learning_rate": 9.715514135989432e-05,
      "loss": 3.1095,
      "step": 5330
    },
    {
      "epoch": 0.3242849335033704,
      "grad_norm": 3.7724175453186035,
      "learning_rate": 9.714455933329259e-05,
      "loss": 2.7103,
      "step": 5340
    },
    {
      "epoch": 0.3248922086597437,
      "grad_norm": 1.6243420839309692,
      "learning_rate": 9.713395824068024e-05,
      "loss": 2.3982,
      "step": 5350
    },
    {
      "epoch": 0.3254994838161171,
      "grad_norm": 3.3213744163513184,
      "learning_rate": 9.712333808634448e-05,
      "loss": 2.3912,
      "step": 5360
    },
    {
      "epoch": 0.3261067589724904,
      "grad_norm": 3.3812777996063232,
      "learning_rate": 9.711269887458032e-05,
      "loss": 2.6176,
      "step": 5370
    },
    {
      "epoch": 0.3267140341288638,
      "grad_norm": 2.8287436962127686,
      "learning_rate": 9.710204060969038e-05,
      "loss": 2.7881,
      "step": 5380
    },
    {
      "epoch": 0.3273213092852371,
      "grad_norm": 4.676497936248779,
      "learning_rate": 9.709136329598505e-05,
      "loss": 2.7891,
      "step": 5390
    },
    {
      "epoch": 0.3279285844416105,
      "grad_norm": 2.073455572128296,
      "learning_rate": 9.708066693778241e-05,
      "loss": 2.2933,
      "step": 5400
    },
    {
      "epoch": 0.32853585959798387,
      "grad_norm": 2.725923538208008,
      "learning_rate": 9.706995153940825e-05,
      "loss": 2.628,
      "step": 5410
    },
    {
      "epoch": 0.3291431347543572,
      "grad_norm": 2.788832187652588,
      "learning_rate": 9.705921710519602e-05,
      "loss": 2.5013,
      "step": 5420
    },
    {
      "epoch": 0.32975040991073057,
      "grad_norm": 3.5404584407806396,
      "learning_rate": 9.704846363948692e-05,
      "loss": 2.6669,
      "step": 5430
    },
    {
      "epoch": 0.3303576850671039,
      "grad_norm": 2.4285194873809814,
      "learning_rate": 9.70376911466298e-05,
      "loss": 2.2676,
      "step": 5440
    },
    {
      "epoch": 0.33096496022347727,
      "grad_norm": 1.98625910282135,
      "learning_rate": 9.702689963098124e-05,
      "loss": 2.4375,
      "step": 5450
    },
    {
      "epoch": 0.3315722353798506,
      "grad_norm": 2.273041248321533,
      "learning_rate": 9.701608909690549e-05,
      "loss": 2.7615,
      "step": 5460
    },
    {
      "epoch": 0.33217951053622397,
      "grad_norm": 3.0551483631134033,
      "learning_rate": 9.700525954877453e-05,
      "loss": 2.5322,
      "step": 5470
    },
    {
      "epoch": 0.33278678569259734,
      "grad_norm": 1.9035460948944092,
      "learning_rate": 9.699441099096797e-05,
      "loss": 2.6966,
      "step": 5480
    },
    {
      "epoch": 0.33339406084897066,
      "grad_norm": 3.1844379901885986,
      "learning_rate": 9.698354342787317e-05,
      "loss": 2.5792,
      "step": 5490
    },
    {
      "epoch": 0.33400133600534404,
      "grad_norm": 2.7493135929107666,
      "learning_rate": 9.697265686388512e-05,
      "loss": 3.1563,
      "step": 5500
    },
    {
      "epoch": 0.33460861116171736,
      "grad_norm": 4.200898170471191,
      "learning_rate": 9.696175130340653e-05,
      "loss": 2.7244,
      "step": 5510
    },
    {
      "epoch": 0.33521588631809074,
      "grad_norm": 3.4090373516082764,
      "learning_rate": 9.695082675084778e-05,
      "loss": 2.6671,
      "step": 5520
    },
    {
      "epoch": 0.33582316147446406,
      "grad_norm": 4.021493434906006,
      "learning_rate": 9.693988321062692e-05,
      "loss": 2.809,
      "step": 5530
    },
    {
      "epoch": 0.33643043663083744,
      "grad_norm": 2.8112053871154785,
      "learning_rate": 9.692892068716974e-05,
      "loss": 3.1483,
      "step": 5540
    },
    {
      "epoch": 0.33703771178721076,
      "grad_norm": 3.90509295463562,
      "learning_rate": 9.69179391849096e-05,
      "loss": 2.617,
      "step": 5550
    },
    {
      "epoch": 0.33764498694358414,
      "grad_norm": 2.5465035438537598,
      "learning_rate": 9.690693870828763e-05,
      "loss": 2.735,
      "step": 5560
    },
    {
      "epoch": 0.3382522620999575,
      "grad_norm": 2.8536598682403564,
      "learning_rate": 9.689591926175257e-05,
      "loss": 2.8422,
      "step": 5570
    },
    {
      "epoch": 0.33885953725633083,
      "grad_norm": 3.5907583236694336,
      "learning_rate": 9.68848808497609e-05,
      "loss": 2.3543,
      "step": 5580
    },
    {
      "epoch": 0.3394668124127042,
      "grad_norm": 1.7455558776855469,
      "learning_rate": 9.68738234767767e-05,
      "loss": 2.5034,
      "step": 5590
    },
    {
      "epoch": 0.34007408756907753,
      "grad_norm": 3.4365475177764893,
      "learning_rate": 9.686274714727175e-05,
      "loss": 2.388,
      "step": 5600
    },
    {
      "epoch": 0.3406813627254509,
      "grad_norm": 3.4610817432403564,
      "learning_rate": 9.68516518657255e-05,
      "loss": 2.9911,
      "step": 5610
    },
    {
      "epoch": 0.34128863788182423,
      "grad_norm": 2.9483425617218018,
      "learning_rate": 9.684053763662507e-05,
      "loss": 3.0189,
      "step": 5620
    },
    {
      "epoch": 0.3418959130381976,
      "grad_norm": 4.086541175842285,
      "learning_rate": 9.68294044644652e-05,
      "loss": 2.9267,
      "step": 5630
    },
    {
      "epoch": 0.342503188194571,
      "grad_norm": 3.5564351081848145,
      "learning_rate": 9.681825235374835e-05,
      "loss": 2.735,
      "step": 5640
    },
    {
      "epoch": 0.3431104633509443,
      "grad_norm": 2.5905704498291016,
      "learning_rate": 9.68070813089846e-05,
      "loss": 2.976,
      "step": 5650
    },
    {
      "epoch": 0.3437177385073177,
      "grad_norm": 3.4880199432373047,
      "learning_rate": 9.67958913346917e-05,
      "loss": 2.5428,
      "step": 5660
    },
    {
      "epoch": 0.344325013663691,
      "grad_norm": 2.2489869594573975,
      "learning_rate": 9.678468243539505e-05,
      "loss": 2.2887,
      "step": 5670
    },
    {
      "epoch": 0.3449322888200644,
      "grad_norm": 1.8352601528167725,
      "learning_rate": 9.677345461562773e-05,
      "loss": 2.5947,
      "step": 5680
    },
    {
      "epoch": 0.3455395639764377,
      "grad_norm": 1.9699002504348755,
      "learning_rate": 9.67622078799304e-05,
      "loss": 2.2721,
      "step": 5690
    },
    {
      "epoch": 0.3461468391328111,
      "grad_norm": 1.9468005895614624,
      "learning_rate": 9.675094223285147e-05,
      "loss": 2.1432,
      "step": 5700
    },
    {
      "epoch": 0.34675411428918446,
      "grad_norm": 1.7849065065383911,
      "learning_rate": 9.673965767894693e-05,
      "loss": 2.4846,
      "step": 5710
    },
    {
      "epoch": 0.3473613894455578,
      "grad_norm": 2.2585389614105225,
      "learning_rate": 9.672835422278042e-05,
      "loss": 2.7742,
      "step": 5720
    },
    {
      "epoch": 0.34796866460193115,
      "grad_norm": 2.461836576461792,
      "learning_rate": 9.671703186892325e-05,
      "loss": 2.3772,
      "step": 5730
    },
    {
      "epoch": 0.3485759397583045,
      "grad_norm": 1.7330503463745117,
      "learning_rate": 9.670569062195436e-05,
      "loss": 2.5005,
      "step": 5740
    },
    {
      "epoch": 0.34918321491467785,
      "grad_norm": 2.814467191696167,
      "learning_rate": 9.669433048646032e-05,
      "loss": 2.4543,
      "step": 5750
    },
    {
      "epoch": 0.3497904900710512,
      "grad_norm": 2.3687150478363037,
      "learning_rate": 9.668295146703537e-05,
      "loss": 2.7108,
      "step": 5760
    },
    {
      "epoch": 0.35039776522742455,
      "grad_norm": 3.235748052597046,
      "learning_rate": 9.667155356828135e-05,
      "loss": 2.4751,
      "step": 5770
    },
    {
      "epoch": 0.3510050403837979,
      "grad_norm": 2.880159854888916,
      "learning_rate": 9.666013679480777e-05,
      "loss": 2.6458,
      "step": 5780
    },
    {
      "epoch": 0.35161231554017125,
      "grad_norm": 2.7512199878692627,
      "learning_rate": 9.664870115123172e-05,
      "loss": 2.7487,
      "step": 5790
    },
    {
      "epoch": 0.3522195906965446,
      "grad_norm": 2.948134660720825,
      "learning_rate": 9.6637246642178e-05,
      "loss": 2.3833,
      "step": 5800
    },
    {
      "epoch": 0.35282686585291795,
      "grad_norm": 2.062732696533203,
      "learning_rate": 9.662577327227896e-05,
      "loss": 2.4633,
      "step": 5810
    },
    {
      "epoch": 0.3534341410092913,
      "grad_norm": 4.2675395011901855,
      "learning_rate": 9.661428104617463e-05,
      "loss": 2.6464,
      "step": 5820
    },
    {
      "epoch": 0.35404141616566465,
      "grad_norm": 3.093010663986206,
      "learning_rate": 9.660276996851265e-05,
      "loss": 2.3541,
      "step": 5830
    },
    {
      "epoch": 0.354648691322038,
      "grad_norm": 3.0793302059173584,
      "learning_rate": 9.659124004394828e-05,
      "loss": 2.3254,
      "step": 5840
    },
    {
      "epoch": 0.35525596647841134,
      "grad_norm": 1.8483396768569946,
      "learning_rate": 9.657969127714441e-05,
      "loss": 2.2173,
      "step": 5850
    },
    {
      "epoch": 0.3558632416347847,
      "grad_norm": 1.734941005706787,
      "learning_rate": 9.656812367277154e-05,
      "loss": 2.0666,
      "step": 5860
    },
    {
      "epoch": 0.3564705167911581,
      "grad_norm": 2.4158875942230225,
      "learning_rate": 9.655653723550779e-05,
      "loss": 2.4716,
      "step": 5870
    },
    {
      "epoch": 0.3570777919475314,
      "grad_norm": 3.6479389667510986,
      "learning_rate": 9.65449319700389e-05,
      "loss": 2.7466,
      "step": 5880
    },
    {
      "epoch": 0.3576850671039048,
      "grad_norm": 3.620398759841919,
      "learning_rate": 9.653330788105823e-05,
      "loss": 2.2851,
      "step": 5890
    },
    {
      "epoch": 0.3582923422602781,
      "grad_norm": 2.0576295852661133,
      "learning_rate": 9.652166497326675e-05,
      "loss": 2.3337,
      "step": 5900
    },
    {
      "epoch": 0.3588996174166515,
      "grad_norm": 3.9837324619293213,
      "learning_rate": 9.651000325137304e-05,
      "loss": 2.7013,
      "step": 5910
    },
    {
      "epoch": 0.3595068925730248,
      "grad_norm": 2.426441192626953,
      "learning_rate": 9.649832272009327e-05,
      "loss": 2.6629,
      "step": 5920
    },
    {
      "epoch": 0.3601141677293982,
      "grad_norm": 1.9288657903671265,
      "learning_rate": 9.648662338415124e-05,
      "loss": 2.7753,
      "step": 5930
    },
    {
      "epoch": 0.36072144288577157,
      "grad_norm": 3.571359395980835,
      "learning_rate": 9.647490524827834e-05,
      "loss": 2.8653,
      "step": 5940
    },
    {
      "epoch": 0.3613287180421449,
      "grad_norm": 4.056645393371582,
      "learning_rate": 9.646316831721359e-05,
      "loss": 2.7246,
      "step": 5950
    },
    {
      "epoch": 0.36193599319851827,
      "grad_norm": 4.044543743133545,
      "learning_rate": 9.645141259570358e-05,
      "loss": 2.9822,
      "step": 5960
    },
    {
      "epoch": 0.3625432683548916,
      "grad_norm": 1.899647831916809,
      "learning_rate": 9.643963808850252e-05,
      "loss": 2.6269,
      "step": 5970
    },
    {
      "epoch": 0.36315054351126497,
      "grad_norm": 2.770113706588745,
      "learning_rate": 9.642784480037218e-05,
      "loss": 2.5157,
      "step": 5980
    },
    {
      "epoch": 0.3637578186676383,
      "grad_norm": 3.5850327014923096,
      "learning_rate": 9.6416032736082e-05,
      "loss": 2.3941,
      "step": 5990
    },
    {
      "epoch": 0.36436509382401167,
      "grad_norm": 2.645689010620117,
      "learning_rate": 9.640420190040893e-05,
      "loss": 2.4616,
      "step": 6000
    },
    {
      "epoch": 0.364972368980385,
      "grad_norm": 3.212184429168701,
      "learning_rate": 9.639235229813754e-05,
      "loss": 2.4048,
      "step": 6010
    },
    {
      "epoch": 0.36557964413675836,
      "grad_norm": 2.152174234390259,
      "learning_rate": 9.638048393406006e-05,
      "loss": 2.3119,
      "step": 6020
    },
    {
      "epoch": 0.36618691929313174,
      "grad_norm": 2.0382027626037598,
      "learning_rate": 9.636859681297616e-05,
      "loss": 2.4328,
      "step": 6030
    },
    {
      "epoch": 0.36679419444950506,
      "grad_norm": 1.8242830038070679,
      "learning_rate": 9.635669093969323e-05,
      "loss": 2.4276,
      "step": 6040
    },
    {
      "epoch": 0.36740146960587844,
      "grad_norm": 3.223503589630127,
      "learning_rate": 9.634476631902623e-05,
      "loss": 2.3513,
      "step": 6050
    },
    {
      "epoch": 0.36800874476225176,
      "grad_norm": 2.917818069458008,
      "learning_rate": 9.633282295579758e-05,
      "loss": 2.6678,
      "step": 6060
    },
    {
      "epoch": 0.36861601991862514,
      "grad_norm": 2.7738428115844727,
      "learning_rate": 9.632086085483742e-05,
      "loss": 2.6119,
      "step": 6070
    },
    {
      "epoch": 0.36922329507499846,
      "grad_norm": 3.361091375350952,
      "learning_rate": 9.630888002098343e-05,
      "loss": 2.7862,
      "step": 6080
    },
    {
      "epoch": 0.36983057023137184,
      "grad_norm": 2.45442795753479,
      "learning_rate": 9.629688045908081e-05,
      "loss": 2.9131,
      "step": 6090
    },
    {
      "epoch": 0.3704378453877452,
      "grad_norm": 3.6359691619873047,
      "learning_rate": 9.628486217398238e-05,
      "loss": 2.8073,
      "step": 6100
    },
    {
      "epoch": 0.37104512054411853,
      "grad_norm": 3.5657355785369873,
      "learning_rate": 9.627282517054854e-05,
      "loss": 2.4888,
      "step": 6110
    },
    {
      "epoch": 0.3716523957004919,
      "grad_norm": 2.302048683166504,
      "learning_rate": 9.626076945364726e-05,
      "loss": 2.4152,
      "step": 6120
    },
    {
      "epoch": 0.37225967085686523,
      "grad_norm": 3.246180295944214,
      "learning_rate": 9.624869502815404e-05,
      "loss": 2.3139,
      "step": 6130
    },
    {
      "epoch": 0.3728669460132386,
      "grad_norm": 2.1596319675445557,
      "learning_rate": 9.623660189895196e-05,
      "loss": 2.3836,
      "step": 6140
    },
    {
      "epoch": 0.37347422116961193,
      "grad_norm": 2.8426425457000732,
      "learning_rate": 9.622449007093169e-05,
      "loss": 2.1422,
      "step": 6150
    },
    {
      "epoch": 0.3740814963259853,
      "grad_norm": 2.9144046306610107,
      "learning_rate": 9.621235954899146e-05,
      "loss": 2.4923,
      "step": 6160
    },
    {
      "epoch": 0.37468877148235863,
      "grad_norm": 2.1653025150299072,
      "learning_rate": 9.620021033803701e-05,
      "loss": 2.3489,
      "step": 6170
    },
    {
      "epoch": 0.375296046638732,
      "grad_norm": 1.9786434173583984,
      "learning_rate": 9.618804244298171e-05,
      "loss": 2.7954,
      "step": 6180
    },
    {
      "epoch": 0.3759033217951054,
      "grad_norm": 1.9633774757385254,
      "learning_rate": 9.617585586874645e-05,
      "loss": 3.0363,
      "step": 6190
    },
    {
      "epoch": 0.3765105969514787,
      "grad_norm": 3.5665221214294434,
      "learning_rate": 9.616365062025965e-05,
      "loss": 2.6774,
      "step": 6200
    },
    {
      "epoch": 0.3771178721078521,
      "grad_norm": 3.683419704437256,
      "learning_rate": 9.615142670245731e-05,
      "loss": 2.6537,
      "step": 6210
    },
    {
      "epoch": 0.3777251472642254,
      "grad_norm": 3.5384700298309326,
      "learning_rate": 9.6139184120283e-05,
      "loss": 2.3428,
      "step": 6220
    },
    {
      "epoch": 0.3783324224205988,
      "grad_norm": 2.1741151809692383,
      "learning_rate": 9.612692287868778e-05,
      "loss": 2.6709,
      "step": 6230
    },
    {
      "epoch": 0.3789396975769721,
      "grad_norm": 4.12810754776001,
      "learning_rate": 9.611464298263034e-05,
      "loss": 2.8755,
      "step": 6240
    },
    {
      "epoch": 0.3795469727333455,
      "grad_norm": 3.328441858291626,
      "learning_rate": 9.610234443707682e-05,
      "loss": 2.5798,
      "step": 6250
    },
    {
      "epoch": 0.38015424788971885,
      "grad_norm": 2.308426856994629,
      "learning_rate": 9.609002724700097e-05,
      "loss": 2.4755,
      "step": 6260
    },
    {
      "epoch": 0.3807615230460922,
      "grad_norm": 3.780794858932495,
      "learning_rate": 9.607769141738405e-05,
      "loss": 2.9129,
      "step": 6270
    },
    {
      "epoch": 0.38136879820246555,
      "grad_norm": 2.2972891330718994,
      "learning_rate": 9.606533695321486e-05,
      "loss": 2.2802,
      "step": 6280
    },
    {
      "epoch": 0.3819760733588389,
      "grad_norm": 1.5943816900253296,
      "learning_rate": 9.605296385948976e-05,
      "loss": 2.2624,
      "step": 6290
    },
    {
      "epoch": 0.38258334851521225,
      "grad_norm": 1.7775726318359375,
      "learning_rate": 9.604057214121263e-05,
      "loss": 2.6631,
      "step": 6300
    },
    {
      "epoch": 0.3831906236715856,
      "grad_norm": 2.653024196624756,
      "learning_rate": 9.602816180339484e-05,
      "loss": 2.7701,
      "step": 6310
    },
    {
      "epoch": 0.38379789882795895,
      "grad_norm": 3.1426491737365723,
      "learning_rate": 9.601573285105539e-05,
      "loss": 2.6401,
      "step": 6320
    },
    {
      "epoch": 0.3844051739843323,
      "grad_norm": 2.406773805618286,
      "learning_rate": 9.600328528922068e-05,
      "loss": 2.3394,
      "step": 6330
    },
    {
      "epoch": 0.38501244914070565,
      "grad_norm": 1.4471969604492188,
      "learning_rate": 9.599081912292473e-05,
      "loss": 2.3784,
      "step": 6340
    },
    {
      "epoch": 0.385619724297079,
      "grad_norm": 2.186854124069214,
      "learning_rate": 9.597833435720908e-05,
      "loss": 2.5582,
      "step": 6350
    },
    {
      "epoch": 0.38622699945345235,
      "grad_norm": 3.2903380393981934,
      "learning_rate": 9.596583099712272e-05,
      "loss": 2.8977,
      "step": 6360
    },
    {
      "epoch": 0.3868342746098257,
      "grad_norm": 2.983762264251709,
      "learning_rate": 9.595330904772226e-05,
      "loss": 2.5649,
      "step": 6370
    },
    {
      "epoch": 0.38744154976619904,
      "grad_norm": 2.9107296466827393,
      "learning_rate": 9.594076851407175e-05,
      "loss": 2.6033,
      "step": 6380
    },
    {
      "epoch": 0.3880488249225724,
      "grad_norm": 3.713864803314209,
      "learning_rate": 9.592820940124276e-05,
      "loss": 2.5969,
      "step": 6390
    },
    {
      "epoch": 0.38865610007894574,
      "grad_norm": 2.0372941493988037,
      "learning_rate": 9.591563171431444e-05,
      "loss": 2.5867,
      "step": 6400
    },
    {
      "epoch": 0.3892633752353191,
      "grad_norm": 2.4364500045776367,
      "learning_rate": 9.590303545837337e-05,
      "loss": 2.3277,
      "step": 6410
    },
    {
      "epoch": 0.3898706503916925,
      "grad_norm": 2.3127830028533936,
      "learning_rate": 9.58904206385137e-05,
      "loss": 2.8594,
      "step": 6420
    },
    {
      "epoch": 0.3904779255480658,
      "grad_norm": 5.502495288848877,
      "learning_rate": 9.587778725983705e-05,
      "loss": 2.3264,
      "step": 6430
    },
    {
      "epoch": 0.3910852007044392,
      "grad_norm": 1.8831835985183716,
      "learning_rate": 9.586513532745256e-05,
      "loss": 2.4031,
      "step": 6440
    },
    {
      "epoch": 0.3916924758608125,
      "grad_norm": 2.8312697410583496,
      "learning_rate": 9.585246484647688e-05,
      "loss": 2.5702,
      "step": 6450
    },
    {
      "epoch": 0.3922997510171859,
      "grad_norm": 2.2462260723114014,
      "learning_rate": 9.583977582203415e-05,
      "loss": 2.4924,
      "step": 6460
    },
    {
      "epoch": 0.3929070261735592,
      "grad_norm": 2.6554434299468994,
      "learning_rate": 9.582706825925601e-05,
      "loss": 2.9364,
      "step": 6470
    },
    {
      "epoch": 0.3935143013299326,
      "grad_norm": 4.090837478637695,
      "learning_rate": 9.581434216328162e-05,
      "loss": 2.7923,
      "step": 6480
    },
    {
      "epoch": 0.39412157648630597,
      "grad_norm": 5.550185680389404,
      "learning_rate": 9.580159753925759e-05,
      "loss": 2.592,
      "step": 6490
    },
    {
      "epoch": 0.3947288516426793,
      "grad_norm": 3.940237283706665,
      "learning_rate": 9.578883439233807e-05,
      "loss": 2.4947,
      "step": 6500
    },
    {
      "epoch": 0.39533612679905267,
      "grad_norm": 6.1874799728393555,
      "learning_rate": 9.577605272768466e-05,
      "loss": 2.8036,
      "step": 6510
    },
    {
      "epoch": 0.395943401955426,
      "grad_norm": 3.4556233882904053,
      "learning_rate": 9.576325255046649e-05,
      "loss": 2.6134,
      "step": 6520
    },
    {
      "epoch": 0.39655067711179937,
      "grad_norm": 1.9538158178329468,
      "learning_rate": 9.575043386586013e-05,
      "loss": 2.8077,
      "step": 6530
    },
    {
      "epoch": 0.3971579522681727,
      "grad_norm": 3.221627950668335,
      "learning_rate": 9.573759667904968e-05,
      "loss": 2.511,
      "step": 6540
    },
    {
      "epoch": 0.39776522742454606,
      "grad_norm": 1.9716154336929321,
      "learning_rate": 9.57247409952267e-05,
      "loss": 2.7303,
      "step": 6550
    },
    {
      "epoch": 0.39837250258091944,
      "grad_norm": 5.112796306610107,
      "learning_rate": 9.571186681959023e-05,
      "loss": 2.8032,
      "step": 6560
    },
    {
      "epoch": 0.39897977773729276,
      "grad_norm": 2.2915725708007812,
      "learning_rate": 9.569897415734681e-05,
      "loss": 2.2758,
      "step": 6570
    },
    {
      "epoch": 0.39958705289366614,
      "grad_norm": 2.2393112182617188,
      "learning_rate": 9.56860630137104e-05,
      "loss": 2.2547,
      "step": 6580
    },
    {
      "epoch": 0.40019432805003946,
      "grad_norm": 2.2078495025634766,
      "learning_rate": 9.567313339390251e-05,
      "loss": 2.6223,
      "step": 6590
    },
    {
      "epoch": 0.40080160320641284,
      "grad_norm": 2.9799387454986572,
      "learning_rate": 9.566018530315204e-05,
      "loss": 2.4083,
      "step": 6600
    },
    {
      "epoch": 0.40140887836278616,
      "grad_norm": 3.1604864597320557,
      "learning_rate": 9.564721874669545e-05,
      "loss": 2.6593,
      "step": 6610
    },
    {
      "epoch": 0.40201615351915954,
      "grad_norm": 3.535609483718872,
      "learning_rate": 9.563423372977661e-05,
      "loss": 2.569,
      "step": 6620
    },
    {
      "epoch": 0.40262342867553286,
      "grad_norm": 3.2493913173675537,
      "learning_rate": 9.562123025764684e-05,
      "loss": 2.4588,
      "step": 6630
    },
    {
      "epoch": 0.40323070383190623,
      "grad_norm": 3.36810564994812,
      "learning_rate": 9.560820833556498e-05,
      "loss": 2.6289,
      "step": 6640
    },
    {
      "epoch": 0.4038379789882796,
      "grad_norm": 4.481655120849609,
      "learning_rate": 9.55951679687973e-05,
      "loss": 2.8437,
      "step": 6650
    },
    {
      "epoch": 0.40444525414465293,
      "grad_norm": 2.7945375442504883,
      "learning_rate": 9.558210916261751e-05,
      "loss": 2.5957,
      "step": 6660
    },
    {
      "epoch": 0.4050525293010263,
      "grad_norm": 3.196676015853882,
      "learning_rate": 9.556903192230684e-05,
      "loss": 2.4798,
      "step": 6670
    },
    {
      "epoch": 0.40565980445739963,
      "grad_norm": 3.2260711193084717,
      "learning_rate": 9.55559362531539e-05,
      "loss": 2.502,
      "step": 6680
    },
    {
      "epoch": 0.406267079613773,
      "grad_norm": 2.8825058937072754,
      "learning_rate": 9.55428221604548e-05,
      "loss": 2.69,
      "step": 6690
    },
    {
      "epoch": 0.40687435477014633,
      "grad_norm": 2.5269992351531982,
      "learning_rate": 9.552968964951307e-05,
      "loss": 2.5891,
      "step": 6700
    },
    {
      "epoch": 0.4074816299265197,
      "grad_norm": 2.4735569953918457,
      "learning_rate": 9.551653872563975e-05,
      "loss": 2.9437,
      "step": 6710
    },
    {
      "epoch": 0.4080889050828931,
      "grad_norm": 3.534066915512085,
      "learning_rate": 9.550336939415323e-05,
      "loss": 2.766,
      "step": 6720
    },
    {
      "epoch": 0.4086961802392664,
      "grad_norm": 2.8585851192474365,
      "learning_rate": 9.549018166037943e-05,
      "loss": 2.4819,
      "step": 6730
    },
    {
      "epoch": 0.4093034553956398,
      "grad_norm": 2.6258301734924316,
      "learning_rate": 9.547697552965167e-05,
      "loss": 2.4516,
      "step": 6740
    },
    {
      "epoch": 0.4099107305520131,
      "grad_norm": 2.4010605812072754,
      "learning_rate": 9.546375100731073e-05,
      "loss": 2.6557,
      "step": 6750
    },
    {
      "epoch": 0.4105180057083865,
      "grad_norm": 2.8973047733306885,
      "learning_rate": 9.54505080987048e-05,
      "loss": 2.6761,
      "step": 6760
    },
    {
      "epoch": 0.4111252808647598,
      "grad_norm": 3.275704860687256,
      "learning_rate": 9.543724680918953e-05,
      "loss": 2.372,
      "step": 6770
    },
    {
      "epoch": 0.4117325560211332,
      "grad_norm": 1.9340436458587646,
      "learning_rate": 9.5423967144128e-05,
      "loss": 2.3927,
      "step": 6780
    },
    {
      "epoch": 0.41233983117750656,
      "grad_norm": 1.4836126565933228,
      "learning_rate": 9.541066910889071e-05,
      "loss": 2.5071,
      "step": 6790
    },
    {
      "epoch": 0.4129471063338799,
      "grad_norm": 2.805004596710205,
      "learning_rate": 9.539735270885562e-05,
      "loss": 2.4898,
      "step": 6800
    },
    {
      "epoch": 0.41355438149025325,
      "grad_norm": 2.3302114009857178,
      "learning_rate": 9.538401794940808e-05,
      "loss": 2.2788,
      "step": 6810
    },
    {
      "epoch": 0.4141616566466266,
      "grad_norm": 2.43017578125,
      "learning_rate": 9.537066483594086e-05,
      "loss": 2.2734,
      "step": 6820
    },
    {
      "epoch": 0.41476893180299995,
      "grad_norm": 1.5651804208755493,
      "learning_rate": 9.53572933738542e-05,
      "loss": 2.1212,
      "step": 6830
    },
    {
      "epoch": 0.4153762069593733,
      "grad_norm": 1.97849440574646,
      "learning_rate": 9.534390356855571e-05,
      "loss": 2.207,
      "step": 6840
    },
    {
      "epoch": 0.41598348211574665,
      "grad_norm": 2.558380126953125,
      "learning_rate": 9.533049542546046e-05,
      "loss": 2.4412,
      "step": 6850
    },
    {
      "epoch": 0.41659075727211997,
      "grad_norm": 1.959463119506836,
      "learning_rate": 9.531706894999091e-05,
      "loss": 2.3049,
      "step": 6860
    },
    {
      "epoch": 0.41719803242849335,
      "grad_norm": 1.793039321899414,
      "learning_rate": 9.530362414757694e-05,
      "loss": 2.329,
      "step": 6870
    },
    {
      "epoch": 0.4178053075848667,
      "grad_norm": 2.201921224594116,
      "learning_rate": 9.529016102365584e-05,
      "loss": 2.5112,
      "step": 6880
    },
    {
      "epoch": 0.41841258274124005,
      "grad_norm": 2.999833106994629,
      "learning_rate": 9.52766795836723e-05,
      "loss": 2.2657,
      "step": 6890
    },
    {
      "epoch": 0.4190198578976134,
      "grad_norm": 1.904390573501587,
      "learning_rate": 9.526317983307847e-05,
      "loss": 2.5569,
      "step": 6900
    },
    {
      "epoch": 0.41962713305398675,
      "grad_norm": 3.3410770893096924,
      "learning_rate": 9.524966177733382e-05,
      "loss": 2.524,
      "step": 6910
    },
    {
      "epoch": 0.4202344082103601,
      "grad_norm": 2.7304599285125732,
      "learning_rate": 9.523612542190528e-05,
      "loss": 2.7249,
      "step": 6920
    },
    {
      "epoch": 0.42084168336673344,
      "grad_norm": 3.790069103240967,
      "learning_rate": 9.522257077226717e-05,
      "loss": 2.128,
      "step": 6930
    },
    {
      "epoch": 0.4214489585231068,
      "grad_norm": 2.271986961364746,
      "learning_rate": 9.52089978339012e-05,
      "loss": 2.5377,
      "step": 6940
    },
    {
      "epoch": 0.4220562336794802,
      "grad_norm": 3.3071982860565186,
      "learning_rate": 9.519540661229651e-05,
      "loss": 2.6311,
      "step": 6950
    },
    {
      "epoch": 0.4226635088358535,
      "grad_norm": 4.1074724197387695,
      "learning_rate": 9.518179711294956e-05,
      "loss": 2.6324,
      "step": 6960
    },
    {
      "epoch": 0.4232707839922269,
      "grad_norm": 2.819727659225464,
      "learning_rate": 9.51681693413643e-05,
      "loss": 2.3383,
      "step": 6970
    },
    {
      "epoch": 0.4238780591486002,
      "grad_norm": 3.6407546997070312,
      "learning_rate": 9.515452330305198e-05,
      "loss": 2.4282,
      "step": 6980
    },
    {
      "epoch": 0.4244853343049736,
      "grad_norm": 3.0504987239837646,
      "learning_rate": 9.514085900353128e-05,
      "loss": 3.1475,
      "step": 6990
    },
    {
      "epoch": 0.4250926094613469,
      "grad_norm": 5.783546447753906,
      "learning_rate": 9.512717644832828e-05,
      "loss": 2.607,
      "step": 7000
    },
    {
      "epoch": 0.4256998846177203,
      "grad_norm": 3.1271045207977295,
      "learning_rate": 9.511347564297642e-05,
      "loss": 2.8311,
      "step": 7010
    },
    {
      "epoch": 0.42630715977409367,
      "grad_norm": 1.9040294885635376,
      "learning_rate": 9.509975659301649e-05,
      "loss": 2.7186,
      "step": 7020
    },
    {
      "epoch": 0.426914434930467,
      "grad_norm": 1.7764972448349,
      "learning_rate": 9.508601930399673e-05,
      "loss": 2.3434,
      "step": 7030
    },
    {
      "epoch": 0.42752171008684037,
      "grad_norm": 1.4445196390151978,
      "learning_rate": 9.50722637814727e-05,
      "loss": 2.0834,
      "step": 7040
    },
    {
      "epoch": 0.4281289852432137,
      "grad_norm": 1.4342163801193237,
      "learning_rate": 9.505849003100736e-05,
      "loss": 2.2241,
      "step": 7050
    },
    {
      "epoch": 0.42873626039958707,
      "grad_norm": 3.122532367706299,
      "learning_rate": 9.504469805817105e-05,
      "loss": 3.0578,
      "step": 7060
    },
    {
      "epoch": 0.4293435355559604,
      "grad_norm": 4.518110275268555,
      "learning_rate": 9.503088786854141e-05,
      "loss": 2.7352,
      "step": 7070
    },
    {
      "epoch": 0.42995081071233376,
      "grad_norm": 3.5267205238342285,
      "learning_rate": 9.501705946770356e-05,
      "loss": 2.629,
      "step": 7080
    },
    {
      "epoch": 0.4305580858687071,
      "grad_norm": 1.3078187704086304,
      "learning_rate": 9.50032128612499e-05,
      "loss": 2.3815,
      "step": 7090
    },
    {
      "epoch": 0.43116536102508046,
      "grad_norm": 2.044771909713745,
      "learning_rate": 9.498934805478021e-05,
      "loss": 2.4246,
      "step": 7100
    },
    {
      "epoch": 0.43177263618145384,
      "grad_norm": 3.038041591644287,
      "learning_rate": 9.497546505390167e-05,
      "loss": 2.7609,
      "step": 7110
    },
    {
      "epoch": 0.43237991133782716,
      "grad_norm": 2.6125526428222656,
      "learning_rate": 9.496156386422875e-05,
      "loss": 2.6718,
      "step": 7120
    },
    {
      "epoch": 0.43298718649420054,
      "grad_norm": 2.3341405391693115,
      "learning_rate": 9.494764449138331e-05,
      "loss": 2.3215,
      "step": 7130
    },
    {
      "epoch": 0.43359446165057386,
      "grad_norm": 2.62197208404541,
      "learning_rate": 9.493370694099462e-05,
      "loss": 2.4163,
      "step": 7140
    },
    {
      "epoch": 0.43420173680694724,
      "grad_norm": 2.6565566062927246,
      "learning_rate": 9.491975121869919e-05,
      "loss": 2.5138,
      "step": 7150
    },
    {
      "epoch": 0.43480901196332056,
      "grad_norm": 5.283926010131836,
      "learning_rate": 9.490577733014094e-05,
      "loss": 2.3474,
      "step": 7160
    },
    {
      "epoch": 0.43541628711969393,
      "grad_norm": 2.083625316619873,
      "learning_rate": 9.489178528097117e-05,
      "loss": 2.372,
      "step": 7170
    },
    {
      "epoch": 0.4360235622760673,
      "grad_norm": 3.0178773403167725,
      "learning_rate": 9.487777507684848e-05,
      "loss": 3.1013,
      "step": 7180
    },
    {
      "epoch": 0.43663083743244063,
      "grad_norm": 3.7908904552459717,
      "learning_rate": 9.486374672343878e-05,
      "loss": 2.7359,
      "step": 7190
    },
    {
      "epoch": 0.437238112588814,
      "grad_norm": 2.7196524143218994,
      "learning_rate": 9.484970022641541e-05,
      "loss": 2.4006,
      "step": 7200
    },
    {
      "epoch": 0.43784538774518733,
      "grad_norm": 4.560878753662109,
      "learning_rate": 9.483563559145898e-05,
      "loss": 2.6072,
      "step": 7210
    },
    {
      "epoch": 0.4384526629015607,
      "grad_norm": 3.3567259311676025,
      "learning_rate": 9.482155282425742e-05,
      "loss": 2.4336,
      "step": 7220
    },
    {
      "epoch": 0.43905993805793403,
      "grad_norm": 2.9329910278320312,
      "learning_rate": 9.480745193050607e-05,
      "loss": 2.641,
      "step": 7230
    },
    {
      "epoch": 0.4396672132143074,
      "grad_norm": 3.1050922870635986,
      "learning_rate": 9.479333291590753e-05,
      "loss": 3.2285,
      "step": 7240
    },
    {
      "epoch": 0.4402744883706807,
      "grad_norm": 5.255746841430664,
      "learning_rate": 9.477919578617176e-05,
      "loss": 2.5787,
      "step": 7250
    },
    {
      "epoch": 0.4408817635270541,
      "grad_norm": 3.5249173641204834,
      "learning_rate": 9.476504054701605e-05,
      "loss": 2.263,
      "step": 7260
    },
    {
      "epoch": 0.4414890386834275,
      "grad_norm": 1.4400535821914673,
      "learning_rate": 9.475086720416499e-05,
      "loss": 2.3638,
      "step": 7270
    },
    {
      "epoch": 0.4420963138398008,
      "grad_norm": 5.359386444091797,
      "learning_rate": 9.473667576335052e-05,
      "loss": 2.3326,
      "step": 7280
    },
    {
      "epoch": 0.4427035889961742,
      "grad_norm": 4.084681987762451,
      "learning_rate": 9.472246623031186e-05,
      "loss": 2.2995,
      "step": 7290
    },
    {
      "epoch": 0.4433108641525475,
      "grad_norm": 3.9391560554504395,
      "learning_rate": 9.470823861079561e-05,
      "loss": 2.7228,
      "step": 7300
    },
    {
      "epoch": 0.4439181393089209,
      "grad_norm": 7.8190155029296875,
      "learning_rate": 9.469399291055562e-05,
      "loss": 2.5351,
      "step": 7310
    },
    {
      "epoch": 0.4445254144652942,
      "grad_norm": 2.7113735675811768,
      "learning_rate": 9.467972913535308e-05,
      "loss": 2.9699,
      "step": 7320
    },
    {
      "epoch": 0.4451326896216676,
      "grad_norm": 3.1036882400512695,
      "learning_rate": 9.46654472909565e-05,
      "loss": 2.1459,
      "step": 7330
    },
    {
      "epoch": 0.44573996477804095,
      "grad_norm": 1.736986756324768,
      "learning_rate": 9.465114738314166e-05,
      "loss": 2.2287,
      "step": 7340
    },
    {
      "epoch": 0.4463472399344143,
      "grad_norm": 1.990052580833435,
      "learning_rate": 9.46368294176917e-05,
      "loss": 2.2948,
      "step": 7350
    },
    {
      "epoch": 0.44695451509078765,
      "grad_norm": 2.227933883666992,
      "learning_rate": 9.4622493400397e-05,
      "loss": 2.4761,
      "step": 7360
    },
    {
      "epoch": 0.447561790247161,
      "grad_norm": 2.3538217544555664,
      "learning_rate": 9.460813933705531e-05,
      "loss": 2.6532,
      "step": 7370
    },
    {
      "epoch": 0.44816906540353435,
      "grad_norm": 3.5950870513916016,
      "learning_rate": 9.459376723347161e-05,
      "loss": 2.4128,
      "step": 7380
    },
    {
      "epoch": 0.44877634055990767,
      "grad_norm": 1.4991557598114014,
      "learning_rate": 9.457937709545823e-05,
      "loss": 2.1169,
      "step": 7390
    },
    {
      "epoch": 0.44938361571628105,
      "grad_norm": 2.792180299758911,
      "learning_rate": 9.456496892883477e-05,
      "loss": 2.0965,
      "step": 7400
    },
    {
      "epoch": 0.4499908908726544,
      "grad_norm": 2.634004592895508,
      "learning_rate": 9.455054273942811e-05,
      "loss": 2.5012,
      "step": 7410
    },
    {
      "epoch": 0.45059816602902775,
      "grad_norm": 4.450744152069092,
      "learning_rate": 9.453609853307244e-05,
      "loss": 2.723,
      "step": 7420
    },
    {
      "epoch": 0.4512054411854011,
      "grad_norm": 2.6657700538635254,
      "learning_rate": 9.452163631560922e-05,
      "loss": 2.6843,
      "step": 7430
    },
    {
      "epoch": 0.45181271634177445,
      "grad_norm": 2.125448226928711,
      "learning_rate": 9.45071560928872e-05,
      "loss": 2.7006,
      "step": 7440
    },
    {
      "epoch": 0.4524199914981478,
      "grad_norm": 3.011261463165283,
      "learning_rate": 9.449265787076243e-05,
      "loss": 2.5433,
      "step": 7450
    },
    {
      "epoch": 0.45302726665452114,
      "grad_norm": 3.3293473720550537,
      "learning_rate": 9.44781416550982e-05,
      "loss": 2.6216,
      "step": 7460
    },
    {
      "epoch": 0.4536345418108945,
      "grad_norm": 3.0404953956604004,
      "learning_rate": 9.446360745176511e-05,
      "loss": 2.4369,
      "step": 7470
    },
    {
      "epoch": 0.45424181696726784,
      "grad_norm": 2.4808309078216553,
      "learning_rate": 9.444905526664103e-05,
      "loss": 2.7247,
      "step": 7480
    },
    {
      "epoch": 0.4548490921236412,
      "grad_norm": 3.364408254623413,
      "learning_rate": 9.443448510561109e-05,
      "loss": 2.6495,
      "step": 7490
    },
    {
      "epoch": 0.4554563672800146,
      "grad_norm": 3.59797739982605,
      "learning_rate": 9.441989697456767e-05,
      "loss": 2.4655,
      "step": 7500
    },
    {
      "epoch": 0.4560636424363879,
      "grad_norm": 2.721339464187622,
      "learning_rate": 9.440529087941047e-05,
      "loss": 2.857,
      "step": 7510
    },
    {
      "epoch": 0.4566709175927613,
      "grad_norm": 2.698099136352539,
      "learning_rate": 9.439066682604643e-05,
      "loss": 2.5713,
      "step": 7520
    },
    {
      "epoch": 0.4572781927491346,
      "grad_norm": 2.32311749458313,
      "learning_rate": 9.437602482038974e-05,
      "loss": 2.5252,
      "step": 7530
    },
    {
      "epoch": 0.457885467905508,
      "grad_norm": 2.642758846282959,
      "learning_rate": 9.436136486836186e-05,
      "loss": 2.2922,
      "step": 7540
    },
    {
      "epoch": 0.4584927430618813,
      "grad_norm": 2.9288697242736816,
      "learning_rate": 9.43466869758915e-05,
      "loss": 2.2564,
      "step": 7550
    },
    {
      "epoch": 0.4591000182182547,
      "grad_norm": 1.605661153793335,
      "learning_rate": 9.433199114891467e-05,
      "loss": 2.2749,
      "step": 7560
    },
    {
      "epoch": 0.45970729337462807,
      "grad_norm": 6.775206565856934,
      "learning_rate": 9.431727739337454e-05,
      "loss": 2.8181,
      "step": 7570
    },
    {
      "epoch": 0.4603145685310014,
      "grad_norm": 2.811607599258423,
      "learning_rate": 9.430254571522163e-05,
      "loss": 2.5018,
      "step": 7580
    },
    {
      "epoch": 0.46092184368737477,
      "grad_norm": 2.536494016647339,
      "learning_rate": 9.428779612041368e-05,
      "loss": 2.5337,
      "step": 7590
    },
    {
      "epoch": 0.4615291188437481,
      "grad_norm": 2.322028875350952,
      "learning_rate": 9.427302861491561e-05,
      "loss": 2.8019,
      "step": 7600
    },
    {
      "epoch": 0.46213639400012146,
      "grad_norm": 6.6271071434021,
      "learning_rate": 9.425824320469964e-05,
      "loss": 2.3037,
      "step": 7610
    },
    {
      "epoch": 0.4627436691564948,
      "grad_norm": 2.064951181411743,
      "learning_rate": 9.424343989574526e-05,
      "loss": 2.0578,
      "step": 7620
    },
    {
      "epoch": 0.46335094431286816,
      "grad_norm": 2.8219897747039795,
      "learning_rate": 9.422861869403916e-05,
      "loss": 2.4841,
      "step": 7630
    },
    {
      "epoch": 0.46395821946924154,
      "grad_norm": 3.9589405059814453,
      "learning_rate": 9.421377960557525e-05,
      "loss": 2.6274,
      "step": 7640
    },
    {
      "epoch": 0.46456549462561486,
      "grad_norm": 3.2575364112854004,
      "learning_rate": 9.419892263635468e-05,
      "loss": 2.5832,
      "step": 7650
    },
    {
      "epoch": 0.46517276978198824,
      "grad_norm": 3.0187571048736572,
      "learning_rate": 9.41840477923859e-05,
      "loss": 2.8226,
      "step": 7660
    },
    {
      "epoch": 0.46578004493836156,
      "grad_norm": 3.099646806716919,
      "learning_rate": 9.416915507968449e-05,
      "loss": 2.9858,
      "step": 7670
    },
    {
      "epoch": 0.46638732009473494,
      "grad_norm": 3.9471912384033203,
      "learning_rate": 9.41542445042733e-05,
      "loss": 2.4171,
      "step": 7680
    },
    {
      "epoch": 0.46699459525110826,
      "grad_norm": 1.6442005634307861,
      "learning_rate": 9.41393160721824e-05,
      "loss": 2.3506,
      "step": 7690
    },
    {
      "epoch": 0.46760187040748163,
      "grad_norm": 2.0049915313720703,
      "learning_rate": 9.412436978944912e-05,
      "loss": 2.2025,
      "step": 7700
    },
    {
      "epoch": 0.46820914556385496,
      "grad_norm": 2.3059773445129395,
      "learning_rate": 9.410940566211797e-05,
      "loss": 2.459,
      "step": 7710
    },
    {
      "epoch": 0.46881642072022833,
      "grad_norm": 2.8079028129577637,
      "learning_rate": 9.409442369624065e-05,
      "loss": 2.5502,
      "step": 7720
    },
    {
      "epoch": 0.4694236958766017,
      "grad_norm": 4.0988640785217285,
      "learning_rate": 9.40794238978761e-05,
      "loss": 2.5722,
      "step": 7730
    },
    {
      "epoch": 0.47003097103297503,
      "grad_norm": 2.1008057594299316,
      "learning_rate": 9.406440627309053e-05,
      "loss": 2.5841,
      "step": 7740
    },
    {
      "epoch": 0.4706382461893484,
      "grad_norm": 2.213907241821289,
      "learning_rate": 9.404937082795726e-05,
      "loss": 2.3623,
      "step": 7750
    },
    {
      "epoch": 0.47124552134572173,
      "grad_norm": 1.881065845489502,
      "learning_rate": 9.40343175685569e-05,
      "loss": 2.4822,
      "step": 7760
    },
    {
      "epoch": 0.4718527965020951,
      "grad_norm": 2.2961854934692383,
      "learning_rate": 9.401924650097718e-05,
      "loss": 2.4485,
      "step": 7770
    },
    {
      "epoch": 0.47246007165846843,
      "grad_norm": 2.2445356845855713,
      "learning_rate": 9.400415763131312e-05,
      "loss": 2.3201,
      "step": 7780
    },
    {
      "epoch": 0.4730673468148418,
      "grad_norm": 2.4774019718170166,
      "learning_rate": 9.398905096566688e-05,
      "loss": 2.4478,
      "step": 7790
    },
    {
      "epoch": 0.4736746219712152,
      "grad_norm": 3.193199872970581,
      "learning_rate": 9.397392651014785e-05,
      "loss": 2.7434,
      "step": 7800
    },
    {
      "epoch": 0.4742818971275885,
      "grad_norm": 2.8147432804107666,
      "learning_rate": 9.39587842708726e-05,
      "loss": 2.4104,
      "step": 7810
    },
    {
      "epoch": 0.4748891722839619,
      "grad_norm": 2.2249224185943604,
      "learning_rate": 9.394362425396486e-05,
      "loss": 2.5148,
      "step": 7820
    },
    {
      "epoch": 0.4754964474403352,
      "grad_norm": 2.6423826217651367,
      "learning_rate": 9.392844646555563e-05,
      "loss": 2.1811,
      "step": 7830
    },
    {
      "epoch": 0.4761037225967086,
      "grad_norm": 2.9791035652160645,
      "learning_rate": 9.391325091178303e-05,
      "loss": 2.4796,
      "step": 7840
    },
    {
      "epoch": 0.4767109977530819,
      "grad_norm": 2.477844476699829,
      "learning_rate": 9.389803759879237e-05,
      "loss": 2.363,
      "step": 7850
    },
    {
      "epoch": 0.4773182729094553,
      "grad_norm": 2.568680763244629,
      "learning_rate": 9.388280653273617e-05,
      "loss": 2.8172,
      "step": 7860
    },
    {
      "epoch": 0.47792554806582865,
      "grad_norm": 3.1237146854400635,
      "learning_rate": 9.386755771977412e-05,
      "loss": 2.7337,
      "step": 7870
    },
    {
      "epoch": 0.478532823222202,
      "grad_norm": 2.9446589946746826,
      "learning_rate": 9.385229116607306e-05,
      "loss": 2.4484,
      "step": 7880
    },
    {
      "epoch": 0.47914009837857535,
      "grad_norm": 2.2867212295532227,
      "learning_rate": 9.383700687780706e-05,
      "loss": 2.1593,
      "step": 7890
    },
    {
      "epoch": 0.4797473735349487,
      "grad_norm": 2.8926541805267334,
      "learning_rate": 9.382170486115728e-05,
      "loss": 2.3686,
      "step": 7900
    },
    {
      "epoch": 0.48035464869132205,
      "grad_norm": 1.5645568370819092,
      "learning_rate": 9.380638512231216e-05,
      "loss": 2.1285,
      "step": 7910
    },
    {
      "epoch": 0.48096192384769537,
      "grad_norm": 1.7086042165756226,
      "learning_rate": 9.379104766746722e-05,
      "loss": 2.3893,
      "step": 7920
    },
    {
      "epoch": 0.48156919900406875,
      "grad_norm": 2.87491512298584,
      "learning_rate": 9.377569250282517e-05,
      "loss": 2.5674,
      "step": 7930
    },
    {
      "epoch": 0.48217647416044207,
      "grad_norm": 2.034264087677002,
      "learning_rate": 9.376031963459589e-05,
      "loss": 2.9346,
      "step": 7940
    },
    {
      "epoch": 0.48278374931681545,
      "grad_norm": 4.426326751708984,
      "learning_rate": 9.37449290689964e-05,
      "loss": 2.5844,
      "step": 7950
    },
    {
      "epoch": 0.4833910244731888,
      "grad_norm": 2.483025312423706,
      "learning_rate": 9.372952081225088e-05,
      "loss": 2.9293,
      "step": 7960
    },
    {
      "epoch": 0.48399829962956215,
      "grad_norm": 4.655477523803711,
      "learning_rate": 9.371409487059069e-05,
      "loss": 3.0947,
      "step": 7970
    },
    {
      "epoch": 0.4846055747859355,
      "grad_norm": 4.066415786743164,
      "learning_rate": 9.369865125025435e-05,
      "loss": 2.9336,
      "step": 7980
    },
    {
      "epoch": 0.48521284994230884,
      "grad_norm": 4.145140647888184,
      "learning_rate": 9.368318995748746e-05,
      "loss": 2.6267,
      "step": 7990
    },
    {
      "epoch": 0.4858201250986822,
      "grad_norm": 2.0782315731048584,
      "learning_rate": 9.366771099854283e-05,
      "loss": 2.837,
      "step": 8000
    },
    {
      "epoch": 0.48642740025505554,
      "grad_norm": 4.426705837249756,
      "learning_rate": 9.365221437968042e-05,
      "loss": 2.5609,
      "step": 8010
    },
    {
      "epoch": 0.4870346754114289,
      "grad_norm": 3.0449514389038086,
      "learning_rate": 9.363670010716729e-05,
      "loss": 2.8438,
      "step": 8020
    },
    {
      "epoch": 0.4876419505678023,
      "grad_norm": 2.9630210399627686,
      "learning_rate": 9.362116818727767e-05,
      "loss": 2.4277,
      "step": 8030
    },
    {
      "epoch": 0.4882492257241756,
      "grad_norm": 2.468177556991577,
      "learning_rate": 9.360561862629287e-05,
      "loss": 2.7228,
      "step": 8040
    },
    {
      "epoch": 0.488856500880549,
      "grad_norm": 5.127236366271973,
      "learning_rate": 9.359005143050146e-05,
      "loss": 2.6297,
      "step": 8050
    },
    {
      "epoch": 0.4894637760369223,
      "grad_norm": 2.267310857772827,
      "learning_rate": 9.3574466606199e-05,
      "loss": 2.3677,
      "step": 8060
    },
    {
      "epoch": 0.4900710511932957,
      "grad_norm": 2.511033296585083,
      "learning_rate": 9.355886415968827e-05,
      "loss": 2.7575,
      "step": 8070
    },
    {
      "epoch": 0.490678326349669,
      "grad_norm": 2.1425158977508545,
      "learning_rate": 9.354324409727911e-05,
      "loss": 2.4077,
      "step": 8080
    },
    {
      "epoch": 0.4912856015060424,
      "grad_norm": 2.956350326538086,
      "learning_rate": 9.352760642528857e-05,
      "loss": 2.4938,
      "step": 8090
    },
    {
      "epoch": 0.49189287666241577,
      "grad_norm": 2.9924001693725586,
      "learning_rate": 9.351195115004076e-05,
      "loss": 2.5948,
      "step": 8100
    },
    {
      "epoch": 0.4925001518187891,
      "grad_norm": 2.2809035778045654,
      "learning_rate": 9.349627827786691e-05,
      "loss": 2.39,
      "step": 8110
    },
    {
      "epoch": 0.49310742697516247,
      "grad_norm": 2.567007303237915,
      "learning_rate": 9.348058781510538e-05,
      "loss": 2.8342,
      "step": 8120
    },
    {
      "epoch": 0.4937147021315358,
      "grad_norm": 3.279766798019409,
      "learning_rate": 9.346487976810166e-05,
      "loss": 2.4439,
      "step": 8130
    },
    {
      "epoch": 0.49432197728790916,
      "grad_norm": 2.2140555381774902,
      "learning_rate": 9.344915414320831e-05,
      "loss": 2.5409,
      "step": 8140
    },
    {
      "epoch": 0.4949292524442825,
      "grad_norm": 3.304486036300659,
      "learning_rate": 9.343341094678504e-05,
      "loss": 2.8112,
      "step": 8150
    },
    {
      "epoch": 0.49553652760065586,
      "grad_norm": 3.662257432937622,
      "learning_rate": 9.341765018519865e-05,
      "loss": 2.3601,
      "step": 8160
    },
    {
      "epoch": 0.4961438027570292,
      "grad_norm": 1.7995479106903076,
      "learning_rate": 9.340187186482304e-05,
      "loss": 2.4799,
      "step": 8170
    },
    {
      "epoch": 0.49675107791340256,
      "grad_norm": 1.566380500793457,
      "learning_rate": 9.338607599203919e-05,
      "loss": 2.6543,
      "step": 8180
    },
    {
      "epoch": 0.49735835306977594,
      "grad_norm": 4.468143463134766,
      "learning_rate": 9.337026257323524e-05,
      "loss": 2.4874,
      "step": 8190
    },
    {
      "epoch": 0.49796562822614926,
      "grad_norm": 2.3735294342041016,
      "learning_rate": 9.33544316148064e-05,
      "loss": 2.4208,
      "step": 8200
    },
    {
      "epoch": 0.49857290338252264,
      "grad_norm": 3.549135208129883,
      "learning_rate": 9.333858312315489e-05,
      "loss": 2.7751,
      "step": 8210
    },
    {
      "epoch": 0.49918017853889596,
      "grad_norm": 5.2206220626831055,
      "learning_rate": 9.332271710469016e-05,
      "loss": 2.5915,
      "step": 8220
    },
    {
      "epoch": 0.49978745369526933,
      "grad_norm": 2.187800645828247,
      "learning_rate": 9.330683356582866e-05,
      "loss": 2.2875,
      "step": 8230
    },
    {
      "epoch": 0.5003947288516427,
      "grad_norm": 2.0768632888793945,
      "learning_rate": 9.329093251299393e-05,
      "loss": 2.0601,
      "step": 8240
    },
    {
      "epoch": 0.501002004008016,
      "grad_norm": 3.2875900268554688,
      "learning_rate": 9.327501395261664e-05,
      "loss": 2.7665,
      "step": 8250
    },
    {
      "epoch": 0.5016092791643894,
      "grad_norm": 2.3767271041870117,
      "learning_rate": 9.325907789113448e-05,
      "loss": 2.7249,
      "step": 8260
    },
    {
      "epoch": 0.5022165543207627,
      "grad_norm": 2.4210739135742188,
      "learning_rate": 9.324312433499227e-05,
      "loss": 2.2586,
      "step": 8270
    },
    {
      "epoch": 0.5028238294771361,
      "grad_norm": 1.5918452739715576,
      "learning_rate": 9.322715329064187e-05,
      "loss": 2.27,
      "step": 8280
    },
    {
      "epoch": 0.5034311046335095,
      "grad_norm": 2.8737852573394775,
      "learning_rate": 9.321116476454222e-05,
      "loss": 2.336,
      "step": 8290
    },
    {
      "epoch": 0.5040383797898828,
      "grad_norm": 1.7413668632507324,
      "learning_rate": 9.319515876315934e-05,
      "loss": 2.5603,
      "step": 8300
    },
    {
      "epoch": 0.5046456549462561,
      "grad_norm": 2.4790902137756348,
      "learning_rate": 9.317913529296631e-05,
      "loss": 2.4584,
      "step": 8310
    },
    {
      "epoch": 0.5052529301026295,
      "grad_norm": 2.080993413925171,
      "learning_rate": 9.316309436044328e-05,
      "loss": 2.3901,
      "step": 8320
    },
    {
      "epoch": 0.5058602052590029,
      "grad_norm": 2.425281047821045,
      "learning_rate": 9.314703597207747e-05,
      "loss": 2.0926,
      "step": 8330
    },
    {
      "epoch": 0.5064674804153763,
      "grad_norm": 1.6941583156585693,
      "learning_rate": 9.313096013436313e-05,
      "loss": 2.3259,
      "step": 8340
    },
    {
      "epoch": 0.5070747555717495,
      "grad_norm": 2.405672073364258,
      "learning_rate": 9.311486685380158e-05,
      "loss": 2.5686,
      "step": 8350
    },
    {
      "epoch": 0.5076820307281229,
      "grad_norm": 2.3601489067077637,
      "learning_rate": 9.309875613690122e-05,
      "loss": 2.2101,
      "step": 8360
    },
    {
      "epoch": 0.5082893058844963,
      "grad_norm": 2.2470037937164307,
      "learning_rate": 9.308262799017748e-05,
      "loss": 2.5983,
      "step": 8370
    },
    {
      "epoch": 0.5088965810408697,
      "grad_norm": 3.8188443183898926,
      "learning_rate": 9.306648242015281e-05,
      "loss": 2.7569,
      "step": 8380
    },
    {
      "epoch": 0.5095038561972429,
      "grad_norm": 3.3722052574157715,
      "learning_rate": 9.305031943335679e-05,
      "loss": 2.6754,
      "step": 8390
    },
    {
      "epoch": 0.5101111313536163,
      "grad_norm": 2.8479950428009033,
      "learning_rate": 9.303413903632591e-05,
      "loss": 2.8225,
      "step": 8400
    },
    {
      "epoch": 0.5107184065099897,
      "grad_norm": 4.241398811340332,
      "learning_rate": 9.301794123560384e-05,
      "loss": 2.5676,
      "step": 8410
    },
    {
      "epoch": 0.511325681666363,
      "grad_norm": 2.8258047103881836,
      "learning_rate": 9.300172603774123e-05,
      "loss": 2.9235,
      "step": 8420
    },
    {
      "epoch": 0.5119329568227364,
      "grad_norm": 3.3932950496673584,
      "learning_rate": 9.298549344929573e-05,
      "loss": 2.8427,
      "step": 8430
    },
    {
      "epoch": 0.5125402319791097,
      "grad_norm": 3.3268444538116455,
      "learning_rate": 9.296924347683208e-05,
      "loss": 2.7745,
      "step": 8440
    },
    {
      "epoch": 0.5131475071354831,
      "grad_norm": 4.169103145599365,
      "learning_rate": 9.295297612692202e-05,
      "loss": 2.6378,
      "step": 8450
    },
    {
      "epoch": 0.5137547822918564,
      "grad_norm": 2.4215517044067383,
      "learning_rate": 9.293669140614433e-05,
      "loss": 2.2369,
      "step": 8460
    },
    {
      "epoch": 0.5143620574482298,
      "grad_norm": 3.521707057952881,
      "learning_rate": 9.29203893210848e-05,
      "loss": 3.0993,
      "step": 8470
    },
    {
      "epoch": 0.5149693326046031,
      "grad_norm": 4.297608852386475,
      "learning_rate": 9.290406987833629e-05,
      "loss": 2.4451,
      "step": 8480
    },
    {
      "epoch": 0.5155766077609765,
      "grad_norm": 2.7457594871520996,
      "learning_rate": 9.288773308449859e-05,
      "loss": 2.5904,
      "step": 8490
    },
    {
      "epoch": 0.5161838829173498,
      "grad_norm": 3.168519973754883,
      "learning_rate": 9.287137894617858e-05,
      "loss": 2.6114,
      "step": 8500
    },
    {
      "epoch": 0.5167911580737232,
      "grad_norm": 2.792043685913086,
      "learning_rate": 9.285500746999014e-05,
      "loss": 2.6976,
      "step": 8510
    },
    {
      "epoch": 0.5173984332300966,
      "grad_norm": 2.657944679260254,
      "learning_rate": 9.283861866255416e-05,
      "loss": 2.7227,
      "step": 8520
    },
    {
      "epoch": 0.5180057083864699,
      "grad_norm": 3.365320920944214,
      "learning_rate": 9.282221253049853e-05,
      "loss": 2.4333,
      "step": 8530
    },
    {
      "epoch": 0.5186129835428432,
      "grad_norm": 2.5189900398254395,
      "learning_rate": 9.280578908045814e-05,
      "loss": 2.3042,
      "step": 8540
    },
    {
      "epoch": 0.5192202586992166,
      "grad_norm": 2.487299680709839,
      "learning_rate": 9.278934831907491e-05,
      "loss": 2.5268,
      "step": 8550
    },
    {
      "epoch": 0.51982753385559,
      "grad_norm": 3.208329677581787,
      "learning_rate": 9.277289025299773e-05,
      "loss": 2.2965,
      "step": 8560
    },
    {
      "epoch": 0.5204348090119634,
      "grad_norm": 2.99489426612854,
      "learning_rate": 9.275641488888253e-05,
      "loss": 2.5589,
      "step": 8570
    },
    {
      "epoch": 0.5210420841683366,
      "grad_norm": 2.725825548171997,
      "learning_rate": 9.273992223339219e-05,
      "loss": 2.6164,
      "step": 8580
    },
    {
      "epoch": 0.52164935932471,
      "grad_norm": 3.0565147399902344,
      "learning_rate": 9.27234122931966e-05,
      "loss": 2.5928,
      "step": 8590
    },
    {
      "epoch": 0.5222566344810834,
      "grad_norm": 2.9571340084075928,
      "learning_rate": 9.270688507497265e-05,
      "loss": 2.4654,
      "step": 8600
    },
    {
      "epoch": 0.5228639096374568,
      "grad_norm": 1.6460801362991333,
      "learning_rate": 9.26903405854042e-05,
      "loss": 2.2962,
      "step": 8610
    },
    {
      "epoch": 0.52347118479383,
      "grad_norm": 2.196638584136963,
      "learning_rate": 9.267377883118214e-05,
      "loss": 2.3665,
      "step": 8620
    },
    {
      "epoch": 0.5240784599502034,
      "grad_norm": 3.373619318008423,
      "learning_rate": 9.265719981900424e-05,
      "loss": 2.8717,
      "step": 8630
    },
    {
      "epoch": 0.5246857351065768,
      "grad_norm": 5.956878662109375,
      "learning_rate": 9.264060355557539e-05,
      "loss": 2.7637,
      "step": 8640
    },
    {
      "epoch": 0.5252930102629502,
      "grad_norm": 3.629153251647949,
      "learning_rate": 9.262399004760734e-05,
      "loss": 2.6365,
      "step": 8650
    },
    {
      "epoch": 0.5259002854193235,
      "grad_norm": 3.097426176071167,
      "learning_rate": 9.260735930181888e-05,
      "loss": 2.3189,
      "step": 8660
    },
    {
      "epoch": 0.5265075605756968,
      "grad_norm": 1.7411129474639893,
      "learning_rate": 9.259071132493571e-05,
      "loss": 2.2004,
      "step": 8670
    },
    {
      "epoch": 0.5271148357320702,
      "grad_norm": 2.9429659843444824,
      "learning_rate": 9.257404612369059e-05,
      "loss": 2.318,
      "step": 8680
    },
    {
      "epoch": 0.5277221108884436,
      "grad_norm": 2.6911280155181885,
      "learning_rate": 9.255736370482315e-05,
      "loss": 2.3571,
      "step": 8690
    },
    {
      "epoch": 0.5283293860448169,
      "grad_norm": 3.3159782886505127,
      "learning_rate": 9.254066407508005e-05,
      "loss": 2.7079,
      "step": 8700
    },
    {
      "epoch": 0.5289366612011902,
      "grad_norm": 2.239410161972046,
      "learning_rate": 9.252394724121486e-05,
      "loss": 2.7012,
      "step": 8710
    },
    {
      "epoch": 0.5295439363575636,
      "grad_norm": 3.213658094406128,
      "learning_rate": 9.250721320998819e-05,
      "loss": 2.8355,
      "step": 8720
    },
    {
      "epoch": 0.530151211513937,
      "grad_norm": 3.4942750930786133,
      "learning_rate": 9.249046198816749e-05,
      "loss": 2.2983,
      "step": 8730
    },
    {
      "epoch": 0.5307584866703103,
      "grad_norm": 2.6915221214294434,
      "learning_rate": 9.247369358252723e-05,
      "loss": 3.0905,
      "step": 8740
    },
    {
      "epoch": 0.5313657618266837,
      "grad_norm": 2.9591939449310303,
      "learning_rate": 9.245690799984885e-05,
      "loss": 2.5459,
      "step": 8750
    },
    {
      "epoch": 0.531973036983057,
      "grad_norm": 1.5241374969482422,
      "learning_rate": 9.244010524692068e-05,
      "loss": 1.9899,
      "step": 8760
    },
    {
      "epoch": 0.5325803121394304,
      "grad_norm": 1.7669579982757568,
      "learning_rate": 9.242328533053804e-05,
      "loss": 2.1031,
      "step": 8770
    },
    {
      "epoch": 0.5331875872958037,
      "grad_norm": 1.575698733329773,
      "learning_rate": 9.240644825750315e-05,
      "loss": 2.4433,
      "step": 8780
    },
    {
      "epoch": 0.5337948624521771,
      "grad_norm": 2.6101441383361816,
      "learning_rate": 9.23895940346252e-05,
      "loss": 2.4171,
      "step": 8790
    },
    {
      "epoch": 0.5344021376085505,
      "grad_norm": 1.8948440551757812,
      "learning_rate": 9.237272266872032e-05,
      "loss": 2.544,
      "step": 8800
    },
    {
      "epoch": 0.5350094127649238,
      "grad_norm": 2.173553705215454,
      "learning_rate": 9.235583416661154e-05,
      "loss": 2.5664,
      "step": 8810
    },
    {
      "epoch": 0.5356166879212971,
      "grad_norm": 1.9105868339538574,
      "learning_rate": 9.233892853512887e-05,
      "loss": 2.6918,
      "step": 8820
    },
    {
      "epoch": 0.5362239630776705,
      "grad_norm": 2.5718681812286377,
      "learning_rate": 9.232200578110917e-05,
      "loss": 2.6601,
      "step": 8830
    },
    {
      "epoch": 0.5368312382340439,
      "grad_norm": 1.9243773221969604,
      "learning_rate": 9.23050659113963e-05,
      "loss": 2.3174,
      "step": 8840
    },
    {
      "epoch": 0.5374385133904171,
      "grad_norm": 2.3969576358795166,
      "learning_rate": 9.2288108932841e-05,
      "loss": 2.3395,
      "step": 8850
    },
    {
      "epoch": 0.5380457885467905,
      "grad_norm": 1.5673489570617676,
      "learning_rate": 9.227113485230095e-05,
      "loss": 2.2778,
      "step": 8860
    },
    {
      "epoch": 0.5386530637031639,
      "grad_norm": 3.3187739849090576,
      "learning_rate": 9.225414367664076e-05,
      "loss": 2.4104,
      "step": 8870
    },
    {
      "epoch": 0.5392603388595373,
      "grad_norm": 2.0055112838745117,
      "learning_rate": 9.22371354127319e-05,
      "loss": 2.7621,
      "step": 8880
    },
    {
      "epoch": 0.5398676140159107,
      "grad_norm": 2.786675453186035,
      "learning_rate": 9.222011006745279e-05,
      "loss": 2.6655,
      "step": 8890
    },
    {
      "epoch": 0.5404748891722839,
      "grad_norm": 2.4771339893341064,
      "learning_rate": 9.220306764768876e-05,
      "loss": 2.7034,
      "step": 8900
    },
    {
      "epoch": 0.5410821643286573,
      "grad_norm": 3.0360453128814697,
      "learning_rate": 9.2186008160332e-05,
      "loss": 2.4427,
      "step": 8910
    },
    {
      "epoch": 0.5416894394850307,
      "grad_norm": 2.101039171218872,
      "learning_rate": 9.21689316122817e-05,
      "loss": 2.2527,
      "step": 8920
    },
    {
      "epoch": 0.542296714641404,
      "grad_norm": 3.0840237140655518,
      "learning_rate": 9.215183801044385e-05,
      "loss": 2.4056,
      "step": 8930
    },
    {
      "epoch": 0.5429039897977773,
      "grad_norm": 3.750338315963745,
      "learning_rate": 9.213472736173139e-05,
      "loss": 2.1872,
      "step": 8940
    },
    {
      "epoch": 0.5435112649541507,
      "grad_norm": 1.6616802215576172,
      "learning_rate": 9.211759967306412e-05,
      "loss": 2.5448,
      "step": 8950
    },
    {
      "epoch": 0.5441185401105241,
      "grad_norm": 2.598639726638794,
      "learning_rate": 9.210045495136876e-05,
      "loss": 2.427,
      "step": 8960
    },
    {
      "epoch": 0.5447258152668975,
      "grad_norm": 2.9213547706604004,
      "learning_rate": 9.208329320357891e-05,
      "loss": 2.9029,
      "step": 8970
    },
    {
      "epoch": 0.5453330904232708,
      "grad_norm": 2.659151554107666,
      "learning_rate": 9.206611443663506e-05,
      "loss": 2.5062,
      "step": 8980
    },
    {
      "epoch": 0.5459403655796441,
      "grad_norm": 2.189408302307129,
      "learning_rate": 9.204891865748457e-05,
      "loss": 2.4944,
      "step": 8990
    },
    {
      "epoch": 0.5465476407360175,
      "grad_norm": 3.2065200805664062,
      "learning_rate": 9.203170587308169e-05,
      "loss": 2.2783,
      "step": 9000
    },
    {
      "epoch": 0.5471549158923908,
      "grad_norm": 3.0260839462280273,
      "learning_rate": 9.201447609038754e-05,
      "loss": 2.8667,
      "step": 9010
    },
    {
      "epoch": 0.5477621910487642,
      "grad_norm": 3.9019992351531982,
      "learning_rate": 9.19972293163701e-05,
      "loss": 2.2164,
      "step": 9020
    },
    {
      "epoch": 0.5483694662051376,
      "grad_norm": 2.112363338470459,
      "learning_rate": 9.197996555800427e-05,
      "loss": 2.4123,
      "step": 9030
    },
    {
      "epoch": 0.5489767413615109,
      "grad_norm": 2.479797124862671,
      "learning_rate": 9.196268482227179e-05,
      "loss": 2.509,
      "step": 9040
    },
    {
      "epoch": 0.5495840165178842,
      "grad_norm": 2.725674629211426,
      "learning_rate": 9.194538711616126e-05,
      "loss": 2.3684,
      "step": 9050
    },
    {
      "epoch": 0.5501912916742576,
      "grad_norm": 4.0709662437438965,
      "learning_rate": 9.192807244666811e-05,
      "loss": 2.5961,
      "step": 9060
    },
    {
      "epoch": 0.550798566830631,
      "grad_norm": 3.7287497520446777,
      "learning_rate": 9.191074082079472e-05,
      "loss": 2.9992,
      "step": 9070
    },
    {
      "epoch": 0.5514058419870043,
      "grad_norm": 4.769763469696045,
      "learning_rate": 9.189339224555025e-05,
      "loss": 2.6328,
      "step": 9080
    },
    {
      "epoch": 0.5520131171433776,
      "grad_norm": 4.153842449188232,
      "learning_rate": 9.187602672795074e-05,
      "loss": 2.5317,
      "step": 9090
    },
    {
      "epoch": 0.552620392299751,
      "grad_norm": 2.487879753112793,
      "learning_rate": 9.18586442750191e-05,
      "loss": 2.0679,
      "step": 9100
    },
    {
      "epoch": 0.5532276674561244,
      "grad_norm": 2.6940436363220215,
      "learning_rate": 9.184124489378505e-05,
      "loss": 2.316,
      "step": 9110
    },
    {
      "epoch": 0.5538349426124978,
      "grad_norm": 3.454097270965576,
      "learning_rate": 9.182382859128518e-05,
      "loss": 2.5558,
      "step": 9120
    },
    {
      "epoch": 0.554442217768871,
      "grad_norm": 2.3994202613830566,
      "learning_rate": 9.180639537456293e-05,
      "loss": 2.4688,
      "step": 9130
    },
    {
      "epoch": 0.5550494929252444,
      "grad_norm": 2.5144002437591553,
      "learning_rate": 9.178894525066857e-05,
      "loss": 2.6252,
      "step": 9140
    },
    {
      "epoch": 0.5556567680816178,
      "grad_norm": 3.087794303894043,
      "learning_rate": 9.177147822665919e-05,
      "loss": 2.5555,
      "step": 9150
    },
    {
      "epoch": 0.5562640432379912,
      "grad_norm": 2.323120594024658,
      "learning_rate": 9.175399430959877e-05,
      "loss": 2.4064,
      "step": 9160
    },
    {
      "epoch": 0.5568713183943644,
      "grad_norm": 2.3239688873291016,
      "learning_rate": 9.173649350655804e-05,
      "loss": 2.1732,
      "step": 9170
    },
    {
      "epoch": 0.5574785935507378,
      "grad_norm": 3.296776056289673,
      "learning_rate": 9.171897582461461e-05,
      "loss": 2.4186,
      "step": 9180
    },
    {
      "epoch": 0.5580858687071112,
      "grad_norm": 1.9341621398925781,
      "learning_rate": 9.170144127085296e-05,
      "loss": 2.3931,
      "step": 9190
    },
    {
      "epoch": 0.5586931438634846,
      "grad_norm": 1.985984444618225,
      "learning_rate": 9.168388985236428e-05,
      "loss": 2.4363,
      "step": 9200
    },
    {
      "epoch": 0.5593004190198579,
      "grad_norm": 2.456603765487671,
      "learning_rate": 9.166632157624668e-05,
      "loss": 2.4155,
      "step": 9210
    },
    {
      "epoch": 0.5599076941762312,
      "grad_norm": 3.137023687362671,
      "learning_rate": 9.164873644960503e-05,
      "loss": 2.2484,
      "step": 9220
    },
    {
      "epoch": 0.5605149693326046,
      "grad_norm": 1.8423420190811157,
      "learning_rate": 9.163113447955106e-05,
      "loss": 2.5563,
      "step": 9230
    },
    {
      "epoch": 0.561122244488978,
      "grad_norm": 3.128898859024048,
      "learning_rate": 9.161351567320327e-05,
      "loss": 2.4705,
      "step": 9240
    },
    {
      "epoch": 0.5617295196453513,
      "grad_norm": 1.4238096475601196,
      "learning_rate": 9.159588003768698e-05,
      "loss": 2.3052,
      "step": 9250
    },
    {
      "epoch": 0.5623367948017247,
      "grad_norm": 2.697199821472168,
      "learning_rate": 9.157822758013433e-05,
      "loss": 2.5731,
      "step": 9260
    },
    {
      "epoch": 0.562944069958098,
      "grad_norm": 2.900585412979126,
      "learning_rate": 9.156055830768426e-05,
      "loss": 2.5653,
      "step": 9270
    },
    {
      "epoch": 0.5635513451144714,
      "grad_norm": 1.354722499847412,
      "learning_rate": 9.15428722274825e-05,
      "loss": 2.3286,
      "step": 9280
    },
    {
      "epoch": 0.5641586202708447,
      "grad_norm": 2.9299933910369873,
      "learning_rate": 9.152516934668158e-05,
      "loss": 2.6438,
      "step": 9290
    },
    {
      "epoch": 0.5647658954272181,
      "grad_norm": 2.886495590209961,
      "learning_rate": 9.150744967244082e-05,
      "loss": 2.7906,
      "step": 9300
    },
    {
      "epoch": 0.5653731705835914,
      "grad_norm": 2.379913806915283,
      "learning_rate": 9.148971321192637e-05,
      "loss": 2.583,
      "step": 9310
    },
    {
      "epoch": 0.5659804457399648,
      "grad_norm": 3.1914608478546143,
      "learning_rate": 9.147195997231111e-05,
      "loss": 2.1796,
      "step": 9320
    },
    {
      "epoch": 0.5665877208963381,
      "grad_norm": 2.312002182006836,
      "learning_rate": 9.145418996077473e-05,
      "loss": 2.6289,
      "step": 9330
    },
    {
      "epoch": 0.5671949960527115,
      "grad_norm": 2.495377540588379,
      "learning_rate": 9.143640318450371e-05,
      "loss": 2.653,
      "step": 9340
    },
    {
      "epoch": 0.5678022712090849,
      "grad_norm": 2.999154567718506,
      "learning_rate": 9.141859965069132e-05,
      "loss": 2.6557,
      "step": 9350
    },
    {
      "epoch": 0.5684095463654582,
      "grad_norm": 4.194555759429932,
      "learning_rate": 9.140077936653759e-05,
      "loss": 2.4244,
      "step": 9360
    },
    {
      "epoch": 0.5690168215218315,
      "grad_norm": 2.220982789993286,
      "learning_rate": 9.13829423392493e-05,
      "loss": 2.6754,
      "step": 9370
    },
    {
      "epoch": 0.5696240966782049,
      "grad_norm": 1.877602458000183,
      "learning_rate": 9.136508857604005e-05,
      "loss": 2.2346,
      "step": 9380
    },
    {
      "epoch": 0.5702313718345783,
      "grad_norm": 2.279038429260254,
      "learning_rate": 9.134721808413019e-05,
      "loss": 2.5616,
      "step": 9390
    },
    {
      "epoch": 0.5708386469909515,
      "grad_norm": 3.0587923526763916,
      "learning_rate": 9.132933087074682e-05,
      "loss": 2.68,
      "step": 9400
    },
    {
      "epoch": 0.5714459221473249,
      "grad_norm": 2.90105938911438,
      "learning_rate": 9.131142694312382e-05,
      "loss": 2.7356,
      "step": 9410
    },
    {
      "epoch": 0.5720531973036983,
      "grad_norm": 3.086871385574341,
      "learning_rate": 9.129350630850182e-05,
      "loss": 2.7204,
      "step": 9420
    },
    {
      "epoch": 0.5726604724600717,
      "grad_norm": 2.7255003452301025,
      "learning_rate": 9.127556897412821e-05,
      "loss": 2.5455,
      "step": 9430
    },
    {
      "epoch": 0.5732677476164451,
      "grad_norm": 1.8335353136062622,
      "learning_rate": 9.125761494725715e-05,
      "loss": 2.3739,
      "step": 9440
    },
    {
      "epoch": 0.5738750227728183,
      "grad_norm": 1.3352797031402588,
      "learning_rate": 9.12396442351495e-05,
      "loss": 2.3522,
      "step": 9450
    },
    {
      "epoch": 0.5744822979291917,
      "grad_norm": 2.2693068981170654,
      "learning_rate": 9.122165684507293e-05,
      "loss": 2.933,
      "step": 9460
    },
    {
      "epoch": 0.5750895730855651,
      "grad_norm": 1.6518694162368774,
      "learning_rate": 9.120365278430183e-05,
      "loss": 2.602,
      "step": 9470
    },
    {
      "epoch": 0.5756968482419385,
      "grad_norm": 1.6351338624954224,
      "learning_rate": 9.118563206011731e-05,
      "loss": 2.0357,
      "step": 9480
    },
    {
      "epoch": 0.5763041233983118,
      "grad_norm": 1.794082760810852,
      "learning_rate": 9.116759467980725e-05,
      "loss": 2.1533,
      "step": 9490
    },
    {
      "epoch": 0.5769113985546851,
      "grad_norm": 2.724379301071167,
      "learning_rate": 9.114954065066624e-05,
      "loss": 2.2415,
      "step": 9500
    },
    {
      "epoch": 0.5775186737110585,
      "grad_norm": 2.5686569213867188,
      "learning_rate": 9.113146997999563e-05,
      "loss": 2.3221,
      "step": 9510
    },
    {
      "epoch": 0.5781259488674318,
      "grad_norm": 2.770348072052002,
      "learning_rate": 9.111338267510349e-05,
      "loss": 2.5863,
      "step": 9520
    },
    {
      "epoch": 0.5787332240238052,
      "grad_norm": 2.3239896297454834,
      "learning_rate": 9.10952787433046e-05,
      "loss": 2.4664,
      "step": 9530
    },
    {
      "epoch": 0.5793404991801785,
      "grad_norm": 1.8462122678756714,
      "learning_rate": 9.107715819192048e-05,
      "loss": 2.2761,
      "step": 9540
    },
    {
      "epoch": 0.5799477743365519,
      "grad_norm": 1.2186552286148071,
      "learning_rate": 9.105902102827939e-05,
      "loss": 2.1616,
      "step": 9550
    },
    {
      "epoch": 0.5805550494929252,
      "grad_norm": 1.521883249282837,
      "learning_rate": 9.104086725971628e-05,
      "loss": 1.9259,
      "step": 9560
    },
    {
      "epoch": 0.5811623246492986,
      "grad_norm": 2.165473222732544,
      "learning_rate": 9.102269689357281e-05,
      "loss": 2.1603,
      "step": 9570
    },
    {
      "epoch": 0.581769599805672,
      "grad_norm": 2.505690097808838,
      "learning_rate": 9.100450993719735e-05,
      "loss": 2.6077,
      "step": 9580
    },
    {
      "epoch": 0.5823768749620453,
      "grad_norm": 2.958092212677002,
      "learning_rate": 9.098630639794506e-05,
      "loss": 2.2884,
      "step": 9590
    },
    {
      "epoch": 0.5829841501184186,
      "grad_norm": 3.051650047302246,
      "learning_rate": 9.096808628317767e-05,
      "loss": 2.3864,
      "step": 9600
    },
    {
      "epoch": 0.583591425274792,
      "grad_norm": 3.931284189224243,
      "learning_rate": 9.094984960026372e-05,
      "loss": 2.5111,
      "step": 9610
    },
    {
      "epoch": 0.5841987004311654,
      "grad_norm": 3.674762725830078,
      "learning_rate": 9.093159635657839e-05,
      "loss": 2.4269,
      "step": 9620
    },
    {
      "epoch": 0.5848059755875387,
      "grad_norm": 3.7031919956207275,
      "learning_rate": 9.091332655950362e-05,
      "loss": 2.8626,
      "step": 9630
    },
    {
      "epoch": 0.585413250743912,
      "grad_norm": 2.0926623344421387,
      "learning_rate": 9.089504021642798e-05,
      "loss": 2.6317,
      "step": 9640
    },
    {
      "epoch": 0.5860205259002854,
      "grad_norm": 2.809067964553833,
      "learning_rate": 9.087673733474678e-05,
      "loss": 2.8596,
      "step": 9650
    },
    {
      "epoch": 0.5866278010566588,
      "grad_norm": 4.281638145446777,
      "learning_rate": 9.085841792186196e-05,
      "loss": 3.0084,
      "step": 9660
    },
    {
      "epoch": 0.5872350762130322,
      "grad_norm": 3.3353207111358643,
      "learning_rate": 9.084008198518222e-05,
      "loss": 2.9987,
      "step": 9670
    },
    {
      "epoch": 0.5878423513694054,
      "grad_norm": 4.044825077056885,
      "learning_rate": 9.08217295321229e-05,
      "loss": 2.3106,
      "step": 9680
    },
    {
      "epoch": 0.5884496265257788,
      "grad_norm": 1.9802145957946777,
      "learning_rate": 9.080336057010599e-05,
      "loss": 2.5262,
      "step": 9690
    },
    {
      "epoch": 0.5890569016821522,
      "grad_norm": 2.1142120361328125,
      "learning_rate": 9.078497510656024e-05,
      "loss": 2.5851,
      "step": 9700
    },
    {
      "epoch": 0.5896641768385256,
      "grad_norm": 1.7885438203811646,
      "learning_rate": 9.076657314892096e-05,
      "loss": 2.3755,
      "step": 9710
    },
    {
      "epoch": 0.5902714519948989,
      "grad_norm": 2.3488941192626953,
      "learning_rate": 9.074815470463027e-05,
      "loss": 2.2145,
      "step": 9720
    },
    {
      "epoch": 0.5908787271512722,
      "grad_norm": 2.8608455657958984,
      "learning_rate": 9.072971978113684e-05,
      "loss": 2.994,
      "step": 9730
    },
    {
      "epoch": 0.5914860023076456,
      "grad_norm": 3.9730896949768066,
      "learning_rate": 9.071126838589603e-05,
      "loss": 2.9198,
      "step": 9740
    },
    {
      "epoch": 0.592093277464019,
      "grad_norm": 2.5613226890563965,
      "learning_rate": 9.069280052636993e-05,
      "loss": 2.853,
      "step": 9750
    },
    {
      "epoch": 0.5927005526203923,
      "grad_norm": 1.679639458656311,
      "learning_rate": 9.067431621002719e-05,
      "loss": 2.7064,
      "step": 9760
    },
    {
      "epoch": 0.5933078277767656,
      "grad_norm": 1.482369303703308,
      "learning_rate": 9.065581544434319e-05,
      "loss": 2.2095,
      "step": 9770
    },
    {
      "epoch": 0.593915102933139,
      "grad_norm": 1.9043338298797607,
      "learning_rate": 9.063729823679991e-05,
      "loss": 2.1853,
      "step": 9780
    },
    {
      "epoch": 0.5945223780895124,
      "grad_norm": 2.1812925338745117,
      "learning_rate": 9.061876459488603e-05,
      "loss": 2.3918,
      "step": 9790
    },
    {
      "epoch": 0.5951296532458857,
      "grad_norm": 2.207256555557251,
      "learning_rate": 9.060021452609684e-05,
      "loss": 2.5148,
      "step": 9800
    },
    {
      "epoch": 0.5957369284022591,
      "grad_norm": 2.2263755798339844,
      "learning_rate": 9.058164803793426e-05,
      "loss": 2.1129,
      "step": 9810
    },
    {
      "epoch": 0.5963442035586324,
      "grad_norm": 2.68636417388916,
      "learning_rate": 9.056306513790692e-05,
      "loss": 2.3785,
      "step": 9820
    },
    {
      "epoch": 0.5969514787150058,
      "grad_norm": 4.874732494354248,
      "learning_rate": 9.054446583352999e-05,
      "loss": 2.3083,
      "step": 9830
    },
    {
      "epoch": 0.5975587538713791,
      "grad_norm": 2.111842393875122,
      "learning_rate": 9.052585013232535e-05,
      "loss": 2.7522,
      "step": 9840
    },
    {
      "epoch": 0.5981660290277525,
      "grad_norm": 2.907121419906616,
      "learning_rate": 9.05072180418215e-05,
      "loss": 3.1136,
      "step": 9850
    },
    {
      "epoch": 0.5987733041841258,
      "grad_norm": 5.161039352416992,
      "learning_rate": 9.048856956955354e-05,
      "loss": 3.0639,
      "step": 9860
    },
    {
      "epoch": 0.5993805793404992,
      "grad_norm": 1.929857611656189,
      "learning_rate": 9.046990472306321e-05,
      "loss": 2.7769,
      "step": 9870
    },
    {
      "epoch": 0.5999878544968725,
      "grad_norm": 2.4187803268432617,
      "learning_rate": 9.045122350989885e-05,
      "loss": 2.4865,
      "step": 9880
    },
    {
      "epoch": 0.6005951296532459,
      "grad_norm": 2.848289728164673,
      "learning_rate": 9.043252593761548e-05,
      "loss": 2.2188,
      "step": 9890
    },
    {
      "epoch": 0.6012024048096193,
      "grad_norm": 2.1121842861175537,
      "learning_rate": 9.041381201377468e-05,
      "loss": 2.2873,
      "step": 9900
    },
    {
      "epoch": 0.6018096799659925,
      "grad_norm": 2.5564963817596436,
      "learning_rate": 9.039508174594464e-05,
      "loss": 2.5817,
      "step": 9910
    },
    {
      "epoch": 0.6024169551223659,
      "grad_norm": 4.278826713562012,
      "learning_rate": 9.03763351417002e-05,
      "loss": 2.6845,
      "step": 9920
    },
    {
      "epoch": 0.6030242302787393,
      "grad_norm": 2.555363178253174,
      "learning_rate": 9.035757220862277e-05,
      "loss": 2.2949,
      "step": 9930
    },
    {
      "epoch": 0.6036315054351127,
      "grad_norm": 2.6103625297546387,
      "learning_rate": 9.033879295430041e-05,
      "loss": 2.5046,
      "step": 9940
    },
    {
      "epoch": 0.6042387805914861,
      "grad_norm": 2.807713508605957,
      "learning_rate": 9.031999738632772e-05,
      "loss": 2.7496,
      "step": 9950
    },
    {
      "epoch": 0.6048460557478593,
      "grad_norm": 2.4578003883361816,
      "learning_rate": 9.030118551230593e-05,
      "loss": 2.5224,
      "step": 9960
    },
    {
      "epoch": 0.6054533309042327,
      "grad_norm": 2.8829402923583984,
      "learning_rate": 9.028235733984286e-05,
      "loss": 2.2572,
      "step": 9970
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.2058815956115723,
      "learning_rate": 9.026351287655294e-05,
      "loss": 2.4009,
      "step": 9980
    },
    {
      "epoch": 0.6066678812169795,
      "grad_norm": 2.3183200359344482,
      "learning_rate": 9.024465213005715e-05,
      "loss": 2.2544,
      "step": 9990
    },
    {
      "epoch": 0.6072751563733527,
      "grad_norm": 2.3965699672698975,
      "learning_rate": 9.022577510798308e-05,
      "loss": 2.4502,
      "step": 10000
    },
    {
      "epoch": 0.6073966114046274,
      "eval_loss": 4.87440299987793,
      "eval_runtime": 2263.904,
      "eval_samples_per_second": 7.274,
      "eval_steps_per_second": 1.819,
      "step": 10002
    },
    {
      "epoch": 0.6078824315297261,
      "grad_norm": 3.3228437900543213,
      "learning_rate": 9.020688181796493e-05,
      "loss": 4.1092,
      "step": 10010
    },
    {
      "epoch": 0.6084897066860995,
      "grad_norm": 5.20359992980957,
      "learning_rate": 9.01879722676434e-05,
      "loss": 3.3527,
      "step": 10020
    },
    {
      "epoch": 0.6090969818424729,
      "grad_norm": 4.2720746994018555,
      "learning_rate": 9.016904646466584e-05,
      "loss": 2.3019,
      "step": 10030
    },
    {
      "epoch": 0.6097042569988462,
      "grad_norm": 2.7660162448883057,
      "learning_rate": 9.015010441668615e-05,
      "loss": 2.5572,
      "step": 10040
    },
    {
      "epoch": 0.6103115321552195,
      "grad_norm": 3.4933412075042725,
      "learning_rate": 9.013114613136478e-05,
      "loss": 2.7544,
      "step": 10050
    },
    {
      "epoch": 0.6109188073115929,
      "grad_norm": 2.4275436401367188,
      "learning_rate": 9.011217161636877e-05,
      "loss": 2.5457,
      "step": 10060
    },
    {
      "epoch": 0.6115260824679662,
      "grad_norm": 3.0177035331726074,
      "learning_rate": 9.009318087937171e-05,
      "loss": 2.7811,
      "step": 10070
    },
    {
      "epoch": 0.6121333576243396,
      "grad_norm": 3.306544542312622,
      "learning_rate": 9.007417392805377e-05,
      "loss": 2.5273,
      "step": 10080
    },
    {
      "epoch": 0.6127406327807129,
      "grad_norm": 1.4508968591690063,
      "learning_rate": 9.005515077010166e-05,
      "loss": 2.8301,
      "step": 10090
    },
    {
      "epoch": 0.6133479079370863,
      "grad_norm": 3.4063172340393066,
      "learning_rate": 9.003611141320863e-05,
      "loss": 2.7901,
      "step": 10100
    },
    {
      "epoch": 0.6139551830934596,
      "grad_norm": 2.848604440689087,
      "learning_rate": 9.001705586507453e-05,
      "loss": 2.7744,
      "step": 10110
    },
    {
      "epoch": 0.614562458249833,
      "grad_norm": 3.2302942276000977,
      "learning_rate": 8.99979841334057e-05,
      "loss": 2.5935,
      "step": 10120
    },
    {
      "epoch": 0.6151697334062064,
      "grad_norm": 2.1525051593780518,
      "learning_rate": 8.997889622591507e-05,
      "loss": 2.3824,
      "step": 10130
    },
    {
      "epoch": 0.6157770085625797,
      "grad_norm": 2.754324197769165,
      "learning_rate": 8.995979215032207e-05,
      "loss": 2.4528,
      "step": 10140
    },
    {
      "epoch": 0.616384283718953,
      "grad_norm": 3.7029967308044434,
      "learning_rate": 8.994067191435274e-05,
      "loss": 2.439,
      "step": 10150
    },
    {
      "epoch": 0.6169915588753264,
      "grad_norm": 2.6139845848083496,
      "learning_rate": 8.992153552573954e-05,
      "loss": 2.6505,
      "step": 10160
    },
    {
      "epoch": 0.6175988340316998,
      "grad_norm": 3.6021318435668945,
      "learning_rate": 8.990238299222159e-05,
      "loss": 2.9317,
      "step": 10170
    },
    {
      "epoch": 0.6182061091880731,
      "grad_norm": 4.154824256896973,
      "learning_rate": 8.988321432154445e-05,
      "loss": 2.3849,
      "step": 10180
    },
    {
      "epoch": 0.6188133843444464,
      "grad_norm": 2.7430953979492188,
      "learning_rate": 8.986402952146025e-05,
      "loss": 2.2439,
      "step": 10190
    },
    {
      "epoch": 0.6194206595008198,
      "grad_norm": 1.666829228401184,
      "learning_rate": 8.98448285997276e-05,
      "loss": 2.191,
      "step": 10200
    },
    {
      "epoch": 0.6200279346571932,
      "grad_norm": 2.0815229415893555,
      "learning_rate": 8.982561156411172e-05,
      "loss": 2.1338,
      "step": 10210
    },
    {
      "epoch": 0.6206352098135666,
      "grad_norm": 4.061728000640869,
      "learning_rate": 8.980637842238421e-05,
      "loss": 2.3692,
      "step": 10220
    },
    {
      "epoch": 0.6212424849699398,
      "grad_norm": 3.6601531505584717,
      "learning_rate": 8.97871291823233e-05,
      "loss": 2.2528,
      "step": 10230
    },
    {
      "epoch": 0.6218497601263132,
      "grad_norm": 3.23398494720459,
      "learning_rate": 8.976786385171368e-05,
      "loss": 2.2825,
      "step": 10240
    },
    {
      "epoch": 0.6224570352826866,
      "grad_norm": 2.1624553203582764,
      "learning_rate": 8.974858243834656e-05,
      "loss": 2.4409,
      "step": 10250
    },
    {
      "epoch": 0.62306431043906,
      "grad_norm": 3.4366209506988525,
      "learning_rate": 8.972928495001967e-05,
      "loss": 2.5289,
      "step": 10260
    },
    {
      "epoch": 0.6236715855954333,
      "grad_norm": 2.3075616359710693,
      "learning_rate": 8.970997139453719e-05,
      "loss": 2.2738,
      "step": 10270
    },
    {
      "epoch": 0.6242788607518066,
      "grad_norm": 3.2994792461395264,
      "learning_rate": 8.969064177970982e-05,
      "loss": 2.4711,
      "step": 10280
    },
    {
      "epoch": 0.62488613590818,
      "grad_norm": 2.4378843307495117,
      "learning_rate": 8.967129611335481e-05,
      "loss": 2.3612,
      "step": 10290
    },
    {
      "epoch": 0.6254934110645534,
      "grad_norm": 2.3643929958343506,
      "learning_rate": 8.965193440329583e-05,
      "loss": 2.3669,
      "step": 10300
    },
    {
      "epoch": 0.6261006862209267,
      "grad_norm": 2.5255088806152344,
      "learning_rate": 8.963255665736305e-05,
      "loss": 2.5143,
      "step": 10310
    },
    {
      "epoch": 0.6267079613773,
      "grad_norm": 3.0703423023223877,
      "learning_rate": 8.961316288339316e-05,
      "loss": 2.5231,
      "step": 10320
    },
    {
      "epoch": 0.6273152365336734,
      "grad_norm": 2.2456793785095215,
      "learning_rate": 8.959375308922932e-05,
      "loss": 2.1943,
      "step": 10330
    },
    {
      "epoch": 0.6279225116900468,
      "grad_norm": 4.914292335510254,
      "learning_rate": 8.957432728272113e-05,
      "loss": 2.6187,
      "step": 10340
    },
    {
      "epoch": 0.6285297868464201,
      "grad_norm": 3.769244432449341,
      "learning_rate": 8.95548854717247e-05,
      "loss": 3.0072,
      "step": 10350
    },
    {
      "epoch": 0.6291370620027935,
      "grad_norm": 5.719825744628906,
      "learning_rate": 8.953542766410263e-05,
      "loss": 2.8444,
      "step": 10360
    },
    {
      "epoch": 0.6297443371591668,
      "grad_norm": 4.365283012390137,
      "learning_rate": 8.951595386772397e-05,
      "loss": 2.5112,
      "step": 10370
    },
    {
      "epoch": 0.6303516123155402,
      "grad_norm": 3.822444200515747,
      "learning_rate": 8.94964640904642e-05,
      "loss": 2.3913,
      "step": 10380
    },
    {
      "epoch": 0.6309588874719135,
      "grad_norm": 4.943605899810791,
      "learning_rate": 8.947695834020532e-05,
      "loss": 2.3712,
      "step": 10390
    },
    {
      "epoch": 0.6315661626282869,
      "grad_norm": 2.2141122817993164,
      "learning_rate": 8.945743662483577e-05,
      "loss": 2.077,
      "step": 10400
    },
    {
      "epoch": 0.6321734377846602,
      "grad_norm": 3.104752779006958,
      "learning_rate": 8.943789895225043e-05,
      "loss": 2.3866,
      "step": 10410
    },
    {
      "epoch": 0.6327807129410336,
      "grad_norm": 3.065980911254883,
      "learning_rate": 8.941834533035064e-05,
      "loss": 2.5169,
      "step": 10420
    },
    {
      "epoch": 0.6333879880974069,
      "grad_norm": 2.950214385986328,
      "learning_rate": 8.93987757670442e-05,
      "loss": 2.8384,
      "step": 10430
    },
    {
      "epoch": 0.6339952632537803,
      "grad_norm": 6.666264057159424,
      "learning_rate": 8.937919027024539e-05,
      "loss": 2.3647,
      "step": 10440
    },
    {
      "epoch": 0.6346025384101537,
      "grad_norm": 2.9136860370635986,
      "learning_rate": 8.935958884787485e-05,
      "loss": 2.3851,
      "step": 10450
    },
    {
      "epoch": 0.635209813566527,
      "grad_norm": 2.5608134269714355,
      "learning_rate": 8.933997150785971e-05,
      "loss": 2.613,
      "step": 10460
    },
    {
      "epoch": 0.6358170887229003,
      "grad_norm": 3.3344039916992188,
      "learning_rate": 8.932033825813356e-05,
      "loss": 2.0891,
      "step": 10470
    },
    {
      "epoch": 0.6364243638792737,
      "grad_norm": 1.8463948965072632,
      "learning_rate": 8.930068910663638e-05,
      "loss": 2.3697,
      "step": 10480
    },
    {
      "epoch": 0.6370316390356471,
      "grad_norm": 2.3469629287719727,
      "learning_rate": 8.928102406131463e-05,
      "loss": 2.2694,
      "step": 10490
    },
    {
      "epoch": 0.6376389141920205,
      "grad_norm": 2.083887815475464,
      "learning_rate": 8.926134313012112e-05,
      "loss": 2.3239,
      "step": 10500
    },
    {
      "epoch": 0.6382461893483937,
      "grad_norm": 2.035071611404419,
      "learning_rate": 8.924164632101518e-05,
      "loss": 2.5366,
      "step": 10510
    },
    {
      "epoch": 0.6388534645047671,
      "grad_norm": 5.150882720947266,
      "learning_rate": 8.922193364196245e-05,
      "loss": 2.6236,
      "step": 10520
    },
    {
      "epoch": 0.6394607396611405,
      "grad_norm": 2.8706300258636475,
      "learning_rate": 8.920220510093511e-05,
      "loss": 2.2694,
      "step": 10530
    },
    {
      "epoch": 0.6400680148175139,
      "grad_norm": 1.983695149421692,
      "learning_rate": 8.91824607059117e-05,
      "loss": 2.2969,
      "step": 10540
    },
    {
      "epoch": 0.6406752899738871,
      "grad_norm": 1.5155842304229736,
      "learning_rate": 8.916270046487711e-05,
      "loss": 2.132,
      "step": 10550
    },
    {
      "epoch": 0.6412825651302605,
      "grad_norm": 1.8717682361602783,
      "learning_rate": 8.914292438582275e-05,
      "loss": 2.812,
      "step": 10560
    },
    {
      "epoch": 0.6418898402866339,
      "grad_norm": 2.2809364795684814,
      "learning_rate": 8.912313247674636e-05,
      "loss": 2.2388,
      "step": 10570
    },
    {
      "epoch": 0.6424971154430072,
      "grad_norm": 2.253598928451538,
      "learning_rate": 8.91033247456521e-05,
      "loss": 2.9213,
      "step": 10580
    },
    {
      "epoch": 0.6431043905993806,
      "grad_norm": 3.0436055660247803,
      "learning_rate": 8.908350120055056e-05,
      "loss": 2.4018,
      "step": 10590
    },
    {
      "epoch": 0.6437116657557539,
      "grad_norm": 2.1712491512298584,
      "learning_rate": 8.906366184945865e-05,
      "loss": 2.4385,
      "step": 10600
    },
    {
      "epoch": 0.6443189409121273,
      "grad_norm": 3.192767381668091,
      "learning_rate": 8.904380670039975e-05,
      "loss": 2.7219,
      "step": 10610
    },
    {
      "epoch": 0.6449262160685006,
      "grad_norm": 3.2504477500915527,
      "learning_rate": 8.90239357614036e-05,
      "loss": 2.7263,
      "step": 10620
    },
    {
      "epoch": 0.645533491224874,
      "grad_norm": 4.368812084197998,
      "learning_rate": 8.900404904050632e-05,
      "loss": 2.5837,
      "step": 10630
    },
    {
      "epoch": 0.6461407663812473,
      "grad_norm": 2.9172627925872803,
      "learning_rate": 8.89841465457504e-05,
      "loss": 2.506,
      "step": 10640
    },
    {
      "epoch": 0.6467480415376207,
      "grad_norm": 2.7561259269714355,
      "learning_rate": 8.896422828518475e-05,
      "loss": 2.5347,
      "step": 10650
    },
    {
      "epoch": 0.647355316693994,
      "grad_norm": 3.685168504714966,
      "learning_rate": 8.894429426686461e-05,
      "loss": 2.7422,
      "step": 10660
    },
    {
      "epoch": 0.6479625918503674,
      "grad_norm": 3.364013195037842,
      "learning_rate": 8.892434449885164e-05,
      "loss": 2.7811,
      "step": 10670
    },
    {
      "epoch": 0.6485698670067408,
      "grad_norm": 2.8424785137176514,
      "learning_rate": 8.890437898921382e-05,
      "loss": 2.9497,
      "step": 10680
    },
    {
      "epoch": 0.6491771421631141,
      "grad_norm": 2.6483638286590576,
      "learning_rate": 8.888439774602554e-05,
      "loss": 2.9663,
      "step": 10690
    },
    {
      "epoch": 0.6497844173194874,
      "grad_norm": 3.600252866744995,
      "learning_rate": 8.886440077736752e-05,
      "loss": 2.7403,
      "step": 10700
    },
    {
      "epoch": 0.6503916924758608,
      "grad_norm": 3.467076539993286,
      "learning_rate": 8.884438809132685e-05,
      "loss": 2.4715,
      "step": 10710
    },
    {
      "epoch": 0.6509989676322342,
      "grad_norm": 2.749890089035034,
      "learning_rate": 8.882435969599699e-05,
      "loss": 2.6199,
      "step": 10720
    },
    {
      "epoch": 0.6516062427886076,
      "grad_norm": 4.895919322967529,
      "learning_rate": 8.880431559947774e-05,
      "loss": 2.8516,
      "step": 10730
    },
    {
      "epoch": 0.6522135179449808,
      "grad_norm": 4.225321292877197,
      "learning_rate": 8.878425580987524e-05,
      "loss": 2.5933,
      "step": 10740
    },
    {
      "epoch": 0.6528207931013542,
      "grad_norm": 3.0961122512817383,
      "learning_rate": 8.876418033530201e-05,
      "loss": 2.4141,
      "step": 10750
    },
    {
      "epoch": 0.6534280682577276,
      "grad_norm": 2.662951946258545,
      "learning_rate": 8.874408918387685e-05,
      "loss": 2.1653,
      "step": 10760
    },
    {
      "epoch": 0.654035343414101,
      "grad_norm": 1.5607211589813232,
      "learning_rate": 8.872398236372499e-05,
      "loss": 2.0971,
      "step": 10770
    },
    {
      "epoch": 0.6546426185704742,
      "grad_norm": 1.6043403148651123,
      "learning_rate": 8.870385988297794e-05,
      "loss": 2.3463,
      "step": 10780
    },
    {
      "epoch": 0.6552498937268476,
      "grad_norm": 3.468993902206421,
      "learning_rate": 8.868372174977352e-05,
      "loss": 2.6878,
      "step": 10790
    },
    {
      "epoch": 0.655857168883221,
      "grad_norm": 2.711749792098999,
      "learning_rate": 8.866356797225594e-05,
      "loss": 2.2417,
      "step": 10800
    },
    {
      "epoch": 0.6564644440395944,
      "grad_norm": 1.9240520000457764,
      "learning_rate": 8.86433985585757e-05,
      "loss": 2.1399,
      "step": 10810
    },
    {
      "epoch": 0.6570717191959677,
      "grad_norm": 2.441174030303955,
      "learning_rate": 8.862321351688963e-05,
      "loss": 2.5713,
      "step": 10820
    },
    {
      "epoch": 0.657678994352341,
      "grad_norm": 5.838700771331787,
      "learning_rate": 8.86030128553609e-05,
      "loss": 2.4965,
      "step": 10830
    },
    {
      "epoch": 0.6582862695087144,
      "grad_norm": 3.846709728240967,
      "learning_rate": 8.858279658215895e-05,
      "loss": 2.4066,
      "step": 10840
    },
    {
      "epoch": 0.6588935446650878,
      "grad_norm": 3.831381320953369,
      "learning_rate": 8.856256470545957e-05,
      "loss": 2.5227,
      "step": 10850
    },
    {
      "epoch": 0.6595008198214611,
      "grad_norm": 3.528681993484497,
      "learning_rate": 8.854231723344487e-05,
      "loss": 2.6556,
      "step": 10860
    },
    {
      "epoch": 0.6601080949778344,
      "grad_norm": 2.8658411502838135,
      "learning_rate": 8.852205417430323e-05,
      "loss": 2.3905,
      "step": 10870
    },
    {
      "epoch": 0.6607153701342078,
      "grad_norm": 2.386774778366089,
      "learning_rate": 8.850177553622938e-05,
      "loss": 2.6274,
      "step": 10880
    },
    {
      "epoch": 0.6613226452905812,
      "grad_norm": 3.0485377311706543,
      "learning_rate": 8.848148132742431e-05,
      "loss": 2.587,
      "step": 10890
    },
    {
      "epoch": 0.6619299204469545,
      "grad_norm": 1.6677119731903076,
      "learning_rate": 8.846117155609532e-05,
      "loss": 2.3033,
      "step": 10900
    },
    {
      "epoch": 0.6625371956033279,
      "grad_norm": 1.8614333868026733,
      "learning_rate": 8.844084623045599e-05,
      "loss": 2.2457,
      "step": 10910
    },
    {
      "epoch": 0.6631444707597012,
      "grad_norm": 3.2817835807800293,
      "learning_rate": 8.842050535872623e-05,
      "loss": 2.567,
      "step": 10920
    },
    {
      "epoch": 0.6637517459160746,
      "grad_norm": 2.6699044704437256,
      "learning_rate": 8.84001489491322e-05,
      "loss": 2.4098,
      "step": 10930
    },
    {
      "epoch": 0.6643590210724479,
      "grad_norm": 2.9969699382781982,
      "learning_rate": 8.837977700990636e-05,
      "loss": 2.6021,
      "step": 10940
    },
    {
      "epoch": 0.6649662962288213,
      "grad_norm": 2.897136688232422,
      "learning_rate": 8.835938954928744e-05,
      "loss": 2.7893,
      "step": 10950
    },
    {
      "epoch": 0.6655735713851947,
      "grad_norm": 3.533891439437866,
      "learning_rate": 8.833898657552046e-05,
      "loss": 2.6963,
      "step": 10960
    },
    {
      "epoch": 0.666180846541568,
      "grad_norm": 3.979959726333618,
      "learning_rate": 8.831856809685672e-05,
      "loss": 3.0215,
      "step": 10970
    },
    {
      "epoch": 0.6667881216979413,
      "grad_norm": 3.1960015296936035,
      "learning_rate": 8.829813412155375e-05,
      "loss": 2.5707,
      "step": 10980
    },
    {
      "epoch": 0.6673953968543147,
      "grad_norm": 3.5149619579315186,
      "learning_rate": 8.82776846578754e-05,
      "loss": 2.3074,
      "step": 10990
    },
    {
      "epoch": 0.6680026720106881,
      "grad_norm": 1.994879961013794,
      "learning_rate": 8.825721971409173e-05,
      "loss": 2.7095,
      "step": 11000
    },
    {
      "epoch": 0.6686099471670613,
      "grad_norm": 2.587444543838501,
      "learning_rate": 8.823673929847914e-05,
      "loss": 2.2443,
      "step": 11010
    },
    {
      "epoch": 0.6692172223234347,
      "grad_norm": 2.088688373565674,
      "learning_rate": 8.821624341932018e-05,
      "loss": 2.2242,
      "step": 11020
    },
    {
      "epoch": 0.6698244974798081,
      "grad_norm": 1.699973225593567,
      "learning_rate": 8.819573208490373e-05,
      "loss": 2.585,
      "step": 11030
    },
    {
      "epoch": 0.6704317726361815,
      "grad_norm": 3.000051259994507,
      "learning_rate": 8.817520530352491e-05,
      "loss": 2.5736,
      "step": 11040
    },
    {
      "epoch": 0.6710390477925549,
      "grad_norm": 6.5334906578063965,
      "learning_rate": 8.815466308348508e-05,
      "loss": 2.318,
      "step": 11050
    },
    {
      "epoch": 0.6716463229489281,
      "grad_norm": 5.0103864669799805,
      "learning_rate": 8.813410543309184e-05,
      "loss": 2.256,
      "step": 11060
    },
    {
      "epoch": 0.6722535981053015,
      "grad_norm": 2.4965860843658447,
      "learning_rate": 8.8113532360659e-05,
      "loss": 2.1051,
      "step": 11070
    },
    {
      "epoch": 0.6728608732616749,
      "grad_norm": 4.164797782897949,
      "learning_rate": 8.809294387450668e-05,
      "loss": 2.7265,
      "step": 11080
    },
    {
      "epoch": 0.6734681484180483,
      "grad_norm": 2.8441388607025146,
      "learning_rate": 8.807233998296117e-05,
      "loss": 2.6916,
      "step": 11090
    },
    {
      "epoch": 0.6740754235744215,
      "grad_norm": 3.308903455734253,
      "learning_rate": 8.805172069435501e-05,
      "loss": 2.7707,
      "step": 11100
    },
    {
      "epoch": 0.6746826987307949,
      "grad_norm": 2.4701712131500244,
      "learning_rate": 8.803108601702698e-05,
      "loss": 2.3197,
      "step": 11110
    },
    {
      "epoch": 0.6752899738871683,
      "grad_norm": 2.2833468914031982,
      "learning_rate": 8.801043595932206e-05,
      "loss": 2.3151,
      "step": 11120
    },
    {
      "epoch": 0.6758972490435416,
      "grad_norm": 3.1311230659484863,
      "learning_rate": 8.798977052959148e-05,
      "loss": 2.1687,
      "step": 11130
    },
    {
      "epoch": 0.676504524199915,
      "grad_norm": 0.935635507106781,
      "learning_rate": 8.796908973619265e-05,
      "loss": 1.9931,
      "step": 11140
    },
    {
      "epoch": 0.6771117993562883,
      "grad_norm": 2.280280828475952,
      "learning_rate": 8.794839358748923e-05,
      "loss": 2.3454,
      "step": 11150
    },
    {
      "epoch": 0.6777190745126617,
      "grad_norm": 2.8030853271484375,
      "learning_rate": 8.792768209185105e-05,
      "loss": 2.6203,
      "step": 11160
    },
    {
      "epoch": 0.678326349669035,
      "grad_norm": 3.877730131149292,
      "learning_rate": 8.790695525765418e-05,
      "loss": 2.1577,
      "step": 11170
    },
    {
      "epoch": 0.6789336248254084,
      "grad_norm": 2.007049083709717,
      "learning_rate": 8.788621309328087e-05,
      "loss": 2.1643,
      "step": 11180
    },
    {
      "epoch": 0.6795408999817818,
      "grad_norm": 4.5134992599487305,
      "learning_rate": 8.786545560711962e-05,
      "loss": 2.4497,
      "step": 11190
    },
    {
      "epoch": 0.6801481751381551,
      "grad_norm": 3.2414166927337646,
      "learning_rate": 8.784468280756505e-05,
      "loss": 2.3829,
      "step": 11200
    },
    {
      "epoch": 0.6807554502945284,
      "grad_norm": 2.5370278358459473,
      "learning_rate": 8.782389470301803e-05,
      "loss": 2.7114,
      "step": 11210
    },
    {
      "epoch": 0.6813627254509018,
      "grad_norm": 6.506296634674072,
      "learning_rate": 8.780309130188558e-05,
      "loss": 2.389,
      "step": 11220
    },
    {
      "epoch": 0.6819700006072752,
      "grad_norm": 3.7199716567993164,
      "learning_rate": 8.778227261258095e-05,
      "loss": 2.391,
      "step": 11230
    },
    {
      "epoch": 0.6825772757636485,
      "grad_norm": 2.9331204891204834,
      "learning_rate": 8.776143864352352e-05,
      "loss": 2.5268,
      "step": 11240
    },
    {
      "epoch": 0.6831845509200218,
      "grad_norm": 4.080738067626953,
      "learning_rate": 8.774058940313894e-05,
      "loss": 2.2572,
      "step": 11250
    },
    {
      "epoch": 0.6837918260763952,
      "grad_norm": 2.6593098640441895,
      "learning_rate": 8.771972489985891e-05,
      "loss": 3.0673,
      "step": 11260
    },
    {
      "epoch": 0.6843991012327686,
      "grad_norm": 3.2573087215423584,
      "learning_rate": 8.769884514212139e-05,
      "loss": 2.7258,
      "step": 11270
    },
    {
      "epoch": 0.685006376389142,
      "grad_norm": 3.442018747329712,
      "learning_rate": 8.767795013837048e-05,
      "loss": 2.46,
      "step": 11280
    },
    {
      "epoch": 0.6856136515455152,
      "grad_norm": 2.887796640396118,
      "learning_rate": 8.765703989705647e-05,
      "loss": 2.5344,
      "step": 11290
    },
    {
      "epoch": 0.6862209267018886,
      "grad_norm": 4.760140895843506,
      "learning_rate": 8.76361144266358e-05,
      "loss": 2.0413,
      "step": 11300
    },
    {
      "epoch": 0.686828201858262,
      "grad_norm": 1.7822891473770142,
      "learning_rate": 8.761517373557102e-05,
      "loss": 2.5357,
      "step": 11310
    },
    {
      "epoch": 0.6874354770146354,
      "grad_norm": 2.2745306491851807,
      "learning_rate": 8.759421783233092e-05,
      "loss": 2.4792,
      "step": 11320
    },
    {
      "epoch": 0.6880427521710086,
      "grad_norm": 2.810046434402466,
      "learning_rate": 8.757324672539039e-05,
      "loss": 2.7805,
      "step": 11330
    },
    {
      "epoch": 0.688650027327382,
      "grad_norm": 2.6413328647613525,
      "learning_rate": 8.755226042323048e-05,
      "loss": 2.5531,
      "step": 11340
    },
    {
      "epoch": 0.6892573024837554,
      "grad_norm": 1.4079152345657349,
      "learning_rate": 8.753125893433838e-05,
      "loss": 1.9894,
      "step": 11350
    },
    {
      "epoch": 0.6898645776401288,
      "grad_norm": 1.0995352268218994,
      "learning_rate": 8.751024226720742e-05,
      "loss": 1.7711,
      "step": 11360
    },
    {
      "epoch": 0.6904718527965021,
      "grad_norm": 1.9425890445709229,
      "learning_rate": 8.748921043033708e-05,
      "loss": 2.2423,
      "step": 11370
    },
    {
      "epoch": 0.6910791279528754,
      "grad_norm": 1.9972096681594849,
      "learning_rate": 8.746816343223298e-05,
      "loss": 2.6469,
      "step": 11380
    },
    {
      "epoch": 0.6916864031092488,
      "grad_norm": 3.4727747440338135,
      "learning_rate": 8.744710128140688e-05,
      "loss": 2.4529,
      "step": 11390
    },
    {
      "epoch": 0.6922936782656222,
      "grad_norm": 2.4167869091033936,
      "learning_rate": 8.74260239863766e-05,
      "loss": 2.5715,
      "step": 11400
    },
    {
      "epoch": 0.6929009534219955,
      "grad_norm": 3.3651492595672607,
      "learning_rate": 8.740493155566616e-05,
      "loss": 2.7494,
      "step": 11410
    },
    {
      "epoch": 0.6935082285783689,
      "grad_norm": 2.737031936645508,
      "learning_rate": 8.738382399780567e-05,
      "loss": 2.6008,
      "step": 11420
    },
    {
      "epoch": 0.6941155037347422,
      "grad_norm": 3.416959285736084,
      "learning_rate": 8.736270132133138e-05,
      "loss": 2.8297,
      "step": 11430
    },
    {
      "epoch": 0.6947227788911156,
      "grad_norm": 3.342590808868408,
      "learning_rate": 8.734156353478561e-05,
      "loss": 2.2218,
      "step": 11440
    },
    {
      "epoch": 0.6953300540474889,
      "grad_norm": 2.9010980129241943,
      "learning_rate": 8.732041064671684e-05,
      "loss": 2.4401,
      "step": 11450
    },
    {
      "epoch": 0.6959373292038623,
      "grad_norm": 1.8994362354278564,
      "learning_rate": 8.72992426656796e-05,
      "loss": 2.1499,
      "step": 11460
    },
    {
      "epoch": 0.6965446043602356,
      "grad_norm": 1.4053677320480347,
      "learning_rate": 8.72780596002346e-05,
      "loss": 2.2814,
      "step": 11470
    },
    {
      "epoch": 0.697151879516609,
      "grad_norm": 2.6589019298553467,
      "learning_rate": 8.72568614589486e-05,
      "loss": 2.5686,
      "step": 11480
    },
    {
      "epoch": 0.6977591546729823,
      "grad_norm": 2.1260557174682617,
      "learning_rate": 8.723564825039446e-05,
      "loss": 2.4583,
      "step": 11490
    },
    {
      "epoch": 0.6983664298293557,
      "grad_norm": 3.538309335708618,
      "learning_rate": 8.721441998315112e-05,
      "loss": 2.3324,
      "step": 11500
    },
    {
      "epoch": 0.6989737049857291,
      "grad_norm": 2.3715875148773193,
      "learning_rate": 8.719317666580365e-05,
      "loss": 2.1055,
      "step": 11510
    },
    {
      "epoch": 0.6995809801421023,
      "grad_norm": 2.161717653274536,
      "learning_rate": 8.71719183069432e-05,
      "loss": 2.5824,
      "step": 11520
    },
    {
      "epoch": 0.7001882552984757,
      "grad_norm": 2.954040288925171,
      "learning_rate": 8.715064491516696e-05,
      "loss": 3.0389,
      "step": 11530
    },
    {
      "epoch": 0.7007955304548491,
      "grad_norm": 3.533721923828125,
      "learning_rate": 8.712935649907824e-05,
      "loss": 2.3634,
      "step": 11540
    },
    {
      "epoch": 0.7014028056112225,
      "grad_norm": 2.287060022354126,
      "learning_rate": 8.710805306728641e-05,
      "loss": 2.6555,
      "step": 11550
    },
    {
      "epoch": 0.7020100807675957,
      "grad_norm": 2.368189573287964,
      "learning_rate": 8.708673462840693e-05,
      "loss": 2.3734,
      "step": 11560
    },
    {
      "epoch": 0.7026173559239691,
      "grad_norm": 1.9973392486572266,
      "learning_rate": 8.70654011910613e-05,
      "loss": 2.7784,
      "step": 11570
    },
    {
      "epoch": 0.7032246310803425,
      "grad_norm": 3.813711643218994,
      "learning_rate": 8.704405276387713e-05,
      "loss": 2.8612,
      "step": 11580
    },
    {
      "epoch": 0.7038319062367159,
      "grad_norm": 3.54384183883667,
      "learning_rate": 8.702268935548802e-05,
      "loss": 2.7681,
      "step": 11590
    },
    {
      "epoch": 0.7044391813930893,
      "grad_norm": 2.9659247398376465,
      "learning_rate": 8.70013109745337e-05,
      "loss": 2.2178,
      "step": 11600
    },
    {
      "epoch": 0.7050464565494625,
      "grad_norm": 1.2802150249481201,
      "learning_rate": 8.697991762965993e-05,
      "loss": 2.0925,
      "step": 11610
    },
    {
      "epoch": 0.7056537317058359,
      "grad_norm": 2.105222463607788,
      "learning_rate": 8.695850932951852e-05,
      "loss": 2.7548,
      "step": 11620
    },
    {
      "epoch": 0.7062610068622093,
      "grad_norm": 4.5314106941223145,
      "learning_rate": 8.693708608276732e-05,
      "loss": 2.8701,
      "step": 11630
    },
    {
      "epoch": 0.7068682820185826,
      "grad_norm": 4.354351043701172,
      "learning_rate": 8.691564789807023e-05,
      "loss": 2.4745,
      "step": 11640
    },
    {
      "epoch": 0.707475557174956,
      "grad_norm": 3.265119791030884,
      "learning_rate": 8.689419478409719e-05,
      "loss": 2.911,
      "step": 11650
    },
    {
      "epoch": 0.7080828323313293,
      "grad_norm": 4.033328056335449,
      "learning_rate": 8.68727267495242e-05,
      "loss": 2.6005,
      "step": 11660
    },
    {
      "epoch": 0.7086901074877027,
      "grad_norm": 4.0294575691223145,
      "learning_rate": 8.685124380303325e-05,
      "loss": 2.468,
      "step": 11670
    },
    {
      "epoch": 0.709297382644076,
      "grad_norm": 3.4301838874816895,
      "learning_rate": 8.682974595331242e-05,
      "loss": 2.5571,
      "step": 11680
    },
    {
      "epoch": 0.7099046578004494,
      "grad_norm": 2.7208755016326904,
      "learning_rate": 8.680823320905573e-05,
      "loss": 2.4373,
      "step": 11690
    },
    {
      "epoch": 0.7105119329568227,
      "grad_norm": 1.932278037071228,
      "learning_rate": 8.678670557896332e-05,
      "loss": 2.2539,
      "step": 11700
    },
    {
      "epoch": 0.7111192081131961,
      "grad_norm": 1.8511635065078735,
      "learning_rate": 8.676516307174129e-05,
      "loss": 2.3909,
      "step": 11710
    },
    {
      "epoch": 0.7117264832695694,
      "grad_norm": 3.708554983139038,
      "learning_rate": 8.674360569610179e-05,
      "loss": 2.7991,
      "step": 11720
    },
    {
      "epoch": 0.7123337584259428,
      "grad_norm": 3.4265975952148438,
      "learning_rate": 8.672203346076296e-05,
      "loss": 2.6793,
      "step": 11730
    },
    {
      "epoch": 0.7129410335823162,
      "grad_norm": 2.8297042846679688,
      "learning_rate": 8.670044637444891e-05,
      "loss": 2.6672,
      "step": 11740
    },
    {
      "epoch": 0.7135483087386895,
      "grad_norm": 1.712507963180542,
      "learning_rate": 8.667884444588988e-05,
      "loss": 2.3607,
      "step": 11750
    },
    {
      "epoch": 0.7141555838950628,
      "grad_norm": 1.6748040914535522,
      "learning_rate": 8.665722768382199e-05,
      "loss": 2.2089,
      "step": 11760
    },
    {
      "epoch": 0.7147628590514362,
      "grad_norm": 1.7342253923416138,
      "learning_rate": 8.663559609698739e-05,
      "loss": 2.584,
      "step": 11770
    },
    {
      "epoch": 0.7153701342078096,
      "grad_norm": 3.874382972717285,
      "learning_rate": 8.661394969413427e-05,
      "loss": 2.6211,
      "step": 11780
    },
    {
      "epoch": 0.7159774093641829,
      "grad_norm": 3.4614226818084717,
      "learning_rate": 8.659228848401675e-05,
      "loss": 3.0633,
      "step": 11790
    },
    {
      "epoch": 0.7165846845205562,
      "grad_norm": 3.6878087520599365,
      "learning_rate": 8.657061247539499e-05,
      "loss": 2.7167,
      "step": 11800
    },
    {
      "epoch": 0.7171919596769296,
      "grad_norm": 2.2905080318450928,
      "learning_rate": 8.65489216770351e-05,
      "loss": 2.4906,
      "step": 11810
    },
    {
      "epoch": 0.717799234833303,
      "grad_norm": 2.7152750492095947,
      "learning_rate": 8.65272160977092e-05,
      "loss": 2.4945,
      "step": 11820
    },
    {
      "epoch": 0.7184065099896764,
      "grad_norm": 2.367062568664551,
      "learning_rate": 8.650549574619537e-05,
      "loss": 2.7214,
      "step": 11830
    },
    {
      "epoch": 0.7190137851460496,
      "grad_norm": 2.510831594467163,
      "learning_rate": 8.648376063127765e-05,
      "loss": 2.5164,
      "step": 11840
    },
    {
      "epoch": 0.719621060302423,
      "grad_norm": 1.6286613941192627,
      "learning_rate": 8.646201076174608e-05,
      "loss": 2.1187,
      "step": 11850
    },
    {
      "epoch": 0.7202283354587964,
      "grad_norm": 1.5965510606765747,
      "learning_rate": 8.644024614639665e-05,
      "loss": 2.2467,
      "step": 11860
    },
    {
      "epoch": 0.7208356106151698,
      "grad_norm": 2.3933534622192383,
      "learning_rate": 8.641846679403131e-05,
      "loss": 2.6519,
      "step": 11870
    },
    {
      "epoch": 0.7214428857715431,
      "grad_norm": 2.3515665531158447,
      "learning_rate": 8.639667271345798e-05,
      "loss": 2.5174,
      "step": 11880
    },
    {
      "epoch": 0.7220501609279164,
      "grad_norm": 3.7553038597106934,
      "learning_rate": 8.637486391349055e-05,
      "loss": 2.5086,
      "step": 11890
    },
    {
      "epoch": 0.7226574360842898,
      "grad_norm": 2.82210636138916,
      "learning_rate": 8.635304040294883e-05,
      "loss": 2.6263,
      "step": 11900
    },
    {
      "epoch": 0.7232647112406632,
      "grad_norm": 5.5038228034973145,
      "learning_rate": 8.63312021906586e-05,
      "loss": 2.5174,
      "step": 11910
    },
    {
      "epoch": 0.7238719863970365,
      "grad_norm": 2.410322427749634,
      "learning_rate": 8.630934928545156e-05,
      "loss": 2.7475,
      "step": 11920
    },
    {
      "epoch": 0.7244792615534098,
      "grad_norm": 4.554124355316162,
      "learning_rate": 8.62874816961654e-05,
      "loss": 2.549,
      "step": 11930
    },
    {
      "epoch": 0.7250865367097832,
      "grad_norm": 2.554035186767578,
      "learning_rate": 8.626559943164371e-05,
      "loss": 2.2746,
      "step": 11940
    },
    {
      "epoch": 0.7256938118661566,
      "grad_norm": 2.4692511558532715,
      "learning_rate": 8.624370250073606e-05,
      "loss": 2.4697,
      "step": 11950
    },
    {
      "epoch": 0.7263010870225299,
      "grad_norm": 4.0665388107299805,
      "learning_rate": 8.622179091229785e-05,
      "loss": 2.9763,
      "step": 11960
    },
    {
      "epoch": 0.7269083621789033,
      "grad_norm": 3.6282260417938232,
      "learning_rate": 8.619986467519052e-05,
      "loss": 2.6488,
      "step": 11970
    },
    {
      "epoch": 0.7275156373352766,
      "grad_norm": 2.9815902709960938,
      "learning_rate": 8.61779237982814e-05,
      "loss": 2.5356,
      "step": 11980
    },
    {
      "epoch": 0.72812291249165,
      "grad_norm": 2.5579867362976074,
      "learning_rate": 8.615596829044371e-05,
      "loss": 2.5631,
      "step": 11990
    },
    {
      "epoch": 0.7287301876480233,
      "grad_norm": 2.2907872200012207,
      "learning_rate": 8.613399816055658e-05,
      "loss": 2.7221,
      "step": 12000
    },
    {
      "epoch": 0.7293374628043967,
      "grad_norm": 4.127490520477295,
      "learning_rate": 8.611201341750514e-05,
      "loss": 2.4702,
      "step": 12010
    },
    {
      "epoch": 0.72994473796077,
      "grad_norm": 2.083357572555542,
      "learning_rate": 8.609001407018032e-05,
      "loss": 2.301,
      "step": 12020
    },
    {
      "epoch": 0.7305520131171434,
      "grad_norm": 2.482703924179077,
      "learning_rate": 8.606800012747904e-05,
      "loss": 2.3263,
      "step": 12030
    },
    {
      "epoch": 0.7311592882735167,
      "grad_norm": 2.9380152225494385,
      "learning_rate": 8.604597159830407e-05,
      "loss": 2.3825,
      "step": 12040
    },
    {
      "epoch": 0.7317665634298901,
      "grad_norm": 3.2425944805145264,
      "learning_rate": 8.60239284915641e-05,
      "loss": 2.5301,
      "step": 12050
    },
    {
      "epoch": 0.7323738385862635,
      "grad_norm": 2.3856546878814697,
      "learning_rate": 8.600187081617372e-05,
      "loss": 2.3959,
      "step": 12060
    },
    {
      "epoch": 0.7329811137426367,
      "grad_norm": 1.9480715990066528,
      "learning_rate": 8.59797985810534e-05,
      "loss": 2.5315,
      "step": 12070
    },
    {
      "epoch": 0.7335883888990101,
      "grad_norm": 4.169482707977295,
      "learning_rate": 8.59577117951295e-05,
      "loss": 2.5322,
      "step": 12080
    },
    {
      "epoch": 0.7341956640553835,
      "grad_norm": 3.9851677417755127,
      "learning_rate": 8.593561046733429e-05,
      "loss": 2.905,
      "step": 12090
    },
    {
      "epoch": 0.7348029392117569,
      "grad_norm": 2.3589980602264404,
      "learning_rate": 8.591349460660586e-05,
      "loss": 2.7986,
      "step": 12100
    },
    {
      "epoch": 0.7354102143681303,
      "grad_norm": 4.960108280181885,
      "learning_rate": 8.589136422188825e-05,
      "loss": 2.9707,
      "step": 12110
    },
    {
      "epoch": 0.7360174895245035,
      "grad_norm": 3.9063751697540283,
      "learning_rate": 8.586921932213133e-05,
      "loss": 2.3679,
      "step": 12120
    },
    {
      "epoch": 0.7366247646808769,
      "grad_norm": 2.3410685062408447,
      "learning_rate": 8.584705991629085e-05,
      "loss": 2.2307,
      "step": 12130
    },
    {
      "epoch": 0.7372320398372503,
      "grad_norm": 3.320397138595581,
      "learning_rate": 8.582488601332842e-05,
      "loss": 2.7699,
      "step": 12140
    },
    {
      "epoch": 0.7378393149936237,
      "grad_norm": 3.449104070663452,
      "learning_rate": 8.580269762221152e-05,
      "loss": 2.9865,
      "step": 12150
    },
    {
      "epoch": 0.7384465901499969,
      "grad_norm": 4.642445087432861,
      "learning_rate": 8.578049475191349e-05,
      "loss": 2.7348,
      "step": 12160
    },
    {
      "epoch": 0.7390538653063703,
      "grad_norm": 2.631683826446533,
      "learning_rate": 8.575827741141356e-05,
      "loss": 2.3757,
      "step": 12170
    },
    {
      "epoch": 0.7396611404627437,
      "grad_norm": 3.2988593578338623,
      "learning_rate": 8.573604560969672e-05,
      "loss": 2.7262,
      "step": 12180
    },
    {
      "epoch": 0.740268415619117,
      "grad_norm": 3.3551158905029297,
      "learning_rate": 8.571379935575388e-05,
      "loss": 2.5552,
      "step": 12190
    },
    {
      "epoch": 0.7408756907754904,
      "grad_norm": 2.020345687866211,
      "learning_rate": 8.56915386585818e-05,
      "loss": 2.1725,
      "step": 12200
    },
    {
      "epoch": 0.7414829659318637,
      "grad_norm": 2.195716142654419,
      "learning_rate": 8.566926352718305e-05,
      "loss": 2.5121,
      "step": 12210
    },
    {
      "epoch": 0.7420902410882371,
      "grad_norm": 2.804353952407837,
      "learning_rate": 8.564697397056605e-05,
      "loss": 2.8907,
      "step": 12220
    },
    {
      "epoch": 0.7426975162446104,
      "grad_norm": 3.9870293140411377,
      "learning_rate": 8.562466999774503e-05,
      "loss": 2.6383,
      "step": 12230
    },
    {
      "epoch": 0.7433047914009838,
      "grad_norm": 2.4484710693359375,
      "learning_rate": 8.56023516177401e-05,
      "loss": 2.8208,
      "step": 12240
    },
    {
      "epoch": 0.7439120665573571,
      "grad_norm": 2.685448169708252,
      "learning_rate": 8.558001883957717e-05,
      "loss": 2.7254,
      "step": 12250
    },
    {
      "epoch": 0.7445193417137305,
      "grad_norm": 2.2661547660827637,
      "learning_rate": 8.555767167228796e-05,
      "loss": 2.4185,
      "step": 12260
    },
    {
      "epoch": 0.7451266168701038,
      "grad_norm": 2.602473258972168,
      "learning_rate": 8.553531012491e-05,
      "loss": 2.5444,
      "step": 12270
    },
    {
      "epoch": 0.7457338920264772,
      "grad_norm": 2.4099934101104736,
      "learning_rate": 8.55129342064867e-05,
      "loss": 2.1561,
      "step": 12280
    },
    {
      "epoch": 0.7463411671828506,
      "grad_norm": 1.9889905452728271,
      "learning_rate": 8.549054392606719e-05,
      "loss": 2.2029,
      "step": 12290
    },
    {
      "epoch": 0.7469484423392239,
      "grad_norm": 1.427160620689392,
      "learning_rate": 8.54681392927065e-05,
      "loss": 2.2326,
      "step": 12300
    },
    {
      "epoch": 0.7475557174955972,
      "grad_norm": 1.6829954385757446,
      "learning_rate": 8.544572031546539e-05,
      "loss": 2.1735,
      "step": 12310
    },
    {
      "epoch": 0.7481629926519706,
      "grad_norm": 2.298722505569458,
      "learning_rate": 8.542328700341046e-05,
      "loss": 2.3039,
      "step": 12320
    },
    {
      "epoch": 0.748770267808344,
      "grad_norm": 2.367161989212036,
      "learning_rate": 8.54008393656141e-05,
      "loss": 2.4927,
      "step": 12330
    },
    {
      "epoch": 0.7493775429647173,
      "grad_norm": 1.292523741722107,
      "learning_rate": 8.537837741115449e-05,
      "loss": 2.3177,
      "step": 12340
    },
    {
      "epoch": 0.7499848181210906,
      "grad_norm": 1.629451870918274,
      "learning_rate": 8.535590114911561e-05,
      "loss": 2.0671,
      "step": 12350
    },
    {
      "epoch": 0.750592093277464,
      "grad_norm": 1.9883320331573486,
      "learning_rate": 8.533341058858721e-05,
      "loss": 2.3407,
      "step": 12360
    },
    {
      "epoch": 0.7511993684338374,
      "grad_norm": 1.7368723154067993,
      "learning_rate": 8.531090573866485e-05,
      "loss": 2.2881,
      "step": 12370
    },
    {
      "epoch": 0.7518066435902108,
      "grad_norm": 1.8307616710662842,
      "learning_rate": 8.528838660844982e-05,
      "loss": 2.5488,
      "step": 12380
    },
    {
      "epoch": 0.752413918746584,
      "grad_norm": 2.095053195953369,
      "learning_rate": 8.526585320704923e-05,
      "loss": 2.0862,
      "step": 12390
    },
    {
      "epoch": 0.7530211939029574,
      "grad_norm": 1.9407196044921875,
      "learning_rate": 8.524330554357594e-05,
      "loss": 2.4531,
      "step": 12400
    },
    {
      "epoch": 0.7536284690593308,
      "grad_norm": 2.0376124382019043,
      "learning_rate": 8.522074362714859e-05,
      "loss": 2.4825,
      "step": 12410
    },
    {
      "epoch": 0.7542357442157042,
      "grad_norm": 2.9444565773010254,
      "learning_rate": 8.519816746689157e-05,
      "loss": 2.6911,
      "step": 12420
    },
    {
      "epoch": 0.7548430193720775,
      "grad_norm": 3.3410415649414062,
      "learning_rate": 8.517557707193506e-05,
      "loss": 2.5068,
      "step": 12430
    },
    {
      "epoch": 0.7554502945284508,
      "grad_norm": 2.4086990356445312,
      "learning_rate": 8.515297245141497e-05,
      "loss": 2.6127,
      "step": 12440
    },
    {
      "epoch": 0.7560575696848242,
      "grad_norm": 2.7177772521972656,
      "learning_rate": 8.513035361447294e-05,
      "loss": 2.7316,
      "step": 12450
    },
    {
      "epoch": 0.7566648448411976,
      "grad_norm": 1.6825549602508545,
      "learning_rate": 8.510772057025643e-05,
      "loss": 2.1345,
      "step": 12460
    },
    {
      "epoch": 0.7572721199975709,
      "grad_norm": 1.9317388534545898,
      "learning_rate": 8.508507332791857e-05,
      "loss": 2.4682,
      "step": 12470
    },
    {
      "epoch": 0.7578793951539442,
      "grad_norm": 3.309476137161255,
      "learning_rate": 8.506241189661827e-05,
      "loss": 2.3302,
      "step": 12480
    },
    {
      "epoch": 0.7584866703103176,
      "grad_norm": 1.859839677810669,
      "learning_rate": 8.50397362855202e-05,
      "loss": 1.9408,
      "step": 12490
    },
    {
      "epoch": 0.759093945466691,
      "grad_norm": 1.459124207496643,
      "learning_rate": 8.50170465037947e-05,
      "loss": 2.1485,
      "step": 12500
    },
    {
      "epoch": 0.7597012206230643,
      "grad_norm": 1.0410512685775757,
      "learning_rate": 8.49943425606179e-05,
      "loss": 2.0481,
      "step": 12510
    },
    {
      "epoch": 0.7603084957794377,
      "grad_norm": 1.0356611013412476,
      "learning_rate": 8.497162446517164e-05,
      "loss": 1.8196,
      "step": 12520
    },
    {
      "epoch": 0.760915770935811,
      "grad_norm": 2.2021400928497314,
      "learning_rate": 8.494889222664348e-05,
      "loss": 2.4323,
      "step": 12530
    },
    {
      "epoch": 0.7615230460921844,
      "grad_norm": 2.3735203742980957,
      "learning_rate": 8.492614585422669e-05,
      "loss": 2.8658,
      "step": 12540
    },
    {
      "epoch": 0.7621303212485577,
      "grad_norm": 3.0863265991210938,
      "learning_rate": 8.490338535712026e-05,
      "loss": 2.3499,
      "step": 12550
    },
    {
      "epoch": 0.7627375964049311,
      "grad_norm": 2.730555772781372,
      "learning_rate": 8.488061074452891e-05,
      "loss": 2.414,
      "step": 12560
    },
    {
      "epoch": 0.7633448715613044,
      "grad_norm": 3.928560733795166,
      "learning_rate": 8.485782202566306e-05,
      "loss": 2.6671,
      "step": 12570
    },
    {
      "epoch": 0.7639521467176777,
      "grad_norm": 6.450390338897705,
      "learning_rate": 8.483501920973882e-05,
      "loss": 2.548,
      "step": 12580
    },
    {
      "epoch": 0.7645594218740511,
      "grad_norm": 3.4004361629486084,
      "learning_rate": 8.481220230597801e-05,
      "loss": 2.5146,
      "step": 12590
    },
    {
      "epoch": 0.7651666970304245,
      "grad_norm": 1.7330594062805176,
      "learning_rate": 8.478937132360816e-05,
      "loss": 2.3994,
      "step": 12600
    },
    {
      "epoch": 0.7657739721867979,
      "grad_norm": 3.00627064704895,
      "learning_rate": 8.476652627186248e-05,
      "loss": 2.5771,
      "step": 12610
    },
    {
      "epoch": 0.7663812473431711,
      "grad_norm": 7.128241539001465,
      "learning_rate": 8.474366715997986e-05,
      "loss": 2.6336,
      "step": 12620
    },
    {
      "epoch": 0.7669885224995445,
      "grad_norm": 3.4288289546966553,
      "learning_rate": 8.47207939972049e-05,
      "loss": 3.089,
      "step": 12630
    },
    {
      "epoch": 0.7675957976559179,
      "grad_norm": 4.700195789337158,
      "learning_rate": 8.469790679278789e-05,
      "loss": 2.8137,
      "step": 12640
    },
    {
      "epoch": 0.7682030728122913,
      "grad_norm": 3.2817933559417725,
      "learning_rate": 8.467500555598473e-05,
      "loss": 2.466,
      "step": 12650
    },
    {
      "epoch": 0.7688103479686647,
      "grad_norm": 3.635267496109009,
      "learning_rate": 8.465209029605709e-05,
      "loss": 2.7356,
      "step": 12660
    },
    {
      "epoch": 0.7694176231250379,
      "grad_norm": 2.351529359817505,
      "learning_rate": 8.462916102227226e-05,
      "loss": 2.3176,
      "step": 12670
    },
    {
      "epoch": 0.7700248982814113,
      "grad_norm": 1.876102089881897,
      "learning_rate": 8.460621774390319e-05,
      "loss": 2.3651,
      "step": 12680
    },
    {
      "epoch": 0.7706321734377847,
      "grad_norm": 1.9611821174621582,
      "learning_rate": 8.458326047022852e-05,
      "loss": 2.2849,
      "step": 12690
    },
    {
      "epoch": 0.771239448594158,
      "grad_norm": 2.0260138511657715,
      "learning_rate": 8.45602892105325e-05,
      "loss": 3.1043,
      "step": 12700
    },
    {
      "epoch": 0.7718467237505313,
      "grad_norm": 3.062626361846924,
      "learning_rate": 8.453730397410512e-05,
      "loss": 2.6477,
      "step": 12710
    },
    {
      "epoch": 0.7724539989069047,
      "grad_norm": 2.758742570877075,
      "learning_rate": 8.451430477024196e-05,
      "loss": 2.4825,
      "step": 12720
    },
    {
      "epoch": 0.7730612740632781,
      "grad_norm": 2.8642354011535645,
      "learning_rate": 8.449129160824427e-05,
      "loss": 2.4363,
      "step": 12730
    },
    {
      "epoch": 0.7736685492196514,
      "grad_norm": 2.541844606399536,
      "learning_rate": 8.446826449741891e-05,
      "loss": 2.5599,
      "step": 12740
    },
    {
      "epoch": 0.7742758243760248,
      "grad_norm": 5.834018230438232,
      "learning_rate": 8.444522344707844e-05,
      "loss": 2.6359,
      "step": 12750
    },
    {
      "epoch": 0.7748830995323981,
      "grad_norm": 3.472008466720581,
      "learning_rate": 8.442216846654102e-05,
      "loss": 2.3415,
      "step": 12760
    },
    {
      "epoch": 0.7754903746887715,
      "grad_norm": 2.1439666748046875,
      "learning_rate": 8.439909956513046e-05,
      "loss": 2.135,
      "step": 12770
    },
    {
      "epoch": 0.7760976498451448,
      "grad_norm": 3.7304089069366455,
      "learning_rate": 8.437601675217616e-05,
      "loss": 2.5596,
      "step": 12780
    },
    {
      "epoch": 0.7767049250015182,
      "grad_norm": 1.5036545991897583,
      "learning_rate": 8.435292003701323e-05,
      "loss": 2.2213,
      "step": 12790
    },
    {
      "epoch": 0.7773122001578915,
      "grad_norm": 2.005058765411377,
      "learning_rate": 8.43298094289823e-05,
      "loss": 2.2357,
      "step": 12800
    },
    {
      "epoch": 0.7779194753142649,
      "grad_norm": 2.5616648197174072,
      "learning_rate": 8.43066849374297e-05,
      "loss": 2.4601,
      "step": 12810
    },
    {
      "epoch": 0.7785267504706382,
      "grad_norm": 2.1473047733306885,
      "learning_rate": 8.42835465717073e-05,
      "loss": 2.0917,
      "step": 12820
    },
    {
      "epoch": 0.7791340256270116,
      "grad_norm": 2.136707067489624,
      "learning_rate": 8.426039434117268e-05,
      "loss": 2.6254,
      "step": 12830
    },
    {
      "epoch": 0.779741300783385,
      "grad_norm": 2.4741482734680176,
      "learning_rate": 8.423722825518893e-05,
      "loss": 2.4613,
      "step": 12840
    },
    {
      "epoch": 0.7803485759397583,
      "grad_norm": 2.643094301223755,
      "learning_rate": 8.421404832312482e-05,
      "loss": 2.338,
      "step": 12850
    },
    {
      "epoch": 0.7809558510961316,
      "grad_norm": 2.091867685317993,
      "learning_rate": 8.419085455435465e-05,
      "loss": 2.1584,
      "step": 12860
    },
    {
      "epoch": 0.781563126252505,
      "grad_norm": 1.9694041013717651,
      "learning_rate": 8.416764695825835e-05,
      "loss": 2.3327,
      "step": 12870
    },
    {
      "epoch": 0.7821704014088784,
      "grad_norm": 1.7031898498535156,
      "learning_rate": 8.414442554422146e-05,
      "loss": 2.731,
      "step": 12880
    },
    {
      "epoch": 0.7827776765652518,
      "grad_norm": 2.5709309577941895,
      "learning_rate": 8.412119032163508e-05,
      "loss": 2.4037,
      "step": 12890
    },
    {
      "epoch": 0.783384951721625,
      "grad_norm": 2.479111433029175,
      "learning_rate": 8.40979412998959e-05,
      "loss": 2.8875,
      "step": 12900
    },
    {
      "epoch": 0.7839922268779984,
      "grad_norm": 3.3739397525787354,
      "learning_rate": 8.407467848840621e-05,
      "loss": 2.6635,
      "step": 12910
    },
    {
      "epoch": 0.7845995020343718,
      "grad_norm": 2.9242236614227295,
      "learning_rate": 8.405140189657385e-05,
      "loss": 2.7224,
      "step": 12920
    },
    {
      "epoch": 0.7852067771907452,
      "grad_norm": 2.5716285705566406,
      "learning_rate": 8.402811153381224e-05,
      "loss": 2.6009,
      "step": 12930
    },
    {
      "epoch": 0.7858140523471184,
      "grad_norm": 2.881065845489502,
      "learning_rate": 8.40048074095404e-05,
      "loss": 2.3889,
      "step": 12940
    },
    {
      "epoch": 0.7864213275034918,
      "grad_norm": 3.3019120693206787,
      "learning_rate": 8.398148953318285e-05,
      "loss": 2.5101,
      "step": 12950
    },
    {
      "epoch": 0.7870286026598652,
      "grad_norm": 2.1673166751861572,
      "learning_rate": 8.395815791416975e-05,
      "loss": 2.4984,
      "step": 12960
    },
    {
      "epoch": 0.7876358778162386,
      "grad_norm": 2.124929666519165,
      "learning_rate": 8.393481256193674e-05,
      "loss": 2.5633,
      "step": 12970
    },
    {
      "epoch": 0.7882431529726119,
      "grad_norm": 2.5952885150909424,
      "learning_rate": 8.391145348592506e-05,
      "loss": 2.1786,
      "step": 12980
    },
    {
      "epoch": 0.7888504281289852,
      "grad_norm": 2.5406508445739746,
      "learning_rate": 8.388808069558153e-05,
      "loss": 2.0521,
      "step": 12990
    },
    {
      "epoch": 0.7894577032853586,
      "grad_norm": 2.3589870929718018,
      "learning_rate": 8.386469420035845e-05,
      "loss": 2.5765,
      "step": 13000
    },
    {
      "epoch": 0.790064978441732,
      "grad_norm": 2.960855484008789,
      "learning_rate": 8.384129400971368e-05,
      "loss": 2.1828,
      "step": 13010
    },
    {
      "epoch": 0.7906722535981053,
      "grad_norm": 2.806478261947632,
      "learning_rate": 8.381788013311065e-05,
      "loss": 2.5048,
      "step": 13020
    },
    {
      "epoch": 0.7912795287544786,
      "grad_norm": 2.5773706436157227,
      "learning_rate": 8.379445258001828e-05,
      "loss": 2.473,
      "step": 13030
    },
    {
      "epoch": 0.791886803910852,
      "grad_norm": 2.9111194610595703,
      "learning_rate": 8.377101135991108e-05,
      "loss": 2.629,
      "step": 13040
    },
    {
      "epoch": 0.7924940790672254,
      "grad_norm": 3.5611307621002197,
      "learning_rate": 8.374755648226903e-05,
      "loss": 2.7101,
      "step": 13050
    },
    {
      "epoch": 0.7931013542235987,
      "grad_norm": 2.315772533416748,
      "learning_rate": 8.372408795657766e-05,
      "loss": 2.2226,
      "step": 13060
    },
    {
      "epoch": 0.7937086293799721,
      "grad_norm": 1.711525559425354,
      "learning_rate": 8.370060579232802e-05,
      "loss": 2.4281,
      "step": 13070
    },
    {
      "epoch": 0.7943159045363454,
      "grad_norm": 2.324127435684204,
      "learning_rate": 8.367710999901667e-05,
      "loss": 2.0477,
      "step": 13080
    },
    {
      "epoch": 0.7949231796927188,
      "grad_norm": 1.2778127193450928,
      "learning_rate": 8.365360058614567e-05,
      "loss": 1.8017,
      "step": 13090
    },
    {
      "epoch": 0.7955304548490921,
      "grad_norm": 1.9032167196273804,
      "learning_rate": 8.363007756322263e-05,
      "loss": 2.9376,
      "step": 13100
    },
    {
      "epoch": 0.7961377300054655,
      "grad_norm": 2.874929189682007,
      "learning_rate": 8.360654093976061e-05,
      "loss": 2.6261,
      "step": 13110
    },
    {
      "epoch": 0.7967450051618389,
      "grad_norm": 4.557077407836914,
      "learning_rate": 8.35829907252782e-05,
      "loss": 2.477,
      "step": 13120
    },
    {
      "epoch": 0.7973522803182121,
      "grad_norm": 4.758883953094482,
      "learning_rate": 8.355942692929948e-05,
      "loss": 2.4263,
      "step": 13130
    },
    {
      "epoch": 0.7979595554745855,
      "grad_norm": 2.719919443130493,
      "learning_rate": 8.353584956135405e-05,
      "loss": 2.4558,
      "step": 13140
    },
    {
      "epoch": 0.7985668306309589,
      "grad_norm": 2.0837626457214355,
      "learning_rate": 8.351225863097693e-05,
      "loss": 2.6519,
      "step": 13150
    },
    {
      "epoch": 0.7991741057873323,
      "grad_norm": 1.9154090881347656,
      "learning_rate": 8.34886541477087e-05,
      "loss": 2.3142,
      "step": 13160
    },
    {
      "epoch": 0.7997813809437055,
      "grad_norm": 2.6755120754241943,
      "learning_rate": 8.346503612109537e-05,
      "loss": 2.3519,
      "step": 13170
    },
    {
      "epoch": 0.8003886561000789,
      "grad_norm": 2.5423855781555176,
      "learning_rate": 8.344140456068846e-05,
      "loss": 2.3536,
      "step": 13180
    },
    {
      "epoch": 0.8009959312564523,
      "grad_norm": 2.4292235374450684,
      "learning_rate": 8.341775947604495e-05,
      "loss": 2.3336,
      "step": 13190
    },
    {
      "epoch": 0.8016032064128257,
      "grad_norm": 1.8967987298965454,
      "learning_rate": 8.339410087672727e-05,
      "loss": 2.2914,
      "step": 13200
    },
    {
      "epoch": 0.802210481569199,
      "grad_norm": 2.057884931564331,
      "learning_rate": 8.337042877230337e-05,
      "loss": 2.9064,
      "step": 13210
    },
    {
      "epoch": 0.8028177567255723,
      "grad_norm": 2.6186962127685547,
      "learning_rate": 8.334674317234659e-05,
      "loss": 2.7126,
      "step": 13220
    },
    {
      "epoch": 0.8034250318819457,
      "grad_norm": 3.4090452194213867,
      "learning_rate": 8.332304408643577e-05,
      "loss": 3.0608,
      "step": 13230
    },
    {
      "epoch": 0.8040323070383191,
      "grad_norm": 3.7857513427734375,
      "learning_rate": 8.32993315241552e-05,
      "loss": 2.6376,
      "step": 13240
    },
    {
      "epoch": 0.8046395821946924,
      "grad_norm": 2.4018149375915527,
      "learning_rate": 8.327560549509465e-05,
      "loss": 2.3394,
      "step": 13250
    },
    {
      "epoch": 0.8052468573510657,
      "grad_norm": 1.6329115629196167,
      "learning_rate": 8.325186600884926e-05,
      "loss": 2.3094,
      "step": 13260
    },
    {
      "epoch": 0.8058541325074391,
      "grad_norm": 2.6282739639282227,
      "learning_rate": 8.322811307501968e-05,
      "loss": 2.6324,
      "step": 13270
    },
    {
      "epoch": 0.8064614076638125,
      "grad_norm": 3.3258767127990723,
      "learning_rate": 8.320434670321196e-05,
      "loss": 2.4961,
      "step": 13280
    },
    {
      "epoch": 0.8070686828201858,
      "grad_norm": 2.4998958110809326,
      "learning_rate": 8.318056690303762e-05,
      "loss": 2.6439,
      "step": 13290
    },
    {
      "epoch": 0.8076759579765592,
      "grad_norm": 5.0452494621276855,
      "learning_rate": 8.315677368411356e-05,
      "loss": 2.5144,
      "step": 13300
    },
    {
      "epoch": 0.8082832331329325,
      "grad_norm": 2.682633876800537,
      "learning_rate": 8.313296705606217e-05,
      "loss": 2.4193,
      "step": 13310
    },
    {
      "epoch": 0.8088905082893059,
      "grad_norm": 1.8968080282211304,
      "learning_rate": 8.31091470285112e-05,
      "loss": 2.6738,
      "step": 13320
    },
    {
      "epoch": 0.8094977834456792,
      "grad_norm": 3.1589014530181885,
      "learning_rate": 8.308531361109389e-05,
      "loss": 3.0168,
      "step": 13330
    },
    {
      "epoch": 0.8101050586020526,
      "grad_norm": 4.218478202819824,
      "learning_rate": 8.30614668134488e-05,
      "loss": 2.9548,
      "step": 13340
    },
    {
      "epoch": 0.810712333758426,
      "grad_norm": 4.336192607879639,
      "learning_rate": 8.303760664521998e-05,
      "loss": 3.0472,
      "step": 13350
    },
    {
      "epoch": 0.8113196089147993,
      "grad_norm": 3.2313005924224854,
      "learning_rate": 8.301373311605687e-05,
      "loss": 2.6008,
      "step": 13360
    },
    {
      "epoch": 0.8119268840711726,
      "grad_norm": 2.6195454597473145,
      "learning_rate": 8.29898462356143e-05,
      "loss": 2.4358,
      "step": 13370
    },
    {
      "epoch": 0.812534159227546,
      "grad_norm": 2.825472593307495,
      "learning_rate": 8.29659460135525e-05,
      "loss": 2.6959,
      "step": 13380
    },
    {
      "epoch": 0.8131414343839194,
      "grad_norm": 1.622677206993103,
      "learning_rate": 8.294203245953709e-05,
      "loss": 2.2056,
      "step": 13390
    },
    {
      "epoch": 0.8137487095402927,
      "grad_norm": 1.7132394313812256,
      "learning_rate": 8.291810558323911e-05,
      "loss": 2.2276,
      "step": 13400
    },
    {
      "epoch": 0.814355984696666,
      "grad_norm": 1.8683552742004395,
      "learning_rate": 8.289416539433498e-05,
      "loss": 2.1035,
      "step": 13410
    },
    {
      "epoch": 0.8149632598530394,
      "grad_norm": 2.5628504753112793,
      "learning_rate": 8.287021190250647e-05,
      "loss": 2.3154,
      "step": 13420
    },
    {
      "epoch": 0.8155705350094128,
      "grad_norm": 2.6364221572875977,
      "learning_rate": 8.284624511744076e-05,
      "loss": 2.474,
      "step": 13430
    },
    {
      "epoch": 0.8161778101657862,
      "grad_norm": 2.844710350036621,
      "learning_rate": 8.282226504883042e-05,
      "loss": 2.1227,
      "step": 13440
    },
    {
      "epoch": 0.8167850853221594,
      "grad_norm": 2.473994493484497,
      "learning_rate": 8.279827170637333e-05,
      "loss": 2.386,
      "step": 13450
    },
    {
      "epoch": 0.8173923604785328,
      "grad_norm": 2.2195279598236084,
      "learning_rate": 8.277426509977281e-05,
      "loss": 2.8405,
      "step": 13460
    },
    {
      "epoch": 0.8179996356349062,
      "grad_norm": 3.744962215423584,
      "learning_rate": 8.275024523873753e-05,
      "loss": 2.6105,
      "step": 13470
    },
    {
      "epoch": 0.8186069107912796,
      "grad_norm": 1.6180806159973145,
      "learning_rate": 8.272621213298146e-05,
      "loss": 2.302,
      "step": 13480
    },
    {
      "epoch": 0.8192141859476528,
      "grad_norm": 2.8582286834716797,
      "learning_rate": 8.270216579222398e-05,
      "loss": 2.1161,
      "step": 13490
    },
    {
      "epoch": 0.8198214611040262,
      "grad_norm": 2.333277463912964,
      "learning_rate": 8.267810622618986e-05,
      "loss": 2.4821,
      "step": 13500
    },
    {
      "epoch": 0.8204287362603996,
      "grad_norm": 1.9967674016952515,
      "learning_rate": 8.265403344460911e-05,
      "loss": 2.5312,
      "step": 13510
    },
    {
      "epoch": 0.821036011416773,
      "grad_norm": 2.542839765548706,
      "learning_rate": 8.26299474572172e-05,
      "loss": 2.3433,
      "step": 13520
    },
    {
      "epoch": 0.8216432865731463,
      "grad_norm": 3.862605333328247,
      "learning_rate": 8.260584827375484e-05,
      "loss": 2.2608,
      "step": 13530
    },
    {
      "epoch": 0.8222505617295196,
      "grad_norm": 1.4928942918777466,
      "learning_rate": 8.258173590396814e-05,
      "loss": 2.6259,
      "step": 13540
    },
    {
      "epoch": 0.822857836885893,
      "grad_norm": 2.0023536682128906,
      "learning_rate": 8.255761035760853e-05,
      "loss": 2.2278,
      "step": 13550
    },
    {
      "epoch": 0.8234651120422664,
      "grad_norm": 3.051513195037842,
      "learning_rate": 8.25334716444328e-05,
      "loss": 2.2445,
      "step": 13560
    },
    {
      "epoch": 0.8240723871986397,
      "grad_norm": 1.7742516994476318,
      "learning_rate": 8.250931977420296e-05,
      "loss": 1.996,
      "step": 13570
    },
    {
      "epoch": 0.8246796623550131,
      "grad_norm": 1.3505935668945312,
      "learning_rate": 8.248515475668645e-05,
      "loss": 2.2175,
      "step": 13580
    },
    {
      "epoch": 0.8252869375113864,
      "grad_norm": 2.2673656940460205,
      "learning_rate": 8.2460976601656e-05,
      "loss": 2.2615,
      "step": 13590
    },
    {
      "epoch": 0.8258942126677598,
      "grad_norm": 3.0157172679901123,
      "learning_rate": 8.243678531888962e-05,
      "loss": 2.3413,
      "step": 13600
    },
    {
      "epoch": 0.8265014878241331,
      "grad_norm": 2.931645154953003,
      "learning_rate": 8.241258091817066e-05,
      "loss": 2.4041,
      "step": 13610
    },
    {
      "epoch": 0.8271087629805065,
      "grad_norm": 2.87677001953125,
      "learning_rate": 8.238836340928775e-05,
      "loss": 2.6186,
      "step": 13620
    },
    {
      "epoch": 0.8277160381368798,
      "grad_norm": 3.2362022399902344,
      "learning_rate": 8.236413280203486e-05,
      "loss": 2.3927,
      "step": 13630
    },
    {
      "epoch": 0.8283233132932531,
      "grad_norm": 3.1075081825256348,
      "learning_rate": 8.233988910621121e-05,
      "loss": 2.7662,
      "step": 13640
    },
    {
      "epoch": 0.8289305884496265,
      "grad_norm": 2.1110453605651855,
      "learning_rate": 8.231563233162134e-05,
      "loss": 2.4271,
      "step": 13650
    },
    {
      "epoch": 0.8295378636059999,
      "grad_norm": 1.6828699111938477,
      "learning_rate": 8.22913624880751e-05,
      "loss": 2.246,
      "step": 13660
    },
    {
      "epoch": 0.8301451387623733,
      "grad_norm": 1.2686536312103271,
      "learning_rate": 8.226707958538757e-05,
      "loss": 2.4472,
      "step": 13670
    },
    {
      "epoch": 0.8307524139187465,
      "grad_norm": 3.2288880348205566,
      "learning_rate": 8.224278363337916e-05,
      "loss": 2.2723,
      "step": 13680
    },
    {
      "epoch": 0.8313596890751199,
      "grad_norm": 2.7412126064300537,
      "learning_rate": 8.221847464187556e-05,
      "loss": 2.5299,
      "step": 13690
    },
    {
      "epoch": 0.8319669642314933,
      "grad_norm": 4.39555549621582,
      "learning_rate": 8.219415262070766e-05,
      "loss": 2.5416,
      "step": 13700
    },
    {
      "epoch": 0.8325742393878667,
      "grad_norm": 2.048339605331421,
      "learning_rate": 8.216981757971173e-05,
      "loss": 2.3627,
      "step": 13710
    },
    {
      "epoch": 0.8331815145442399,
      "grad_norm": 2.835141181945801,
      "learning_rate": 8.21454695287292e-05,
      "loss": 2.3699,
      "step": 13720
    },
    {
      "epoch": 0.8337887897006133,
      "grad_norm": 1.7608753442764282,
      "learning_rate": 8.212110847760684e-05,
      "loss": 2.4842,
      "step": 13730
    },
    {
      "epoch": 0.8343960648569867,
      "grad_norm": 4.060382843017578,
      "learning_rate": 8.209673443619664e-05,
      "loss": 2.0942,
      "step": 13740
    },
    {
      "epoch": 0.8350033400133601,
      "grad_norm": 3.5765583515167236,
      "learning_rate": 8.207234741435585e-05,
      "loss": 2.5347,
      "step": 13750
    },
    {
      "epoch": 0.8356106151697335,
      "grad_norm": 2.8071422576904297,
      "learning_rate": 8.204794742194698e-05,
      "loss": 2.4106,
      "step": 13760
    },
    {
      "epoch": 0.8362178903261067,
      "grad_norm": 3.2632009983062744,
      "learning_rate": 8.202353446883773e-05,
      "loss": 2.5065,
      "step": 13770
    },
    {
      "epoch": 0.8368251654824801,
      "grad_norm": 1.4529907703399658,
      "learning_rate": 8.199910856490114e-05,
      "loss": 2.6212,
      "step": 13780
    },
    {
      "epoch": 0.8374324406388535,
      "grad_norm": 3.3968262672424316,
      "learning_rate": 8.197466972001542e-05,
      "loss": 2.4654,
      "step": 13790
    },
    {
      "epoch": 0.8380397157952268,
      "grad_norm": 2.1333422660827637,
      "learning_rate": 8.195021794406401e-05,
      "loss": 2.1672,
      "step": 13800
    },
    {
      "epoch": 0.8386469909516002,
      "grad_norm": 2.8348331451416016,
      "learning_rate": 8.192575324693564e-05,
      "loss": 2.6651,
      "step": 13810
    },
    {
      "epoch": 0.8392542661079735,
      "grad_norm": 4.144172668457031,
      "learning_rate": 8.190127563852417e-05,
      "loss": 2.8167,
      "step": 13820
    },
    {
      "epoch": 0.8398615412643469,
      "grad_norm": 2.990058422088623,
      "learning_rate": 8.187678512872875e-05,
      "loss": 2.4382,
      "step": 13830
    },
    {
      "epoch": 0.8404688164207202,
      "grad_norm": 2.9836502075195312,
      "learning_rate": 8.185228172745376e-05,
      "loss": 2.114,
      "step": 13840
    },
    {
      "epoch": 0.8410760915770936,
      "grad_norm": 2.6517345905303955,
      "learning_rate": 8.182776544460875e-05,
      "loss": 2.2129,
      "step": 13850
    },
    {
      "epoch": 0.8416833667334669,
      "grad_norm": 2.177889585494995,
      "learning_rate": 8.180323629010848e-05,
      "loss": 2.402,
      "step": 13860
    },
    {
      "epoch": 0.8422906418898403,
      "grad_norm": 2.4541287422180176,
      "learning_rate": 8.177869427387296e-05,
      "loss": 2.1343,
      "step": 13870
    },
    {
      "epoch": 0.8428979170462136,
      "grad_norm": 1.1903932094573975,
      "learning_rate": 8.175413940582734e-05,
      "loss": 2.027,
      "step": 13880
    },
    {
      "epoch": 0.843505192202587,
      "grad_norm": 2.189837694168091,
      "learning_rate": 8.172957169590202e-05,
      "loss": 2.2826,
      "step": 13890
    },
    {
      "epoch": 0.8441124673589604,
      "grad_norm": 2.6319968700408936,
      "learning_rate": 8.170499115403256e-05,
      "loss": 2.5356,
      "step": 13900
    },
    {
      "epoch": 0.8447197425153337,
      "grad_norm": 3.175452470779419,
      "learning_rate": 8.168039779015974e-05,
      "loss": 2.4329,
      "step": 13910
    },
    {
      "epoch": 0.845327017671707,
      "grad_norm": 2.3415842056274414,
      "learning_rate": 8.16557916142295e-05,
      "loss": 2.6731,
      "step": 13920
    },
    {
      "epoch": 0.8459342928280804,
      "grad_norm": 1.3133344650268555,
      "learning_rate": 8.163117263619297e-05,
      "loss": 2.2982,
      "step": 13930
    },
    {
      "epoch": 0.8465415679844538,
      "grad_norm": 1.575184941291809,
      "learning_rate": 8.160654086600646e-05,
      "loss": 2.1976,
      "step": 13940
    },
    {
      "epoch": 0.8471488431408271,
      "grad_norm": 2.800210475921631,
      "learning_rate": 8.158189631363142e-05,
      "loss": 2.4166,
      "step": 13950
    },
    {
      "epoch": 0.8477561182972004,
      "grad_norm": 2.47287917137146,
      "learning_rate": 8.155723898903455e-05,
      "loss": 2.3407,
      "step": 13960
    },
    {
      "epoch": 0.8483633934535738,
      "grad_norm": 1.5428630113601685,
      "learning_rate": 8.153256890218764e-05,
      "loss": 2.4305,
      "step": 13970
    },
    {
      "epoch": 0.8489706686099472,
      "grad_norm": 1.8768900632858276,
      "learning_rate": 8.150788606306765e-05,
      "loss": 2.4644,
      "step": 13980
    },
    {
      "epoch": 0.8495779437663206,
      "grad_norm": 2.8523519039154053,
      "learning_rate": 8.148319048165674e-05,
      "loss": 2.5282,
      "step": 13990
    },
    {
      "epoch": 0.8501852189226938,
      "grad_norm": 2.360447645187378,
      "learning_rate": 8.145848216794219e-05,
      "loss": 2.6921,
      "step": 14000
    },
    {
      "epoch": 0.8507924940790672,
      "grad_norm": 3.7220401763916016,
      "learning_rate": 8.143376113191641e-05,
      "loss": 2.6553,
      "step": 14010
    },
    {
      "epoch": 0.8513997692354406,
      "grad_norm": 3.6995880603790283,
      "learning_rate": 8.140902738357702e-05,
      "loss": 2.1895,
      "step": 14020
    },
    {
      "epoch": 0.852007044391814,
      "grad_norm": 2.649040937423706,
      "learning_rate": 8.138428093292672e-05,
      "loss": 2.1989,
      "step": 14030
    },
    {
      "epoch": 0.8526143195481873,
      "grad_norm": 2.2280173301696777,
      "learning_rate": 8.135952178997338e-05,
      "loss": 2.2253,
      "step": 14040
    },
    {
      "epoch": 0.8532215947045606,
      "grad_norm": 2.132962465286255,
      "learning_rate": 8.133474996472996e-05,
      "loss": 2.4112,
      "step": 14050
    },
    {
      "epoch": 0.853828869860934,
      "grad_norm": 2.3102171421051025,
      "learning_rate": 8.130996546721462e-05,
      "loss": 2.1681,
      "step": 14060
    },
    {
      "epoch": 0.8544361450173074,
      "grad_norm": 2.109811544418335,
      "learning_rate": 8.128516830745058e-05,
      "loss": 2.3382,
      "step": 14070
    },
    {
      "epoch": 0.8550434201736807,
      "grad_norm": 2.0714871883392334,
      "learning_rate": 8.126035849546623e-05,
      "loss": 2.5596,
      "step": 14080
    },
    {
      "epoch": 0.855650695330054,
      "grad_norm": 3.6590170860290527,
      "learning_rate": 8.123553604129504e-05,
      "loss": 2.3262,
      "step": 14090
    },
    {
      "epoch": 0.8562579704864274,
      "grad_norm": 2.3014512062072754,
      "learning_rate": 8.12107009549756e-05,
      "loss": 2.0562,
      "step": 14100
    },
    {
      "epoch": 0.8568652456428008,
      "grad_norm": 3.3457398414611816,
      "learning_rate": 8.118585324655161e-05,
      "loss": 2.3507,
      "step": 14110
    },
    {
      "epoch": 0.8574725207991741,
      "grad_norm": 2.981548547744751,
      "learning_rate": 8.116099292607189e-05,
      "loss": 2.6772,
      "step": 14120
    },
    {
      "epoch": 0.8580797959555475,
      "grad_norm": 1.813355565071106,
      "learning_rate": 8.113612000359036e-05,
      "loss": 2.2055,
      "step": 14130
    },
    {
      "epoch": 0.8586870711119208,
      "grad_norm": 1.5853006839752197,
      "learning_rate": 8.111123448916602e-05,
      "loss": 2.6819,
      "step": 14140
    },
    {
      "epoch": 0.8592943462682942,
      "grad_norm": 2.876767873764038,
      "learning_rate": 8.108633639286294e-05,
      "loss": 2.5329,
      "step": 14150
    },
    {
      "epoch": 0.8599016214246675,
      "grad_norm": 1.3732216358184814,
      "learning_rate": 8.106142572475035e-05,
      "loss": 1.9446,
      "step": 14160
    },
    {
      "epoch": 0.8605088965810409,
      "grad_norm": 1.814455270767212,
      "learning_rate": 8.103650249490249e-05,
      "loss": 2.1733,
      "step": 14170
    },
    {
      "epoch": 0.8611161717374142,
      "grad_norm": 2.2597944736480713,
      "learning_rate": 8.101156671339873e-05,
      "loss": 2.5973,
      "step": 14180
    },
    {
      "epoch": 0.8617234468937875,
      "grad_norm": 1.9215071201324463,
      "learning_rate": 8.098661839032349e-05,
      "loss": 2.3681,
      "step": 14190
    },
    {
      "epoch": 0.8623307220501609,
      "grad_norm": 2.8064026832580566,
      "learning_rate": 8.096165753576625e-05,
      "loss": 2.3254,
      "step": 14200
    },
    {
      "epoch": 0.8629379972065343,
      "grad_norm": 1.8613202571868896,
      "learning_rate": 8.093668415982161e-05,
      "loss": 2.4581,
      "step": 14210
    },
    {
      "epoch": 0.8635452723629077,
      "grad_norm": 3.495070457458496,
      "learning_rate": 8.091169827258918e-05,
      "loss": 2.871,
      "step": 14220
    },
    {
      "epoch": 0.864152547519281,
      "grad_norm": 4.918758869171143,
      "learning_rate": 8.088669988417366e-05,
      "loss": 2.6349,
      "step": 14230
    },
    {
      "epoch": 0.8647598226756543,
      "grad_norm": 2.881579875946045,
      "learning_rate": 8.086168900468478e-05,
      "loss": 3.1681,
      "step": 14240
    },
    {
      "epoch": 0.8653670978320277,
      "grad_norm": 3.4151830673217773,
      "learning_rate": 8.083666564423736e-05,
      "loss": 2.7448,
      "step": 14250
    },
    {
      "epoch": 0.8659743729884011,
      "grad_norm": 2.0789084434509277,
      "learning_rate": 8.081162981295123e-05,
      "loss": 2.2524,
      "step": 14260
    },
    {
      "epoch": 0.8665816481447745,
      "grad_norm": 2.2574095726013184,
      "learning_rate": 8.07865815209513e-05,
      "loss": 2.3894,
      "step": 14270
    },
    {
      "epoch": 0.8671889233011477,
      "grad_norm": 2.1313610076904297,
      "learning_rate": 8.076152077836747e-05,
      "loss": 2.559,
      "step": 14280
    },
    {
      "epoch": 0.8677961984575211,
      "grad_norm": 2.239670753479004,
      "learning_rate": 8.073644759533473e-05,
      "loss": 2.5135,
      "step": 14290
    },
    {
      "epoch": 0.8684034736138945,
      "grad_norm": 3.7109780311584473,
      "learning_rate": 8.071136198199305e-05,
      "loss": 2.1408,
      "step": 14300
    },
    {
      "epoch": 0.8690107487702678,
      "grad_norm": 1.7204209566116333,
      "learning_rate": 8.068626394848748e-05,
      "loss": 2.447,
      "step": 14310
    },
    {
      "epoch": 0.8696180239266411,
      "grad_norm": 2.8504374027252197,
      "learning_rate": 8.066115350496802e-05,
      "loss": 2.6088,
      "step": 14320
    },
    {
      "epoch": 0.8702252990830145,
      "grad_norm": 1.808363676071167,
      "learning_rate": 8.063603066158978e-05,
      "loss": 2.4599,
      "step": 14330
    },
    {
      "epoch": 0.8708325742393879,
      "grad_norm": 1.5743000507354736,
      "learning_rate": 8.06108954285128e-05,
      "loss": 2.4164,
      "step": 14340
    },
    {
      "epoch": 0.8714398493957612,
      "grad_norm": 2.0879335403442383,
      "learning_rate": 8.058574781590221e-05,
      "loss": 2.4542,
      "step": 14350
    },
    {
      "epoch": 0.8720471245521346,
      "grad_norm": 1.8936529159545898,
      "learning_rate": 8.056058783392807e-05,
      "loss": 2.3614,
      "step": 14360
    },
    {
      "epoch": 0.8726543997085079,
      "grad_norm": 2.3419861793518066,
      "learning_rate": 8.053541549276549e-05,
      "loss": 2.4959,
      "step": 14370
    },
    {
      "epoch": 0.8732616748648813,
      "grad_norm": 3.9795079231262207,
      "learning_rate": 8.051023080259459e-05,
      "loss": 2.6176,
      "step": 14380
    },
    {
      "epoch": 0.8738689500212546,
      "grad_norm": 3.4352641105651855,
      "learning_rate": 8.04850337736004e-05,
      "loss": 2.4473,
      "step": 14390
    },
    {
      "epoch": 0.874476225177628,
      "grad_norm": 2.3498103618621826,
      "learning_rate": 8.045982441597307e-05,
      "loss": 2.3904,
      "step": 14400
    },
    {
      "epoch": 0.8750835003340013,
      "grad_norm": 2.1589627265930176,
      "learning_rate": 8.04346027399076e-05,
      "loss": 2.2676,
      "step": 14410
    },
    {
      "epoch": 0.8756907754903747,
      "grad_norm": 3.1897470951080322,
      "learning_rate": 8.040936875560408e-05,
      "loss": 2.8094,
      "step": 14420
    },
    {
      "epoch": 0.876298050646748,
      "grad_norm": 2.512303590774536,
      "learning_rate": 8.038412247326752e-05,
      "loss": 2.6731,
      "step": 14430
    },
    {
      "epoch": 0.8769053258031214,
      "grad_norm": 4.151480674743652,
      "learning_rate": 8.035886390310792e-05,
      "loss": 2.8509,
      "step": 14440
    },
    {
      "epoch": 0.8775126009594948,
      "grad_norm": 3.727976083755493,
      "learning_rate": 8.033359305534025e-05,
      "loss": 2.4852,
      "step": 14450
    },
    {
      "epoch": 0.8781198761158681,
      "grad_norm": 2.1578946113586426,
      "learning_rate": 8.030830994018446e-05,
      "loss": 2.1491,
      "step": 14460
    },
    {
      "epoch": 0.8787271512722414,
      "grad_norm": 1.788666009902954,
      "learning_rate": 8.02830145678654e-05,
      "loss": 2.0136,
      "step": 14470
    },
    {
      "epoch": 0.8793344264286148,
      "grad_norm": 1.789258360862732,
      "learning_rate": 8.025770694861299e-05,
      "loss": 2.1495,
      "step": 14480
    },
    {
      "epoch": 0.8799417015849882,
      "grad_norm": 2.4654128551483154,
      "learning_rate": 8.023238709266196e-05,
      "loss": 2.5395,
      "step": 14490
    },
    {
      "epoch": 0.8805489767413615,
      "grad_norm": 2.7974510192871094,
      "learning_rate": 8.02070550102521e-05,
      "loss": 2.0708,
      "step": 14500
    },
    {
      "epoch": 0.8811562518977348,
      "grad_norm": 1.6571016311645508,
      "learning_rate": 8.018171071162812e-05,
      "loss": 2.1512,
      "step": 14510
    },
    {
      "epoch": 0.8817635270541082,
      "grad_norm": 2.8650360107421875,
      "learning_rate": 8.015635420703963e-05,
      "loss": 2.3669,
      "step": 14520
    },
    {
      "epoch": 0.8823708022104816,
      "grad_norm": 3.1920647621154785,
      "learning_rate": 8.013098550674122e-05,
      "loss": 2.7443,
      "step": 14530
    },
    {
      "epoch": 0.882978077366855,
      "grad_norm": 4.367587089538574,
      "learning_rate": 8.010560462099238e-05,
      "loss": 2.9754,
      "step": 14540
    },
    {
      "epoch": 0.8835853525232282,
      "grad_norm": 4.049655914306641,
      "learning_rate": 8.008021156005758e-05,
      "loss": 2.3614,
      "step": 14550
    },
    {
      "epoch": 0.8841926276796016,
      "grad_norm": 2.5558342933654785,
      "learning_rate": 8.005480633420614e-05,
      "loss": 2.8934,
      "step": 14560
    },
    {
      "epoch": 0.884799902835975,
      "grad_norm": 2.371664524078369,
      "learning_rate": 8.002938895371237e-05,
      "loss": 3.0711,
      "step": 14570
    },
    {
      "epoch": 0.8854071779923484,
      "grad_norm": 1.7377221584320068,
      "learning_rate": 8.000395942885543e-05,
      "loss": 2.296,
      "step": 14580
    },
    {
      "epoch": 0.8860144531487217,
      "grad_norm": 1.8490080833435059,
      "learning_rate": 7.997851776991945e-05,
      "loss": 2.2931,
      "step": 14590
    },
    {
      "epoch": 0.886621728305095,
      "grad_norm": 1.832748532295227,
      "learning_rate": 7.995306398719342e-05,
      "loss": 2.2782,
      "step": 14600
    },
    {
      "epoch": 0.8872290034614684,
      "grad_norm": 2.516735315322876,
      "learning_rate": 7.992759809097128e-05,
      "loss": 2.6955,
      "step": 14610
    },
    {
      "epoch": 0.8878362786178418,
      "grad_norm": 2.9285168647766113,
      "learning_rate": 7.990212009155184e-05,
      "loss": 2.4884,
      "step": 14620
    },
    {
      "epoch": 0.8884435537742151,
      "grad_norm": 2.779439687728882,
      "learning_rate": 7.987662999923879e-05,
      "loss": 2.8564,
      "step": 14630
    },
    {
      "epoch": 0.8890508289305884,
      "grad_norm": 3.486205816268921,
      "learning_rate": 7.985112782434074e-05,
      "loss": 2.375,
      "step": 14640
    },
    {
      "epoch": 0.8896581040869618,
      "grad_norm": 3.814786672592163,
      "learning_rate": 7.98256135771712e-05,
      "loss": 2.278,
      "step": 14650
    },
    {
      "epoch": 0.8902653792433352,
      "grad_norm": 3.5412020683288574,
      "learning_rate": 7.980008726804849e-05,
      "loss": 2.3876,
      "step": 14660
    },
    {
      "epoch": 0.8908726543997085,
      "grad_norm": 3.5556771755218506,
      "learning_rate": 7.97745489072959e-05,
      "loss": 2.3736,
      "step": 14670
    },
    {
      "epoch": 0.8914799295560819,
      "grad_norm": 2.403332233428955,
      "learning_rate": 7.974899850524151e-05,
      "loss": 2.5403,
      "step": 14680
    },
    {
      "epoch": 0.8920872047124552,
      "grad_norm": 3.4171156883239746,
      "learning_rate": 7.972343607221836e-05,
      "loss": 2.4093,
      "step": 14690
    },
    {
      "epoch": 0.8926944798688285,
      "grad_norm": 2.9310600757598877,
      "learning_rate": 7.969786161856424e-05,
      "loss": 2.0482,
      "step": 14700
    },
    {
      "epoch": 0.8933017550252019,
      "grad_norm": 1.4467486143112183,
      "learning_rate": 7.967227515462191e-05,
      "loss": 2.284,
      "step": 14710
    },
    {
      "epoch": 0.8939090301815753,
      "grad_norm": 2.8390300273895264,
      "learning_rate": 7.964667669073894e-05,
      "loss": 2.2462,
      "step": 14720
    },
    {
      "epoch": 0.8945163053379486,
      "grad_norm": 3.9782779216766357,
      "learning_rate": 7.962106623726775e-05,
      "loss": 2.29,
      "step": 14730
    },
    {
      "epoch": 0.895123580494322,
      "grad_norm": 1.9151089191436768,
      "learning_rate": 7.959544380456563e-05,
      "loss": 2.0701,
      "step": 14740
    },
    {
      "epoch": 0.8957308556506953,
      "grad_norm": 2.6230833530426025,
      "learning_rate": 7.956980940299467e-05,
      "loss": 2.5214,
      "step": 14750
    },
    {
      "epoch": 0.8963381308070687,
      "grad_norm": 4.862583160400391,
      "learning_rate": 7.954416304292185e-05,
      "loss": 2.5188,
      "step": 14760
    },
    {
      "epoch": 0.8969454059634421,
      "grad_norm": 2.775786876678467,
      "learning_rate": 7.951850473471897e-05,
      "loss": 2.0076,
      "step": 14770
    },
    {
      "epoch": 0.8975526811198153,
      "grad_norm": 3.1819043159484863,
      "learning_rate": 7.949283448876264e-05,
      "loss": 2.6531,
      "step": 14780
    },
    {
      "epoch": 0.8981599562761887,
      "grad_norm": 4.957727909088135,
      "learning_rate": 7.946715231543434e-05,
      "loss": 2.6867,
      "step": 14790
    },
    {
      "epoch": 0.8987672314325621,
      "grad_norm": 3.7349653244018555,
      "learning_rate": 7.944145822512034e-05,
      "loss": 2.6197,
      "step": 14800
    },
    {
      "epoch": 0.8993745065889355,
      "grad_norm": 2.166565179824829,
      "learning_rate": 7.941575222821171e-05,
      "loss": 2.32,
      "step": 14810
    },
    {
      "epoch": 0.8999817817453089,
      "grad_norm": 1.8019078969955444,
      "learning_rate": 7.939003433510442e-05,
      "loss": 1.9526,
      "step": 14820
    },
    {
      "epoch": 0.9005890569016821,
      "grad_norm": 1.717832326889038,
      "learning_rate": 7.936430455619917e-05,
      "loss": 2.2671,
      "step": 14830
    },
    {
      "epoch": 0.9011963320580555,
      "grad_norm": 2.6785995960235596,
      "learning_rate": 7.933856290190149e-05,
      "loss": 2.3013,
      "step": 14840
    },
    {
      "epoch": 0.9018036072144289,
      "grad_norm": 2.3188323974609375,
      "learning_rate": 7.931280938262169e-05,
      "loss": 2.2861,
      "step": 14850
    },
    {
      "epoch": 0.9024108823708022,
      "grad_norm": 1.9764552116394043,
      "learning_rate": 7.928704400877495e-05,
      "loss": 2.2652,
      "step": 14860
    },
    {
      "epoch": 0.9030181575271755,
      "grad_norm": 3.1202142238616943,
      "learning_rate": 7.926126679078116e-05,
      "loss": 2.3702,
      "step": 14870
    },
    {
      "epoch": 0.9036254326835489,
      "grad_norm": 2.226175308227539,
      "learning_rate": 7.923547773906507e-05,
      "loss": 2.4236,
      "step": 14880
    },
    {
      "epoch": 0.9042327078399223,
      "grad_norm": 2.6553168296813965,
      "learning_rate": 7.920967686405616e-05,
      "loss": 2.7328,
      "step": 14890
    },
    {
      "epoch": 0.9048399829962956,
      "grad_norm": 3.43151593208313,
      "learning_rate": 7.918386417618872e-05,
      "loss": 2.5852,
      "step": 14900
    },
    {
      "epoch": 0.905447258152669,
      "grad_norm": 1.8898957967758179,
      "learning_rate": 7.915803968590181e-05,
      "loss": 2.5824,
      "step": 14910
    },
    {
      "epoch": 0.9060545333090423,
      "grad_norm": 2.6855783462524414,
      "learning_rate": 7.913220340363927e-05,
      "loss": 2.2231,
      "step": 14920
    },
    {
      "epoch": 0.9066618084654157,
      "grad_norm": 2.775252103805542,
      "learning_rate": 7.91063553398497e-05,
      "loss": 2.3945,
      "step": 14930
    },
    {
      "epoch": 0.907269083621789,
      "grad_norm": 2.539144992828369,
      "learning_rate": 7.908049550498648e-05,
      "loss": 2.6163,
      "step": 14940
    },
    {
      "epoch": 0.9078763587781624,
      "grad_norm": 1.9513890743255615,
      "learning_rate": 7.905462390950772e-05,
      "loss": 2.3562,
      "step": 14950
    },
    {
      "epoch": 0.9084836339345357,
      "grad_norm": 2.3795104026794434,
      "learning_rate": 7.902874056387633e-05,
      "loss": 2.4214,
      "step": 14960
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.6715095043182373,
      "learning_rate": 7.900284547855991e-05,
      "loss": 2.7066,
      "step": 14970
    },
    {
      "epoch": 0.9096981842472824,
      "grad_norm": 3.3640217781066895,
      "learning_rate": 7.897693866403089e-05,
      "loss": 2.6013,
      "step": 14980
    },
    {
      "epoch": 0.9103054594036558,
      "grad_norm": 2.447582721710205,
      "learning_rate": 7.895102013076637e-05,
      "loss": 2.5466,
      "step": 14990
    },
    {
      "epoch": 0.9109127345600292,
      "grad_norm": 2.505021572113037,
      "learning_rate": 7.892508988924822e-05,
      "loss": 2.3578,
      "step": 15000
    }
  ],
  "logging_steps": 10,
  "max_steps": 49401,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "total_flos": 3.135504384e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
