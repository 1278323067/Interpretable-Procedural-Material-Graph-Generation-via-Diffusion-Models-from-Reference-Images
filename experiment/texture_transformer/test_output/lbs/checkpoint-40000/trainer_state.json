{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.429100625493411,
  "eval_steps": 5001,
  "global_step": 40000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006072751563733527,
      "grad_norm": 7.186389446258545,
      "learning_rate": 9.999998988960184e-05,
      "loss": 28.5336,
      "step": 10
    },
    {
      "epoch": 0.0012145503127467055,
      "grad_norm": 6.450451374053955,
      "learning_rate": 9.999995955841146e-05,
      "loss": 5.8003,
      "step": 20
    },
    {
      "epoch": 0.0018218254691200583,
      "grad_norm": 19.285921096801758,
      "learning_rate": 9.99999090064411e-05,
      "loss": 4.1025,
      "step": 30
    },
    {
      "epoch": 0.002429100625493411,
      "grad_norm": 6.392780303955078,
      "learning_rate": 9.999983823371122e-05,
      "loss": 3.7077,
      "step": 40
    },
    {
      "epoch": 0.0030363757818667636,
      "grad_norm": 3.809990406036377,
      "learning_rate": 9.999974724025045e-05,
      "loss": 3.4147,
      "step": 50
    },
    {
      "epoch": 0.0036436509382401167,
      "grad_norm": 3.4429056644439697,
      "learning_rate": 9.999963602609556e-05,
      "loss": 3.434,
      "step": 60
    },
    {
      "epoch": 0.004250926094613469,
      "grad_norm": 4.669011116027832,
      "learning_rate": 9.999950459129158e-05,
      "loss": 3.4633,
      "step": 70
    },
    {
      "epoch": 0.004858201250986822,
      "grad_norm": 4.70878267288208,
      "learning_rate": 9.99993529358916e-05,
      "loss": 3.8761,
      "step": 80
    },
    {
      "epoch": 0.005465476407360175,
      "grad_norm": 5.6883344650268555,
      "learning_rate": 9.9999181059957e-05,
      "loss": 3.4797,
      "step": 90
    },
    {
      "epoch": 0.006072751563733527,
      "grad_norm": 4.667462348937988,
      "learning_rate": 9.999898896355726e-05,
      "loss": 3.2826,
      "step": 100
    },
    {
      "epoch": 0.006680026720106881,
      "grad_norm": 5.330298900604248,
      "learning_rate": 9.999877664677009e-05,
      "loss": 2.9715,
      "step": 110
    },
    {
      "epoch": 0.007287301876480233,
      "grad_norm": 6.202093601226807,
      "learning_rate": 9.999854410968134e-05,
      "loss": 2.9419,
      "step": 120
    },
    {
      "epoch": 0.007894577032853586,
      "grad_norm": 4.214776039123535,
      "learning_rate": 9.999829135238505e-05,
      "loss": 3.3057,
      "step": 130
    },
    {
      "epoch": 0.008501852189226939,
      "grad_norm": 3.475522994995117,
      "learning_rate": 9.999801837498346e-05,
      "loss": 3.3707,
      "step": 140
    },
    {
      "epoch": 0.009109127345600291,
      "grad_norm": 5.682331562042236,
      "learning_rate": 9.999772517758694e-05,
      "loss": 3.1139,
      "step": 150
    },
    {
      "epoch": 0.009716402501973644,
      "grad_norm": 4.191882133483887,
      "learning_rate": 9.999741176031408e-05,
      "loss": 3.0561,
      "step": 160
    },
    {
      "epoch": 0.010323677658346997,
      "grad_norm": 4.490618705749512,
      "learning_rate": 9.999707812329162e-05,
      "loss": 2.8682,
      "step": 170
    },
    {
      "epoch": 0.01093095281472035,
      "grad_norm": 3.096764326095581,
      "learning_rate": 9.99967242666545e-05,
      "loss": 3.1537,
      "step": 180
    },
    {
      "epoch": 0.011538227971093702,
      "grad_norm": 4.115010738372803,
      "learning_rate": 9.99963501905458e-05,
      "loss": 3.1016,
      "step": 190
    },
    {
      "epoch": 0.012145503127467054,
      "grad_norm": 2.6895015239715576,
      "learning_rate": 9.999595589511684e-05,
      "loss": 3.0048,
      "step": 200
    },
    {
      "epoch": 0.012752778283840409,
      "grad_norm": 5.5280046463012695,
      "learning_rate": 9.999554138052704e-05,
      "loss": 2.6903,
      "step": 210
    },
    {
      "epoch": 0.013360053440213761,
      "grad_norm": 4.60183572769165,
      "learning_rate": 9.999510664694408e-05,
      "loss": 2.6745,
      "step": 220
    },
    {
      "epoch": 0.013967328596587114,
      "grad_norm": 5.704331398010254,
      "learning_rate": 9.999465169454374e-05,
      "loss": 2.9479,
      "step": 230
    },
    {
      "epoch": 0.014574603752960467,
      "grad_norm": 2.7345144748687744,
      "learning_rate": 9.999417652351002e-05,
      "loss": 2.9361,
      "step": 240
    },
    {
      "epoch": 0.01518187890933382,
      "grad_norm": 3.391403913497925,
      "learning_rate": 9.999368113403508e-05,
      "loss": 3.0954,
      "step": 250
    },
    {
      "epoch": 0.015789154065707172,
      "grad_norm": 3.0357401371002197,
      "learning_rate": 9.999316552631928e-05,
      "loss": 3.1664,
      "step": 260
    },
    {
      "epoch": 0.016396429222080525,
      "grad_norm": 2.4493205547332764,
      "learning_rate": 9.999262970057113e-05,
      "loss": 2.8576,
      "step": 270
    },
    {
      "epoch": 0.017003704378453877,
      "grad_norm": 3.509894847869873,
      "learning_rate": 9.999207365700733e-05,
      "loss": 3.0379,
      "step": 280
    },
    {
      "epoch": 0.01761097953482723,
      "grad_norm": 2.7530264854431152,
      "learning_rate": 9.999149739585273e-05,
      "loss": 2.7517,
      "step": 290
    },
    {
      "epoch": 0.018218254691200583,
      "grad_norm": 4.6260576248168945,
      "learning_rate": 9.999090091734043e-05,
      "loss": 2.8676,
      "step": 300
    },
    {
      "epoch": 0.018825529847573935,
      "grad_norm": 5.038768768310547,
      "learning_rate": 9.999028422171159e-05,
      "loss": 3.1278,
      "step": 310
    },
    {
      "epoch": 0.019432805003947288,
      "grad_norm": 2.4521424770355225,
      "learning_rate": 9.998964730921568e-05,
      "loss": 2.8147,
      "step": 320
    },
    {
      "epoch": 0.02004008016032064,
      "grad_norm": 3.764702796936035,
      "learning_rate": 9.99889901801102e-05,
      "loss": 2.9978,
      "step": 330
    },
    {
      "epoch": 0.020647355316693993,
      "grad_norm": 3.569005250930786,
      "learning_rate": 9.998831283466099e-05,
      "loss": 2.8446,
      "step": 340
    },
    {
      "epoch": 0.021254630473067346,
      "grad_norm": 2.5747430324554443,
      "learning_rate": 9.998761527314191e-05,
      "loss": 2.6242,
      "step": 350
    },
    {
      "epoch": 0.0218619056294407,
      "grad_norm": 2.6841931343078613,
      "learning_rate": 9.99868974958351e-05,
      "loss": 2.7303,
      "step": 360
    },
    {
      "epoch": 0.02246918078581405,
      "grad_norm": 4.337249279022217,
      "learning_rate": 9.998615950303083e-05,
      "loss": 3.0008,
      "step": 370
    },
    {
      "epoch": 0.023076455942187404,
      "grad_norm": 5.8416595458984375,
      "learning_rate": 9.998540129502756e-05,
      "loss": 3.0236,
      "step": 380
    },
    {
      "epoch": 0.023683731098560756,
      "grad_norm": 3.5167787075042725,
      "learning_rate": 9.998462287213191e-05,
      "loss": 3.0139,
      "step": 390
    },
    {
      "epoch": 0.02429100625493411,
      "grad_norm": 3.1287996768951416,
      "learning_rate": 9.998382423465871e-05,
      "loss": 2.6025,
      "step": 400
    },
    {
      "epoch": 0.024898281411307465,
      "grad_norm": 2.7561633586883545,
      "learning_rate": 9.998300538293091e-05,
      "loss": 2.7249,
      "step": 410
    },
    {
      "epoch": 0.025505556567680818,
      "grad_norm": 2.219248056411743,
      "learning_rate": 9.99821663172797e-05,
      "loss": 2.5885,
      "step": 420
    },
    {
      "epoch": 0.02611283172405417,
      "grad_norm": 2.8309640884399414,
      "learning_rate": 9.998130703804438e-05,
      "loss": 2.6203,
      "step": 430
    },
    {
      "epoch": 0.026720106880427523,
      "grad_norm": 2.67535662651062,
      "learning_rate": 9.998042754557249e-05,
      "loss": 2.8381,
      "step": 440
    },
    {
      "epoch": 0.027327382036800876,
      "grad_norm": 3.424489736557007,
      "learning_rate": 9.997952784021967e-05,
      "loss": 2.8307,
      "step": 450
    },
    {
      "epoch": 0.027934657193174228,
      "grad_norm": 2.4972362518310547,
      "learning_rate": 9.997860792234981e-05,
      "loss": 2.727,
      "step": 460
    },
    {
      "epoch": 0.02854193234954758,
      "grad_norm": 2.3320651054382324,
      "learning_rate": 9.997766779233493e-05,
      "loss": 2.733,
      "step": 470
    },
    {
      "epoch": 0.029149207505920934,
      "grad_norm": 4.219310283660889,
      "learning_rate": 9.997670745055522e-05,
      "loss": 2.5761,
      "step": 480
    },
    {
      "epoch": 0.029756482662294286,
      "grad_norm": 2.828003168106079,
      "learning_rate": 9.997572689739907e-05,
      "loss": 2.8755,
      "step": 490
    },
    {
      "epoch": 0.03036375781866764,
      "grad_norm": 2.7965126037597656,
      "learning_rate": 9.997472613326304e-05,
      "loss": 2.8552,
      "step": 500
    },
    {
      "epoch": 0.03097103297504099,
      "grad_norm": 3.650280475616455,
      "learning_rate": 9.997370515855182e-05,
      "loss": 2.6381,
      "step": 510
    },
    {
      "epoch": 0.031578308131414344,
      "grad_norm": 3.817025661468506,
      "learning_rate": 9.997266397367836e-05,
      "loss": 2.4954,
      "step": 520
    },
    {
      "epoch": 0.03218558328778769,
      "grad_norm": 1.6731635332107544,
      "learning_rate": 9.99716025790637e-05,
      "loss": 2.4817,
      "step": 530
    },
    {
      "epoch": 0.03279285844416105,
      "grad_norm": 2.273855209350586,
      "learning_rate": 9.997052097513709e-05,
      "loss": 2.7781,
      "step": 540
    },
    {
      "epoch": 0.033400133600534405,
      "grad_norm": 2.9378833770751953,
      "learning_rate": 9.996941916233594e-05,
      "loss": 2.5581,
      "step": 550
    },
    {
      "epoch": 0.034007408756907755,
      "grad_norm": 3.8389534950256348,
      "learning_rate": 9.996829714110583e-05,
      "loss": 3.0879,
      "step": 560
    },
    {
      "epoch": 0.03461468391328111,
      "grad_norm": 3.4130256175994873,
      "learning_rate": 9.996715491190057e-05,
      "loss": 2.9079,
      "step": 570
    },
    {
      "epoch": 0.03522195906965446,
      "grad_norm": 2.044156551361084,
      "learning_rate": 9.996599247518206e-05,
      "loss": 2.6539,
      "step": 580
    },
    {
      "epoch": 0.035829234226027816,
      "grad_norm": 3.4560179710388184,
      "learning_rate": 9.996480983142041e-05,
      "loss": 2.9231,
      "step": 590
    },
    {
      "epoch": 0.036436509382401165,
      "grad_norm": 3.428556442260742,
      "learning_rate": 9.99636069810939e-05,
      "loss": 2.9796,
      "step": 600
    },
    {
      "epoch": 0.03704378453877452,
      "grad_norm": 2.4829540252685547,
      "learning_rate": 9.9962383924689e-05,
      "loss": 2.8804,
      "step": 610
    },
    {
      "epoch": 0.03765105969514787,
      "grad_norm": 3.457402229309082,
      "learning_rate": 9.99611406627003e-05,
      "loss": 3.1318,
      "step": 620
    },
    {
      "epoch": 0.03825833485152123,
      "grad_norm": 2.778350830078125,
      "learning_rate": 9.995987719563062e-05,
      "loss": 2.9028,
      "step": 630
    },
    {
      "epoch": 0.038865610007894576,
      "grad_norm": 3.100822687149048,
      "learning_rate": 9.995859352399094e-05,
      "loss": 2.8307,
      "step": 640
    },
    {
      "epoch": 0.03947288516426793,
      "grad_norm": 2.6477577686309814,
      "learning_rate": 9.995728964830036e-05,
      "loss": 2.8336,
      "step": 650
    },
    {
      "epoch": 0.04008016032064128,
      "grad_norm": 3.6645150184631348,
      "learning_rate": 9.995596556908622e-05,
      "loss": 2.6316,
      "step": 660
    },
    {
      "epoch": 0.04068743547701464,
      "grad_norm": 4.381188869476318,
      "learning_rate": 9.995462128688397e-05,
      "loss": 2.6818,
      "step": 670
    },
    {
      "epoch": 0.041294710633387986,
      "grad_norm": 3.122107982635498,
      "learning_rate": 9.995325680223728e-05,
      "loss": 2.9583,
      "step": 680
    },
    {
      "epoch": 0.04190198578976134,
      "grad_norm": 2.9715704917907715,
      "learning_rate": 9.995187211569797e-05,
      "loss": 2.8996,
      "step": 690
    },
    {
      "epoch": 0.04250926094613469,
      "grad_norm": 2.5021092891693115,
      "learning_rate": 9.995046722782601e-05,
      "loss": 2.6537,
      "step": 700
    },
    {
      "epoch": 0.04311653610250805,
      "grad_norm": 2.472275733947754,
      "learning_rate": 9.994904213918959e-05,
      "loss": 2.7742,
      "step": 710
    },
    {
      "epoch": 0.0437238112588814,
      "grad_norm": 2.840935707092285,
      "learning_rate": 9.994759685036501e-05,
      "loss": 2.7137,
      "step": 720
    },
    {
      "epoch": 0.04433108641525475,
      "grad_norm": 4.177767753601074,
      "learning_rate": 9.994613136193679e-05,
      "loss": 2.6396,
      "step": 730
    },
    {
      "epoch": 0.0449383615716281,
      "grad_norm": 2.6978330612182617,
      "learning_rate": 9.994464567449757e-05,
      "loss": 3.107,
      "step": 740
    },
    {
      "epoch": 0.04554563672800146,
      "grad_norm": 2.9850990772247314,
      "learning_rate": 9.99431397886482e-05,
      "loss": 3.0693,
      "step": 750
    },
    {
      "epoch": 0.04615291188437481,
      "grad_norm": 3.750215768814087,
      "learning_rate": 9.994161370499769e-05,
      "loss": 2.6602,
      "step": 760
    },
    {
      "epoch": 0.04676018704074816,
      "grad_norm": 2.1821258068084717,
      "learning_rate": 9.994006742416321e-05,
      "loss": 2.6184,
      "step": 770
    },
    {
      "epoch": 0.04736746219712151,
      "grad_norm": 2.588514804840088,
      "learning_rate": 9.99385009467701e-05,
      "loss": 2.6797,
      "step": 780
    },
    {
      "epoch": 0.04797473735349487,
      "grad_norm": 2.306520700454712,
      "learning_rate": 9.993691427345187e-05,
      "loss": 2.5766,
      "step": 790
    },
    {
      "epoch": 0.04858201250986822,
      "grad_norm": 2.2094011306762695,
      "learning_rate": 9.993530740485018e-05,
      "loss": 2.815,
      "step": 800
    },
    {
      "epoch": 0.049189287666241574,
      "grad_norm": 2.3844077587127686,
      "learning_rate": 9.993368034161489e-05,
      "loss": 2.5661,
      "step": 810
    },
    {
      "epoch": 0.04979656282261493,
      "grad_norm": 2.4530060291290283,
      "learning_rate": 9.9932033084404e-05,
      "loss": 2.9215,
      "step": 820
    },
    {
      "epoch": 0.05040383797898828,
      "grad_norm": 2.7166547775268555,
      "learning_rate": 9.99303656338837e-05,
      "loss": 2.6268,
      "step": 830
    },
    {
      "epoch": 0.051011113135361635,
      "grad_norm": 2.2288105487823486,
      "learning_rate": 9.992867799072833e-05,
      "loss": 2.6068,
      "step": 840
    },
    {
      "epoch": 0.051618388291734985,
      "grad_norm": 2.3088345527648926,
      "learning_rate": 9.99269701556204e-05,
      "loss": 2.4566,
      "step": 850
    },
    {
      "epoch": 0.05222566344810834,
      "grad_norm": 2.3499398231506348,
      "learning_rate": 9.992524212925056e-05,
      "loss": 2.7308,
      "step": 860
    },
    {
      "epoch": 0.05283293860448169,
      "grad_norm": 2.4770405292510986,
      "learning_rate": 9.99234939123177e-05,
      "loss": 2.7754,
      "step": 870
    },
    {
      "epoch": 0.053440213760855046,
      "grad_norm": 4.333104133605957,
      "learning_rate": 9.992172550552879e-05,
      "loss": 2.8169,
      "step": 880
    },
    {
      "epoch": 0.054047488917228395,
      "grad_norm": 3.036212205886841,
      "learning_rate": 9.9919936909599e-05,
      "loss": 2.8564,
      "step": 890
    },
    {
      "epoch": 0.05465476407360175,
      "grad_norm": 2.8859453201293945,
      "learning_rate": 9.99181281252517e-05,
      "loss": 3.1683,
      "step": 900
    },
    {
      "epoch": 0.0552620392299751,
      "grad_norm": 3.3369028568267822,
      "learning_rate": 9.991629915321836e-05,
      "loss": 2.7407,
      "step": 910
    },
    {
      "epoch": 0.055869314386348456,
      "grad_norm": 4.032517433166504,
      "learning_rate": 9.991444999423865e-05,
      "loss": 2.9512,
      "step": 920
    },
    {
      "epoch": 0.056476589542721806,
      "grad_norm": 2.22833251953125,
      "learning_rate": 9.991258064906041e-05,
      "loss": 2.9532,
      "step": 930
    },
    {
      "epoch": 0.05708386469909516,
      "grad_norm": 6.261169910430908,
      "learning_rate": 9.991069111843964e-05,
      "loss": 2.8817,
      "step": 940
    },
    {
      "epoch": 0.05769113985546851,
      "grad_norm": 3.274970054626465,
      "learning_rate": 9.990878140314047e-05,
      "loss": 2.8114,
      "step": 950
    },
    {
      "epoch": 0.05829841501184187,
      "grad_norm": 2.3626973628997803,
      "learning_rate": 9.990685150393523e-05,
      "loss": 2.455,
      "step": 960
    },
    {
      "epoch": 0.058905690168215216,
      "grad_norm": 3.046497106552124,
      "learning_rate": 9.990490142160442e-05,
      "loss": 2.5229,
      "step": 970
    },
    {
      "epoch": 0.05951296532458857,
      "grad_norm": 2.2845818996429443,
      "learning_rate": 9.990293115693667e-05,
      "loss": 2.8898,
      "step": 980
    },
    {
      "epoch": 0.06012024048096192,
      "grad_norm": 4.140326976776123,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.9451,
      "step": 990
    },
    {
      "epoch": 0.06072751563733528,
      "grad_norm": 5.539736747741699,
      "learning_rate": 9.989893008378572e-05,
      "loss": 3.0845,
      "step": 1000
    },
    {
      "epoch": 0.06133479079370863,
      "grad_norm": 4.5421013832092285,
      "learning_rate": 9.989689927692062e-05,
      "loss": 3.3652,
      "step": 1010
    },
    {
      "epoch": 0.06194206595008198,
      "grad_norm": 2.962928295135498,
      "learning_rate": 9.989484829095478e-05,
      "loss": 2.8889,
      "step": 1020
    },
    {
      "epoch": 0.06254934110645534,
      "grad_norm": 3.3312432765960693,
      "learning_rate": 9.989277712671766e-05,
      "loss": 2.6089,
      "step": 1030
    },
    {
      "epoch": 0.06315661626282869,
      "grad_norm": 2.9687602519989014,
      "learning_rate": 9.989068578504684e-05,
      "loss": 2.7753,
      "step": 1040
    },
    {
      "epoch": 0.06376389141920204,
      "grad_norm": 4.476959705352783,
      "learning_rate": 9.988857426678811e-05,
      "loss": 2.3883,
      "step": 1050
    },
    {
      "epoch": 0.06437116657557539,
      "grad_norm": 3.7683510780334473,
      "learning_rate": 9.98864425727954e-05,
      "loss": 2.6421,
      "step": 1060
    },
    {
      "epoch": 0.06497844173194875,
      "grad_norm": 2.254359722137451,
      "learning_rate": 9.98842907039308e-05,
      "loss": 2.5235,
      "step": 1070
    },
    {
      "epoch": 0.0655857168883221,
      "grad_norm": 2.0745208263397217,
      "learning_rate": 9.988211866106457e-05,
      "loss": 2.6447,
      "step": 1080
    },
    {
      "epoch": 0.06619299204469545,
      "grad_norm": 2.1825270652770996,
      "learning_rate": 9.987992644507511e-05,
      "loss": 2.6447,
      "step": 1090
    },
    {
      "epoch": 0.06680026720106881,
      "grad_norm": 2.115710973739624,
      "learning_rate": 9.987771405684897e-05,
      "loss": 2.3655,
      "step": 1100
    },
    {
      "epoch": 0.06740754235744216,
      "grad_norm": 1.6710492372512817,
      "learning_rate": 9.987548149728092e-05,
      "loss": 2.3085,
      "step": 1110
    },
    {
      "epoch": 0.06801481751381551,
      "grad_norm": 2.5440642833709717,
      "learning_rate": 9.987322876727381e-05,
      "loss": 2.8059,
      "step": 1120
    },
    {
      "epoch": 0.06862209267018886,
      "grad_norm": 2.6967358589172363,
      "learning_rate": 9.98709558677387e-05,
      "loss": 3.04,
      "step": 1130
    },
    {
      "epoch": 0.06922936782656222,
      "grad_norm": 3.9061031341552734,
      "learning_rate": 9.986866279959474e-05,
      "loss": 3.1271,
      "step": 1140
    },
    {
      "epoch": 0.06983664298293557,
      "grad_norm": 3.2761754989624023,
      "learning_rate": 9.986634956376932e-05,
      "loss": 2.6869,
      "step": 1150
    },
    {
      "epoch": 0.07044391813930892,
      "grad_norm": 2.55582857131958,
      "learning_rate": 9.986401616119795e-05,
      "loss": 2.9894,
      "step": 1160
    },
    {
      "epoch": 0.07105119329568227,
      "grad_norm": 2.079308032989502,
      "learning_rate": 9.98616625928243e-05,
      "loss": 2.6543,
      "step": 1170
    },
    {
      "epoch": 0.07165846845205563,
      "grad_norm": 2.0200679302215576,
      "learning_rate": 9.985928885960019e-05,
      "loss": 2.5643,
      "step": 1180
    },
    {
      "epoch": 0.07226574360842898,
      "grad_norm": 2.95072078704834,
      "learning_rate": 9.985689496248556e-05,
      "loss": 2.428,
      "step": 1190
    },
    {
      "epoch": 0.07287301876480233,
      "grad_norm": 2.0961287021636963,
      "learning_rate": 9.985448090244858e-05,
      "loss": 2.7229,
      "step": 1200
    },
    {
      "epoch": 0.07348029392117568,
      "grad_norm": 3.2801177501678467,
      "learning_rate": 9.985204668046553e-05,
      "loss": 2.8076,
      "step": 1210
    },
    {
      "epoch": 0.07408756907754904,
      "grad_norm": 6.785343170166016,
      "learning_rate": 9.984959229752082e-05,
      "loss": 2.756,
      "step": 1220
    },
    {
      "epoch": 0.07469484423392239,
      "grad_norm": 2.543053150177002,
      "learning_rate": 9.984711775460707e-05,
      "loss": 3.0355,
      "step": 1230
    },
    {
      "epoch": 0.07530211939029574,
      "grad_norm": 2.2619287967681885,
      "learning_rate": 9.9844623052725e-05,
      "loss": 2.9984,
      "step": 1240
    },
    {
      "epoch": 0.07590939454666909,
      "grad_norm": 6.4721198081970215,
      "learning_rate": 9.984210819288354e-05,
      "loss": 2.6957,
      "step": 1250
    },
    {
      "epoch": 0.07651666970304245,
      "grad_norm": 2.721078872680664,
      "learning_rate": 9.983957317609971e-05,
      "loss": 2.7023,
      "step": 1260
    },
    {
      "epoch": 0.0771239448594158,
      "grad_norm": 1.7425986528396606,
      "learning_rate": 9.983701800339873e-05,
      "loss": 2.7241,
      "step": 1270
    },
    {
      "epoch": 0.07773122001578915,
      "grad_norm": 2.0464706420898438,
      "learning_rate": 9.983444267581394e-05,
      "loss": 2.6473,
      "step": 1280
    },
    {
      "epoch": 0.0783384951721625,
      "grad_norm": 7.386662006378174,
      "learning_rate": 9.983184719438687e-05,
      "loss": 2.8031,
      "step": 1290
    },
    {
      "epoch": 0.07894577032853586,
      "grad_norm": 5.077014923095703,
      "learning_rate": 9.982923156016713e-05,
      "loss": 2.5077,
      "step": 1300
    },
    {
      "epoch": 0.07955304548490921,
      "grad_norm": 5.140445232391357,
      "learning_rate": 9.982659577421255e-05,
      "loss": 2.6643,
      "step": 1310
    },
    {
      "epoch": 0.08016032064128256,
      "grad_norm": 4.758424282073975,
      "learning_rate": 9.982393983758908e-05,
      "loss": 2.9352,
      "step": 1320
    },
    {
      "epoch": 0.08076759579765591,
      "grad_norm": 3.2941057682037354,
      "learning_rate": 9.982126375137083e-05,
      "loss": 2.9868,
      "step": 1330
    },
    {
      "epoch": 0.08137487095402927,
      "grad_norm": 3.253530979156494,
      "learning_rate": 9.981856751664004e-05,
      "loss": 3.0862,
      "step": 1340
    },
    {
      "epoch": 0.08198214611040262,
      "grad_norm": 7.478233814239502,
      "learning_rate": 9.981585113448713e-05,
      "loss": 2.6801,
      "step": 1350
    },
    {
      "epoch": 0.08258942126677597,
      "grad_norm": 2.1226282119750977,
      "learning_rate": 9.981311460601061e-05,
      "loss": 2.3369,
      "step": 1360
    },
    {
      "epoch": 0.08319669642314934,
      "grad_norm": 2.751624345779419,
      "learning_rate": 9.981035793231722e-05,
      "loss": 2.6989,
      "step": 1370
    },
    {
      "epoch": 0.08380397157952268,
      "grad_norm": 2.239170789718628,
      "learning_rate": 9.980758111452177e-05,
      "loss": 2.8433,
      "step": 1380
    },
    {
      "epoch": 0.08441124673589603,
      "grad_norm": 3.954367160797119,
      "learning_rate": 9.980478415374726e-05,
      "loss": 2.9201,
      "step": 1390
    },
    {
      "epoch": 0.08501852189226938,
      "grad_norm": 5.653280735015869,
      "learning_rate": 9.980196705112484e-05,
      "loss": 3.2969,
      "step": 1400
    },
    {
      "epoch": 0.08562579704864275,
      "grad_norm": 4.375555515289307,
      "learning_rate": 9.979912980779377e-05,
      "loss": 3.0847,
      "step": 1410
    },
    {
      "epoch": 0.0862330722050161,
      "grad_norm": 3.537280559539795,
      "learning_rate": 9.979627242490148e-05,
      "loss": 2.9035,
      "step": 1420
    },
    {
      "epoch": 0.08684034736138944,
      "grad_norm": 2.722881317138672,
      "learning_rate": 9.979339490360355e-05,
      "loss": 2.952,
      "step": 1430
    },
    {
      "epoch": 0.0874476225177628,
      "grad_norm": 2.460787057876587,
      "learning_rate": 9.979049724506369e-05,
      "loss": 2.8785,
      "step": 1440
    },
    {
      "epoch": 0.08805489767413616,
      "grad_norm": 2.098292827606201,
      "learning_rate": 9.978757945045378e-05,
      "loss": 2.8513,
      "step": 1450
    },
    {
      "epoch": 0.0886621728305095,
      "grad_norm": 2.7332565784454346,
      "learning_rate": 9.978464152095378e-05,
      "loss": 2.7216,
      "step": 1460
    },
    {
      "epoch": 0.08926944798688285,
      "grad_norm": 4.435109615325928,
      "learning_rate": 9.978168345775187e-05,
      "loss": 3.0856,
      "step": 1470
    },
    {
      "epoch": 0.0898767231432562,
      "grad_norm": 3.5636463165283203,
      "learning_rate": 9.977870526204431e-05,
      "loss": 2.8489,
      "step": 1480
    },
    {
      "epoch": 0.09048399829962957,
      "grad_norm": 3.468517780303955,
      "learning_rate": 9.977570693503557e-05,
      "loss": 2.7209,
      "step": 1490
    },
    {
      "epoch": 0.09109127345600292,
      "grad_norm": 3.9199130535125732,
      "learning_rate": 9.977268847793819e-05,
      "loss": 3.0091,
      "step": 1500
    },
    {
      "epoch": 0.09169854861237627,
      "grad_norm": 3.373481512069702,
      "learning_rate": 9.976964989197288e-05,
      "loss": 2.6523,
      "step": 1510
    },
    {
      "epoch": 0.09230582376874961,
      "grad_norm": 3.3667783737182617,
      "learning_rate": 9.976659117836851e-05,
      "loss": 2.668,
      "step": 1520
    },
    {
      "epoch": 0.09291309892512298,
      "grad_norm": 2.519352436065674,
      "learning_rate": 9.976351233836207e-05,
      "loss": 2.8741,
      "step": 1530
    },
    {
      "epoch": 0.09352037408149633,
      "grad_norm": 2.5113368034362793,
      "learning_rate": 9.976041337319867e-05,
      "loss": 2.7558,
      "step": 1540
    },
    {
      "epoch": 0.09412764923786968,
      "grad_norm": 2.948071002960205,
      "learning_rate": 9.975729428413162e-05,
      "loss": 2.933,
      "step": 1550
    },
    {
      "epoch": 0.09473492439424303,
      "grad_norm": 4.039847373962402,
      "learning_rate": 9.975415507242229e-05,
      "loss": 2.7249,
      "step": 1560
    },
    {
      "epoch": 0.09534219955061639,
      "grad_norm": 2.9555819034576416,
      "learning_rate": 9.975099573934026e-05,
      "loss": 2.8253,
      "step": 1570
    },
    {
      "epoch": 0.09594947470698974,
      "grad_norm": 7.770235061645508,
      "learning_rate": 9.974781628616318e-05,
      "loss": 2.7095,
      "step": 1580
    },
    {
      "epoch": 0.09655674986336309,
      "grad_norm": 2.452521324157715,
      "learning_rate": 9.974461671417689e-05,
      "loss": 2.9011,
      "step": 1590
    },
    {
      "epoch": 0.09716402501973644,
      "grad_norm": 3.272028923034668,
      "learning_rate": 9.974139702467538e-05,
      "loss": 2.6145,
      "step": 1600
    },
    {
      "epoch": 0.0977713001761098,
      "grad_norm": 2.645780086517334,
      "learning_rate": 9.973815721896068e-05,
      "loss": 2.4509,
      "step": 1610
    },
    {
      "epoch": 0.09837857533248315,
      "grad_norm": 1.6018285751342773,
      "learning_rate": 9.973489729834307e-05,
      "loss": 2.4907,
      "step": 1620
    },
    {
      "epoch": 0.0989858504888565,
      "grad_norm": 2.603994607925415,
      "learning_rate": 9.973161726414088e-05,
      "loss": 2.7411,
      "step": 1630
    },
    {
      "epoch": 0.09959312564522986,
      "grad_norm": 2.387714147567749,
      "learning_rate": 9.972831711768063e-05,
      "loss": 2.7527,
      "step": 1640
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 2.995133876800537,
      "learning_rate": 9.972499686029694e-05,
      "loss": 2.9751,
      "step": 1650
    },
    {
      "epoch": 0.10080767595797656,
      "grad_norm": 3.6726107597351074,
      "learning_rate": 9.972165649333259e-05,
      "loss": 2.7097,
      "step": 1660
    },
    {
      "epoch": 0.10141495111434991,
      "grad_norm": 2.033721685409546,
      "learning_rate": 9.971829601813845e-05,
      "loss": 2.6913,
      "step": 1670
    },
    {
      "epoch": 0.10202222627072327,
      "grad_norm": 2.304652214050293,
      "learning_rate": 9.971491543607356e-05,
      "loss": 3.0866,
      "step": 1680
    },
    {
      "epoch": 0.10262950142709662,
      "grad_norm": 4.654269695281982,
      "learning_rate": 9.971151474850511e-05,
      "loss": 2.8732,
      "step": 1690
    },
    {
      "epoch": 0.10323677658346997,
      "grad_norm": 3.589049816131592,
      "learning_rate": 9.970809395680837e-05,
      "loss": 2.6607,
      "step": 1700
    },
    {
      "epoch": 0.10384405173984332,
      "grad_norm": 3.154991865158081,
      "learning_rate": 9.970465306236676e-05,
      "loss": 2.4037,
      "step": 1710
    },
    {
      "epoch": 0.10445132689621668,
      "grad_norm": 2.4109930992126465,
      "learning_rate": 9.970119206657182e-05,
      "loss": 2.6353,
      "step": 1720
    },
    {
      "epoch": 0.10505860205259003,
      "grad_norm": 2.327291250228882,
      "learning_rate": 9.969771097082326e-05,
      "loss": 2.8521,
      "step": 1730
    },
    {
      "epoch": 0.10566587720896338,
      "grad_norm": 2.3901686668395996,
      "learning_rate": 9.969420977652888e-05,
      "loss": 2.6906,
      "step": 1740
    },
    {
      "epoch": 0.10627315236533673,
      "grad_norm": 3.146944999694824,
      "learning_rate": 9.969068848510461e-05,
      "loss": 2.6366,
      "step": 1750
    },
    {
      "epoch": 0.10688042752171009,
      "grad_norm": 5.862473011016846,
      "learning_rate": 9.968714709797453e-05,
      "loss": 2.6662,
      "step": 1760
    },
    {
      "epoch": 0.10748770267808344,
      "grad_norm": 4.54126501083374,
      "learning_rate": 9.968358561657083e-05,
      "loss": 2.7903,
      "step": 1770
    },
    {
      "epoch": 0.10809497783445679,
      "grad_norm": 2.4159276485443115,
      "learning_rate": 9.968000404233382e-05,
      "loss": 3.2408,
      "step": 1780
    },
    {
      "epoch": 0.10870225299083014,
      "grad_norm": 2.532350540161133,
      "learning_rate": 9.967640237671195e-05,
      "loss": 2.8161,
      "step": 1790
    },
    {
      "epoch": 0.1093095281472035,
      "grad_norm": 2.955338954925537,
      "learning_rate": 9.967278062116179e-05,
      "loss": 2.7066,
      "step": 1800
    },
    {
      "epoch": 0.10991680330357685,
      "grad_norm": 5.425250053405762,
      "learning_rate": 9.966913877714803e-05,
      "loss": 3.1352,
      "step": 1810
    },
    {
      "epoch": 0.1105240784599502,
      "grad_norm": 7.161690711975098,
      "learning_rate": 9.966547684614352e-05,
      "loss": 2.8076,
      "step": 1820
    },
    {
      "epoch": 0.11113135361632355,
      "grad_norm": 4.221362113952637,
      "learning_rate": 9.966179482962916e-05,
      "loss": 2.6411,
      "step": 1830
    },
    {
      "epoch": 0.11173862877269691,
      "grad_norm": 2.1997218132019043,
      "learning_rate": 9.965809272909406e-05,
      "loss": 2.6229,
      "step": 1840
    },
    {
      "epoch": 0.11234590392907026,
      "grad_norm": 3.221076726913452,
      "learning_rate": 9.965437054603538e-05,
      "loss": 2.4802,
      "step": 1850
    },
    {
      "epoch": 0.11295317908544361,
      "grad_norm": 2.018519878387451,
      "learning_rate": 9.965062828195841e-05,
      "loss": 2.7864,
      "step": 1860
    },
    {
      "epoch": 0.11356045424181696,
      "grad_norm": 3.077029228210449,
      "learning_rate": 9.964686593837663e-05,
      "loss": 2.8535,
      "step": 1870
    },
    {
      "epoch": 0.11416772939819032,
      "grad_norm": 2.1237363815307617,
      "learning_rate": 9.964308351681157e-05,
      "loss": 2.6879,
      "step": 1880
    },
    {
      "epoch": 0.11477500455456367,
      "grad_norm": 3.0007996559143066,
      "learning_rate": 9.96392810187929e-05,
      "loss": 2.4494,
      "step": 1890
    },
    {
      "epoch": 0.11538227971093702,
      "grad_norm": 1.815516710281372,
      "learning_rate": 9.96354584458584e-05,
      "loss": 2.7779,
      "step": 1900
    },
    {
      "epoch": 0.11598955486731038,
      "grad_norm": 2.029073715209961,
      "learning_rate": 9.9631615799554e-05,
      "loss": 2.6198,
      "step": 1910
    },
    {
      "epoch": 0.11659683002368373,
      "grad_norm": 2.1800644397735596,
      "learning_rate": 9.96277530814337e-05,
      "loss": 2.4975,
      "step": 1920
    },
    {
      "epoch": 0.11720410518005708,
      "grad_norm": 3.3478686809539795,
      "learning_rate": 9.962387029305968e-05,
      "loss": 2.8814,
      "step": 1930
    },
    {
      "epoch": 0.11781138033643043,
      "grad_norm": 2.567033529281616,
      "learning_rate": 9.96199674360022e-05,
      "loss": 2.7724,
      "step": 1940
    },
    {
      "epoch": 0.1184186554928038,
      "grad_norm": 3.720099449157715,
      "learning_rate": 9.961604451183958e-05,
      "loss": 2.6863,
      "step": 1950
    },
    {
      "epoch": 0.11902593064917714,
      "grad_norm": 2.103940725326538,
      "learning_rate": 9.961210152215839e-05,
      "loss": 2.6206,
      "step": 1960
    },
    {
      "epoch": 0.1196332058055505,
      "grad_norm": 3.92439866065979,
      "learning_rate": 9.960813846855318e-05,
      "loss": 2.7931,
      "step": 1970
    },
    {
      "epoch": 0.12024048096192384,
      "grad_norm": 4.141117572784424,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.7756,
      "step": 1980
    },
    {
      "epoch": 0.1208477561182972,
      "grad_norm": 2.5056381225585938,
      "learning_rate": 9.96001521759898e-05,
      "loss": 2.5133,
      "step": 1990
    },
    {
      "epoch": 0.12145503127467056,
      "grad_norm": 2.824169635772705,
      "learning_rate": 9.959612894026138e-05,
      "loss": 2.6841,
      "step": 2000
    },
    {
      "epoch": 0.1220623064310439,
      "grad_norm": 3.8654754161834717,
      "learning_rate": 9.959208564706854e-05,
      "loss": 2.9506,
      "step": 2010
    },
    {
      "epoch": 0.12266958158741725,
      "grad_norm": 3.2301290035247803,
      "learning_rate": 9.958802229804643e-05,
      "loss": 2.8329,
      "step": 2020
    },
    {
      "epoch": 0.12327685674379062,
      "grad_norm": 3.3621160984039307,
      "learning_rate": 9.958393889483837e-05,
      "loss": 3.0437,
      "step": 2030
    },
    {
      "epoch": 0.12388413190016397,
      "grad_norm": 3.243455648422241,
      "learning_rate": 9.95798354390957e-05,
      "loss": 2.5902,
      "step": 2040
    },
    {
      "epoch": 0.12449140705653731,
      "grad_norm": 2.0370466709136963,
      "learning_rate": 9.957571193247797e-05,
      "loss": 2.8775,
      "step": 2050
    },
    {
      "epoch": 0.12509868221291068,
      "grad_norm": 3.824655771255493,
      "learning_rate": 9.957156837665276e-05,
      "loss": 2.7865,
      "step": 2060
    },
    {
      "epoch": 0.12570595736928403,
      "grad_norm": 1.9630728960037231,
      "learning_rate": 9.95674047732958e-05,
      "loss": 2.4911,
      "step": 2070
    },
    {
      "epoch": 0.12631323252565738,
      "grad_norm": 3.390956401824951,
      "learning_rate": 9.956322112409093e-05,
      "loss": 2.7021,
      "step": 2080
    },
    {
      "epoch": 0.12692050768203073,
      "grad_norm": 2.7686266899108887,
      "learning_rate": 9.955901743073006e-05,
      "loss": 2.9021,
      "step": 2090
    },
    {
      "epoch": 0.12752778283840407,
      "grad_norm": 4.272260665893555,
      "learning_rate": 9.955479369491326e-05,
      "loss": 3.1487,
      "step": 2100
    },
    {
      "epoch": 0.12813505799477742,
      "grad_norm": 3.350961446762085,
      "learning_rate": 9.955054991834866e-05,
      "loss": 3.1143,
      "step": 2110
    },
    {
      "epoch": 0.12874233315115077,
      "grad_norm": 2.762260913848877,
      "learning_rate": 9.954628610275249e-05,
      "loss": 2.8706,
      "step": 2120
    },
    {
      "epoch": 0.12934960830752415,
      "grad_norm": 3.9122231006622314,
      "learning_rate": 9.954200224984915e-05,
      "loss": 2.6773,
      "step": 2130
    },
    {
      "epoch": 0.1299568834638975,
      "grad_norm": 3.533756971359253,
      "learning_rate": 9.953769836137106e-05,
      "loss": 3.2229,
      "step": 2140
    },
    {
      "epoch": 0.13056415862027085,
      "grad_norm": 5.022109031677246,
      "learning_rate": 9.95333744390588e-05,
      "loss": 3.1665,
      "step": 2150
    },
    {
      "epoch": 0.1311714337766442,
      "grad_norm": 4.488645076751709,
      "learning_rate": 9.952903048466104e-05,
      "loss": 2.7469,
      "step": 2160
    },
    {
      "epoch": 0.13177870893301755,
      "grad_norm": 2.999912738800049,
      "learning_rate": 9.952466649993451e-05,
      "loss": 3.1273,
      "step": 2170
    },
    {
      "epoch": 0.1323859840893909,
      "grad_norm": 7.09088134765625,
      "learning_rate": 9.952028248664411e-05,
      "loss": 3.1609,
      "step": 2180
    },
    {
      "epoch": 0.13299325924576424,
      "grad_norm": 4.756022930145264,
      "learning_rate": 9.95158784465628e-05,
      "loss": 2.9298,
      "step": 2190
    },
    {
      "epoch": 0.13360053440213762,
      "grad_norm": 6.609065532684326,
      "learning_rate": 9.951145438147162e-05,
      "loss": 2.8606,
      "step": 2200
    },
    {
      "epoch": 0.13420780955851097,
      "grad_norm": 2.513035774230957,
      "learning_rate": 9.950701029315977e-05,
      "loss": 3.0793,
      "step": 2210
    },
    {
      "epoch": 0.13481508471488432,
      "grad_norm": 3.002197027206421,
      "learning_rate": 9.950254618342447e-05,
      "loss": 2.7873,
      "step": 2220
    },
    {
      "epoch": 0.13542235987125767,
      "grad_norm": 2.292532444000244,
      "learning_rate": 9.949806205407111e-05,
      "loss": 2.8288,
      "step": 2230
    },
    {
      "epoch": 0.13602963502763102,
      "grad_norm": 3.5423099994659424,
      "learning_rate": 9.949355790691311e-05,
      "loss": 2.893,
      "step": 2240
    },
    {
      "epoch": 0.13663691018400437,
      "grad_norm": 2.4385581016540527,
      "learning_rate": 9.948903374377205e-05,
      "loss": 2.7611,
      "step": 2250
    },
    {
      "epoch": 0.13724418534037772,
      "grad_norm": 3.24420428276062,
      "learning_rate": 9.948448956647757e-05,
      "loss": 2.9368,
      "step": 2260
    },
    {
      "epoch": 0.13785146049675107,
      "grad_norm": 2.549713373184204,
      "learning_rate": 9.947992537686739e-05,
      "loss": 2.9616,
      "step": 2270
    },
    {
      "epoch": 0.13845873565312444,
      "grad_norm": 6.837342739105225,
      "learning_rate": 9.947534117678735e-05,
      "loss": 2.5777,
      "step": 2280
    },
    {
      "epoch": 0.1390660108094978,
      "grad_norm": 1.8564249277114868,
      "learning_rate": 9.947073696809137e-05,
      "loss": 2.8812,
      "step": 2290
    },
    {
      "epoch": 0.13967328596587114,
      "grad_norm": 3.8077194690704346,
      "learning_rate": 9.946611275264148e-05,
      "loss": 2.5728,
      "step": 2300
    },
    {
      "epoch": 0.1402805611222445,
      "grad_norm": 2.3424782752990723,
      "learning_rate": 9.946146853230777e-05,
      "loss": 2.5292,
      "step": 2310
    },
    {
      "epoch": 0.14088783627861784,
      "grad_norm": 3.4835190773010254,
      "learning_rate": 9.945680430896844e-05,
      "loss": 2.6803,
      "step": 2320
    },
    {
      "epoch": 0.1414951114349912,
      "grad_norm": 8.598039627075195,
      "learning_rate": 9.94521200845098e-05,
      "loss": 2.8117,
      "step": 2330
    },
    {
      "epoch": 0.14210238659136454,
      "grad_norm": 2.788564682006836,
      "learning_rate": 9.944741586082617e-05,
      "loss": 2.4584,
      "step": 2340
    },
    {
      "epoch": 0.1427096617477379,
      "grad_norm": 4.074861526489258,
      "learning_rate": 9.944269163982007e-05,
      "loss": 2.6019,
      "step": 2350
    },
    {
      "epoch": 0.14331693690411126,
      "grad_norm": 4.219268321990967,
      "learning_rate": 9.943794742340202e-05,
      "loss": 2.6492,
      "step": 2360
    },
    {
      "epoch": 0.1439242120604846,
      "grad_norm": 3.0509867668151855,
      "learning_rate": 9.943318321349067e-05,
      "loss": 3.151,
      "step": 2370
    },
    {
      "epoch": 0.14453148721685796,
      "grad_norm": 2.362668514251709,
      "learning_rate": 9.942839901201272e-05,
      "loss": 2.7968,
      "step": 2380
    },
    {
      "epoch": 0.1451387623732313,
      "grad_norm": 3.283731460571289,
      "learning_rate": 9.9423594820903e-05,
      "loss": 2.5412,
      "step": 2390
    },
    {
      "epoch": 0.14574603752960466,
      "grad_norm": 2.352623701095581,
      "learning_rate": 9.94187706421044e-05,
      "loss": 2.7589,
      "step": 2400
    },
    {
      "epoch": 0.146353312685978,
      "grad_norm": 2.5188491344451904,
      "learning_rate": 9.941392647756789e-05,
      "loss": 2.8871,
      "step": 2410
    },
    {
      "epoch": 0.14696058784235136,
      "grad_norm": 3.1891841888427734,
      "learning_rate": 9.940906232925251e-05,
      "loss": 2.7545,
      "step": 2420
    },
    {
      "epoch": 0.14756786299872474,
      "grad_norm": 2.551603078842163,
      "learning_rate": 9.940417819912544e-05,
      "loss": 2.6814,
      "step": 2430
    },
    {
      "epoch": 0.14817513815509809,
      "grad_norm": 1.3717840909957886,
      "learning_rate": 9.939927408916185e-05,
      "loss": 2.5997,
      "step": 2440
    },
    {
      "epoch": 0.14878241331147143,
      "grad_norm": 4.24821662902832,
      "learning_rate": 9.939435000134509e-05,
      "loss": 2.7488,
      "step": 2450
    },
    {
      "epoch": 0.14938968846784478,
      "grad_norm": 2.477276563644409,
      "learning_rate": 9.93894059376665e-05,
      "loss": 2.304,
      "step": 2460
    },
    {
      "epoch": 0.14999696362421813,
      "grad_norm": 1.2770167589187622,
      "learning_rate": 9.938444190012556e-05,
      "loss": 2.0069,
      "step": 2470
    },
    {
      "epoch": 0.15060423878059148,
      "grad_norm": 1.531536340713501,
      "learning_rate": 9.93794578907298e-05,
      "loss": 2.2887,
      "step": 2480
    },
    {
      "epoch": 0.15121151393696483,
      "grad_norm": 2.0877492427825928,
      "learning_rate": 9.937445391149483e-05,
      "loss": 2.9653,
      "step": 2490
    },
    {
      "epoch": 0.15181878909333818,
      "grad_norm": 6.722013473510742,
      "learning_rate": 9.936942996444434e-05,
      "loss": 2.4992,
      "step": 2500
    },
    {
      "epoch": 0.15242606424971156,
      "grad_norm": 2.083906412124634,
      "learning_rate": 9.936438605161009e-05,
      "loss": 2.9464,
      "step": 2510
    },
    {
      "epoch": 0.1530333394060849,
      "grad_norm": 2.181049346923828,
      "learning_rate": 9.935932217503193e-05,
      "loss": 2.9778,
      "step": 2520
    },
    {
      "epoch": 0.15364061456245826,
      "grad_norm": 2.2514467239379883,
      "learning_rate": 9.935423833675777e-05,
      "loss": 2.6186,
      "step": 2530
    },
    {
      "epoch": 0.1542478897188316,
      "grad_norm": 2.5771031379699707,
      "learning_rate": 9.934913453884358e-05,
      "loss": 2.4291,
      "step": 2540
    },
    {
      "epoch": 0.15485516487520495,
      "grad_norm": 1.4450653791427612,
      "learning_rate": 9.934401078335342e-05,
      "loss": 2.4076,
      "step": 2550
    },
    {
      "epoch": 0.1554624400315783,
      "grad_norm": 2.9123857021331787,
      "learning_rate": 9.933886707235946e-05,
      "loss": 2.4538,
      "step": 2560
    },
    {
      "epoch": 0.15606971518795165,
      "grad_norm": 2.603353500366211,
      "learning_rate": 9.933370340794183e-05,
      "loss": 2.8142,
      "step": 2570
    },
    {
      "epoch": 0.156676990344325,
      "grad_norm": 2.504354953765869,
      "learning_rate": 9.932851979218886e-05,
      "loss": 3.0566,
      "step": 2580
    },
    {
      "epoch": 0.15728426550069838,
      "grad_norm": 6.334001064300537,
      "learning_rate": 9.932331622719685e-05,
      "loss": 2.9577,
      "step": 2590
    },
    {
      "epoch": 0.15789154065707173,
      "grad_norm": 2.168508291244507,
      "learning_rate": 9.931809271507023e-05,
      "loss": 2.6132,
      "step": 2600
    },
    {
      "epoch": 0.15849881581344508,
      "grad_norm": 3.2646288871765137,
      "learning_rate": 9.931284925792142e-05,
      "loss": 2.7586,
      "step": 2610
    },
    {
      "epoch": 0.15910609096981843,
      "grad_norm": 5.00638484954834,
      "learning_rate": 9.930758585787102e-05,
      "loss": 2.9365,
      "step": 2620
    },
    {
      "epoch": 0.15971336612619177,
      "grad_norm": 3.4067678451538086,
      "learning_rate": 9.930230251704759e-05,
      "loss": 2.7673,
      "step": 2630
    },
    {
      "epoch": 0.16032064128256512,
      "grad_norm": 3.0509109497070312,
      "learning_rate": 9.929699923758783e-05,
      "loss": 2.4575,
      "step": 2640
    },
    {
      "epoch": 0.16092791643893847,
      "grad_norm": 3.062145233154297,
      "learning_rate": 9.929167602163647e-05,
      "loss": 2.3961,
      "step": 2650
    },
    {
      "epoch": 0.16153519159531182,
      "grad_norm": 2.161017417907715,
      "learning_rate": 9.928633287134626e-05,
      "loss": 2.9916,
      "step": 2660
    },
    {
      "epoch": 0.1621424667516852,
      "grad_norm": 5.448423385620117,
      "learning_rate": 9.928096978887809e-05,
      "loss": 2.9117,
      "step": 2670
    },
    {
      "epoch": 0.16274974190805855,
      "grad_norm": 4.232268810272217,
      "learning_rate": 9.927558677640088e-05,
      "loss": 2.7004,
      "step": 2680
    },
    {
      "epoch": 0.1633570170644319,
      "grad_norm": 5.422119140625,
      "learning_rate": 9.927018383609158e-05,
      "loss": 2.6605,
      "step": 2690
    },
    {
      "epoch": 0.16396429222080525,
      "grad_norm": 2.119215965270996,
      "learning_rate": 9.926476097013524e-05,
      "loss": 2.6782,
      "step": 2700
    },
    {
      "epoch": 0.1645715673771786,
      "grad_norm": 2.2161812782287598,
      "learning_rate": 9.925931818072496e-05,
      "loss": 2.6402,
      "step": 2710
    },
    {
      "epoch": 0.16517884253355195,
      "grad_norm": 2.8675131797790527,
      "learning_rate": 9.925385547006189e-05,
      "loss": 3.1298,
      "step": 2720
    },
    {
      "epoch": 0.1657861176899253,
      "grad_norm": 4.072360038757324,
      "learning_rate": 9.924837284035522e-05,
      "loss": 2.8118,
      "step": 2730
    },
    {
      "epoch": 0.16639339284629867,
      "grad_norm": 4.261361122131348,
      "learning_rate": 9.924287029382223e-05,
      "loss": 2.9002,
      "step": 2740
    },
    {
      "epoch": 0.16700066800267202,
      "grad_norm": 1.7519627809524536,
      "learning_rate": 9.923734783268823e-05,
      "loss": 2.7755,
      "step": 2750
    },
    {
      "epoch": 0.16760794315904537,
      "grad_norm": 3.7197766304016113,
      "learning_rate": 9.92318054591866e-05,
      "loss": 2.8496,
      "step": 2760
    },
    {
      "epoch": 0.16821521831541872,
      "grad_norm": 3.0197410583496094,
      "learning_rate": 9.922624317555874e-05,
      "loss": 2.6811,
      "step": 2770
    },
    {
      "epoch": 0.16882249347179207,
      "grad_norm": 2.217505693435669,
      "learning_rate": 9.922066098405415e-05,
      "loss": 2.4643,
      "step": 2780
    },
    {
      "epoch": 0.16942976862816542,
      "grad_norm": 4.421277046203613,
      "learning_rate": 9.921505888693036e-05,
      "loss": 2.3106,
      "step": 2790
    },
    {
      "epoch": 0.17003704378453877,
      "grad_norm": 1.5161346197128296,
      "learning_rate": 9.920943688645292e-05,
      "loss": 2.6155,
      "step": 2800
    },
    {
      "epoch": 0.17064431894091212,
      "grad_norm": 1.6676315069198608,
      "learning_rate": 9.920379498489549e-05,
      "loss": 2.6192,
      "step": 2810
    },
    {
      "epoch": 0.1712515940972855,
      "grad_norm": 2.112467050552368,
      "learning_rate": 9.919813318453973e-05,
      "loss": 2.7792,
      "step": 2820
    },
    {
      "epoch": 0.17185886925365884,
      "grad_norm": 3.6455938816070557,
      "learning_rate": 9.919245148767535e-05,
      "loss": 3.0264,
      "step": 2830
    },
    {
      "epoch": 0.1724661444100322,
      "grad_norm": 3.838238000869751,
      "learning_rate": 9.918674989660013e-05,
      "loss": 2.8347,
      "step": 2840
    },
    {
      "epoch": 0.17307341956640554,
      "grad_norm": 2.5430784225463867,
      "learning_rate": 9.91810284136199e-05,
      "loss": 3.1742,
      "step": 2850
    },
    {
      "epoch": 0.1736806947227789,
      "grad_norm": 2.5899415016174316,
      "learning_rate": 9.917528704104848e-05,
      "loss": 2.5676,
      "step": 2860
    },
    {
      "epoch": 0.17428796987915224,
      "grad_norm": 2.067662239074707,
      "learning_rate": 9.916952578120781e-05,
      "loss": 2.5503,
      "step": 2870
    },
    {
      "epoch": 0.1748952450355256,
      "grad_norm": 2.4310338497161865,
      "learning_rate": 9.916374463642781e-05,
      "loss": 2.5014,
      "step": 2880
    },
    {
      "epoch": 0.17550252019189894,
      "grad_norm": 2.253208637237549,
      "learning_rate": 9.915794360904649e-05,
      "loss": 2.5005,
      "step": 2890
    },
    {
      "epoch": 0.1761097953482723,
      "grad_norm": 2.0732288360595703,
      "learning_rate": 9.915212270140985e-05,
      "loss": 2.6896,
      "step": 2900
    },
    {
      "epoch": 0.17671707050464566,
      "grad_norm": 2.6870555877685547,
      "learning_rate": 9.914628191587198e-05,
      "loss": 2.5548,
      "step": 2910
    },
    {
      "epoch": 0.177324345661019,
      "grad_norm": 2.1309797763824463,
      "learning_rate": 9.914042125479499e-05,
      "loss": 2.5855,
      "step": 2920
    },
    {
      "epoch": 0.17793162081739236,
      "grad_norm": 2.771636724472046,
      "learning_rate": 9.913454072054901e-05,
      "loss": 2.4663,
      "step": 2930
    },
    {
      "epoch": 0.1785388959737657,
      "grad_norm": 2.417732000350952,
      "learning_rate": 9.912864031551222e-05,
      "loss": 2.4668,
      "step": 2940
    },
    {
      "epoch": 0.17914617113013906,
      "grad_norm": 2.665285348892212,
      "learning_rate": 9.912272004207085e-05,
      "loss": 2.6413,
      "step": 2950
    },
    {
      "epoch": 0.1797534462865124,
      "grad_norm": 2.796678304672241,
      "learning_rate": 9.911677990261913e-05,
      "loss": 3.0202,
      "step": 2960
    },
    {
      "epoch": 0.18036072144288579,
      "grad_norm": 2.566383123397827,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.6453,
      "step": 2970
    },
    {
      "epoch": 0.18096799659925913,
      "grad_norm": 1.996123194694519,
      "learning_rate": 9.910484003530192e-05,
      "loss": 2.4306,
      "step": 2980
    },
    {
      "epoch": 0.18157527175563248,
      "grad_norm": 1.706026554107666,
      "learning_rate": 9.909884031226506e-05,
      "loss": 2.2706,
      "step": 2990
    },
    {
      "epoch": 0.18218254691200583,
      "grad_norm": 1.729822039604187,
      "learning_rate": 9.909282073287522e-05,
      "loss": 2.457,
      "step": 3000
    },
    {
      "epoch": 0.18278982206837918,
      "grad_norm": 2.3749873638153076,
      "learning_rate": 9.908678129956681e-05,
      "loss": 2.6506,
      "step": 3010
    },
    {
      "epoch": 0.18339709722475253,
      "grad_norm": 2.04215407371521,
      "learning_rate": 9.908072201478225e-05,
      "loss": 2.7665,
      "step": 3020
    },
    {
      "epoch": 0.18400437238112588,
      "grad_norm": 3.055835723876953,
      "learning_rate": 9.907464288097203e-05,
      "loss": 2.7069,
      "step": 3030
    },
    {
      "epoch": 0.18461164753749923,
      "grad_norm": 1.7778457403182983,
      "learning_rate": 9.906854390059467e-05,
      "loss": 2.5494,
      "step": 3040
    },
    {
      "epoch": 0.1852189226938726,
      "grad_norm": 2.1889514923095703,
      "learning_rate": 9.906242507611665e-05,
      "loss": 2.882,
      "step": 3050
    },
    {
      "epoch": 0.18582619785024596,
      "grad_norm": 4.832243919372559,
      "learning_rate": 9.905628641001255e-05,
      "loss": 2.7401,
      "step": 3060
    },
    {
      "epoch": 0.1864334730066193,
      "grad_norm": 2.737074136734009,
      "learning_rate": 9.905012790476493e-05,
      "loss": 2.7053,
      "step": 3070
    },
    {
      "epoch": 0.18704074816299265,
      "grad_norm": 4.174485206604004,
      "learning_rate": 9.904394956286441e-05,
      "loss": 2.5649,
      "step": 3080
    },
    {
      "epoch": 0.187648023319366,
      "grad_norm": 2.3177812099456787,
      "learning_rate": 9.903775138680956e-05,
      "loss": 2.9224,
      "step": 3090
    },
    {
      "epoch": 0.18825529847573935,
      "grad_norm": 3.777872085571289,
      "learning_rate": 9.903153337910708e-05,
      "loss": 2.7775,
      "step": 3100
    },
    {
      "epoch": 0.1888625736321127,
      "grad_norm": 2.733771324157715,
      "learning_rate": 9.90252955422716e-05,
      "loss": 2.4246,
      "step": 3110
    },
    {
      "epoch": 0.18946984878848605,
      "grad_norm": 1.4292099475860596,
      "learning_rate": 9.90190378788258e-05,
      "loss": 2.4589,
      "step": 3120
    },
    {
      "epoch": 0.19007712394485943,
      "grad_norm": 3.246751546859741,
      "learning_rate": 9.901276039130039e-05,
      "loss": 2.9849,
      "step": 3130
    },
    {
      "epoch": 0.19068439910123278,
      "grad_norm": 2.8562397956848145,
      "learning_rate": 9.900646308223408e-05,
      "loss": 2.7477,
      "step": 3140
    },
    {
      "epoch": 0.19129167425760613,
      "grad_norm": 4.488560199737549,
      "learning_rate": 9.90001459541736e-05,
      "loss": 2.462,
      "step": 3150
    },
    {
      "epoch": 0.19189894941397947,
      "grad_norm": 2.5003440380096436,
      "learning_rate": 9.899380900967371e-05,
      "loss": 2.801,
      "step": 3160
    },
    {
      "epoch": 0.19250622457035282,
      "grad_norm": 3.0068466663360596,
      "learning_rate": 9.898745225129715e-05,
      "loss": 2.5794,
      "step": 3170
    },
    {
      "epoch": 0.19311349972672617,
      "grad_norm": 3.695321798324585,
      "learning_rate": 9.898107568161472e-05,
      "loss": 3.0487,
      "step": 3180
    },
    {
      "epoch": 0.19372077488309952,
      "grad_norm": 3.300095796585083,
      "learning_rate": 9.897467930320519e-05,
      "loss": 3.0191,
      "step": 3190
    },
    {
      "epoch": 0.19432805003947287,
      "grad_norm": 2.3612868785858154,
      "learning_rate": 9.896826311865534e-05,
      "loss": 2.9502,
      "step": 3200
    },
    {
      "epoch": 0.19493532519584625,
      "grad_norm": 1.7939715385437012,
      "learning_rate": 9.896182713056001e-05,
      "loss": 2.899,
      "step": 3210
    },
    {
      "epoch": 0.1955426003522196,
      "grad_norm": 2.2416467666625977,
      "learning_rate": 9.8955371341522e-05,
      "loss": 2.6435,
      "step": 3220
    },
    {
      "epoch": 0.19614987550859295,
      "grad_norm": 3.1400442123413086,
      "learning_rate": 9.894889575415214e-05,
      "loss": 2.7126,
      "step": 3230
    },
    {
      "epoch": 0.1967571506649663,
      "grad_norm": 5.574371337890625,
      "learning_rate": 9.894240037106927e-05,
      "loss": 2.9349,
      "step": 3240
    },
    {
      "epoch": 0.19736442582133965,
      "grad_norm": 3.5921504497528076,
      "learning_rate": 9.89358851949002e-05,
      "loss": 2.9763,
      "step": 3250
    },
    {
      "epoch": 0.197971700977713,
      "grad_norm": 4.121638298034668,
      "learning_rate": 9.892935022827978e-05,
      "loss": 2.6553,
      "step": 3260
    },
    {
      "epoch": 0.19857897613408634,
      "grad_norm": 2.0624380111694336,
      "learning_rate": 9.892279547385087e-05,
      "loss": 2.542,
      "step": 3270
    },
    {
      "epoch": 0.19918625129045972,
      "grad_norm": 1.2536828517913818,
      "learning_rate": 9.891622093426429e-05,
      "loss": 2.411,
      "step": 3280
    },
    {
      "epoch": 0.19979352644683307,
      "grad_norm": 1.965015172958374,
      "learning_rate": 9.890962661217892e-05,
      "loss": 2.6575,
      "step": 3290
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 2.6900694370269775,
      "learning_rate": 9.89030125102616e-05,
      "loss": 2.2768,
      "step": 3300
    },
    {
      "epoch": 0.20100807675957977,
      "grad_norm": 4.045104503631592,
      "learning_rate": 9.889637863118715e-05,
      "loss": 3.1668,
      "step": 3310
    },
    {
      "epoch": 0.20161535191595312,
      "grad_norm": 4.006162166595459,
      "learning_rate": 9.888972497763844e-05,
      "loss": 2.6059,
      "step": 3320
    },
    {
      "epoch": 0.20222262707232647,
      "grad_norm": 1.8307310342788696,
      "learning_rate": 9.888305155230632e-05,
      "loss": 2.6234,
      "step": 3330
    },
    {
      "epoch": 0.20282990222869982,
      "grad_norm": 3.2577028274536133,
      "learning_rate": 9.887635835788965e-05,
      "loss": 2.4421,
      "step": 3340
    },
    {
      "epoch": 0.20343717738507316,
      "grad_norm": 3.091216564178467,
      "learning_rate": 9.886964539709519e-05,
      "loss": 3.2579,
      "step": 3350
    },
    {
      "epoch": 0.20404445254144654,
      "grad_norm": 2.574180841445923,
      "learning_rate": 9.886291267263783e-05,
      "loss": 2.8493,
      "step": 3360
    },
    {
      "epoch": 0.2046517276978199,
      "grad_norm": 2.3522448539733887,
      "learning_rate": 9.885616018724037e-05,
      "loss": 3.0643,
      "step": 3370
    },
    {
      "epoch": 0.20525900285419324,
      "grad_norm": 3.863027811050415,
      "learning_rate": 9.884938794363365e-05,
      "loss": 2.718,
      "step": 3380
    },
    {
      "epoch": 0.2058662780105666,
      "grad_norm": 2.705446720123291,
      "learning_rate": 9.884259594455643e-05,
      "loss": 3.0853,
      "step": 3390
    },
    {
      "epoch": 0.20647355316693994,
      "grad_norm": 5.110780715942383,
      "learning_rate": 9.883578419275553e-05,
      "loss": 3.1031,
      "step": 3400
    },
    {
      "epoch": 0.2070808283233133,
      "grad_norm": 9.48508071899414,
      "learning_rate": 9.882895269098572e-05,
      "loss": 2.8142,
      "step": 3410
    },
    {
      "epoch": 0.20768810347968664,
      "grad_norm": 2.7131142616271973,
      "learning_rate": 9.882210144200979e-05,
      "loss": 2.9707,
      "step": 3420
    },
    {
      "epoch": 0.20829537863605999,
      "grad_norm": 6.04401159286499,
      "learning_rate": 9.881523044859844e-05,
      "loss": 2.9288,
      "step": 3430
    },
    {
      "epoch": 0.20890265379243336,
      "grad_norm": 2.5574951171875,
      "learning_rate": 9.880833971353048e-05,
      "loss": 2.8821,
      "step": 3440
    },
    {
      "epoch": 0.2095099289488067,
      "grad_norm": 3.428109645843506,
      "learning_rate": 9.880142923959258e-05,
      "loss": 2.4052,
      "step": 3450
    },
    {
      "epoch": 0.21011720410518006,
      "grad_norm": 3.915019989013672,
      "learning_rate": 9.879449902957946e-05,
      "loss": 2.4556,
      "step": 3460
    },
    {
      "epoch": 0.2107244792615534,
      "grad_norm": 4.7336249351501465,
      "learning_rate": 9.878754908629384e-05,
      "loss": 3.0088,
      "step": 3470
    },
    {
      "epoch": 0.21133175441792676,
      "grad_norm": 5.3492608070373535,
      "learning_rate": 9.878057941254634e-05,
      "loss": 2.9061,
      "step": 3480
    },
    {
      "epoch": 0.2119390295743001,
      "grad_norm": 5.061074733734131,
      "learning_rate": 9.877359001115563e-05,
      "loss": 2.977,
      "step": 3490
    },
    {
      "epoch": 0.21254630473067346,
      "grad_norm": 6.3792009353637695,
      "learning_rate": 9.876658088494832e-05,
      "loss": 2.6252,
      "step": 3500
    },
    {
      "epoch": 0.21315357988704683,
      "grad_norm": 2.6951260566711426,
      "learning_rate": 9.875955203675905e-05,
      "loss": 2.5433,
      "step": 3510
    },
    {
      "epoch": 0.21376085504342018,
      "grad_norm": 2.335087299346924,
      "learning_rate": 9.875250346943035e-05,
      "loss": 2.8298,
      "step": 3520
    },
    {
      "epoch": 0.21436813019979353,
      "grad_norm": 3.5351386070251465,
      "learning_rate": 9.874543518581279e-05,
      "loss": 2.7372,
      "step": 3530
    },
    {
      "epoch": 0.21497540535616688,
      "grad_norm": 3.641818046569824,
      "learning_rate": 9.873834718876491e-05,
      "loss": 2.7473,
      "step": 3540
    },
    {
      "epoch": 0.21558268051254023,
      "grad_norm": 3.574537515640259,
      "learning_rate": 9.873123948115321e-05,
      "loss": 2.402,
      "step": 3550
    },
    {
      "epoch": 0.21618995566891358,
      "grad_norm": 2.720914363861084,
      "learning_rate": 9.872411206585215e-05,
      "loss": 2.566,
      "step": 3560
    },
    {
      "epoch": 0.21679723082528693,
      "grad_norm": 2.3451085090637207,
      "learning_rate": 9.871696494574417e-05,
      "loss": 2.6943,
      "step": 3570
    },
    {
      "epoch": 0.21740450598166028,
      "grad_norm": 3.902515411376953,
      "learning_rate": 9.870979812371967e-05,
      "loss": 2.7848,
      "step": 3580
    },
    {
      "epoch": 0.21801178113803366,
      "grad_norm": 3.8705780506134033,
      "learning_rate": 9.870261160267704e-05,
      "loss": 2.6667,
      "step": 3590
    },
    {
      "epoch": 0.218619056294407,
      "grad_norm": 2.5219051837921143,
      "learning_rate": 9.869540538552263e-05,
      "loss": 2.8836,
      "step": 3600
    },
    {
      "epoch": 0.21922633145078035,
      "grad_norm": 3.556544065475464,
      "learning_rate": 9.868817947517073e-05,
      "loss": 2.7057,
      "step": 3610
    },
    {
      "epoch": 0.2198336066071537,
      "grad_norm": 2.2481508255004883,
      "learning_rate": 9.868093387454362e-05,
      "loss": 2.9842,
      "step": 3620
    },
    {
      "epoch": 0.22044088176352705,
      "grad_norm": 1.7546168565750122,
      "learning_rate": 9.867366858657155e-05,
      "loss": 2.6182,
      "step": 3630
    },
    {
      "epoch": 0.2210481569199004,
      "grad_norm": 3.4513635635375977,
      "learning_rate": 9.866638361419269e-05,
      "loss": 2.5793,
      "step": 3640
    },
    {
      "epoch": 0.22165543207627375,
      "grad_norm": 5.610491752624512,
      "learning_rate": 9.865907896035324e-05,
      "loss": 2.5074,
      "step": 3650
    },
    {
      "epoch": 0.2222627072326471,
      "grad_norm": 2.9097673892974854,
      "learning_rate": 9.865175462800727e-05,
      "loss": 3.0177,
      "step": 3660
    },
    {
      "epoch": 0.22286998238902048,
      "grad_norm": 2.3843166828155518,
      "learning_rate": 9.86444106201169e-05,
      "loss": 2.9034,
      "step": 3670
    },
    {
      "epoch": 0.22347725754539383,
      "grad_norm": 3.045614719390869,
      "learning_rate": 9.863704693965214e-05,
      "loss": 2.9245,
      "step": 3680
    },
    {
      "epoch": 0.22408453270176718,
      "grad_norm": 2.782386064529419,
      "learning_rate": 9.862966358959099e-05,
      "loss": 2.4482,
      "step": 3690
    },
    {
      "epoch": 0.22469180785814052,
      "grad_norm": 1.9459657669067383,
      "learning_rate": 9.862226057291937e-05,
      "loss": 2.3088,
      "step": 3700
    },
    {
      "epoch": 0.22529908301451387,
      "grad_norm": 1.696479082107544,
      "learning_rate": 9.861483789263122e-05,
      "loss": 2.5437,
      "step": 3710
    },
    {
      "epoch": 0.22590635817088722,
      "grad_norm": 2.0274083614349365,
      "learning_rate": 9.860739555172835e-05,
      "loss": 2.6204,
      "step": 3720
    },
    {
      "epoch": 0.22651363332726057,
      "grad_norm": 2.0878758430480957,
      "learning_rate": 9.859993355322058e-05,
      "loss": 2.8906,
      "step": 3730
    },
    {
      "epoch": 0.22712090848363392,
      "grad_norm": 1.5803723335266113,
      "learning_rate": 9.859245190012566e-05,
      "loss": 2.4404,
      "step": 3740
    },
    {
      "epoch": 0.2277281836400073,
      "grad_norm": 1.9750295877456665,
      "learning_rate": 9.85849505954693e-05,
      "loss": 2.7873,
      "step": 3750
    },
    {
      "epoch": 0.22833545879638065,
      "grad_norm": 2.813697576522827,
      "learning_rate": 9.857742964228512e-05,
      "loss": 2.5473,
      "step": 3760
    },
    {
      "epoch": 0.228942733952754,
      "grad_norm": 2.6191413402557373,
      "learning_rate": 9.856988904361474e-05,
      "loss": 2.5801,
      "step": 3770
    },
    {
      "epoch": 0.22955000910912735,
      "grad_norm": 1.8832244873046875,
      "learning_rate": 9.856232880250769e-05,
      "loss": 3.0785,
      "step": 3780
    },
    {
      "epoch": 0.2301572842655007,
      "grad_norm": 4.4343342781066895,
      "learning_rate": 9.855474892202144e-05,
      "loss": 2.8938,
      "step": 3790
    },
    {
      "epoch": 0.23076455942187404,
      "grad_norm": 2.286416530609131,
      "learning_rate": 9.854714940522142e-05,
      "loss": 2.7686,
      "step": 3800
    },
    {
      "epoch": 0.2313718345782474,
      "grad_norm": 3.0311994552612305,
      "learning_rate": 9.853953025518102e-05,
      "loss": 2.7424,
      "step": 3810
    },
    {
      "epoch": 0.23197910973462077,
      "grad_norm": 1.974241018295288,
      "learning_rate": 9.853189147498149e-05,
      "loss": 2.3339,
      "step": 3820
    },
    {
      "epoch": 0.23258638489099412,
      "grad_norm": 2.4821865558624268,
      "learning_rate": 9.852423306771214e-05,
      "loss": 2.4837,
      "step": 3830
    },
    {
      "epoch": 0.23319366004736747,
      "grad_norm": 3.675553560256958,
      "learning_rate": 9.85165550364701e-05,
      "loss": 2.3941,
      "step": 3840
    },
    {
      "epoch": 0.23380093520374082,
      "grad_norm": 2.205625295639038,
      "learning_rate": 9.850885738436053e-05,
      "loss": 2.4988,
      "step": 3850
    },
    {
      "epoch": 0.23440821036011417,
      "grad_norm": 3.900033473968506,
      "learning_rate": 9.850114011449645e-05,
      "loss": 2.7455,
      "step": 3860
    },
    {
      "epoch": 0.23501548551648752,
      "grad_norm": 3.8293356895446777,
      "learning_rate": 9.849340322999886e-05,
      "loss": 2.4713,
      "step": 3870
    },
    {
      "epoch": 0.23562276067286086,
      "grad_norm": 4.01187801361084,
      "learning_rate": 9.848564673399667e-05,
      "loss": 2.5779,
      "step": 3880
    },
    {
      "epoch": 0.23623003582923421,
      "grad_norm": 2.6362264156341553,
      "learning_rate": 9.847787062962675e-05,
      "loss": 2.679,
      "step": 3890
    },
    {
      "epoch": 0.2368373109856076,
      "grad_norm": 4.051872253417969,
      "learning_rate": 9.847007492003388e-05,
      "loss": 2.8158,
      "step": 3900
    },
    {
      "epoch": 0.23744458614198094,
      "grad_norm": 1.8156551122665405,
      "learning_rate": 9.846225960837075e-05,
      "loss": 2.5326,
      "step": 3910
    },
    {
      "epoch": 0.2380518612983543,
      "grad_norm": 3.5477824211120605,
      "learning_rate": 9.8454424697798e-05,
      "loss": 2.4435,
      "step": 3920
    },
    {
      "epoch": 0.23865913645472764,
      "grad_norm": 3.5380911827087402,
      "learning_rate": 9.844657019148418e-05,
      "loss": 2.6089,
      "step": 3930
    },
    {
      "epoch": 0.239266411611101,
      "grad_norm": 1.6853457689285278,
      "learning_rate": 9.843869609260583e-05,
      "loss": 2.392,
      "step": 3940
    },
    {
      "epoch": 0.23987368676747434,
      "grad_norm": 2.7005269527435303,
      "learning_rate": 9.84308024043473e-05,
      "loss": 2.5094,
      "step": 3950
    },
    {
      "epoch": 0.24048096192384769,
      "grad_norm": 2.7162938117980957,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.5644,
      "step": 3960
    },
    {
      "epoch": 0.24108823708022104,
      "grad_norm": 2.0047061443328857,
      "learning_rate": 9.841495627246707e-05,
      "loss": 2.3856,
      "step": 3970
    },
    {
      "epoch": 0.2416955122365944,
      "grad_norm": 2.2503981590270996,
      "learning_rate": 9.840700383525376e-05,
      "loss": 2.5599,
      "step": 3980
    },
    {
      "epoch": 0.24230278739296776,
      "grad_norm": 3.7740635871887207,
      "learning_rate": 9.839903182147717e-05,
      "loss": 2.7236,
      "step": 3990
    },
    {
      "epoch": 0.2429100625493411,
      "grad_norm": 3.18257474899292,
      "learning_rate": 9.839104023436128e-05,
      "loss": 2.6896,
      "step": 4000
    },
    {
      "epoch": 0.24351733770571446,
      "grad_norm": 2.369929313659668,
      "learning_rate": 9.8383029077138e-05,
      "loss": 3.0945,
      "step": 4010
    },
    {
      "epoch": 0.2441246128620878,
      "grad_norm": 4.257975101470947,
      "learning_rate": 9.837499835304724e-05,
      "loss": 3.031,
      "step": 4020
    },
    {
      "epoch": 0.24473188801846116,
      "grad_norm": 3.2274887561798096,
      "learning_rate": 9.836694806533669e-05,
      "loss": 2.4955,
      "step": 4030
    },
    {
      "epoch": 0.2453391631748345,
      "grad_norm": 2.2119297981262207,
      "learning_rate": 9.835887821726202e-05,
      "loss": 2.1706,
      "step": 4040
    },
    {
      "epoch": 0.24594643833120788,
      "grad_norm": 2.16085147857666,
      "learning_rate": 9.835078881208681e-05,
      "loss": 2.4878,
      "step": 4050
    },
    {
      "epoch": 0.24655371348758123,
      "grad_norm": 4.253424167633057,
      "learning_rate": 9.834267985308256e-05,
      "loss": 2.7579,
      "step": 4060
    },
    {
      "epoch": 0.24716098864395458,
      "grad_norm": 3.1904826164245605,
      "learning_rate": 9.833455134352866e-05,
      "loss": 2.6957,
      "step": 4070
    },
    {
      "epoch": 0.24776826380032793,
      "grad_norm": 2.0744168758392334,
      "learning_rate": 9.832640328671238e-05,
      "loss": 2.7579,
      "step": 4080
    },
    {
      "epoch": 0.24837553895670128,
      "grad_norm": 2.6864755153656006,
      "learning_rate": 9.831823568592897e-05,
      "loss": 2.5567,
      "step": 4090
    },
    {
      "epoch": 0.24898281411307463,
      "grad_norm": 3.30214262008667,
      "learning_rate": 9.831004854448152e-05,
      "loss": 2.6107,
      "step": 4100
    },
    {
      "epoch": 0.24959008926944798,
      "grad_norm": 2.6300244331359863,
      "learning_rate": 9.830184186568101e-05,
      "loss": 2.9667,
      "step": 4110
    },
    {
      "epoch": 0.25019736442582136,
      "grad_norm": 3.1646106243133545,
      "learning_rate": 9.829361565284639e-05,
      "loss": 2.7619,
      "step": 4120
    },
    {
      "epoch": 0.2508046395821947,
      "grad_norm": 4.114932537078857,
      "learning_rate": 9.828536990930444e-05,
      "loss": 2.8465,
      "step": 4130
    },
    {
      "epoch": 0.25141191473856805,
      "grad_norm": 2.61739444732666,
      "learning_rate": 9.82771046383899e-05,
      "loss": 2.7121,
      "step": 4140
    },
    {
      "epoch": 0.2520191898949414,
      "grad_norm": 2.7783448696136475,
      "learning_rate": 9.826881984344536e-05,
      "loss": 2.8407,
      "step": 4150
    },
    {
      "epoch": 0.25262646505131475,
      "grad_norm": 4.8291916847229,
      "learning_rate": 9.826051552782132e-05,
      "loss": 2.5552,
      "step": 4160
    },
    {
      "epoch": 0.25323374020768813,
      "grad_norm": 2.3700718879699707,
      "learning_rate": 9.825219169487618e-05,
      "loss": 2.6864,
      "step": 4170
    },
    {
      "epoch": 0.25384101536406145,
      "grad_norm": 2.8589563369750977,
      "learning_rate": 9.824384834797625e-05,
      "loss": 2.95,
      "step": 4180
    },
    {
      "epoch": 0.2544482905204348,
      "grad_norm": 2.6734585762023926,
      "learning_rate": 9.823548549049569e-05,
      "loss": 2.9177,
      "step": 4190
    },
    {
      "epoch": 0.25505556567680815,
      "grad_norm": 3.068936586380005,
      "learning_rate": 9.82271031258166e-05,
      "loss": 2.9135,
      "step": 4200
    },
    {
      "epoch": 0.2556628408331815,
      "grad_norm": 6.355357646942139,
      "learning_rate": 9.82187012573289e-05,
      "loss": 2.7002,
      "step": 4210
    },
    {
      "epoch": 0.25627011598955485,
      "grad_norm": 3.9745824337005615,
      "learning_rate": 9.821027988843046e-05,
      "loss": 3.2746,
      "step": 4220
    },
    {
      "epoch": 0.2568773911459282,
      "grad_norm": 2.342935800552368,
      "learning_rate": 9.820183902252702e-05,
      "loss": 2.5797,
      "step": 4230
    },
    {
      "epoch": 0.25748466630230155,
      "grad_norm": 2.6542980670928955,
      "learning_rate": 9.81933786630322e-05,
      "loss": 2.5502,
      "step": 4240
    },
    {
      "epoch": 0.2580919414586749,
      "grad_norm": 3.0595574378967285,
      "learning_rate": 9.81848988133675e-05,
      "loss": 2.6272,
      "step": 4250
    },
    {
      "epoch": 0.2586992166150483,
      "grad_norm": 4.398388385772705,
      "learning_rate": 9.817639947696231e-05,
      "loss": 2.5148,
      "step": 4260
    },
    {
      "epoch": 0.2593064917714216,
      "grad_norm": 3.045703887939453,
      "learning_rate": 9.816788065725389e-05,
      "loss": 3.2038,
      "step": 4270
    },
    {
      "epoch": 0.259913766927795,
      "grad_norm": 3.259399652481079,
      "learning_rate": 9.81593423576874e-05,
      "loss": 3.2504,
      "step": 4280
    },
    {
      "epoch": 0.2605210420841683,
      "grad_norm": 6.774500370025635,
      "learning_rate": 9.815078458171585e-05,
      "loss": 3.0742,
      "step": 4290
    },
    {
      "epoch": 0.2611283172405417,
      "grad_norm": 3.3682825565338135,
      "learning_rate": 9.814220733280015e-05,
      "loss": 2.6362,
      "step": 4300
    },
    {
      "epoch": 0.261735592396915,
      "grad_norm": 2.7376015186309814,
      "learning_rate": 9.813361061440907e-05,
      "loss": 2.6324,
      "step": 4310
    },
    {
      "epoch": 0.2623428675532884,
      "grad_norm": 3.0718369483947754,
      "learning_rate": 9.812499443001926e-05,
      "loss": 2.506,
      "step": 4320
    },
    {
      "epoch": 0.26295014270966177,
      "grad_norm": 3.2094228267669678,
      "learning_rate": 9.811635878311524e-05,
      "loss": 2.7084,
      "step": 4330
    },
    {
      "epoch": 0.2635574178660351,
      "grad_norm": 2.701871156692505,
      "learning_rate": 9.810770367718942e-05,
      "loss": 2.384,
      "step": 4340
    },
    {
      "epoch": 0.26416469302240847,
      "grad_norm": 3.7065112590789795,
      "learning_rate": 9.809902911574204e-05,
      "loss": 2.4978,
      "step": 4350
    },
    {
      "epoch": 0.2647719681787818,
      "grad_norm": 3.8052141666412354,
      "learning_rate": 9.809033510228126e-05,
      "loss": 3.0138,
      "step": 4360
    },
    {
      "epoch": 0.26537924333515517,
      "grad_norm": 5.367794036865234,
      "learning_rate": 9.808162164032304e-05,
      "loss": 2.7716,
      "step": 4370
    },
    {
      "epoch": 0.2659865184915285,
      "grad_norm": 1.9300705194473267,
      "learning_rate": 9.807288873339126e-05,
      "loss": 2.5247,
      "step": 4380
    },
    {
      "epoch": 0.26659379364790187,
      "grad_norm": 3.477221965789795,
      "learning_rate": 9.806413638501766e-05,
      "loss": 2.6192,
      "step": 4390
    },
    {
      "epoch": 0.26720106880427524,
      "grad_norm": 3.5737721920013428,
      "learning_rate": 9.805536459874182e-05,
      "loss": 2.5503,
      "step": 4400
    },
    {
      "epoch": 0.26780834396064856,
      "grad_norm": 3.9153947830200195,
      "learning_rate": 9.804657337811119e-05,
      "loss": 2.3362,
      "step": 4410
    },
    {
      "epoch": 0.26841561911702194,
      "grad_norm": 2.249321937561035,
      "learning_rate": 9.803776272668106e-05,
      "loss": 2.8203,
      "step": 4420
    },
    {
      "epoch": 0.26902289427339526,
      "grad_norm": 2.5892300605773926,
      "learning_rate": 9.802893264801462e-05,
      "loss": 2.3298,
      "step": 4430
    },
    {
      "epoch": 0.26963016942976864,
      "grad_norm": 2.309278964996338,
      "learning_rate": 9.80200831456829e-05,
      "loss": 2.6912,
      "step": 4440
    },
    {
      "epoch": 0.27023744458614196,
      "grad_norm": 3.7015762329101562,
      "learning_rate": 9.801121422326475e-05,
      "loss": 2.8123,
      "step": 4450
    },
    {
      "epoch": 0.27084471974251534,
      "grad_norm": 3.5909526348114014,
      "learning_rate": 9.800232588434692e-05,
      "loss": 2.8582,
      "step": 4460
    },
    {
      "epoch": 0.27145199489888866,
      "grad_norm": 2.810145854949951,
      "learning_rate": 9.799341813252402e-05,
      "loss": 2.8055,
      "step": 4470
    },
    {
      "epoch": 0.27205927005526204,
      "grad_norm": 4.735893726348877,
      "learning_rate": 9.798449097139845e-05,
      "loss": 3.0835,
      "step": 4480
    },
    {
      "epoch": 0.2726665452116354,
      "grad_norm": 2.36032772064209,
      "learning_rate": 9.797554440458052e-05,
      "loss": 2.7253,
      "step": 4490
    },
    {
      "epoch": 0.27327382036800874,
      "grad_norm": 2.2880494594573975,
      "learning_rate": 9.796657843568835e-05,
      "loss": 2.5313,
      "step": 4500
    },
    {
      "epoch": 0.2738810955243821,
      "grad_norm": 5.525994300842285,
      "learning_rate": 9.795759306834793e-05,
      "loss": 2.6936,
      "step": 4510
    },
    {
      "epoch": 0.27448837068075543,
      "grad_norm": 3.9558563232421875,
      "learning_rate": 9.794858830619307e-05,
      "loss": 2.4912,
      "step": 4520
    },
    {
      "epoch": 0.2750956458371288,
      "grad_norm": 2.2502200603485107,
      "learning_rate": 9.793956415286545e-05,
      "loss": 2.7239,
      "step": 4530
    },
    {
      "epoch": 0.27570292099350213,
      "grad_norm": 4.233541011810303,
      "learning_rate": 9.79305206120146e-05,
      "loss": 2.7652,
      "step": 4540
    },
    {
      "epoch": 0.2763101961498755,
      "grad_norm": 3.0988237857818604,
      "learning_rate": 9.792145768729785e-05,
      "loss": 2.5716,
      "step": 4550
    },
    {
      "epoch": 0.2769174713062489,
      "grad_norm": 2.9098269939422607,
      "learning_rate": 9.791237538238038e-05,
      "loss": 2.4281,
      "step": 4560
    },
    {
      "epoch": 0.2775247464626222,
      "grad_norm": 2.3917245864868164,
      "learning_rate": 9.790327370093525e-05,
      "loss": 2.3824,
      "step": 4570
    },
    {
      "epoch": 0.2781320216189956,
      "grad_norm": 3.6895751953125,
      "learning_rate": 9.78941526466433e-05,
      "loss": 2.3797,
      "step": 4580
    },
    {
      "epoch": 0.2787392967753689,
      "grad_norm": 2.311485528945923,
      "learning_rate": 9.788501222319324e-05,
      "loss": 2.4595,
      "step": 4590
    },
    {
      "epoch": 0.2793465719317423,
      "grad_norm": 2.711461067199707,
      "learning_rate": 9.78758524342816e-05,
      "loss": 2.7317,
      "step": 4600
    },
    {
      "epoch": 0.2799538470881156,
      "grad_norm": 4.229835033416748,
      "learning_rate": 9.786667328361274e-05,
      "loss": 2.6339,
      "step": 4610
    },
    {
      "epoch": 0.280561122244489,
      "grad_norm": 2.738090991973877,
      "learning_rate": 9.785747477489887e-05,
      "loss": 2.5725,
      "step": 4620
    },
    {
      "epoch": 0.28116839740086236,
      "grad_norm": 3.264017105102539,
      "learning_rate": 9.784825691186e-05,
      "loss": 2.6348,
      "step": 4630
    },
    {
      "epoch": 0.2817756725572357,
      "grad_norm": 2.1726365089416504,
      "learning_rate": 9.783901969822399e-05,
      "loss": 2.6744,
      "step": 4640
    },
    {
      "epoch": 0.28238294771360906,
      "grad_norm": 2.5314559936523438,
      "learning_rate": 9.78297631377265e-05,
      "loss": 2.5572,
      "step": 4650
    },
    {
      "epoch": 0.2829902228699824,
      "grad_norm": 2.3580260276794434,
      "learning_rate": 9.782048723411106e-05,
      "loss": 2.5524,
      "step": 4660
    },
    {
      "epoch": 0.28359749802635575,
      "grad_norm": 2.5427303314208984,
      "learning_rate": 9.781119199112896e-05,
      "loss": 2.2684,
      "step": 4670
    },
    {
      "epoch": 0.2842047731827291,
      "grad_norm": 1.748712420463562,
      "learning_rate": 9.780187741253935e-05,
      "loss": 2.5007,
      "step": 4680
    },
    {
      "epoch": 0.28481204833910245,
      "grad_norm": 3.388526439666748,
      "learning_rate": 9.779254350210922e-05,
      "loss": 2.5284,
      "step": 4690
    },
    {
      "epoch": 0.2854193234954758,
      "grad_norm": 2.6103951930999756,
      "learning_rate": 9.778319026361332e-05,
      "loss": 2.5922,
      "step": 4700
    },
    {
      "epoch": 0.28602659865184915,
      "grad_norm": 3.6787071228027344,
      "learning_rate": 9.777381770083426e-05,
      "loss": 2.8014,
      "step": 4710
    },
    {
      "epoch": 0.28663387380822253,
      "grad_norm": 2.8082218170166016,
      "learning_rate": 9.776442581756248e-05,
      "loss": 2.8619,
      "step": 4720
    },
    {
      "epoch": 0.28724114896459585,
      "grad_norm": 2.760582447052002,
      "learning_rate": 9.775501461759617e-05,
      "loss": 3.0972,
      "step": 4730
    },
    {
      "epoch": 0.2878484241209692,
      "grad_norm": 3.080962657928467,
      "learning_rate": 9.774558410474139e-05,
      "loss": 2.728,
      "step": 4740
    },
    {
      "epoch": 0.28845569927734255,
      "grad_norm": 3.8554110527038574,
      "learning_rate": 9.773613428281196e-05,
      "loss": 2.9752,
      "step": 4750
    },
    {
      "epoch": 0.2890629744337159,
      "grad_norm": 4.114931583404541,
      "learning_rate": 9.772666515562958e-05,
      "loss": 2.8765,
      "step": 4760
    },
    {
      "epoch": 0.28967024959008925,
      "grad_norm": 4.620038032531738,
      "learning_rate": 9.77171767270237e-05,
      "loss": 2.8728,
      "step": 4770
    },
    {
      "epoch": 0.2902775247464626,
      "grad_norm": 1.9616671800613403,
      "learning_rate": 9.77076690008316e-05,
      "loss": 2.9237,
      "step": 4780
    },
    {
      "epoch": 0.290884799902836,
      "grad_norm": 3.4501352310180664,
      "learning_rate": 9.769814198089832e-05,
      "loss": 2.871,
      "step": 4790
    },
    {
      "epoch": 0.2914920750592093,
      "grad_norm": 2.927307367324829,
      "learning_rate": 9.768859567107677e-05,
      "loss": 2.8622,
      "step": 4800
    },
    {
      "epoch": 0.2920993502155827,
      "grad_norm": 2.5114638805389404,
      "learning_rate": 9.767903007522763e-05,
      "loss": 2.6475,
      "step": 4810
    },
    {
      "epoch": 0.292706625371956,
      "grad_norm": 3.0498149394989014,
      "learning_rate": 9.766944519721937e-05,
      "loss": 2.5377,
      "step": 4820
    },
    {
      "epoch": 0.2933139005283294,
      "grad_norm": 3.047322988510132,
      "learning_rate": 9.765984104092826e-05,
      "loss": 2.6764,
      "step": 4830
    },
    {
      "epoch": 0.2939211756847027,
      "grad_norm": 2.788280487060547,
      "learning_rate": 9.765021761023838e-05,
      "loss": 2.8318,
      "step": 4840
    },
    {
      "epoch": 0.2945284508410761,
      "grad_norm": 5.985361099243164,
      "learning_rate": 9.764057490904162e-05,
      "loss": 2.8889,
      "step": 4850
    },
    {
      "epoch": 0.29513572599744947,
      "grad_norm": 5.169188976287842,
      "learning_rate": 9.763091294123762e-05,
      "loss": 2.9491,
      "step": 4860
    },
    {
      "epoch": 0.2957430011538228,
      "grad_norm": 3.305724620819092,
      "learning_rate": 9.762123171073383e-05,
      "loss": 2.9127,
      "step": 4870
    },
    {
      "epoch": 0.29635027631019617,
      "grad_norm": 3.5985920429229736,
      "learning_rate": 9.76115312214455e-05,
      "loss": 2.5104,
      "step": 4880
    },
    {
      "epoch": 0.2969575514665695,
      "grad_norm": 2.1707334518432617,
      "learning_rate": 9.760181147729568e-05,
      "loss": 2.7028,
      "step": 4890
    },
    {
      "epoch": 0.29756482662294287,
      "grad_norm": 2.2230172157287598,
      "learning_rate": 9.759207248221516e-05,
      "loss": 2.9218,
      "step": 4900
    },
    {
      "epoch": 0.2981721017793162,
      "grad_norm": 2.9418625831604004,
      "learning_rate": 9.758231424014256e-05,
      "loss": 2.5685,
      "step": 4910
    },
    {
      "epoch": 0.29877937693568957,
      "grad_norm": 3.084195137023926,
      "learning_rate": 9.757253675502426e-05,
      "loss": 2.7473,
      "step": 4920
    },
    {
      "epoch": 0.2993866520920629,
      "grad_norm": 2.7403950691223145,
      "learning_rate": 9.756274003081444e-05,
      "loss": 2.412,
      "step": 4930
    },
    {
      "epoch": 0.29999392724843627,
      "grad_norm": 1.7992706298828125,
      "learning_rate": 9.755292407147505e-05,
      "loss": 2.7033,
      "step": 4940
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 2.1876397132873535,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.6382,
      "step": 4950
    },
    {
      "epoch": 0.30120847756118296,
      "grad_norm": 3.588412284851074,
      "learning_rate": 9.753323446329427e-05,
      "loss": 2.5126,
      "step": 4960
    },
    {
      "epoch": 0.30181575271755634,
      "grad_norm": 2.112494945526123,
      "learning_rate": 9.752336082241564e-05,
      "loss": 2.7804,
      "step": 4970
    },
    {
      "epoch": 0.30242302787392966,
      "grad_norm": 2.2478232383728027,
      "learning_rate": 9.751346796233305e-05,
      "loss": 2.7227,
      "step": 4980
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 2.3947324752807617,
      "learning_rate": 9.750355588704727e-05,
      "loss": 2.5894,
      "step": 4990
    },
    {
      "epoch": 0.30363757818667636,
      "grad_norm": 2.0972561836242676,
      "learning_rate": 9.749362460056694e-05,
      "loss": 2.6187,
      "step": 5000
    },
    {
      "epoch": 0.3036983057023137,
      "eval_loss": 4.65754508972168,
      "eval_runtime": 2394.3048,
      "eval_samples_per_second": 6.878,
      "eval_steps_per_second": 1.719,
      "step": 5001
    },
    {
      "epoch": 0.30424485334304974,
      "grad_norm": 1.952795386314392,
      "learning_rate": 9.748367410690842e-05,
      "loss": 4.0428,
      "step": 5010
    },
    {
      "epoch": 0.3048521284994231,
      "grad_norm": 3.9655306339263916,
      "learning_rate": 9.747370441009584e-05,
      "loss": 3.4581,
      "step": 5020
    },
    {
      "epoch": 0.30545940365579644,
      "grad_norm": 4.936667442321777,
      "learning_rate": 9.746371551416113e-05,
      "loss": 2.8847,
      "step": 5030
    },
    {
      "epoch": 0.3060666788121698,
      "grad_norm": 2.3399593830108643,
      "learning_rate": 9.745370742314393e-05,
      "loss": 2.7816,
      "step": 5040
    },
    {
      "epoch": 0.30667395396854313,
      "grad_norm": 2.794912576675415,
      "learning_rate": 9.744368014109166e-05,
      "loss": 2.6891,
      "step": 5050
    },
    {
      "epoch": 0.3072812291249165,
      "grad_norm": 2.542238712310791,
      "learning_rate": 9.743363367205957e-05,
      "loss": 2.5726,
      "step": 5060
    },
    {
      "epoch": 0.30788850428128983,
      "grad_norm": 2.089010000228882,
      "learning_rate": 9.742356802011054e-05,
      "loss": 2.632,
      "step": 5070
    },
    {
      "epoch": 0.3084957794376632,
      "grad_norm": 3.0076348781585693,
      "learning_rate": 9.741348318931535e-05,
      "loss": 2.4753,
      "step": 5080
    },
    {
      "epoch": 0.30910305459403653,
      "grad_norm": 2.243069648742676,
      "learning_rate": 9.740337918375242e-05,
      "loss": 2.221,
      "step": 5090
    },
    {
      "epoch": 0.3097103297504099,
      "grad_norm": 2.6892781257629395,
      "learning_rate": 9.739325600750797e-05,
      "loss": 3.0242,
      "step": 5100
    },
    {
      "epoch": 0.3103176049067833,
      "grad_norm": 5.9112043380737305,
      "learning_rate": 9.7383113664676e-05,
      "loss": 3.0292,
      "step": 5110
    },
    {
      "epoch": 0.3109248800631566,
      "grad_norm": 2.819478750228882,
      "learning_rate": 9.737295215935822e-05,
      "loss": 2.6084,
      "step": 5120
    },
    {
      "epoch": 0.31153215521953,
      "grad_norm": 2.179046869277954,
      "learning_rate": 9.73627714956641e-05,
      "loss": 2.4918,
      "step": 5130
    },
    {
      "epoch": 0.3121394303759033,
      "grad_norm": 1.5824692249298096,
      "learning_rate": 9.735257167771088e-05,
      "loss": 2.3744,
      "step": 5140
    },
    {
      "epoch": 0.3127467055322767,
      "grad_norm": 1.7063863277435303,
      "learning_rate": 9.73423527096235e-05,
      "loss": 2.4673,
      "step": 5150
    },
    {
      "epoch": 0.31335398068865,
      "grad_norm": 2.7843480110168457,
      "learning_rate": 9.73321145955347e-05,
      "loss": 2.7704,
      "step": 5160
    },
    {
      "epoch": 0.3139612558450234,
      "grad_norm": 2.136676788330078,
      "learning_rate": 9.732185733958493e-05,
      "loss": 2.6841,
      "step": 5170
    },
    {
      "epoch": 0.31456853100139676,
      "grad_norm": 3.3537065982818604,
      "learning_rate": 9.731158094592238e-05,
      "loss": 2.563,
      "step": 5180
    },
    {
      "epoch": 0.3151758061577701,
      "grad_norm": 2.8524134159088135,
      "learning_rate": 9.7301285418703e-05,
      "loss": 2.5413,
      "step": 5190
    },
    {
      "epoch": 0.31578308131414345,
      "grad_norm": 2.492830991744995,
      "learning_rate": 9.729097076209046e-05,
      "loss": 2.5664,
      "step": 5200
    },
    {
      "epoch": 0.3163903564705168,
      "grad_norm": 2.9123501777648926,
      "learning_rate": 9.728063698025616e-05,
      "loss": 2.3671,
      "step": 5210
    },
    {
      "epoch": 0.31699763162689015,
      "grad_norm": 2.315617084503174,
      "learning_rate": 9.727028407737926e-05,
      "loss": 2.4119,
      "step": 5220
    },
    {
      "epoch": 0.3176049067832635,
      "grad_norm": 2.063382625579834,
      "learning_rate": 9.725991205764662e-05,
      "loss": 2.2221,
      "step": 5230
    },
    {
      "epoch": 0.31821218193963685,
      "grad_norm": 2.1030588150024414,
      "learning_rate": 9.724952092525287e-05,
      "loss": 2.5389,
      "step": 5240
    },
    {
      "epoch": 0.31881945709601023,
      "grad_norm": 2.068145275115967,
      "learning_rate": 9.723911068440036e-05,
      "loss": 2.8679,
      "step": 5250
    },
    {
      "epoch": 0.31942673225238355,
      "grad_norm": 2.002915859222412,
      "learning_rate": 9.722868133929912e-05,
      "loss": 2.3099,
      "step": 5260
    },
    {
      "epoch": 0.3200340074087569,
      "grad_norm": 1.9298429489135742,
      "learning_rate": 9.721823289416698e-05,
      "loss": 2.5772,
      "step": 5270
    },
    {
      "epoch": 0.32064128256513025,
      "grad_norm": 2.576030731201172,
      "learning_rate": 9.720776535322943e-05,
      "loss": 2.8001,
      "step": 5280
    },
    {
      "epoch": 0.3212485577215036,
      "grad_norm": 3.1845993995666504,
      "learning_rate": 9.719727872071973e-05,
      "loss": 2.8178,
      "step": 5290
    },
    {
      "epoch": 0.32185583287787695,
      "grad_norm": 2.2578365802764893,
      "learning_rate": 9.718677300087882e-05,
      "loss": 2.3876,
      "step": 5300
    },
    {
      "epoch": 0.3224631080342503,
      "grad_norm": 3.0310754776000977,
      "learning_rate": 9.71762481979554e-05,
      "loss": 2.3081,
      "step": 5310
    },
    {
      "epoch": 0.32307038319062364,
      "grad_norm": 3.4083526134490967,
      "learning_rate": 9.716570431620586e-05,
      "loss": 2.6982,
      "step": 5320
    },
    {
      "epoch": 0.323677658346997,
      "grad_norm": 2.6855759620666504,
      "learning_rate": 9.715514135989432e-05,
      "loss": 3.1095,
      "step": 5330
    },
    {
      "epoch": 0.3242849335033704,
      "grad_norm": 3.7724175453186035,
      "learning_rate": 9.714455933329259e-05,
      "loss": 2.7103,
      "step": 5340
    },
    {
      "epoch": 0.3248922086597437,
      "grad_norm": 1.6243420839309692,
      "learning_rate": 9.713395824068024e-05,
      "loss": 2.3982,
      "step": 5350
    },
    {
      "epoch": 0.3254994838161171,
      "grad_norm": 3.3213744163513184,
      "learning_rate": 9.712333808634448e-05,
      "loss": 2.3912,
      "step": 5360
    },
    {
      "epoch": 0.3261067589724904,
      "grad_norm": 3.3812777996063232,
      "learning_rate": 9.711269887458032e-05,
      "loss": 2.6176,
      "step": 5370
    },
    {
      "epoch": 0.3267140341288638,
      "grad_norm": 2.8287436962127686,
      "learning_rate": 9.710204060969038e-05,
      "loss": 2.7881,
      "step": 5380
    },
    {
      "epoch": 0.3273213092852371,
      "grad_norm": 4.676497936248779,
      "learning_rate": 9.709136329598505e-05,
      "loss": 2.7891,
      "step": 5390
    },
    {
      "epoch": 0.3279285844416105,
      "grad_norm": 2.073455572128296,
      "learning_rate": 9.708066693778241e-05,
      "loss": 2.2933,
      "step": 5400
    },
    {
      "epoch": 0.32853585959798387,
      "grad_norm": 2.725923538208008,
      "learning_rate": 9.706995153940825e-05,
      "loss": 2.628,
      "step": 5410
    },
    {
      "epoch": 0.3291431347543572,
      "grad_norm": 2.788832187652588,
      "learning_rate": 9.705921710519602e-05,
      "loss": 2.5013,
      "step": 5420
    },
    {
      "epoch": 0.32975040991073057,
      "grad_norm": 3.5404584407806396,
      "learning_rate": 9.704846363948692e-05,
      "loss": 2.6669,
      "step": 5430
    },
    {
      "epoch": 0.3303576850671039,
      "grad_norm": 2.4285194873809814,
      "learning_rate": 9.70376911466298e-05,
      "loss": 2.2676,
      "step": 5440
    },
    {
      "epoch": 0.33096496022347727,
      "grad_norm": 1.98625910282135,
      "learning_rate": 9.702689963098124e-05,
      "loss": 2.4375,
      "step": 5450
    },
    {
      "epoch": 0.3315722353798506,
      "grad_norm": 2.273041248321533,
      "learning_rate": 9.701608909690549e-05,
      "loss": 2.7615,
      "step": 5460
    },
    {
      "epoch": 0.33217951053622397,
      "grad_norm": 3.0551483631134033,
      "learning_rate": 9.700525954877453e-05,
      "loss": 2.5322,
      "step": 5470
    },
    {
      "epoch": 0.33278678569259734,
      "grad_norm": 1.9035460948944092,
      "learning_rate": 9.699441099096797e-05,
      "loss": 2.6966,
      "step": 5480
    },
    {
      "epoch": 0.33339406084897066,
      "grad_norm": 3.1844379901885986,
      "learning_rate": 9.698354342787317e-05,
      "loss": 2.5792,
      "step": 5490
    },
    {
      "epoch": 0.33400133600534404,
      "grad_norm": 2.7493135929107666,
      "learning_rate": 9.697265686388512e-05,
      "loss": 3.1563,
      "step": 5500
    },
    {
      "epoch": 0.33460861116171736,
      "grad_norm": 4.200898170471191,
      "learning_rate": 9.696175130340653e-05,
      "loss": 2.7244,
      "step": 5510
    },
    {
      "epoch": 0.33521588631809074,
      "grad_norm": 3.4090373516082764,
      "learning_rate": 9.695082675084778e-05,
      "loss": 2.6671,
      "step": 5520
    },
    {
      "epoch": 0.33582316147446406,
      "grad_norm": 4.021493434906006,
      "learning_rate": 9.693988321062692e-05,
      "loss": 2.809,
      "step": 5530
    },
    {
      "epoch": 0.33643043663083744,
      "grad_norm": 2.8112053871154785,
      "learning_rate": 9.692892068716974e-05,
      "loss": 3.1483,
      "step": 5540
    },
    {
      "epoch": 0.33703771178721076,
      "grad_norm": 3.90509295463562,
      "learning_rate": 9.69179391849096e-05,
      "loss": 2.617,
      "step": 5550
    },
    {
      "epoch": 0.33764498694358414,
      "grad_norm": 2.5465035438537598,
      "learning_rate": 9.690693870828763e-05,
      "loss": 2.735,
      "step": 5560
    },
    {
      "epoch": 0.3382522620999575,
      "grad_norm": 2.8536598682403564,
      "learning_rate": 9.689591926175257e-05,
      "loss": 2.8422,
      "step": 5570
    },
    {
      "epoch": 0.33885953725633083,
      "grad_norm": 3.5907583236694336,
      "learning_rate": 9.68848808497609e-05,
      "loss": 2.3543,
      "step": 5580
    },
    {
      "epoch": 0.3394668124127042,
      "grad_norm": 1.7455558776855469,
      "learning_rate": 9.68738234767767e-05,
      "loss": 2.5034,
      "step": 5590
    },
    {
      "epoch": 0.34007408756907753,
      "grad_norm": 3.4365475177764893,
      "learning_rate": 9.686274714727175e-05,
      "loss": 2.388,
      "step": 5600
    },
    {
      "epoch": 0.3406813627254509,
      "grad_norm": 3.4610817432403564,
      "learning_rate": 9.68516518657255e-05,
      "loss": 2.9911,
      "step": 5610
    },
    {
      "epoch": 0.34128863788182423,
      "grad_norm": 2.9483425617218018,
      "learning_rate": 9.684053763662507e-05,
      "loss": 3.0189,
      "step": 5620
    },
    {
      "epoch": 0.3418959130381976,
      "grad_norm": 4.086541175842285,
      "learning_rate": 9.68294044644652e-05,
      "loss": 2.9267,
      "step": 5630
    },
    {
      "epoch": 0.342503188194571,
      "grad_norm": 3.5564351081848145,
      "learning_rate": 9.681825235374835e-05,
      "loss": 2.735,
      "step": 5640
    },
    {
      "epoch": 0.3431104633509443,
      "grad_norm": 2.5905704498291016,
      "learning_rate": 9.68070813089846e-05,
      "loss": 2.976,
      "step": 5650
    },
    {
      "epoch": 0.3437177385073177,
      "grad_norm": 3.4880199432373047,
      "learning_rate": 9.67958913346917e-05,
      "loss": 2.5428,
      "step": 5660
    },
    {
      "epoch": 0.344325013663691,
      "grad_norm": 2.2489869594573975,
      "learning_rate": 9.678468243539505e-05,
      "loss": 2.2887,
      "step": 5670
    },
    {
      "epoch": 0.3449322888200644,
      "grad_norm": 1.8352601528167725,
      "learning_rate": 9.677345461562773e-05,
      "loss": 2.5947,
      "step": 5680
    },
    {
      "epoch": 0.3455395639764377,
      "grad_norm": 1.9699002504348755,
      "learning_rate": 9.67622078799304e-05,
      "loss": 2.2721,
      "step": 5690
    },
    {
      "epoch": 0.3461468391328111,
      "grad_norm": 1.9468005895614624,
      "learning_rate": 9.675094223285147e-05,
      "loss": 2.1432,
      "step": 5700
    },
    {
      "epoch": 0.34675411428918446,
      "grad_norm": 1.7849065065383911,
      "learning_rate": 9.673965767894693e-05,
      "loss": 2.4846,
      "step": 5710
    },
    {
      "epoch": 0.3473613894455578,
      "grad_norm": 2.2585389614105225,
      "learning_rate": 9.672835422278042e-05,
      "loss": 2.7742,
      "step": 5720
    },
    {
      "epoch": 0.34796866460193115,
      "grad_norm": 2.461836576461792,
      "learning_rate": 9.671703186892325e-05,
      "loss": 2.3772,
      "step": 5730
    },
    {
      "epoch": 0.3485759397583045,
      "grad_norm": 1.7330503463745117,
      "learning_rate": 9.670569062195436e-05,
      "loss": 2.5005,
      "step": 5740
    },
    {
      "epoch": 0.34918321491467785,
      "grad_norm": 2.814467191696167,
      "learning_rate": 9.669433048646032e-05,
      "loss": 2.4543,
      "step": 5750
    },
    {
      "epoch": 0.3497904900710512,
      "grad_norm": 2.3687150478363037,
      "learning_rate": 9.668295146703537e-05,
      "loss": 2.7108,
      "step": 5760
    },
    {
      "epoch": 0.35039776522742455,
      "grad_norm": 3.235748052597046,
      "learning_rate": 9.667155356828135e-05,
      "loss": 2.4751,
      "step": 5770
    },
    {
      "epoch": 0.3510050403837979,
      "grad_norm": 2.880159854888916,
      "learning_rate": 9.666013679480777e-05,
      "loss": 2.6458,
      "step": 5780
    },
    {
      "epoch": 0.35161231554017125,
      "grad_norm": 2.7512199878692627,
      "learning_rate": 9.664870115123172e-05,
      "loss": 2.7487,
      "step": 5790
    },
    {
      "epoch": 0.3522195906965446,
      "grad_norm": 2.948134660720825,
      "learning_rate": 9.6637246642178e-05,
      "loss": 2.3833,
      "step": 5800
    },
    {
      "epoch": 0.35282686585291795,
      "grad_norm": 2.062732696533203,
      "learning_rate": 9.662577327227896e-05,
      "loss": 2.4633,
      "step": 5810
    },
    {
      "epoch": 0.3534341410092913,
      "grad_norm": 4.2675395011901855,
      "learning_rate": 9.661428104617463e-05,
      "loss": 2.6464,
      "step": 5820
    },
    {
      "epoch": 0.35404141616566465,
      "grad_norm": 3.093010663986206,
      "learning_rate": 9.660276996851265e-05,
      "loss": 2.3541,
      "step": 5830
    },
    {
      "epoch": 0.354648691322038,
      "grad_norm": 3.0793302059173584,
      "learning_rate": 9.659124004394828e-05,
      "loss": 2.3254,
      "step": 5840
    },
    {
      "epoch": 0.35525596647841134,
      "grad_norm": 1.8483396768569946,
      "learning_rate": 9.657969127714441e-05,
      "loss": 2.2173,
      "step": 5850
    },
    {
      "epoch": 0.3558632416347847,
      "grad_norm": 1.734941005706787,
      "learning_rate": 9.656812367277154e-05,
      "loss": 2.0666,
      "step": 5860
    },
    {
      "epoch": 0.3564705167911581,
      "grad_norm": 2.4158875942230225,
      "learning_rate": 9.655653723550779e-05,
      "loss": 2.4716,
      "step": 5870
    },
    {
      "epoch": 0.3570777919475314,
      "grad_norm": 3.6479389667510986,
      "learning_rate": 9.65449319700389e-05,
      "loss": 2.7466,
      "step": 5880
    },
    {
      "epoch": 0.3576850671039048,
      "grad_norm": 3.620398759841919,
      "learning_rate": 9.653330788105823e-05,
      "loss": 2.2851,
      "step": 5890
    },
    {
      "epoch": 0.3582923422602781,
      "grad_norm": 2.0576295852661133,
      "learning_rate": 9.652166497326675e-05,
      "loss": 2.3337,
      "step": 5900
    },
    {
      "epoch": 0.3588996174166515,
      "grad_norm": 3.9837324619293213,
      "learning_rate": 9.651000325137304e-05,
      "loss": 2.7013,
      "step": 5910
    },
    {
      "epoch": 0.3595068925730248,
      "grad_norm": 2.426441192626953,
      "learning_rate": 9.649832272009327e-05,
      "loss": 2.6629,
      "step": 5920
    },
    {
      "epoch": 0.3601141677293982,
      "grad_norm": 1.9288657903671265,
      "learning_rate": 9.648662338415124e-05,
      "loss": 2.7753,
      "step": 5930
    },
    {
      "epoch": 0.36072144288577157,
      "grad_norm": 3.571359395980835,
      "learning_rate": 9.647490524827834e-05,
      "loss": 2.8653,
      "step": 5940
    },
    {
      "epoch": 0.3613287180421449,
      "grad_norm": 4.056645393371582,
      "learning_rate": 9.646316831721359e-05,
      "loss": 2.7246,
      "step": 5950
    },
    {
      "epoch": 0.36193599319851827,
      "grad_norm": 4.044543743133545,
      "learning_rate": 9.645141259570358e-05,
      "loss": 2.9822,
      "step": 5960
    },
    {
      "epoch": 0.3625432683548916,
      "grad_norm": 1.899647831916809,
      "learning_rate": 9.643963808850252e-05,
      "loss": 2.6269,
      "step": 5970
    },
    {
      "epoch": 0.36315054351126497,
      "grad_norm": 2.770113706588745,
      "learning_rate": 9.642784480037218e-05,
      "loss": 2.5157,
      "step": 5980
    },
    {
      "epoch": 0.3637578186676383,
      "grad_norm": 3.5850327014923096,
      "learning_rate": 9.6416032736082e-05,
      "loss": 2.3941,
      "step": 5990
    },
    {
      "epoch": 0.36436509382401167,
      "grad_norm": 2.645689010620117,
      "learning_rate": 9.640420190040893e-05,
      "loss": 2.4616,
      "step": 6000
    },
    {
      "epoch": 0.364972368980385,
      "grad_norm": 3.212184429168701,
      "learning_rate": 9.639235229813754e-05,
      "loss": 2.4048,
      "step": 6010
    },
    {
      "epoch": 0.36557964413675836,
      "grad_norm": 2.152174234390259,
      "learning_rate": 9.638048393406006e-05,
      "loss": 2.3119,
      "step": 6020
    },
    {
      "epoch": 0.36618691929313174,
      "grad_norm": 2.0382027626037598,
      "learning_rate": 9.636859681297616e-05,
      "loss": 2.4328,
      "step": 6030
    },
    {
      "epoch": 0.36679419444950506,
      "grad_norm": 1.8242830038070679,
      "learning_rate": 9.635669093969323e-05,
      "loss": 2.4276,
      "step": 6040
    },
    {
      "epoch": 0.36740146960587844,
      "grad_norm": 3.223503589630127,
      "learning_rate": 9.634476631902623e-05,
      "loss": 2.3513,
      "step": 6050
    },
    {
      "epoch": 0.36800874476225176,
      "grad_norm": 2.917818069458008,
      "learning_rate": 9.633282295579758e-05,
      "loss": 2.6678,
      "step": 6060
    },
    {
      "epoch": 0.36861601991862514,
      "grad_norm": 2.7738428115844727,
      "learning_rate": 9.632086085483742e-05,
      "loss": 2.6119,
      "step": 6070
    },
    {
      "epoch": 0.36922329507499846,
      "grad_norm": 3.361091375350952,
      "learning_rate": 9.630888002098343e-05,
      "loss": 2.7862,
      "step": 6080
    },
    {
      "epoch": 0.36983057023137184,
      "grad_norm": 2.45442795753479,
      "learning_rate": 9.629688045908081e-05,
      "loss": 2.9131,
      "step": 6090
    },
    {
      "epoch": 0.3704378453877452,
      "grad_norm": 3.6359691619873047,
      "learning_rate": 9.628486217398238e-05,
      "loss": 2.8073,
      "step": 6100
    },
    {
      "epoch": 0.37104512054411853,
      "grad_norm": 3.5657355785369873,
      "learning_rate": 9.627282517054854e-05,
      "loss": 2.4888,
      "step": 6110
    },
    {
      "epoch": 0.3716523957004919,
      "grad_norm": 2.302048683166504,
      "learning_rate": 9.626076945364726e-05,
      "loss": 2.4152,
      "step": 6120
    },
    {
      "epoch": 0.37225967085686523,
      "grad_norm": 3.246180295944214,
      "learning_rate": 9.624869502815404e-05,
      "loss": 2.3139,
      "step": 6130
    },
    {
      "epoch": 0.3728669460132386,
      "grad_norm": 2.1596319675445557,
      "learning_rate": 9.623660189895196e-05,
      "loss": 2.3836,
      "step": 6140
    },
    {
      "epoch": 0.37347422116961193,
      "grad_norm": 2.8426425457000732,
      "learning_rate": 9.622449007093169e-05,
      "loss": 2.1422,
      "step": 6150
    },
    {
      "epoch": 0.3740814963259853,
      "grad_norm": 2.9144046306610107,
      "learning_rate": 9.621235954899146e-05,
      "loss": 2.4923,
      "step": 6160
    },
    {
      "epoch": 0.37468877148235863,
      "grad_norm": 2.1653025150299072,
      "learning_rate": 9.620021033803701e-05,
      "loss": 2.3489,
      "step": 6170
    },
    {
      "epoch": 0.375296046638732,
      "grad_norm": 1.9786434173583984,
      "learning_rate": 9.618804244298171e-05,
      "loss": 2.7954,
      "step": 6180
    },
    {
      "epoch": 0.3759033217951054,
      "grad_norm": 1.9633774757385254,
      "learning_rate": 9.617585586874645e-05,
      "loss": 3.0363,
      "step": 6190
    },
    {
      "epoch": 0.3765105969514787,
      "grad_norm": 3.5665221214294434,
      "learning_rate": 9.616365062025965e-05,
      "loss": 2.6774,
      "step": 6200
    },
    {
      "epoch": 0.3771178721078521,
      "grad_norm": 3.683419704437256,
      "learning_rate": 9.615142670245731e-05,
      "loss": 2.6537,
      "step": 6210
    },
    {
      "epoch": 0.3777251472642254,
      "grad_norm": 3.5384700298309326,
      "learning_rate": 9.6139184120283e-05,
      "loss": 2.3428,
      "step": 6220
    },
    {
      "epoch": 0.3783324224205988,
      "grad_norm": 2.1741151809692383,
      "learning_rate": 9.612692287868778e-05,
      "loss": 2.6709,
      "step": 6230
    },
    {
      "epoch": 0.3789396975769721,
      "grad_norm": 4.12810754776001,
      "learning_rate": 9.611464298263034e-05,
      "loss": 2.8755,
      "step": 6240
    },
    {
      "epoch": 0.3795469727333455,
      "grad_norm": 3.328441858291626,
      "learning_rate": 9.610234443707682e-05,
      "loss": 2.5798,
      "step": 6250
    },
    {
      "epoch": 0.38015424788971885,
      "grad_norm": 2.308426856994629,
      "learning_rate": 9.609002724700097e-05,
      "loss": 2.4755,
      "step": 6260
    },
    {
      "epoch": 0.3807615230460922,
      "grad_norm": 3.780794858932495,
      "learning_rate": 9.607769141738405e-05,
      "loss": 2.9129,
      "step": 6270
    },
    {
      "epoch": 0.38136879820246555,
      "grad_norm": 2.2972891330718994,
      "learning_rate": 9.606533695321486e-05,
      "loss": 2.2802,
      "step": 6280
    },
    {
      "epoch": 0.3819760733588389,
      "grad_norm": 1.5943816900253296,
      "learning_rate": 9.605296385948976e-05,
      "loss": 2.2624,
      "step": 6290
    },
    {
      "epoch": 0.38258334851521225,
      "grad_norm": 1.7775726318359375,
      "learning_rate": 9.604057214121263e-05,
      "loss": 2.6631,
      "step": 6300
    },
    {
      "epoch": 0.3831906236715856,
      "grad_norm": 2.653024196624756,
      "learning_rate": 9.602816180339484e-05,
      "loss": 2.7701,
      "step": 6310
    },
    {
      "epoch": 0.38379789882795895,
      "grad_norm": 3.1426491737365723,
      "learning_rate": 9.601573285105539e-05,
      "loss": 2.6401,
      "step": 6320
    },
    {
      "epoch": 0.3844051739843323,
      "grad_norm": 2.406773805618286,
      "learning_rate": 9.600328528922068e-05,
      "loss": 2.3394,
      "step": 6330
    },
    {
      "epoch": 0.38501244914070565,
      "grad_norm": 1.4471969604492188,
      "learning_rate": 9.599081912292473e-05,
      "loss": 2.3784,
      "step": 6340
    },
    {
      "epoch": 0.385619724297079,
      "grad_norm": 2.186854124069214,
      "learning_rate": 9.597833435720908e-05,
      "loss": 2.5582,
      "step": 6350
    },
    {
      "epoch": 0.38622699945345235,
      "grad_norm": 3.2903380393981934,
      "learning_rate": 9.596583099712272e-05,
      "loss": 2.8977,
      "step": 6360
    },
    {
      "epoch": 0.3868342746098257,
      "grad_norm": 2.983762264251709,
      "learning_rate": 9.595330904772226e-05,
      "loss": 2.5649,
      "step": 6370
    },
    {
      "epoch": 0.38744154976619904,
      "grad_norm": 2.9107296466827393,
      "learning_rate": 9.594076851407175e-05,
      "loss": 2.6033,
      "step": 6380
    },
    {
      "epoch": 0.3880488249225724,
      "grad_norm": 3.713864803314209,
      "learning_rate": 9.592820940124276e-05,
      "loss": 2.5969,
      "step": 6390
    },
    {
      "epoch": 0.38865610007894574,
      "grad_norm": 2.0372941493988037,
      "learning_rate": 9.591563171431444e-05,
      "loss": 2.5867,
      "step": 6400
    },
    {
      "epoch": 0.3892633752353191,
      "grad_norm": 2.4364500045776367,
      "learning_rate": 9.590303545837337e-05,
      "loss": 2.3277,
      "step": 6410
    },
    {
      "epoch": 0.3898706503916925,
      "grad_norm": 2.3127830028533936,
      "learning_rate": 9.58904206385137e-05,
      "loss": 2.8594,
      "step": 6420
    },
    {
      "epoch": 0.3904779255480658,
      "grad_norm": 5.502495288848877,
      "learning_rate": 9.587778725983705e-05,
      "loss": 2.3264,
      "step": 6430
    },
    {
      "epoch": 0.3910852007044392,
      "grad_norm": 1.8831835985183716,
      "learning_rate": 9.586513532745256e-05,
      "loss": 2.4031,
      "step": 6440
    },
    {
      "epoch": 0.3916924758608125,
      "grad_norm": 2.8312697410583496,
      "learning_rate": 9.585246484647688e-05,
      "loss": 2.5702,
      "step": 6450
    },
    {
      "epoch": 0.3922997510171859,
      "grad_norm": 2.2462260723114014,
      "learning_rate": 9.583977582203415e-05,
      "loss": 2.4924,
      "step": 6460
    },
    {
      "epoch": 0.3929070261735592,
      "grad_norm": 2.6554434299468994,
      "learning_rate": 9.582706825925601e-05,
      "loss": 2.9364,
      "step": 6470
    },
    {
      "epoch": 0.3935143013299326,
      "grad_norm": 4.090837478637695,
      "learning_rate": 9.581434216328162e-05,
      "loss": 2.7923,
      "step": 6480
    },
    {
      "epoch": 0.39412157648630597,
      "grad_norm": 5.550185680389404,
      "learning_rate": 9.580159753925759e-05,
      "loss": 2.592,
      "step": 6490
    },
    {
      "epoch": 0.3947288516426793,
      "grad_norm": 3.940237283706665,
      "learning_rate": 9.578883439233807e-05,
      "loss": 2.4947,
      "step": 6500
    },
    {
      "epoch": 0.39533612679905267,
      "grad_norm": 6.1874799728393555,
      "learning_rate": 9.577605272768466e-05,
      "loss": 2.8036,
      "step": 6510
    },
    {
      "epoch": 0.395943401955426,
      "grad_norm": 3.4556233882904053,
      "learning_rate": 9.576325255046649e-05,
      "loss": 2.6134,
      "step": 6520
    },
    {
      "epoch": 0.39655067711179937,
      "grad_norm": 1.9538158178329468,
      "learning_rate": 9.575043386586013e-05,
      "loss": 2.8077,
      "step": 6530
    },
    {
      "epoch": 0.3971579522681727,
      "grad_norm": 3.221627950668335,
      "learning_rate": 9.573759667904968e-05,
      "loss": 2.511,
      "step": 6540
    },
    {
      "epoch": 0.39776522742454606,
      "grad_norm": 1.9716154336929321,
      "learning_rate": 9.57247409952267e-05,
      "loss": 2.7303,
      "step": 6550
    },
    {
      "epoch": 0.39837250258091944,
      "grad_norm": 5.112796306610107,
      "learning_rate": 9.571186681959023e-05,
      "loss": 2.8032,
      "step": 6560
    },
    {
      "epoch": 0.39897977773729276,
      "grad_norm": 2.2915725708007812,
      "learning_rate": 9.569897415734681e-05,
      "loss": 2.2758,
      "step": 6570
    },
    {
      "epoch": 0.39958705289366614,
      "grad_norm": 2.2393112182617188,
      "learning_rate": 9.56860630137104e-05,
      "loss": 2.2547,
      "step": 6580
    },
    {
      "epoch": 0.40019432805003946,
      "grad_norm": 2.2078495025634766,
      "learning_rate": 9.567313339390251e-05,
      "loss": 2.6223,
      "step": 6590
    },
    {
      "epoch": 0.40080160320641284,
      "grad_norm": 2.9799387454986572,
      "learning_rate": 9.566018530315204e-05,
      "loss": 2.4083,
      "step": 6600
    },
    {
      "epoch": 0.40140887836278616,
      "grad_norm": 3.1604864597320557,
      "learning_rate": 9.564721874669545e-05,
      "loss": 2.6593,
      "step": 6610
    },
    {
      "epoch": 0.40201615351915954,
      "grad_norm": 3.535609483718872,
      "learning_rate": 9.563423372977661e-05,
      "loss": 2.569,
      "step": 6620
    },
    {
      "epoch": 0.40262342867553286,
      "grad_norm": 3.2493913173675537,
      "learning_rate": 9.562123025764684e-05,
      "loss": 2.4588,
      "step": 6630
    },
    {
      "epoch": 0.40323070383190623,
      "grad_norm": 3.36810564994812,
      "learning_rate": 9.560820833556498e-05,
      "loss": 2.6289,
      "step": 6640
    },
    {
      "epoch": 0.4038379789882796,
      "grad_norm": 4.481655120849609,
      "learning_rate": 9.55951679687973e-05,
      "loss": 2.8437,
      "step": 6650
    },
    {
      "epoch": 0.40444525414465293,
      "grad_norm": 2.7945375442504883,
      "learning_rate": 9.558210916261751e-05,
      "loss": 2.5957,
      "step": 6660
    },
    {
      "epoch": 0.4050525293010263,
      "grad_norm": 3.196676015853882,
      "learning_rate": 9.556903192230684e-05,
      "loss": 2.4798,
      "step": 6670
    },
    {
      "epoch": 0.40565980445739963,
      "grad_norm": 3.2260711193084717,
      "learning_rate": 9.55559362531539e-05,
      "loss": 2.502,
      "step": 6680
    },
    {
      "epoch": 0.406267079613773,
      "grad_norm": 2.8825058937072754,
      "learning_rate": 9.55428221604548e-05,
      "loss": 2.69,
      "step": 6690
    },
    {
      "epoch": 0.40687435477014633,
      "grad_norm": 2.5269992351531982,
      "learning_rate": 9.552968964951307e-05,
      "loss": 2.5891,
      "step": 6700
    },
    {
      "epoch": 0.4074816299265197,
      "grad_norm": 2.4735569953918457,
      "learning_rate": 9.551653872563975e-05,
      "loss": 2.9437,
      "step": 6710
    },
    {
      "epoch": 0.4080889050828931,
      "grad_norm": 3.534066915512085,
      "learning_rate": 9.550336939415323e-05,
      "loss": 2.766,
      "step": 6720
    },
    {
      "epoch": 0.4086961802392664,
      "grad_norm": 2.8585851192474365,
      "learning_rate": 9.549018166037943e-05,
      "loss": 2.4819,
      "step": 6730
    },
    {
      "epoch": 0.4093034553956398,
      "grad_norm": 2.6258301734924316,
      "learning_rate": 9.547697552965167e-05,
      "loss": 2.4516,
      "step": 6740
    },
    {
      "epoch": 0.4099107305520131,
      "grad_norm": 2.4010605812072754,
      "learning_rate": 9.546375100731073e-05,
      "loss": 2.6557,
      "step": 6750
    },
    {
      "epoch": 0.4105180057083865,
      "grad_norm": 2.8973047733306885,
      "learning_rate": 9.54505080987048e-05,
      "loss": 2.6761,
      "step": 6760
    },
    {
      "epoch": 0.4111252808647598,
      "grad_norm": 3.275704860687256,
      "learning_rate": 9.543724680918953e-05,
      "loss": 2.372,
      "step": 6770
    },
    {
      "epoch": 0.4117325560211332,
      "grad_norm": 1.9340436458587646,
      "learning_rate": 9.5423967144128e-05,
      "loss": 2.3927,
      "step": 6780
    },
    {
      "epoch": 0.41233983117750656,
      "grad_norm": 1.4836126565933228,
      "learning_rate": 9.541066910889071e-05,
      "loss": 2.5071,
      "step": 6790
    },
    {
      "epoch": 0.4129471063338799,
      "grad_norm": 2.805004596710205,
      "learning_rate": 9.539735270885562e-05,
      "loss": 2.4898,
      "step": 6800
    },
    {
      "epoch": 0.41355438149025325,
      "grad_norm": 2.3302114009857178,
      "learning_rate": 9.538401794940808e-05,
      "loss": 2.2788,
      "step": 6810
    },
    {
      "epoch": 0.4141616566466266,
      "grad_norm": 2.43017578125,
      "learning_rate": 9.537066483594086e-05,
      "loss": 2.2734,
      "step": 6820
    },
    {
      "epoch": 0.41476893180299995,
      "grad_norm": 1.5651804208755493,
      "learning_rate": 9.53572933738542e-05,
      "loss": 2.1212,
      "step": 6830
    },
    {
      "epoch": 0.4153762069593733,
      "grad_norm": 1.97849440574646,
      "learning_rate": 9.534390356855571e-05,
      "loss": 2.207,
      "step": 6840
    },
    {
      "epoch": 0.41598348211574665,
      "grad_norm": 2.558380126953125,
      "learning_rate": 9.533049542546046e-05,
      "loss": 2.4412,
      "step": 6850
    },
    {
      "epoch": 0.41659075727211997,
      "grad_norm": 1.959463119506836,
      "learning_rate": 9.531706894999091e-05,
      "loss": 2.3049,
      "step": 6860
    },
    {
      "epoch": 0.41719803242849335,
      "grad_norm": 1.793039321899414,
      "learning_rate": 9.530362414757694e-05,
      "loss": 2.329,
      "step": 6870
    },
    {
      "epoch": 0.4178053075848667,
      "grad_norm": 2.201921224594116,
      "learning_rate": 9.529016102365584e-05,
      "loss": 2.5112,
      "step": 6880
    },
    {
      "epoch": 0.41841258274124005,
      "grad_norm": 2.999833106994629,
      "learning_rate": 9.52766795836723e-05,
      "loss": 2.2657,
      "step": 6890
    },
    {
      "epoch": 0.4190198578976134,
      "grad_norm": 1.904390573501587,
      "learning_rate": 9.526317983307847e-05,
      "loss": 2.5569,
      "step": 6900
    },
    {
      "epoch": 0.41962713305398675,
      "grad_norm": 3.3410770893096924,
      "learning_rate": 9.524966177733382e-05,
      "loss": 2.524,
      "step": 6910
    },
    {
      "epoch": 0.4202344082103601,
      "grad_norm": 2.7304599285125732,
      "learning_rate": 9.523612542190528e-05,
      "loss": 2.7249,
      "step": 6920
    },
    {
      "epoch": 0.42084168336673344,
      "grad_norm": 3.790069103240967,
      "learning_rate": 9.522257077226717e-05,
      "loss": 2.128,
      "step": 6930
    },
    {
      "epoch": 0.4214489585231068,
      "grad_norm": 2.271986961364746,
      "learning_rate": 9.52089978339012e-05,
      "loss": 2.5377,
      "step": 6940
    },
    {
      "epoch": 0.4220562336794802,
      "grad_norm": 3.3071982860565186,
      "learning_rate": 9.519540661229651e-05,
      "loss": 2.6311,
      "step": 6950
    },
    {
      "epoch": 0.4226635088358535,
      "grad_norm": 4.1074724197387695,
      "learning_rate": 9.518179711294956e-05,
      "loss": 2.6324,
      "step": 6960
    },
    {
      "epoch": 0.4232707839922269,
      "grad_norm": 2.819727659225464,
      "learning_rate": 9.51681693413643e-05,
      "loss": 2.3383,
      "step": 6970
    },
    {
      "epoch": 0.4238780591486002,
      "grad_norm": 3.6407546997070312,
      "learning_rate": 9.515452330305198e-05,
      "loss": 2.4282,
      "step": 6980
    },
    {
      "epoch": 0.4244853343049736,
      "grad_norm": 3.0504987239837646,
      "learning_rate": 9.514085900353128e-05,
      "loss": 3.1475,
      "step": 6990
    },
    {
      "epoch": 0.4250926094613469,
      "grad_norm": 5.783546447753906,
      "learning_rate": 9.512717644832828e-05,
      "loss": 2.607,
      "step": 7000
    },
    {
      "epoch": 0.4256998846177203,
      "grad_norm": 3.1271045207977295,
      "learning_rate": 9.511347564297642e-05,
      "loss": 2.8311,
      "step": 7010
    },
    {
      "epoch": 0.42630715977409367,
      "grad_norm": 1.9040294885635376,
      "learning_rate": 9.509975659301649e-05,
      "loss": 2.7186,
      "step": 7020
    },
    {
      "epoch": 0.426914434930467,
      "grad_norm": 1.7764972448349,
      "learning_rate": 9.508601930399673e-05,
      "loss": 2.3434,
      "step": 7030
    },
    {
      "epoch": 0.42752171008684037,
      "grad_norm": 1.4445196390151978,
      "learning_rate": 9.50722637814727e-05,
      "loss": 2.0834,
      "step": 7040
    },
    {
      "epoch": 0.4281289852432137,
      "grad_norm": 1.4342163801193237,
      "learning_rate": 9.505849003100736e-05,
      "loss": 2.2241,
      "step": 7050
    },
    {
      "epoch": 0.42873626039958707,
      "grad_norm": 3.122532367706299,
      "learning_rate": 9.504469805817105e-05,
      "loss": 3.0578,
      "step": 7060
    },
    {
      "epoch": 0.4293435355559604,
      "grad_norm": 4.518110275268555,
      "learning_rate": 9.503088786854141e-05,
      "loss": 2.7352,
      "step": 7070
    },
    {
      "epoch": 0.42995081071233376,
      "grad_norm": 3.5267205238342285,
      "learning_rate": 9.501705946770356e-05,
      "loss": 2.629,
      "step": 7080
    },
    {
      "epoch": 0.4305580858687071,
      "grad_norm": 1.3078187704086304,
      "learning_rate": 9.50032128612499e-05,
      "loss": 2.3815,
      "step": 7090
    },
    {
      "epoch": 0.43116536102508046,
      "grad_norm": 2.044771909713745,
      "learning_rate": 9.498934805478021e-05,
      "loss": 2.4246,
      "step": 7100
    },
    {
      "epoch": 0.43177263618145384,
      "grad_norm": 3.038041591644287,
      "learning_rate": 9.497546505390167e-05,
      "loss": 2.7609,
      "step": 7110
    },
    {
      "epoch": 0.43237991133782716,
      "grad_norm": 2.6125526428222656,
      "learning_rate": 9.496156386422875e-05,
      "loss": 2.6718,
      "step": 7120
    },
    {
      "epoch": 0.43298718649420054,
      "grad_norm": 2.3341405391693115,
      "learning_rate": 9.494764449138331e-05,
      "loss": 2.3215,
      "step": 7130
    },
    {
      "epoch": 0.43359446165057386,
      "grad_norm": 2.62197208404541,
      "learning_rate": 9.493370694099462e-05,
      "loss": 2.4163,
      "step": 7140
    },
    {
      "epoch": 0.43420173680694724,
      "grad_norm": 2.6565566062927246,
      "learning_rate": 9.491975121869919e-05,
      "loss": 2.5138,
      "step": 7150
    },
    {
      "epoch": 0.43480901196332056,
      "grad_norm": 5.283926010131836,
      "learning_rate": 9.490577733014094e-05,
      "loss": 2.3474,
      "step": 7160
    },
    {
      "epoch": 0.43541628711969393,
      "grad_norm": 2.083625316619873,
      "learning_rate": 9.489178528097117e-05,
      "loss": 2.372,
      "step": 7170
    },
    {
      "epoch": 0.4360235622760673,
      "grad_norm": 3.0178773403167725,
      "learning_rate": 9.487777507684848e-05,
      "loss": 3.1013,
      "step": 7180
    },
    {
      "epoch": 0.43663083743244063,
      "grad_norm": 3.7908904552459717,
      "learning_rate": 9.486374672343878e-05,
      "loss": 2.7359,
      "step": 7190
    },
    {
      "epoch": 0.437238112588814,
      "grad_norm": 2.7196524143218994,
      "learning_rate": 9.484970022641541e-05,
      "loss": 2.4006,
      "step": 7200
    },
    {
      "epoch": 0.43784538774518733,
      "grad_norm": 4.560878753662109,
      "learning_rate": 9.483563559145898e-05,
      "loss": 2.6072,
      "step": 7210
    },
    {
      "epoch": 0.4384526629015607,
      "grad_norm": 3.3567259311676025,
      "learning_rate": 9.482155282425742e-05,
      "loss": 2.4336,
      "step": 7220
    },
    {
      "epoch": 0.43905993805793403,
      "grad_norm": 2.9329910278320312,
      "learning_rate": 9.480745193050607e-05,
      "loss": 2.641,
      "step": 7230
    },
    {
      "epoch": 0.4396672132143074,
      "grad_norm": 3.1050922870635986,
      "learning_rate": 9.479333291590753e-05,
      "loss": 3.2285,
      "step": 7240
    },
    {
      "epoch": 0.4402744883706807,
      "grad_norm": 5.255746841430664,
      "learning_rate": 9.477919578617176e-05,
      "loss": 2.5787,
      "step": 7250
    },
    {
      "epoch": 0.4408817635270541,
      "grad_norm": 3.5249173641204834,
      "learning_rate": 9.476504054701605e-05,
      "loss": 2.263,
      "step": 7260
    },
    {
      "epoch": 0.4414890386834275,
      "grad_norm": 1.4400535821914673,
      "learning_rate": 9.475086720416499e-05,
      "loss": 2.3638,
      "step": 7270
    },
    {
      "epoch": 0.4420963138398008,
      "grad_norm": 5.359386444091797,
      "learning_rate": 9.473667576335052e-05,
      "loss": 2.3326,
      "step": 7280
    },
    {
      "epoch": 0.4427035889961742,
      "grad_norm": 4.084681987762451,
      "learning_rate": 9.472246623031186e-05,
      "loss": 2.2995,
      "step": 7290
    },
    {
      "epoch": 0.4433108641525475,
      "grad_norm": 3.9391560554504395,
      "learning_rate": 9.470823861079561e-05,
      "loss": 2.7228,
      "step": 7300
    },
    {
      "epoch": 0.4439181393089209,
      "grad_norm": 7.8190155029296875,
      "learning_rate": 9.469399291055562e-05,
      "loss": 2.5351,
      "step": 7310
    },
    {
      "epoch": 0.4445254144652942,
      "grad_norm": 2.7113735675811768,
      "learning_rate": 9.467972913535308e-05,
      "loss": 2.9699,
      "step": 7320
    },
    {
      "epoch": 0.4451326896216676,
      "grad_norm": 3.1036882400512695,
      "learning_rate": 9.46654472909565e-05,
      "loss": 2.1459,
      "step": 7330
    },
    {
      "epoch": 0.44573996477804095,
      "grad_norm": 1.736986756324768,
      "learning_rate": 9.465114738314166e-05,
      "loss": 2.2287,
      "step": 7340
    },
    {
      "epoch": 0.4463472399344143,
      "grad_norm": 1.990052580833435,
      "learning_rate": 9.46368294176917e-05,
      "loss": 2.2948,
      "step": 7350
    },
    {
      "epoch": 0.44695451509078765,
      "grad_norm": 2.227933883666992,
      "learning_rate": 9.4622493400397e-05,
      "loss": 2.4761,
      "step": 7360
    },
    {
      "epoch": 0.447561790247161,
      "grad_norm": 2.3538217544555664,
      "learning_rate": 9.460813933705531e-05,
      "loss": 2.6532,
      "step": 7370
    },
    {
      "epoch": 0.44816906540353435,
      "grad_norm": 3.5950870513916016,
      "learning_rate": 9.459376723347161e-05,
      "loss": 2.4128,
      "step": 7380
    },
    {
      "epoch": 0.44877634055990767,
      "grad_norm": 1.4991557598114014,
      "learning_rate": 9.457937709545823e-05,
      "loss": 2.1169,
      "step": 7390
    },
    {
      "epoch": 0.44938361571628105,
      "grad_norm": 2.792180299758911,
      "learning_rate": 9.456496892883477e-05,
      "loss": 2.0965,
      "step": 7400
    },
    {
      "epoch": 0.4499908908726544,
      "grad_norm": 2.634004592895508,
      "learning_rate": 9.455054273942811e-05,
      "loss": 2.5012,
      "step": 7410
    },
    {
      "epoch": 0.45059816602902775,
      "grad_norm": 4.450744152069092,
      "learning_rate": 9.453609853307244e-05,
      "loss": 2.723,
      "step": 7420
    },
    {
      "epoch": 0.4512054411854011,
      "grad_norm": 2.6657700538635254,
      "learning_rate": 9.452163631560922e-05,
      "loss": 2.6843,
      "step": 7430
    },
    {
      "epoch": 0.45181271634177445,
      "grad_norm": 2.125448226928711,
      "learning_rate": 9.45071560928872e-05,
      "loss": 2.7006,
      "step": 7440
    },
    {
      "epoch": 0.4524199914981478,
      "grad_norm": 3.011261463165283,
      "learning_rate": 9.449265787076243e-05,
      "loss": 2.5433,
      "step": 7450
    },
    {
      "epoch": 0.45302726665452114,
      "grad_norm": 3.3293473720550537,
      "learning_rate": 9.44781416550982e-05,
      "loss": 2.6216,
      "step": 7460
    },
    {
      "epoch": 0.4536345418108945,
      "grad_norm": 3.0404953956604004,
      "learning_rate": 9.446360745176511e-05,
      "loss": 2.4369,
      "step": 7470
    },
    {
      "epoch": 0.45424181696726784,
      "grad_norm": 2.4808309078216553,
      "learning_rate": 9.444905526664103e-05,
      "loss": 2.7247,
      "step": 7480
    },
    {
      "epoch": 0.4548490921236412,
      "grad_norm": 3.364408254623413,
      "learning_rate": 9.443448510561109e-05,
      "loss": 2.6495,
      "step": 7490
    },
    {
      "epoch": 0.4554563672800146,
      "grad_norm": 3.59797739982605,
      "learning_rate": 9.441989697456767e-05,
      "loss": 2.4655,
      "step": 7500
    },
    {
      "epoch": 0.4560636424363879,
      "grad_norm": 2.721339464187622,
      "learning_rate": 9.440529087941047e-05,
      "loss": 2.857,
      "step": 7510
    },
    {
      "epoch": 0.4566709175927613,
      "grad_norm": 2.698099136352539,
      "learning_rate": 9.439066682604643e-05,
      "loss": 2.5713,
      "step": 7520
    },
    {
      "epoch": 0.4572781927491346,
      "grad_norm": 2.32311749458313,
      "learning_rate": 9.437602482038974e-05,
      "loss": 2.5252,
      "step": 7530
    },
    {
      "epoch": 0.457885467905508,
      "grad_norm": 2.642758846282959,
      "learning_rate": 9.436136486836186e-05,
      "loss": 2.2922,
      "step": 7540
    },
    {
      "epoch": 0.4584927430618813,
      "grad_norm": 2.9288697242736816,
      "learning_rate": 9.43466869758915e-05,
      "loss": 2.2564,
      "step": 7550
    },
    {
      "epoch": 0.4591000182182547,
      "grad_norm": 1.605661153793335,
      "learning_rate": 9.433199114891467e-05,
      "loss": 2.2749,
      "step": 7560
    },
    {
      "epoch": 0.45970729337462807,
      "grad_norm": 6.775206565856934,
      "learning_rate": 9.431727739337454e-05,
      "loss": 2.8181,
      "step": 7570
    },
    {
      "epoch": 0.4603145685310014,
      "grad_norm": 2.811607599258423,
      "learning_rate": 9.430254571522163e-05,
      "loss": 2.5018,
      "step": 7580
    },
    {
      "epoch": 0.46092184368737477,
      "grad_norm": 2.536494016647339,
      "learning_rate": 9.428779612041368e-05,
      "loss": 2.5337,
      "step": 7590
    },
    {
      "epoch": 0.4615291188437481,
      "grad_norm": 2.322028875350952,
      "learning_rate": 9.427302861491561e-05,
      "loss": 2.8019,
      "step": 7600
    },
    {
      "epoch": 0.46213639400012146,
      "grad_norm": 6.6271071434021,
      "learning_rate": 9.425824320469964e-05,
      "loss": 2.3037,
      "step": 7610
    },
    {
      "epoch": 0.4627436691564948,
      "grad_norm": 2.064951181411743,
      "learning_rate": 9.424343989574526e-05,
      "loss": 2.0578,
      "step": 7620
    },
    {
      "epoch": 0.46335094431286816,
      "grad_norm": 2.8219897747039795,
      "learning_rate": 9.422861869403916e-05,
      "loss": 2.4841,
      "step": 7630
    },
    {
      "epoch": 0.46395821946924154,
      "grad_norm": 3.9589405059814453,
      "learning_rate": 9.421377960557525e-05,
      "loss": 2.6274,
      "step": 7640
    },
    {
      "epoch": 0.46456549462561486,
      "grad_norm": 3.2575364112854004,
      "learning_rate": 9.419892263635468e-05,
      "loss": 2.5832,
      "step": 7650
    },
    {
      "epoch": 0.46517276978198824,
      "grad_norm": 3.0187571048736572,
      "learning_rate": 9.41840477923859e-05,
      "loss": 2.8226,
      "step": 7660
    },
    {
      "epoch": 0.46578004493836156,
      "grad_norm": 3.099646806716919,
      "learning_rate": 9.416915507968449e-05,
      "loss": 2.9858,
      "step": 7670
    },
    {
      "epoch": 0.46638732009473494,
      "grad_norm": 3.9471912384033203,
      "learning_rate": 9.41542445042733e-05,
      "loss": 2.4171,
      "step": 7680
    },
    {
      "epoch": 0.46699459525110826,
      "grad_norm": 1.6442005634307861,
      "learning_rate": 9.41393160721824e-05,
      "loss": 2.3506,
      "step": 7690
    },
    {
      "epoch": 0.46760187040748163,
      "grad_norm": 2.0049915313720703,
      "learning_rate": 9.412436978944912e-05,
      "loss": 2.2025,
      "step": 7700
    },
    {
      "epoch": 0.46820914556385496,
      "grad_norm": 2.3059773445129395,
      "learning_rate": 9.410940566211797e-05,
      "loss": 2.459,
      "step": 7710
    },
    {
      "epoch": 0.46881642072022833,
      "grad_norm": 2.8079028129577637,
      "learning_rate": 9.409442369624065e-05,
      "loss": 2.5502,
      "step": 7720
    },
    {
      "epoch": 0.4694236958766017,
      "grad_norm": 4.0988640785217285,
      "learning_rate": 9.40794238978761e-05,
      "loss": 2.5722,
      "step": 7730
    },
    {
      "epoch": 0.47003097103297503,
      "grad_norm": 2.1008057594299316,
      "learning_rate": 9.406440627309053e-05,
      "loss": 2.5841,
      "step": 7740
    },
    {
      "epoch": 0.4706382461893484,
      "grad_norm": 2.213907241821289,
      "learning_rate": 9.404937082795726e-05,
      "loss": 2.3623,
      "step": 7750
    },
    {
      "epoch": 0.47124552134572173,
      "grad_norm": 1.881065845489502,
      "learning_rate": 9.40343175685569e-05,
      "loss": 2.4822,
      "step": 7760
    },
    {
      "epoch": 0.4718527965020951,
      "grad_norm": 2.2961854934692383,
      "learning_rate": 9.401924650097718e-05,
      "loss": 2.4485,
      "step": 7770
    },
    {
      "epoch": 0.47246007165846843,
      "grad_norm": 2.2445356845855713,
      "learning_rate": 9.400415763131312e-05,
      "loss": 2.3201,
      "step": 7780
    },
    {
      "epoch": 0.4730673468148418,
      "grad_norm": 2.4774019718170166,
      "learning_rate": 9.398905096566688e-05,
      "loss": 2.4478,
      "step": 7790
    },
    {
      "epoch": 0.4736746219712152,
      "grad_norm": 3.193199872970581,
      "learning_rate": 9.397392651014785e-05,
      "loss": 2.7434,
      "step": 7800
    },
    {
      "epoch": 0.4742818971275885,
      "grad_norm": 2.8147432804107666,
      "learning_rate": 9.39587842708726e-05,
      "loss": 2.4104,
      "step": 7810
    },
    {
      "epoch": 0.4748891722839619,
      "grad_norm": 2.2249224185943604,
      "learning_rate": 9.394362425396486e-05,
      "loss": 2.5148,
      "step": 7820
    },
    {
      "epoch": 0.4754964474403352,
      "grad_norm": 2.6423826217651367,
      "learning_rate": 9.392844646555563e-05,
      "loss": 2.1811,
      "step": 7830
    },
    {
      "epoch": 0.4761037225967086,
      "grad_norm": 2.9791035652160645,
      "learning_rate": 9.391325091178303e-05,
      "loss": 2.4796,
      "step": 7840
    },
    {
      "epoch": 0.4767109977530819,
      "grad_norm": 2.477844476699829,
      "learning_rate": 9.389803759879237e-05,
      "loss": 2.363,
      "step": 7850
    },
    {
      "epoch": 0.4773182729094553,
      "grad_norm": 2.568680763244629,
      "learning_rate": 9.388280653273617e-05,
      "loss": 2.8172,
      "step": 7860
    },
    {
      "epoch": 0.47792554806582865,
      "grad_norm": 3.1237146854400635,
      "learning_rate": 9.386755771977412e-05,
      "loss": 2.7337,
      "step": 7870
    },
    {
      "epoch": 0.478532823222202,
      "grad_norm": 2.9446589946746826,
      "learning_rate": 9.385229116607306e-05,
      "loss": 2.4484,
      "step": 7880
    },
    {
      "epoch": 0.47914009837857535,
      "grad_norm": 2.2867212295532227,
      "learning_rate": 9.383700687780706e-05,
      "loss": 2.1593,
      "step": 7890
    },
    {
      "epoch": 0.4797473735349487,
      "grad_norm": 2.8926541805267334,
      "learning_rate": 9.382170486115728e-05,
      "loss": 2.3686,
      "step": 7900
    },
    {
      "epoch": 0.48035464869132205,
      "grad_norm": 1.5645568370819092,
      "learning_rate": 9.380638512231216e-05,
      "loss": 2.1285,
      "step": 7910
    },
    {
      "epoch": 0.48096192384769537,
      "grad_norm": 1.7086042165756226,
      "learning_rate": 9.379104766746722e-05,
      "loss": 2.3893,
      "step": 7920
    },
    {
      "epoch": 0.48156919900406875,
      "grad_norm": 2.87491512298584,
      "learning_rate": 9.377569250282517e-05,
      "loss": 2.5674,
      "step": 7930
    },
    {
      "epoch": 0.48217647416044207,
      "grad_norm": 2.034264087677002,
      "learning_rate": 9.376031963459589e-05,
      "loss": 2.9346,
      "step": 7940
    },
    {
      "epoch": 0.48278374931681545,
      "grad_norm": 4.426326751708984,
      "learning_rate": 9.37449290689964e-05,
      "loss": 2.5844,
      "step": 7950
    },
    {
      "epoch": 0.4833910244731888,
      "grad_norm": 2.483025312423706,
      "learning_rate": 9.372952081225088e-05,
      "loss": 2.9293,
      "step": 7960
    },
    {
      "epoch": 0.48399829962956215,
      "grad_norm": 4.655477523803711,
      "learning_rate": 9.371409487059069e-05,
      "loss": 3.0947,
      "step": 7970
    },
    {
      "epoch": 0.4846055747859355,
      "grad_norm": 4.066415786743164,
      "learning_rate": 9.369865125025435e-05,
      "loss": 2.9336,
      "step": 7980
    },
    {
      "epoch": 0.48521284994230884,
      "grad_norm": 4.145140647888184,
      "learning_rate": 9.368318995748746e-05,
      "loss": 2.6267,
      "step": 7990
    },
    {
      "epoch": 0.4858201250986822,
      "grad_norm": 2.0782315731048584,
      "learning_rate": 9.366771099854283e-05,
      "loss": 2.837,
      "step": 8000
    },
    {
      "epoch": 0.48642740025505554,
      "grad_norm": 4.426705837249756,
      "learning_rate": 9.365221437968042e-05,
      "loss": 2.5609,
      "step": 8010
    },
    {
      "epoch": 0.4870346754114289,
      "grad_norm": 3.0449514389038086,
      "learning_rate": 9.363670010716729e-05,
      "loss": 2.8438,
      "step": 8020
    },
    {
      "epoch": 0.4876419505678023,
      "grad_norm": 2.9630210399627686,
      "learning_rate": 9.362116818727767e-05,
      "loss": 2.4277,
      "step": 8030
    },
    {
      "epoch": 0.4882492257241756,
      "grad_norm": 2.468177556991577,
      "learning_rate": 9.360561862629287e-05,
      "loss": 2.7228,
      "step": 8040
    },
    {
      "epoch": 0.488856500880549,
      "grad_norm": 5.127236366271973,
      "learning_rate": 9.359005143050146e-05,
      "loss": 2.6297,
      "step": 8050
    },
    {
      "epoch": 0.4894637760369223,
      "grad_norm": 2.267310857772827,
      "learning_rate": 9.3574466606199e-05,
      "loss": 2.3677,
      "step": 8060
    },
    {
      "epoch": 0.4900710511932957,
      "grad_norm": 2.511033296585083,
      "learning_rate": 9.355886415968827e-05,
      "loss": 2.7575,
      "step": 8070
    },
    {
      "epoch": 0.490678326349669,
      "grad_norm": 2.1425158977508545,
      "learning_rate": 9.354324409727911e-05,
      "loss": 2.4077,
      "step": 8080
    },
    {
      "epoch": 0.4912856015060424,
      "grad_norm": 2.956350326538086,
      "learning_rate": 9.352760642528857e-05,
      "loss": 2.4938,
      "step": 8090
    },
    {
      "epoch": 0.49189287666241577,
      "grad_norm": 2.9924001693725586,
      "learning_rate": 9.351195115004076e-05,
      "loss": 2.5948,
      "step": 8100
    },
    {
      "epoch": 0.4925001518187891,
      "grad_norm": 2.2809035778045654,
      "learning_rate": 9.349627827786691e-05,
      "loss": 2.39,
      "step": 8110
    },
    {
      "epoch": 0.49310742697516247,
      "grad_norm": 2.567007303237915,
      "learning_rate": 9.348058781510538e-05,
      "loss": 2.8342,
      "step": 8120
    },
    {
      "epoch": 0.4937147021315358,
      "grad_norm": 3.279766798019409,
      "learning_rate": 9.346487976810166e-05,
      "loss": 2.4439,
      "step": 8130
    },
    {
      "epoch": 0.49432197728790916,
      "grad_norm": 2.2140555381774902,
      "learning_rate": 9.344915414320831e-05,
      "loss": 2.5409,
      "step": 8140
    },
    {
      "epoch": 0.4949292524442825,
      "grad_norm": 3.304486036300659,
      "learning_rate": 9.343341094678504e-05,
      "loss": 2.8112,
      "step": 8150
    },
    {
      "epoch": 0.49553652760065586,
      "grad_norm": 3.662257432937622,
      "learning_rate": 9.341765018519865e-05,
      "loss": 2.3601,
      "step": 8160
    },
    {
      "epoch": 0.4961438027570292,
      "grad_norm": 1.7995479106903076,
      "learning_rate": 9.340187186482304e-05,
      "loss": 2.4799,
      "step": 8170
    },
    {
      "epoch": 0.49675107791340256,
      "grad_norm": 1.566380500793457,
      "learning_rate": 9.338607599203919e-05,
      "loss": 2.6543,
      "step": 8180
    },
    {
      "epoch": 0.49735835306977594,
      "grad_norm": 4.468143463134766,
      "learning_rate": 9.337026257323524e-05,
      "loss": 2.4874,
      "step": 8190
    },
    {
      "epoch": 0.49796562822614926,
      "grad_norm": 2.3735294342041016,
      "learning_rate": 9.33544316148064e-05,
      "loss": 2.4208,
      "step": 8200
    },
    {
      "epoch": 0.49857290338252264,
      "grad_norm": 3.549135208129883,
      "learning_rate": 9.333858312315489e-05,
      "loss": 2.7751,
      "step": 8210
    },
    {
      "epoch": 0.49918017853889596,
      "grad_norm": 5.2206220626831055,
      "learning_rate": 9.332271710469016e-05,
      "loss": 2.5915,
      "step": 8220
    },
    {
      "epoch": 0.49978745369526933,
      "grad_norm": 2.187800645828247,
      "learning_rate": 9.330683356582866e-05,
      "loss": 2.2875,
      "step": 8230
    },
    {
      "epoch": 0.5003947288516427,
      "grad_norm": 2.0768632888793945,
      "learning_rate": 9.329093251299393e-05,
      "loss": 2.0601,
      "step": 8240
    },
    {
      "epoch": 0.501002004008016,
      "grad_norm": 3.2875900268554688,
      "learning_rate": 9.327501395261664e-05,
      "loss": 2.7665,
      "step": 8250
    },
    {
      "epoch": 0.5016092791643894,
      "grad_norm": 2.3767271041870117,
      "learning_rate": 9.325907789113448e-05,
      "loss": 2.7249,
      "step": 8260
    },
    {
      "epoch": 0.5022165543207627,
      "grad_norm": 2.4210739135742188,
      "learning_rate": 9.324312433499227e-05,
      "loss": 2.2586,
      "step": 8270
    },
    {
      "epoch": 0.5028238294771361,
      "grad_norm": 1.5918452739715576,
      "learning_rate": 9.322715329064187e-05,
      "loss": 2.27,
      "step": 8280
    },
    {
      "epoch": 0.5034311046335095,
      "grad_norm": 2.8737852573394775,
      "learning_rate": 9.321116476454222e-05,
      "loss": 2.336,
      "step": 8290
    },
    {
      "epoch": 0.5040383797898828,
      "grad_norm": 1.7413668632507324,
      "learning_rate": 9.319515876315934e-05,
      "loss": 2.5603,
      "step": 8300
    },
    {
      "epoch": 0.5046456549462561,
      "grad_norm": 2.4790902137756348,
      "learning_rate": 9.317913529296631e-05,
      "loss": 2.4584,
      "step": 8310
    },
    {
      "epoch": 0.5052529301026295,
      "grad_norm": 2.080993413925171,
      "learning_rate": 9.316309436044328e-05,
      "loss": 2.3901,
      "step": 8320
    },
    {
      "epoch": 0.5058602052590029,
      "grad_norm": 2.425281047821045,
      "learning_rate": 9.314703597207747e-05,
      "loss": 2.0926,
      "step": 8330
    },
    {
      "epoch": 0.5064674804153763,
      "grad_norm": 1.6941583156585693,
      "learning_rate": 9.313096013436313e-05,
      "loss": 2.3259,
      "step": 8340
    },
    {
      "epoch": 0.5070747555717495,
      "grad_norm": 2.405672073364258,
      "learning_rate": 9.311486685380158e-05,
      "loss": 2.5686,
      "step": 8350
    },
    {
      "epoch": 0.5076820307281229,
      "grad_norm": 2.3601489067077637,
      "learning_rate": 9.309875613690122e-05,
      "loss": 2.2101,
      "step": 8360
    },
    {
      "epoch": 0.5082893058844963,
      "grad_norm": 2.2470037937164307,
      "learning_rate": 9.308262799017748e-05,
      "loss": 2.5983,
      "step": 8370
    },
    {
      "epoch": 0.5088965810408697,
      "grad_norm": 3.8188443183898926,
      "learning_rate": 9.306648242015281e-05,
      "loss": 2.7569,
      "step": 8380
    },
    {
      "epoch": 0.5095038561972429,
      "grad_norm": 3.3722052574157715,
      "learning_rate": 9.305031943335679e-05,
      "loss": 2.6754,
      "step": 8390
    },
    {
      "epoch": 0.5101111313536163,
      "grad_norm": 2.8479950428009033,
      "learning_rate": 9.303413903632591e-05,
      "loss": 2.8225,
      "step": 8400
    },
    {
      "epoch": 0.5107184065099897,
      "grad_norm": 4.241398811340332,
      "learning_rate": 9.301794123560384e-05,
      "loss": 2.5676,
      "step": 8410
    },
    {
      "epoch": 0.511325681666363,
      "grad_norm": 2.8258047103881836,
      "learning_rate": 9.300172603774123e-05,
      "loss": 2.9235,
      "step": 8420
    },
    {
      "epoch": 0.5119329568227364,
      "grad_norm": 3.3932950496673584,
      "learning_rate": 9.298549344929573e-05,
      "loss": 2.8427,
      "step": 8430
    },
    {
      "epoch": 0.5125402319791097,
      "grad_norm": 3.3268444538116455,
      "learning_rate": 9.296924347683208e-05,
      "loss": 2.7745,
      "step": 8440
    },
    {
      "epoch": 0.5131475071354831,
      "grad_norm": 4.169103145599365,
      "learning_rate": 9.295297612692202e-05,
      "loss": 2.6378,
      "step": 8450
    },
    {
      "epoch": 0.5137547822918564,
      "grad_norm": 2.4215517044067383,
      "learning_rate": 9.293669140614433e-05,
      "loss": 2.2369,
      "step": 8460
    },
    {
      "epoch": 0.5143620574482298,
      "grad_norm": 3.521707057952881,
      "learning_rate": 9.29203893210848e-05,
      "loss": 3.0993,
      "step": 8470
    },
    {
      "epoch": 0.5149693326046031,
      "grad_norm": 4.297608852386475,
      "learning_rate": 9.290406987833629e-05,
      "loss": 2.4451,
      "step": 8480
    },
    {
      "epoch": 0.5155766077609765,
      "grad_norm": 2.7457594871520996,
      "learning_rate": 9.288773308449859e-05,
      "loss": 2.5904,
      "step": 8490
    },
    {
      "epoch": 0.5161838829173498,
      "grad_norm": 3.168519973754883,
      "learning_rate": 9.287137894617858e-05,
      "loss": 2.6114,
      "step": 8500
    },
    {
      "epoch": 0.5167911580737232,
      "grad_norm": 2.792043685913086,
      "learning_rate": 9.285500746999014e-05,
      "loss": 2.6976,
      "step": 8510
    },
    {
      "epoch": 0.5173984332300966,
      "grad_norm": 2.657944679260254,
      "learning_rate": 9.283861866255416e-05,
      "loss": 2.7227,
      "step": 8520
    },
    {
      "epoch": 0.5180057083864699,
      "grad_norm": 3.365320920944214,
      "learning_rate": 9.282221253049853e-05,
      "loss": 2.4333,
      "step": 8530
    },
    {
      "epoch": 0.5186129835428432,
      "grad_norm": 2.5189900398254395,
      "learning_rate": 9.280578908045814e-05,
      "loss": 2.3042,
      "step": 8540
    },
    {
      "epoch": 0.5192202586992166,
      "grad_norm": 2.487299680709839,
      "learning_rate": 9.278934831907491e-05,
      "loss": 2.5268,
      "step": 8550
    },
    {
      "epoch": 0.51982753385559,
      "grad_norm": 3.208329677581787,
      "learning_rate": 9.277289025299773e-05,
      "loss": 2.2965,
      "step": 8560
    },
    {
      "epoch": 0.5204348090119634,
      "grad_norm": 2.99489426612854,
      "learning_rate": 9.275641488888253e-05,
      "loss": 2.5589,
      "step": 8570
    },
    {
      "epoch": 0.5210420841683366,
      "grad_norm": 2.725825548171997,
      "learning_rate": 9.273992223339219e-05,
      "loss": 2.6164,
      "step": 8580
    },
    {
      "epoch": 0.52164935932471,
      "grad_norm": 3.0565147399902344,
      "learning_rate": 9.27234122931966e-05,
      "loss": 2.5928,
      "step": 8590
    },
    {
      "epoch": 0.5222566344810834,
      "grad_norm": 2.9571340084075928,
      "learning_rate": 9.270688507497265e-05,
      "loss": 2.4654,
      "step": 8600
    },
    {
      "epoch": 0.5228639096374568,
      "grad_norm": 1.6460801362991333,
      "learning_rate": 9.26903405854042e-05,
      "loss": 2.2962,
      "step": 8610
    },
    {
      "epoch": 0.52347118479383,
      "grad_norm": 2.196638584136963,
      "learning_rate": 9.267377883118214e-05,
      "loss": 2.3665,
      "step": 8620
    },
    {
      "epoch": 0.5240784599502034,
      "grad_norm": 3.373619318008423,
      "learning_rate": 9.265719981900424e-05,
      "loss": 2.8717,
      "step": 8630
    },
    {
      "epoch": 0.5246857351065768,
      "grad_norm": 5.956878662109375,
      "learning_rate": 9.264060355557539e-05,
      "loss": 2.7637,
      "step": 8640
    },
    {
      "epoch": 0.5252930102629502,
      "grad_norm": 3.629153251647949,
      "learning_rate": 9.262399004760734e-05,
      "loss": 2.6365,
      "step": 8650
    },
    {
      "epoch": 0.5259002854193235,
      "grad_norm": 3.097426176071167,
      "learning_rate": 9.260735930181888e-05,
      "loss": 2.3189,
      "step": 8660
    },
    {
      "epoch": 0.5265075605756968,
      "grad_norm": 1.7411129474639893,
      "learning_rate": 9.259071132493571e-05,
      "loss": 2.2004,
      "step": 8670
    },
    {
      "epoch": 0.5271148357320702,
      "grad_norm": 2.9429659843444824,
      "learning_rate": 9.257404612369059e-05,
      "loss": 2.318,
      "step": 8680
    },
    {
      "epoch": 0.5277221108884436,
      "grad_norm": 2.6911280155181885,
      "learning_rate": 9.255736370482315e-05,
      "loss": 2.3571,
      "step": 8690
    },
    {
      "epoch": 0.5283293860448169,
      "grad_norm": 3.3159782886505127,
      "learning_rate": 9.254066407508005e-05,
      "loss": 2.7079,
      "step": 8700
    },
    {
      "epoch": 0.5289366612011902,
      "grad_norm": 2.239410161972046,
      "learning_rate": 9.252394724121486e-05,
      "loss": 2.7012,
      "step": 8710
    },
    {
      "epoch": 0.5295439363575636,
      "grad_norm": 3.213658094406128,
      "learning_rate": 9.250721320998819e-05,
      "loss": 2.8355,
      "step": 8720
    },
    {
      "epoch": 0.530151211513937,
      "grad_norm": 3.4942750930786133,
      "learning_rate": 9.249046198816749e-05,
      "loss": 2.2983,
      "step": 8730
    },
    {
      "epoch": 0.5307584866703103,
      "grad_norm": 2.6915221214294434,
      "learning_rate": 9.247369358252723e-05,
      "loss": 3.0905,
      "step": 8740
    },
    {
      "epoch": 0.5313657618266837,
      "grad_norm": 2.9591939449310303,
      "learning_rate": 9.245690799984885e-05,
      "loss": 2.5459,
      "step": 8750
    },
    {
      "epoch": 0.531973036983057,
      "grad_norm": 1.5241374969482422,
      "learning_rate": 9.244010524692068e-05,
      "loss": 1.9899,
      "step": 8760
    },
    {
      "epoch": 0.5325803121394304,
      "grad_norm": 1.7669579982757568,
      "learning_rate": 9.242328533053804e-05,
      "loss": 2.1031,
      "step": 8770
    },
    {
      "epoch": 0.5331875872958037,
      "grad_norm": 1.575698733329773,
      "learning_rate": 9.240644825750315e-05,
      "loss": 2.4433,
      "step": 8780
    },
    {
      "epoch": 0.5337948624521771,
      "grad_norm": 2.6101441383361816,
      "learning_rate": 9.23895940346252e-05,
      "loss": 2.4171,
      "step": 8790
    },
    {
      "epoch": 0.5344021376085505,
      "grad_norm": 1.8948440551757812,
      "learning_rate": 9.237272266872032e-05,
      "loss": 2.544,
      "step": 8800
    },
    {
      "epoch": 0.5350094127649238,
      "grad_norm": 2.173553705215454,
      "learning_rate": 9.235583416661154e-05,
      "loss": 2.5664,
      "step": 8810
    },
    {
      "epoch": 0.5356166879212971,
      "grad_norm": 1.9105868339538574,
      "learning_rate": 9.233892853512887e-05,
      "loss": 2.6918,
      "step": 8820
    },
    {
      "epoch": 0.5362239630776705,
      "grad_norm": 2.5718681812286377,
      "learning_rate": 9.232200578110917e-05,
      "loss": 2.6601,
      "step": 8830
    },
    {
      "epoch": 0.5368312382340439,
      "grad_norm": 1.9243773221969604,
      "learning_rate": 9.23050659113963e-05,
      "loss": 2.3174,
      "step": 8840
    },
    {
      "epoch": 0.5374385133904171,
      "grad_norm": 2.3969576358795166,
      "learning_rate": 9.2288108932841e-05,
      "loss": 2.3395,
      "step": 8850
    },
    {
      "epoch": 0.5380457885467905,
      "grad_norm": 1.5673489570617676,
      "learning_rate": 9.227113485230095e-05,
      "loss": 2.2778,
      "step": 8860
    },
    {
      "epoch": 0.5386530637031639,
      "grad_norm": 3.3187739849090576,
      "learning_rate": 9.225414367664076e-05,
      "loss": 2.4104,
      "step": 8870
    },
    {
      "epoch": 0.5392603388595373,
      "grad_norm": 2.0055112838745117,
      "learning_rate": 9.22371354127319e-05,
      "loss": 2.7621,
      "step": 8880
    },
    {
      "epoch": 0.5398676140159107,
      "grad_norm": 2.786675453186035,
      "learning_rate": 9.222011006745279e-05,
      "loss": 2.6655,
      "step": 8890
    },
    {
      "epoch": 0.5404748891722839,
      "grad_norm": 2.4771339893341064,
      "learning_rate": 9.220306764768876e-05,
      "loss": 2.7034,
      "step": 8900
    },
    {
      "epoch": 0.5410821643286573,
      "grad_norm": 3.0360453128814697,
      "learning_rate": 9.2186008160332e-05,
      "loss": 2.4427,
      "step": 8910
    },
    {
      "epoch": 0.5416894394850307,
      "grad_norm": 2.101039171218872,
      "learning_rate": 9.21689316122817e-05,
      "loss": 2.2527,
      "step": 8920
    },
    {
      "epoch": 0.542296714641404,
      "grad_norm": 3.0840237140655518,
      "learning_rate": 9.215183801044385e-05,
      "loss": 2.4056,
      "step": 8930
    },
    {
      "epoch": 0.5429039897977773,
      "grad_norm": 3.750338315963745,
      "learning_rate": 9.213472736173139e-05,
      "loss": 2.1872,
      "step": 8940
    },
    {
      "epoch": 0.5435112649541507,
      "grad_norm": 1.6616802215576172,
      "learning_rate": 9.211759967306412e-05,
      "loss": 2.5448,
      "step": 8950
    },
    {
      "epoch": 0.5441185401105241,
      "grad_norm": 2.598639726638794,
      "learning_rate": 9.210045495136876e-05,
      "loss": 2.427,
      "step": 8960
    },
    {
      "epoch": 0.5447258152668975,
      "grad_norm": 2.9213547706604004,
      "learning_rate": 9.208329320357891e-05,
      "loss": 2.9029,
      "step": 8970
    },
    {
      "epoch": 0.5453330904232708,
      "grad_norm": 2.659151554107666,
      "learning_rate": 9.206611443663506e-05,
      "loss": 2.5062,
      "step": 8980
    },
    {
      "epoch": 0.5459403655796441,
      "grad_norm": 2.189408302307129,
      "learning_rate": 9.204891865748457e-05,
      "loss": 2.4944,
      "step": 8990
    },
    {
      "epoch": 0.5465476407360175,
      "grad_norm": 3.2065200805664062,
      "learning_rate": 9.203170587308169e-05,
      "loss": 2.2783,
      "step": 9000
    },
    {
      "epoch": 0.5471549158923908,
      "grad_norm": 3.0260839462280273,
      "learning_rate": 9.201447609038754e-05,
      "loss": 2.8667,
      "step": 9010
    },
    {
      "epoch": 0.5477621910487642,
      "grad_norm": 3.9019992351531982,
      "learning_rate": 9.19972293163701e-05,
      "loss": 2.2164,
      "step": 9020
    },
    {
      "epoch": 0.5483694662051376,
      "grad_norm": 2.112363338470459,
      "learning_rate": 9.197996555800427e-05,
      "loss": 2.4123,
      "step": 9030
    },
    {
      "epoch": 0.5489767413615109,
      "grad_norm": 2.479797124862671,
      "learning_rate": 9.196268482227179e-05,
      "loss": 2.509,
      "step": 9040
    },
    {
      "epoch": 0.5495840165178842,
      "grad_norm": 2.725674629211426,
      "learning_rate": 9.194538711616126e-05,
      "loss": 2.3684,
      "step": 9050
    },
    {
      "epoch": 0.5501912916742576,
      "grad_norm": 4.0709662437438965,
      "learning_rate": 9.192807244666811e-05,
      "loss": 2.5961,
      "step": 9060
    },
    {
      "epoch": 0.550798566830631,
      "grad_norm": 3.7287497520446777,
      "learning_rate": 9.191074082079472e-05,
      "loss": 2.9992,
      "step": 9070
    },
    {
      "epoch": 0.5514058419870043,
      "grad_norm": 4.769763469696045,
      "learning_rate": 9.189339224555025e-05,
      "loss": 2.6328,
      "step": 9080
    },
    {
      "epoch": 0.5520131171433776,
      "grad_norm": 4.153842449188232,
      "learning_rate": 9.187602672795074e-05,
      "loss": 2.5317,
      "step": 9090
    },
    {
      "epoch": 0.552620392299751,
      "grad_norm": 2.487879753112793,
      "learning_rate": 9.18586442750191e-05,
      "loss": 2.0679,
      "step": 9100
    },
    {
      "epoch": 0.5532276674561244,
      "grad_norm": 2.6940436363220215,
      "learning_rate": 9.184124489378505e-05,
      "loss": 2.316,
      "step": 9110
    },
    {
      "epoch": 0.5538349426124978,
      "grad_norm": 3.454097270965576,
      "learning_rate": 9.182382859128518e-05,
      "loss": 2.5558,
      "step": 9120
    },
    {
      "epoch": 0.554442217768871,
      "grad_norm": 2.3994202613830566,
      "learning_rate": 9.180639537456293e-05,
      "loss": 2.4688,
      "step": 9130
    },
    {
      "epoch": 0.5550494929252444,
      "grad_norm": 2.5144002437591553,
      "learning_rate": 9.178894525066857e-05,
      "loss": 2.6252,
      "step": 9140
    },
    {
      "epoch": 0.5556567680816178,
      "grad_norm": 3.087794303894043,
      "learning_rate": 9.177147822665919e-05,
      "loss": 2.5555,
      "step": 9150
    },
    {
      "epoch": 0.5562640432379912,
      "grad_norm": 2.323120594024658,
      "learning_rate": 9.175399430959877e-05,
      "loss": 2.4064,
      "step": 9160
    },
    {
      "epoch": 0.5568713183943644,
      "grad_norm": 2.3239688873291016,
      "learning_rate": 9.173649350655804e-05,
      "loss": 2.1732,
      "step": 9170
    },
    {
      "epoch": 0.5574785935507378,
      "grad_norm": 3.296776056289673,
      "learning_rate": 9.171897582461461e-05,
      "loss": 2.4186,
      "step": 9180
    },
    {
      "epoch": 0.5580858687071112,
      "grad_norm": 1.9341621398925781,
      "learning_rate": 9.170144127085296e-05,
      "loss": 2.3931,
      "step": 9190
    },
    {
      "epoch": 0.5586931438634846,
      "grad_norm": 1.985984444618225,
      "learning_rate": 9.168388985236428e-05,
      "loss": 2.4363,
      "step": 9200
    },
    {
      "epoch": 0.5593004190198579,
      "grad_norm": 2.456603765487671,
      "learning_rate": 9.166632157624668e-05,
      "loss": 2.4155,
      "step": 9210
    },
    {
      "epoch": 0.5599076941762312,
      "grad_norm": 3.137023687362671,
      "learning_rate": 9.164873644960503e-05,
      "loss": 2.2484,
      "step": 9220
    },
    {
      "epoch": 0.5605149693326046,
      "grad_norm": 1.8423420190811157,
      "learning_rate": 9.163113447955106e-05,
      "loss": 2.5563,
      "step": 9230
    },
    {
      "epoch": 0.561122244488978,
      "grad_norm": 3.128898859024048,
      "learning_rate": 9.161351567320327e-05,
      "loss": 2.4705,
      "step": 9240
    },
    {
      "epoch": 0.5617295196453513,
      "grad_norm": 1.4238096475601196,
      "learning_rate": 9.159588003768698e-05,
      "loss": 2.3052,
      "step": 9250
    },
    {
      "epoch": 0.5623367948017247,
      "grad_norm": 2.697199821472168,
      "learning_rate": 9.157822758013433e-05,
      "loss": 2.5731,
      "step": 9260
    },
    {
      "epoch": 0.562944069958098,
      "grad_norm": 2.900585412979126,
      "learning_rate": 9.156055830768426e-05,
      "loss": 2.5653,
      "step": 9270
    },
    {
      "epoch": 0.5635513451144714,
      "grad_norm": 1.354722499847412,
      "learning_rate": 9.15428722274825e-05,
      "loss": 2.3286,
      "step": 9280
    },
    {
      "epoch": 0.5641586202708447,
      "grad_norm": 2.9299933910369873,
      "learning_rate": 9.152516934668158e-05,
      "loss": 2.6438,
      "step": 9290
    },
    {
      "epoch": 0.5647658954272181,
      "grad_norm": 2.886495590209961,
      "learning_rate": 9.150744967244082e-05,
      "loss": 2.7906,
      "step": 9300
    },
    {
      "epoch": 0.5653731705835914,
      "grad_norm": 2.379913806915283,
      "learning_rate": 9.148971321192637e-05,
      "loss": 2.583,
      "step": 9310
    },
    {
      "epoch": 0.5659804457399648,
      "grad_norm": 3.1914608478546143,
      "learning_rate": 9.147195997231111e-05,
      "loss": 2.1796,
      "step": 9320
    },
    {
      "epoch": 0.5665877208963381,
      "grad_norm": 2.312002182006836,
      "learning_rate": 9.145418996077473e-05,
      "loss": 2.6289,
      "step": 9330
    },
    {
      "epoch": 0.5671949960527115,
      "grad_norm": 2.495377540588379,
      "learning_rate": 9.143640318450371e-05,
      "loss": 2.653,
      "step": 9340
    },
    {
      "epoch": 0.5678022712090849,
      "grad_norm": 2.999154567718506,
      "learning_rate": 9.141859965069132e-05,
      "loss": 2.6557,
      "step": 9350
    },
    {
      "epoch": 0.5684095463654582,
      "grad_norm": 4.194555759429932,
      "learning_rate": 9.140077936653759e-05,
      "loss": 2.4244,
      "step": 9360
    },
    {
      "epoch": 0.5690168215218315,
      "grad_norm": 2.220982789993286,
      "learning_rate": 9.13829423392493e-05,
      "loss": 2.6754,
      "step": 9370
    },
    {
      "epoch": 0.5696240966782049,
      "grad_norm": 1.877602458000183,
      "learning_rate": 9.136508857604005e-05,
      "loss": 2.2346,
      "step": 9380
    },
    {
      "epoch": 0.5702313718345783,
      "grad_norm": 2.279038429260254,
      "learning_rate": 9.134721808413019e-05,
      "loss": 2.5616,
      "step": 9390
    },
    {
      "epoch": 0.5708386469909515,
      "grad_norm": 3.0587923526763916,
      "learning_rate": 9.132933087074682e-05,
      "loss": 2.68,
      "step": 9400
    },
    {
      "epoch": 0.5714459221473249,
      "grad_norm": 2.90105938911438,
      "learning_rate": 9.131142694312382e-05,
      "loss": 2.7356,
      "step": 9410
    },
    {
      "epoch": 0.5720531973036983,
      "grad_norm": 3.086871385574341,
      "learning_rate": 9.129350630850182e-05,
      "loss": 2.7204,
      "step": 9420
    },
    {
      "epoch": 0.5726604724600717,
      "grad_norm": 2.7255003452301025,
      "learning_rate": 9.127556897412821e-05,
      "loss": 2.5455,
      "step": 9430
    },
    {
      "epoch": 0.5732677476164451,
      "grad_norm": 1.8335353136062622,
      "learning_rate": 9.125761494725715e-05,
      "loss": 2.3739,
      "step": 9440
    },
    {
      "epoch": 0.5738750227728183,
      "grad_norm": 1.3352797031402588,
      "learning_rate": 9.12396442351495e-05,
      "loss": 2.3522,
      "step": 9450
    },
    {
      "epoch": 0.5744822979291917,
      "grad_norm": 2.2693068981170654,
      "learning_rate": 9.122165684507293e-05,
      "loss": 2.933,
      "step": 9460
    },
    {
      "epoch": 0.5750895730855651,
      "grad_norm": 1.6518694162368774,
      "learning_rate": 9.120365278430183e-05,
      "loss": 2.602,
      "step": 9470
    },
    {
      "epoch": 0.5756968482419385,
      "grad_norm": 1.6351338624954224,
      "learning_rate": 9.118563206011731e-05,
      "loss": 2.0357,
      "step": 9480
    },
    {
      "epoch": 0.5763041233983118,
      "grad_norm": 1.794082760810852,
      "learning_rate": 9.116759467980725e-05,
      "loss": 2.1533,
      "step": 9490
    },
    {
      "epoch": 0.5769113985546851,
      "grad_norm": 2.724379301071167,
      "learning_rate": 9.114954065066624e-05,
      "loss": 2.2415,
      "step": 9500
    },
    {
      "epoch": 0.5775186737110585,
      "grad_norm": 2.5686569213867188,
      "learning_rate": 9.113146997999563e-05,
      "loss": 2.3221,
      "step": 9510
    },
    {
      "epoch": 0.5781259488674318,
      "grad_norm": 2.770348072052002,
      "learning_rate": 9.111338267510349e-05,
      "loss": 2.5863,
      "step": 9520
    },
    {
      "epoch": 0.5787332240238052,
      "grad_norm": 2.3239896297454834,
      "learning_rate": 9.10952787433046e-05,
      "loss": 2.4664,
      "step": 9530
    },
    {
      "epoch": 0.5793404991801785,
      "grad_norm": 1.8462122678756714,
      "learning_rate": 9.107715819192048e-05,
      "loss": 2.2761,
      "step": 9540
    },
    {
      "epoch": 0.5799477743365519,
      "grad_norm": 1.2186552286148071,
      "learning_rate": 9.105902102827939e-05,
      "loss": 2.1616,
      "step": 9550
    },
    {
      "epoch": 0.5805550494929252,
      "grad_norm": 1.521883249282837,
      "learning_rate": 9.104086725971628e-05,
      "loss": 1.9259,
      "step": 9560
    },
    {
      "epoch": 0.5811623246492986,
      "grad_norm": 2.165473222732544,
      "learning_rate": 9.102269689357281e-05,
      "loss": 2.1603,
      "step": 9570
    },
    {
      "epoch": 0.581769599805672,
      "grad_norm": 2.505690097808838,
      "learning_rate": 9.100450993719735e-05,
      "loss": 2.6077,
      "step": 9580
    },
    {
      "epoch": 0.5823768749620453,
      "grad_norm": 2.958092212677002,
      "learning_rate": 9.098630639794506e-05,
      "loss": 2.2884,
      "step": 9590
    },
    {
      "epoch": 0.5829841501184186,
      "grad_norm": 3.051650047302246,
      "learning_rate": 9.096808628317767e-05,
      "loss": 2.3864,
      "step": 9600
    },
    {
      "epoch": 0.583591425274792,
      "grad_norm": 3.931284189224243,
      "learning_rate": 9.094984960026372e-05,
      "loss": 2.5111,
      "step": 9610
    },
    {
      "epoch": 0.5841987004311654,
      "grad_norm": 3.674762725830078,
      "learning_rate": 9.093159635657839e-05,
      "loss": 2.4269,
      "step": 9620
    },
    {
      "epoch": 0.5848059755875387,
      "grad_norm": 3.7031919956207275,
      "learning_rate": 9.091332655950362e-05,
      "loss": 2.8626,
      "step": 9630
    },
    {
      "epoch": 0.585413250743912,
      "grad_norm": 2.0926623344421387,
      "learning_rate": 9.089504021642798e-05,
      "loss": 2.6317,
      "step": 9640
    },
    {
      "epoch": 0.5860205259002854,
      "grad_norm": 2.809067964553833,
      "learning_rate": 9.087673733474678e-05,
      "loss": 2.8596,
      "step": 9650
    },
    {
      "epoch": 0.5866278010566588,
      "grad_norm": 4.281638145446777,
      "learning_rate": 9.085841792186196e-05,
      "loss": 3.0084,
      "step": 9660
    },
    {
      "epoch": 0.5872350762130322,
      "grad_norm": 3.3353207111358643,
      "learning_rate": 9.084008198518222e-05,
      "loss": 2.9987,
      "step": 9670
    },
    {
      "epoch": 0.5878423513694054,
      "grad_norm": 4.044825077056885,
      "learning_rate": 9.08217295321229e-05,
      "loss": 2.3106,
      "step": 9680
    },
    {
      "epoch": 0.5884496265257788,
      "grad_norm": 1.9802145957946777,
      "learning_rate": 9.080336057010599e-05,
      "loss": 2.5262,
      "step": 9690
    },
    {
      "epoch": 0.5890569016821522,
      "grad_norm": 2.1142120361328125,
      "learning_rate": 9.078497510656024e-05,
      "loss": 2.5851,
      "step": 9700
    },
    {
      "epoch": 0.5896641768385256,
      "grad_norm": 1.7885438203811646,
      "learning_rate": 9.076657314892096e-05,
      "loss": 2.3755,
      "step": 9710
    },
    {
      "epoch": 0.5902714519948989,
      "grad_norm": 2.3488941192626953,
      "learning_rate": 9.074815470463027e-05,
      "loss": 2.2145,
      "step": 9720
    },
    {
      "epoch": 0.5908787271512722,
      "grad_norm": 2.8608455657958984,
      "learning_rate": 9.072971978113684e-05,
      "loss": 2.994,
      "step": 9730
    },
    {
      "epoch": 0.5914860023076456,
      "grad_norm": 3.9730896949768066,
      "learning_rate": 9.071126838589603e-05,
      "loss": 2.9198,
      "step": 9740
    },
    {
      "epoch": 0.592093277464019,
      "grad_norm": 2.5613226890563965,
      "learning_rate": 9.069280052636993e-05,
      "loss": 2.853,
      "step": 9750
    },
    {
      "epoch": 0.5927005526203923,
      "grad_norm": 1.679639458656311,
      "learning_rate": 9.067431621002719e-05,
      "loss": 2.7064,
      "step": 9760
    },
    {
      "epoch": 0.5933078277767656,
      "grad_norm": 1.482369303703308,
      "learning_rate": 9.065581544434319e-05,
      "loss": 2.2095,
      "step": 9770
    },
    {
      "epoch": 0.593915102933139,
      "grad_norm": 1.9043338298797607,
      "learning_rate": 9.063729823679991e-05,
      "loss": 2.1853,
      "step": 9780
    },
    {
      "epoch": 0.5945223780895124,
      "grad_norm": 2.1812925338745117,
      "learning_rate": 9.061876459488603e-05,
      "loss": 2.3918,
      "step": 9790
    },
    {
      "epoch": 0.5951296532458857,
      "grad_norm": 2.207256555557251,
      "learning_rate": 9.060021452609684e-05,
      "loss": 2.5148,
      "step": 9800
    },
    {
      "epoch": 0.5957369284022591,
      "grad_norm": 2.2263755798339844,
      "learning_rate": 9.058164803793426e-05,
      "loss": 2.1129,
      "step": 9810
    },
    {
      "epoch": 0.5963442035586324,
      "grad_norm": 2.68636417388916,
      "learning_rate": 9.056306513790692e-05,
      "loss": 2.3785,
      "step": 9820
    },
    {
      "epoch": 0.5969514787150058,
      "grad_norm": 4.874732494354248,
      "learning_rate": 9.054446583352999e-05,
      "loss": 2.3083,
      "step": 9830
    },
    {
      "epoch": 0.5975587538713791,
      "grad_norm": 2.111842393875122,
      "learning_rate": 9.052585013232535e-05,
      "loss": 2.7522,
      "step": 9840
    },
    {
      "epoch": 0.5981660290277525,
      "grad_norm": 2.907121419906616,
      "learning_rate": 9.05072180418215e-05,
      "loss": 3.1136,
      "step": 9850
    },
    {
      "epoch": 0.5987733041841258,
      "grad_norm": 5.161039352416992,
      "learning_rate": 9.048856956955354e-05,
      "loss": 3.0639,
      "step": 9860
    },
    {
      "epoch": 0.5993805793404992,
      "grad_norm": 1.929857611656189,
      "learning_rate": 9.046990472306321e-05,
      "loss": 2.7769,
      "step": 9870
    },
    {
      "epoch": 0.5999878544968725,
      "grad_norm": 2.4187803268432617,
      "learning_rate": 9.045122350989885e-05,
      "loss": 2.4865,
      "step": 9880
    },
    {
      "epoch": 0.6005951296532459,
      "grad_norm": 2.848289728164673,
      "learning_rate": 9.043252593761548e-05,
      "loss": 2.2188,
      "step": 9890
    },
    {
      "epoch": 0.6012024048096193,
      "grad_norm": 2.1121842861175537,
      "learning_rate": 9.041381201377468e-05,
      "loss": 2.2873,
      "step": 9900
    },
    {
      "epoch": 0.6018096799659925,
      "grad_norm": 2.5564963817596436,
      "learning_rate": 9.039508174594464e-05,
      "loss": 2.5817,
      "step": 9910
    },
    {
      "epoch": 0.6024169551223659,
      "grad_norm": 4.278826713562012,
      "learning_rate": 9.03763351417002e-05,
      "loss": 2.6845,
      "step": 9920
    },
    {
      "epoch": 0.6030242302787393,
      "grad_norm": 2.555363178253174,
      "learning_rate": 9.035757220862277e-05,
      "loss": 2.2949,
      "step": 9930
    },
    {
      "epoch": 0.6036315054351127,
      "grad_norm": 2.6103625297546387,
      "learning_rate": 9.033879295430041e-05,
      "loss": 2.5046,
      "step": 9940
    },
    {
      "epoch": 0.6042387805914861,
      "grad_norm": 2.807713508605957,
      "learning_rate": 9.031999738632772e-05,
      "loss": 2.7496,
      "step": 9950
    },
    {
      "epoch": 0.6048460557478593,
      "grad_norm": 2.4578003883361816,
      "learning_rate": 9.030118551230593e-05,
      "loss": 2.5224,
      "step": 9960
    },
    {
      "epoch": 0.6054533309042327,
      "grad_norm": 2.8829402923583984,
      "learning_rate": 9.028235733984286e-05,
      "loss": 2.2572,
      "step": 9970
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.2058815956115723,
      "learning_rate": 9.026351287655294e-05,
      "loss": 2.4009,
      "step": 9980
    },
    {
      "epoch": 0.6066678812169795,
      "grad_norm": 2.3183200359344482,
      "learning_rate": 9.024465213005715e-05,
      "loss": 2.2544,
      "step": 9990
    },
    {
      "epoch": 0.6072751563733527,
      "grad_norm": 2.3965699672698975,
      "learning_rate": 9.022577510798308e-05,
      "loss": 2.4502,
      "step": 10000
    },
    {
      "epoch": 0.6073966114046274,
      "eval_loss": 4.87440299987793,
      "eval_runtime": 2263.904,
      "eval_samples_per_second": 7.274,
      "eval_steps_per_second": 1.819,
      "step": 10002
    },
    {
      "epoch": 0.6078824315297261,
      "grad_norm": 3.3228437900543213,
      "learning_rate": 9.020688181796493e-05,
      "loss": 4.1092,
      "step": 10010
    },
    {
      "epoch": 0.6084897066860995,
      "grad_norm": 5.20359992980957,
      "learning_rate": 9.01879722676434e-05,
      "loss": 3.3527,
      "step": 10020
    },
    {
      "epoch": 0.6090969818424729,
      "grad_norm": 4.2720746994018555,
      "learning_rate": 9.016904646466584e-05,
      "loss": 2.3019,
      "step": 10030
    },
    {
      "epoch": 0.6097042569988462,
      "grad_norm": 2.7660162448883057,
      "learning_rate": 9.015010441668615e-05,
      "loss": 2.5572,
      "step": 10040
    },
    {
      "epoch": 0.6103115321552195,
      "grad_norm": 3.4933412075042725,
      "learning_rate": 9.013114613136478e-05,
      "loss": 2.7544,
      "step": 10050
    },
    {
      "epoch": 0.6109188073115929,
      "grad_norm": 2.4275436401367188,
      "learning_rate": 9.011217161636877e-05,
      "loss": 2.5457,
      "step": 10060
    },
    {
      "epoch": 0.6115260824679662,
      "grad_norm": 3.0177035331726074,
      "learning_rate": 9.009318087937171e-05,
      "loss": 2.7811,
      "step": 10070
    },
    {
      "epoch": 0.6121333576243396,
      "grad_norm": 3.306544542312622,
      "learning_rate": 9.007417392805377e-05,
      "loss": 2.5273,
      "step": 10080
    },
    {
      "epoch": 0.6127406327807129,
      "grad_norm": 1.4508968591690063,
      "learning_rate": 9.005515077010166e-05,
      "loss": 2.8301,
      "step": 10090
    },
    {
      "epoch": 0.6133479079370863,
      "grad_norm": 3.4063172340393066,
      "learning_rate": 9.003611141320863e-05,
      "loss": 2.7901,
      "step": 10100
    },
    {
      "epoch": 0.6139551830934596,
      "grad_norm": 2.848604440689087,
      "learning_rate": 9.001705586507453e-05,
      "loss": 2.7744,
      "step": 10110
    },
    {
      "epoch": 0.614562458249833,
      "grad_norm": 3.2302942276000977,
      "learning_rate": 8.99979841334057e-05,
      "loss": 2.5935,
      "step": 10120
    },
    {
      "epoch": 0.6151697334062064,
      "grad_norm": 2.1525051593780518,
      "learning_rate": 8.997889622591507e-05,
      "loss": 2.3824,
      "step": 10130
    },
    {
      "epoch": 0.6157770085625797,
      "grad_norm": 2.754324197769165,
      "learning_rate": 8.995979215032207e-05,
      "loss": 2.4528,
      "step": 10140
    },
    {
      "epoch": 0.616384283718953,
      "grad_norm": 3.7029967308044434,
      "learning_rate": 8.994067191435274e-05,
      "loss": 2.439,
      "step": 10150
    },
    {
      "epoch": 0.6169915588753264,
      "grad_norm": 2.6139845848083496,
      "learning_rate": 8.992153552573954e-05,
      "loss": 2.6505,
      "step": 10160
    },
    {
      "epoch": 0.6175988340316998,
      "grad_norm": 3.6021318435668945,
      "learning_rate": 8.990238299222159e-05,
      "loss": 2.9317,
      "step": 10170
    },
    {
      "epoch": 0.6182061091880731,
      "grad_norm": 4.154824256896973,
      "learning_rate": 8.988321432154445e-05,
      "loss": 2.3849,
      "step": 10180
    },
    {
      "epoch": 0.6188133843444464,
      "grad_norm": 2.7430953979492188,
      "learning_rate": 8.986402952146025e-05,
      "loss": 2.2439,
      "step": 10190
    },
    {
      "epoch": 0.6194206595008198,
      "grad_norm": 1.666829228401184,
      "learning_rate": 8.98448285997276e-05,
      "loss": 2.191,
      "step": 10200
    },
    {
      "epoch": 0.6200279346571932,
      "grad_norm": 2.0815229415893555,
      "learning_rate": 8.982561156411172e-05,
      "loss": 2.1338,
      "step": 10210
    },
    {
      "epoch": 0.6206352098135666,
      "grad_norm": 4.061728000640869,
      "learning_rate": 8.980637842238421e-05,
      "loss": 2.3692,
      "step": 10220
    },
    {
      "epoch": 0.6212424849699398,
      "grad_norm": 3.6601531505584717,
      "learning_rate": 8.97871291823233e-05,
      "loss": 2.2528,
      "step": 10230
    },
    {
      "epoch": 0.6218497601263132,
      "grad_norm": 3.23398494720459,
      "learning_rate": 8.976786385171368e-05,
      "loss": 2.2825,
      "step": 10240
    },
    {
      "epoch": 0.6224570352826866,
      "grad_norm": 2.1624553203582764,
      "learning_rate": 8.974858243834656e-05,
      "loss": 2.4409,
      "step": 10250
    },
    {
      "epoch": 0.62306431043906,
      "grad_norm": 3.4366209506988525,
      "learning_rate": 8.972928495001967e-05,
      "loss": 2.5289,
      "step": 10260
    },
    {
      "epoch": 0.6236715855954333,
      "grad_norm": 2.3075616359710693,
      "learning_rate": 8.970997139453719e-05,
      "loss": 2.2738,
      "step": 10270
    },
    {
      "epoch": 0.6242788607518066,
      "grad_norm": 3.2994792461395264,
      "learning_rate": 8.969064177970982e-05,
      "loss": 2.4711,
      "step": 10280
    },
    {
      "epoch": 0.62488613590818,
      "grad_norm": 2.4378843307495117,
      "learning_rate": 8.967129611335481e-05,
      "loss": 2.3612,
      "step": 10290
    },
    {
      "epoch": 0.6254934110645534,
      "grad_norm": 2.3643929958343506,
      "learning_rate": 8.965193440329583e-05,
      "loss": 2.3669,
      "step": 10300
    },
    {
      "epoch": 0.6261006862209267,
      "grad_norm": 2.5255088806152344,
      "learning_rate": 8.963255665736305e-05,
      "loss": 2.5143,
      "step": 10310
    },
    {
      "epoch": 0.6267079613773,
      "grad_norm": 3.0703423023223877,
      "learning_rate": 8.961316288339316e-05,
      "loss": 2.5231,
      "step": 10320
    },
    {
      "epoch": 0.6273152365336734,
      "grad_norm": 2.2456793785095215,
      "learning_rate": 8.959375308922932e-05,
      "loss": 2.1943,
      "step": 10330
    },
    {
      "epoch": 0.6279225116900468,
      "grad_norm": 4.914292335510254,
      "learning_rate": 8.957432728272113e-05,
      "loss": 2.6187,
      "step": 10340
    },
    {
      "epoch": 0.6285297868464201,
      "grad_norm": 3.769244432449341,
      "learning_rate": 8.95548854717247e-05,
      "loss": 3.0072,
      "step": 10350
    },
    {
      "epoch": 0.6291370620027935,
      "grad_norm": 5.719825744628906,
      "learning_rate": 8.953542766410263e-05,
      "loss": 2.8444,
      "step": 10360
    },
    {
      "epoch": 0.6297443371591668,
      "grad_norm": 4.365283012390137,
      "learning_rate": 8.951595386772397e-05,
      "loss": 2.5112,
      "step": 10370
    },
    {
      "epoch": 0.6303516123155402,
      "grad_norm": 3.822444200515747,
      "learning_rate": 8.94964640904642e-05,
      "loss": 2.3913,
      "step": 10380
    },
    {
      "epoch": 0.6309588874719135,
      "grad_norm": 4.943605899810791,
      "learning_rate": 8.947695834020532e-05,
      "loss": 2.3712,
      "step": 10390
    },
    {
      "epoch": 0.6315661626282869,
      "grad_norm": 2.2141122817993164,
      "learning_rate": 8.945743662483577e-05,
      "loss": 2.077,
      "step": 10400
    },
    {
      "epoch": 0.6321734377846602,
      "grad_norm": 3.104752779006958,
      "learning_rate": 8.943789895225043e-05,
      "loss": 2.3866,
      "step": 10410
    },
    {
      "epoch": 0.6327807129410336,
      "grad_norm": 3.065980911254883,
      "learning_rate": 8.941834533035064e-05,
      "loss": 2.5169,
      "step": 10420
    },
    {
      "epoch": 0.6333879880974069,
      "grad_norm": 2.950214385986328,
      "learning_rate": 8.93987757670442e-05,
      "loss": 2.8384,
      "step": 10430
    },
    {
      "epoch": 0.6339952632537803,
      "grad_norm": 6.666264057159424,
      "learning_rate": 8.937919027024539e-05,
      "loss": 2.3647,
      "step": 10440
    },
    {
      "epoch": 0.6346025384101537,
      "grad_norm": 2.9136860370635986,
      "learning_rate": 8.935958884787485e-05,
      "loss": 2.3851,
      "step": 10450
    },
    {
      "epoch": 0.635209813566527,
      "grad_norm": 2.5608134269714355,
      "learning_rate": 8.933997150785971e-05,
      "loss": 2.613,
      "step": 10460
    },
    {
      "epoch": 0.6358170887229003,
      "grad_norm": 3.3344039916992188,
      "learning_rate": 8.932033825813356e-05,
      "loss": 2.0891,
      "step": 10470
    },
    {
      "epoch": 0.6364243638792737,
      "grad_norm": 1.8463948965072632,
      "learning_rate": 8.930068910663638e-05,
      "loss": 2.3697,
      "step": 10480
    },
    {
      "epoch": 0.6370316390356471,
      "grad_norm": 2.3469629287719727,
      "learning_rate": 8.928102406131463e-05,
      "loss": 2.2694,
      "step": 10490
    },
    {
      "epoch": 0.6376389141920205,
      "grad_norm": 2.083887815475464,
      "learning_rate": 8.926134313012112e-05,
      "loss": 2.3239,
      "step": 10500
    },
    {
      "epoch": 0.6382461893483937,
      "grad_norm": 2.035071611404419,
      "learning_rate": 8.924164632101518e-05,
      "loss": 2.5366,
      "step": 10510
    },
    {
      "epoch": 0.6388534645047671,
      "grad_norm": 5.150882720947266,
      "learning_rate": 8.922193364196245e-05,
      "loss": 2.6236,
      "step": 10520
    },
    {
      "epoch": 0.6394607396611405,
      "grad_norm": 2.8706300258636475,
      "learning_rate": 8.920220510093511e-05,
      "loss": 2.2694,
      "step": 10530
    },
    {
      "epoch": 0.6400680148175139,
      "grad_norm": 1.983695149421692,
      "learning_rate": 8.91824607059117e-05,
      "loss": 2.2969,
      "step": 10540
    },
    {
      "epoch": 0.6406752899738871,
      "grad_norm": 1.5155842304229736,
      "learning_rate": 8.916270046487711e-05,
      "loss": 2.132,
      "step": 10550
    },
    {
      "epoch": 0.6412825651302605,
      "grad_norm": 1.8717682361602783,
      "learning_rate": 8.914292438582275e-05,
      "loss": 2.812,
      "step": 10560
    },
    {
      "epoch": 0.6418898402866339,
      "grad_norm": 2.2809364795684814,
      "learning_rate": 8.912313247674636e-05,
      "loss": 2.2388,
      "step": 10570
    },
    {
      "epoch": 0.6424971154430072,
      "grad_norm": 2.253598928451538,
      "learning_rate": 8.91033247456521e-05,
      "loss": 2.9213,
      "step": 10580
    },
    {
      "epoch": 0.6431043905993806,
      "grad_norm": 3.0436055660247803,
      "learning_rate": 8.908350120055056e-05,
      "loss": 2.4018,
      "step": 10590
    },
    {
      "epoch": 0.6437116657557539,
      "grad_norm": 2.1712491512298584,
      "learning_rate": 8.906366184945865e-05,
      "loss": 2.4385,
      "step": 10600
    },
    {
      "epoch": 0.6443189409121273,
      "grad_norm": 3.192767381668091,
      "learning_rate": 8.904380670039975e-05,
      "loss": 2.7219,
      "step": 10610
    },
    {
      "epoch": 0.6449262160685006,
      "grad_norm": 3.2504477500915527,
      "learning_rate": 8.90239357614036e-05,
      "loss": 2.7263,
      "step": 10620
    },
    {
      "epoch": 0.645533491224874,
      "grad_norm": 4.368812084197998,
      "learning_rate": 8.900404904050632e-05,
      "loss": 2.5837,
      "step": 10630
    },
    {
      "epoch": 0.6461407663812473,
      "grad_norm": 2.9172627925872803,
      "learning_rate": 8.89841465457504e-05,
      "loss": 2.506,
      "step": 10640
    },
    {
      "epoch": 0.6467480415376207,
      "grad_norm": 2.7561259269714355,
      "learning_rate": 8.896422828518475e-05,
      "loss": 2.5347,
      "step": 10650
    },
    {
      "epoch": 0.647355316693994,
      "grad_norm": 3.685168504714966,
      "learning_rate": 8.894429426686461e-05,
      "loss": 2.7422,
      "step": 10660
    },
    {
      "epoch": 0.6479625918503674,
      "grad_norm": 3.364013195037842,
      "learning_rate": 8.892434449885164e-05,
      "loss": 2.7811,
      "step": 10670
    },
    {
      "epoch": 0.6485698670067408,
      "grad_norm": 2.8424785137176514,
      "learning_rate": 8.890437898921382e-05,
      "loss": 2.9497,
      "step": 10680
    },
    {
      "epoch": 0.6491771421631141,
      "grad_norm": 2.6483638286590576,
      "learning_rate": 8.888439774602554e-05,
      "loss": 2.9663,
      "step": 10690
    },
    {
      "epoch": 0.6497844173194874,
      "grad_norm": 3.600252866744995,
      "learning_rate": 8.886440077736752e-05,
      "loss": 2.7403,
      "step": 10700
    },
    {
      "epoch": 0.6503916924758608,
      "grad_norm": 3.467076539993286,
      "learning_rate": 8.884438809132685e-05,
      "loss": 2.4715,
      "step": 10710
    },
    {
      "epoch": 0.6509989676322342,
      "grad_norm": 2.749890089035034,
      "learning_rate": 8.882435969599699e-05,
      "loss": 2.6199,
      "step": 10720
    },
    {
      "epoch": 0.6516062427886076,
      "grad_norm": 4.895919322967529,
      "learning_rate": 8.880431559947774e-05,
      "loss": 2.8516,
      "step": 10730
    },
    {
      "epoch": 0.6522135179449808,
      "grad_norm": 4.225321292877197,
      "learning_rate": 8.878425580987524e-05,
      "loss": 2.5933,
      "step": 10740
    },
    {
      "epoch": 0.6528207931013542,
      "grad_norm": 3.0961122512817383,
      "learning_rate": 8.876418033530201e-05,
      "loss": 2.4141,
      "step": 10750
    },
    {
      "epoch": 0.6534280682577276,
      "grad_norm": 2.662951946258545,
      "learning_rate": 8.874408918387685e-05,
      "loss": 2.1653,
      "step": 10760
    },
    {
      "epoch": 0.654035343414101,
      "grad_norm": 1.5607211589813232,
      "learning_rate": 8.872398236372499e-05,
      "loss": 2.0971,
      "step": 10770
    },
    {
      "epoch": 0.6546426185704742,
      "grad_norm": 1.6043403148651123,
      "learning_rate": 8.870385988297794e-05,
      "loss": 2.3463,
      "step": 10780
    },
    {
      "epoch": 0.6552498937268476,
      "grad_norm": 3.468993902206421,
      "learning_rate": 8.868372174977352e-05,
      "loss": 2.6878,
      "step": 10790
    },
    {
      "epoch": 0.655857168883221,
      "grad_norm": 2.711749792098999,
      "learning_rate": 8.866356797225594e-05,
      "loss": 2.2417,
      "step": 10800
    },
    {
      "epoch": 0.6564644440395944,
      "grad_norm": 1.9240520000457764,
      "learning_rate": 8.86433985585757e-05,
      "loss": 2.1399,
      "step": 10810
    },
    {
      "epoch": 0.6570717191959677,
      "grad_norm": 2.441174030303955,
      "learning_rate": 8.862321351688963e-05,
      "loss": 2.5713,
      "step": 10820
    },
    {
      "epoch": 0.657678994352341,
      "grad_norm": 5.838700771331787,
      "learning_rate": 8.86030128553609e-05,
      "loss": 2.4965,
      "step": 10830
    },
    {
      "epoch": 0.6582862695087144,
      "grad_norm": 3.846709728240967,
      "learning_rate": 8.858279658215895e-05,
      "loss": 2.4066,
      "step": 10840
    },
    {
      "epoch": 0.6588935446650878,
      "grad_norm": 3.831381320953369,
      "learning_rate": 8.856256470545957e-05,
      "loss": 2.5227,
      "step": 10850
    },
    {
      "epoch": 0.6595008198214611,
      "grad_norm": 3.528681993484497,
      "learning_rate": 8.854231723344487e-05,
      "loss": 2.6556,
      "step": 10860
    },
    {
      "epoch": 0.6601080949778344,
      "grad_norm": 2.8658411502838135,
      "learning_rate": 8.852205417430323e-05,
      "loss": 2.3905,
      "step": 10870
    },
    {
      "epoch": 0.6607153701342078,
      "grad_norm": 2.386774778366089,
      "learning_rate": 8.850177553622938e-05,
      "loss": 2.6274,
      "step": 10880
    },
    {
      "epoch": 0.6613226452905812,
      "grad_norm": 3.0485377311706543,
      "learning_rate": 8.848148132742431e-05,
      "loss": 2.587,
      "step": 10890
    },
    {
      "epoch": 0.6619299204469545,
      "grad_norm": 1.6677119731903076,
      "learning_rate": 8.846117155609532e-05,
      "loss": 2.3033,
      "step": 10900
    },
    {
      "epoch": 0.6625371956033279,
      "grad_norm": 1.8614333868026733,
      "learning_rate": 8.844084623045599e-05,
      "loss": 2.2457,
      "step": 10910
    },
    {
      "epoch": 0.6631444707597012,
      "grad_norm": 3.2817835807800293,
      "learning_rate": 8.842050535872623e-05,
      "loss": 2.567,
      "step": 10920
    },
    {
      "epoch": 0.6637517459160746,
      "grad_norm": 2.6699044704437256,
      "learning_rate": 8.84001489491322e-05,
      "loss": 2.4098,
      "step": 10930
    },
    {
      "epoch": 0.6643590210724479,
      "grad_norm": 2.9969699382781982,
      "learning_rate": 8.837977700990636e-05,
      "loss": 2.6021,
      "step": 10940
    },
    {
      "epoch": 0.6649662962288213,
      "grad_norm": 2.897136688232422,
      "learning_rate": 8.835938954928744e-05,
      "loss": 2.7893,
      "step": 10950
    },
    {
      "epoch": 0.6655735713851947,
      "grad_norm": 3.533891439437866,
      "learning_rate": 8.833898657552046e-05,
      "loss": 2.6963,
      "step": 10960
    },
    {
      "epoch": 0.666180846541568,
      "grad_norm": 3.979959726333618,
      "learning_rate": 8.831856809685672e-05,
      "loss": 3.0215,
      "step": 10970
    },
    {
      "epoch": 0.6667881216979413,
      "grad_norm": 3.1960015296936035,
      "learning_rate": 8.829813412155375e-05,
      "loss": 2.5707,
      "step": 10980
    },
    {
      "epoch": 0.6673953968543147,
      "grad_norm": 3.5149619579315186,
      "learning_rate": 8.82776846578754e-05,
      "loss": 2.3074,
      "step": 10990
    },
    {
      "epoch": 0.6680026720106881,
      "grad_norm": 1.994879961013794,
      "learning_rate": 8.825721971409173e-05,
      "loss": 2.7095,
      "step": 11000
    },
    {
      "epoch": 0.6686099471670613,
      "grad_norm": 2.587444543838501,
      "learning_rate": 8.823673929847914e-05,
      "loss": 2.2443,
      "step": 11010
    },
    {
      "epoch": 0.6692172223234347,
      "grad_norm": 2.088688373565674,
      "learning_rate": 8.821624341932018e-05,
      "loss": 2.2242,
      "step": 11020
    },
    {
      "epoch": 0.6698244974798081,
      "grad_norm": 1.699973225593567,
      "learning_rate": 8.819573208490373e-05,
      "loss": 2.585,
      "step": 11030
    },
    {
      "epoch": 0.6704317726361815,
      "grad_norm": 3.000051259994507,
      "learning_rate": 8.817520530352491e-05,
      "loss": 2.5736,
      "step": 11040
    },
    {
      "epoch": 0.6710390477925549,
      "grad_norm": 6.5334906578063965,
      "learning_rate": 8.815466308348508e-05,
      "loss": 2.318,
      "step": 11050
    },
    {
      "epoch": 0.6716463229489281,
      "grad_norm": 5.0103864669799805,
      "learning_rate": 8.813410543309184e-05,
      "loss": 2.256,
      "step": 11060
    },
    {
      "epoch": 0.6722535981053015,
      "grad_norm": 2.4965860843658447,
      "learning_rate": 8.8113532360659e-05,
      "loss": 2.1051,
      "step": 11070
    },
    {
      "epoch": 0.6728608732616749,
      "grad_norm": 4.164797782897949,
      "learning_rate": 8.809294387450668e-05,
      "loss": 2.7265,
      "step": 11080
    },
    {
      "epoch": 0.6734681484180483,
      "grad_norm": 2.8441388607025146,
      "learning_rate": 8.807233998296117e-05,
      "loss": 2.6916,
      "step": 11090
    },
    {
      "epoch": 0.6740754235744215,
      "grad_norm": 3.308903455734253,
      "learning_rate": 8.805172069435501e-05,
      "loss": 2.7707,
      "step": 11100
    },
    {
      "epoch": 0.6746826987307949,
      "grad_norm": 2.4701712131500244,
      "learning_rate": 8.803108601702698e-05,
      "loss": 2.3197,
      "step": 11110
    },
    {
      "epoch": 0.6752899738871683,
      "grad_norm": 2.2833468914031982,
      "learning_rate": 8.801043595932206e-05,
      "loss": 2.3151,
      "step": 11120
    },
    {
      "epoch": 0.6758972490435416,
      "grad_norm": 3.1311230659484863,
      "learning_rate": 8.798977052959148e-05,
      "loss": 2.1687,
      "step": 11130
    },
    {
      "epoch": 0.676504524199915,
      "grad_norm": 0.935635507106781,
      "learning_rate": 8.796908973619265e-05,
      "loss": 1.9931,
      "step": 11140
    },
    {
      "epoch": 0.6771117993562883,
      "grad_norm": 2.280280828475952,
      "learning_rate": 8.794839358748923e-05,
      "loss": 2.3454,
      "step": 11150
    },
    {
      "epoch": 0.6777190745126617,
      "grad_norm": 2.8030853271484375,
      "learning_rate": 8.792768209185105e-05,
      "loss": 2.6203,
      "step": 11160
    },
    {
      "epoch": 0.678326349669035,
      "grad_norm": 3.877730131149292,
      "learning_rate": 8.790695525765418e-05,
      "loss": 2.1577,
      "step": 11170
    },
    {
      "epoch": 0.6789336248254084,
      "grad_norm": 2.007049083709717,
      "learning_rate": 8.788621309328087e-05,
      "loss": 2.1643,
      "step": 11180
    },
    {
      "epoch": 0.6795408999817818,
      "grad_norm": 4.5134992599487305,
      "learning_rate": 8.786545560711962e-05,
      "loss": 2.4497,
      "step": 11190
    },
    {
      "epoch": 0.6801481751381551,
      "grad_norm": 3.2414166927337646,
      "learning_rate": 8.784468280756505e-05,
      "loss": 2.3829,
      "step": 11200
    },
    {
      "epoch": 0.6807554502945284,
      "grad_norm": 2.5370278358459473,
      "learning_rate": 8.782389470301803e-05,
      "loss": 2.7114,
      "step": 11210
    },
    {
      "epoch": 0.6813627254509018,
      "grad_norm": 6.506296634674072,
      "learning_rate": 8.780309130188558e-05,
      "loss": 2.389,
      "step": 11220
    },
    {
      "epoch": 0.6819700006072752,
      "grad_norm": 3.7199716567993164,
      "learning_rate": 8.778227261258095e-05,
      "loss": 2.391,
      "step": 11230
    },
    {
      "epoch": 0.6825772757636485,
      "grad_norm": 2.9331204891204834,
      "learning_rate": 8.776143864352352e-05,
      "loss": 2.5268,
      "step": 11240
    },
    {
      "epoch": 0.6831845509200218,
      "grad_norm": 4.080738067626953,
      "learning_rate": 8.774058940313894e-05,
      "loss": 2.2572,
      "step": 11250
    },
    {
      "epoch": 0.6837918260763952,
      "grad_norm": 2.6593098640441895,
      "learning_rate": 8.771972489985891e-05,
      "loss": 3.0673,
      "step": 11260
    },
    {
      "epoch": 0.6843991012327686,
      "grad_norm": 3.2573087215423584,
      "learning_rate": 8.769884514212139e-05,
      "loss": 2.7258,
      "step": 11270
    },
    {
      "epoch": 0.685006376389142,
      "grad_norm": 3.442018747329712,
      "learning_rate": 8.767795013837048e-05,
      "loss": 2.46,
      "step": 11280
    },
    {
      "epoch": 0.6856136515455152,
      "grad_norm": 2.887796640396118,
      "learning_rate": 8.765703989705647e-05,
      "loss": 2.5344,
      "step": 11290
    },
    {
      "epoch": 0.6862209267018886,
      "grad_norm": 4.760140895843506,
      "learning_rate": 8.76361144266358e-05,
      "loss": 2.0413,
      "step": 11300
    },
    {
      "epoch": 0.686828201858262,
      "grad_norm": 1.7822891473770142,
      "learning_rate": 8.761517373557102e-05,
      "loss": 2.5357,
      "step": 11310
    },
    {
      "epoch": 0.6874354770146354,
      "grad_norm": 2.2745306491851807,
      "learning_rate": 8.759421783233092e-05,
      "loss": 2.4792,
      "step": 11320
    },
    {
      "epoch": 0.6880427521710086,
      "grad_norm": 2.810046434402466,
      "learning_rate": 8.757324672539039e-05,
      "loss": 2.7805,
      "step": 11330
    },
    {
      "epoch": 0.688650027327382,
      "grad_norm": 2.6413328647613525,
      "learning_rate": 8.755226042323048e-05,
      "loss": 2.5531,
      "step": 11340
    },
    {
      "epoch": 0.6892573024837554,
      "grad_norm": 1.4079152345657349,
      "learning_rate": 8.753125893433838e-05,
      "loss": 1.9894,
      "step": 11350
    },
    {
      "epoch": 0.6898645776401288,
      "grad_norm": 1.0995352268218994,
      "learning_rate": 8.751024226720742e-05,
      "loss": 1.7711,
      "step": 11360
    },
    {
      "epoch": 0.6904718527965021,
      "grad_norm": 1.9425890445709229,
      "learning_rate": 8.748921043033708e-05,
      "loss": 2.2423,
      "step": 11370
    },
    {
      "epoch": 0.6910791279528754,
      "grad_norm": 1.9972096681594849,
      "learning_rate": 8.746816343223298e-05,
      "loss": 2.6469,
      "step": 11380
    },
    {
      "epoch": 0.6916864031092488,
      "grad_norm": 3.4727747440338135,
      "learning_rate": 8.744710128140688e-05,
      "loss": 2.4529,
      "step": 11390
    },
    {
      "epoch": 0.6922936782656222,
      "grad_norm": 2.4167869091033936,
      "learning_rate": 8.74260239863766e-05,
      "loss": 2.5715,
      "step": 11400
    },
    {
      "epoch": 0.6929009534219955,
      "grad_norm": 3.3651492595672607,
      "learning_rate": 8.740493155566616e-05,
      "loss": 2.7494,
      "step": 11410
    },
    {
      "epoch": 0.6935082285783689,
      "grad_norm": 2.737031936645508,
      "learning_rate": 8.738382399780567e-05,
      "loss": 2.6008,
      "step": 11420
    },
    {
      "epoch": 0.6941155037347422,
      "grad_norm": 3.416959285736084,
      "learning_rate": 8.736270132133138e-05,
      "loss": 2.8297,
      "step": 11430
    },
    {
      "epoch": 0.6947227788911156,
      "grad_norm": 3.342590808868408,
      "learning_rate": 8.734156353478561e-05,
      "loss": 2.2218,
      "step": 11440
    },
    {
      "epoch": 0.6953300540474889,
      "grad_norm": 2.9010980129241943,
      "learning_rate": 8.732041064671684e-05,
      "loss": 2.4401,
      "step": 11450
    },
    {
      "epoch": 0.6959373292038623,
      "grad_norm": 1.8994362354278564,
      "learning_rate": 8.72992426656796e-05,
      "loss": 2.1499,
      "step": 11460
    },
    {
      "epoch": 0.6965446043602356,
      "grad_norm": 1.4053677320480347,
      "learning_rate": 8.72780596002346e-05,
      "loss": 2.2814,
      "step": 11470
    },
    {
      "epoch": 0.697151879516609,
      "grad_norm": 2.6589019298553467,
      "learning_rate": 8.72568614589486e-05,
      "loss": 2.5686,
      "step": 11480
    },
    {
      "epoch": 0.6977591546729823,
      "grad_norm": 2.1260557174682617,
      "learning_rate": 8.723564825039446e-05,
      "loss": 2.4583,
      "step": 11490
    },
    {
      "epoch": 0.6983664298293557,
      "grad_norm": 3.538309335708618,
      "learning_rate": 8.721441998315112e-05,
      "loss": 2.3324,
      "step": 11500
    },
    {
      "epoch": 0.6989737049857291,
      "grad_norm": 2.3715875148773193,
      "learning_rate": 8.719317666580365e-05,
      "loss": 2.1055,
      "step": 11510
    },
    {
      "epoch": 0.6995809801421023,
      "grad_norm": 2.161717653274536,
      "learning_rate": 8.71719183069432e-05,
      "loss": 2.5824,
      "step": 11520
    },
    {
      "epoch": 0.7001882552984757,
      "grad_norm": 2.954040288925171,
      "learning_rate": 8.715064491516696e-05,
      "loss": 3.0389,
      "step": 11530
    },
    {
      "epoch": 0.7007955304548491,
      "grad_norm": 3.533721923828125,
      "learning_rate": 8.712935649907824e-05,
      "loss": 2.3634,
      "step": 11540
    },
    {
      "epoch": 0.7014028056112225,
      "grad_norm": 2.287060022354126,
      "learning_rate": 8.710805306728641e-05,
      "loss": 2.6555,
      "step": 11550
    },
    {
      "epoch": 0.7020100807675957,
      "grad_norm": 2.368189573287964,
      "learning_rate": 8.708673462840693e-05,
      "loss": 2.3734,
      "step": 11560
    },
    {
      "epoch": 0.7026173559239691,
      "grad_norm": 1.9973392486572266,
      "learning_rate": 8.70654011910613e-05,
      "loss": 2.7784,
      "step": 11570
    },
    {
      "epoch": 0.7032246310803425,
      "grad_norm": 3.813711643218994,
      "learning_rate": 8.704405276387713e-05,
      "loss": 2.8612,
      "step": 11580
    },
    {
      "epoch": 0.7038319062367159,
      "grad_norm": 3.54384183883667,
      "learning_rate": 8.702268935548802e-05,
      "loss": 2.7681,
      "step": 11590
    },
    {
      "epoch": 0.7044391813930893,
      "grad_norm": 2.9659247398376465,
      "learning_rate": 8.70013109745337e-05,
      "loss": 2.2178,
      "step": 11600
    },
    {
      "epoch": 0.7050464565494625,
      "grad_norm": 1.2802150249481201,
      "learning_rate": 8.697991762965993e-05,
      "loss": 2.0925,
      "step": 11610
    },
    {
      "epoch": 0.7056537317058359,
      "grad_norm": 2.105222463607788,
      "learning_rate": 8.695850932951852e-05,
      "loss": 2.7548,
      "step": 11620
    },
    {
      "epoch": 0.7062610068622093,
      "grad_norm": 4.5314106941223145,
      "learning_rate": 8.693708608276732e-05,
      "loss": 2.8701,
      "step": 11630
    },
    {
      "epoch": 0.7068682820185826,
      "grad_norm": 4.354351043701172,
      "learning_rate": 8.691564789807023e-05,
      "loss": 2.4745,
      "step": 11640
    },
    {
      "epoch": 0.707475557174956,
      "grad_norm": 3.265119791030884,
      "learning_rate": 8.689419478409719e-05,
      "loss": 2.911,
      "step": 11650
    },
    {
      "epoch": 0.7080828323313293,
      "grad_norm": 4.033328056335449,
      "learning_rate": 8.68727267495242e-05,
      "loss": 2.6005,
      "step": 11660
    },
    {
      "epoch": 0.7086901074877027,
      "grad_norm": 4.0294575691223145,
      "learning_rate": 8.685124380303325e-05,
      "loss": 2.468,
      "step": 11670
    },
    {
      "epoch": 0.709297382644076,
      "grad_norm": 3.4301838874816895,
      "learning_rate": 8.682974595331242e-05,
      "loss": 2.5571,
      "step": 11680
    },
    {
      "epoch": 0.7099046578004494,
      "grad_norm": 2.7208755016326904,
      "learning_rate": 8.680823320905573e-05,
      "loss": 2.4373,
      "step": 11690
    },
    {
      "epoch": 0.7105119329568227,
      "grad_norm": 1.932278037071228,
      "learning_rate": 8.678670557896332e-05,
      "loss": 2.2539,
      "step": 11700
    },
    {
      "epoch": 0.7111192081131961,
      "grad_norm": 1.8511635065078735,
      "learning_rate": 8.676516307174129e-05,
      "loss": 2.3909,
      "step": 11710
    },
    {
      "epoch": 0.7117264832695694,
      "grad_norm": 3.708554983139038,
      "learning_rate": 8.674360569610179e-05,
      "loss": 2.7991,
      "step": 11720
    },
    {
      "epoch": 0.7123337584259428,
      "grad_norm": 3.4265975952148438,
      "learning_rate": 8.672203346076296e-05,
      "loss": 2.6793,
      "step": 11730
    },
    {
      "epoch": 0.7129410335823162,
      "grad_norm": 2.8297042846679688,
      "learning_rate": 8.670044637444891e-05,
      "loss": 2.6672,
      "step": 11740
    },
    {
      "epoch": 0.7135483087386895,
      "grad_norm": 1.712507963180542,
      "learning_rate": 8.667884444588988e-05,
      "loss": 2.3607,
      "step": 11750
    },
    {
      "epoch": 0.7141555838950628,
      "grad_norm": 1.6748040914535522,
      "learning_rate": 8.665722768382199e-05,
      "loss": 2.2089,
      "step": 11760
    },
    {
      "epoch": 0.7147628590514362,
      "grad_norm": 1.7342253923416138,
      "learning_rate": 8.663559609698739e-05,
      "loss": 2.584,
      "step": 11770
    },
    {
      "epoch": 0.7153701342078096,
      "grad_norm": 3.874382972717285,
      "learning_rate": 8.661394969413427e-05,
      "loss": 2.6211,
      "step": 11780
    },
    {
      "epoch": 0.7159774093641829,
      "grad_norm": 3.4614226818084717,
      "learning_rate": 8.659228848401675e-05,
      "loss": 3.0633,
      "step": 11790
    },
    {
      "epoch": 0.7165846845205562,
      "grad_norm": 3.6878087520599365,
      "learning_rate": 8.657061247539499e-05,
      "loss": 2.7167,
      "step": 11800
    },
    {
      "epoch": 0.7171919596769296,
      "grad_norm": 2.2905080318450928,
      "learning_rate": 8.65489216770351e-05,
      "loss": 2.4906,
      "step": 11810
    },
    {
      "epoch": 0.717799234833303,
      "grad_norm": 2.7152750492095947,
      "learning_rate": 8.65272160977092e-05,
      "loss": 2.4945,
      "step": 11820
    },
    {
      "epoch": 0.7184065099896764,
      "grad_norm": 2.367062568664551,
      "learning_rate": 8.650549574619537e-05,
      "loss": 2.7214,
      "step": 11830
    },
    {
      "epoch": 0.7190137851460496,
      "grad_norm": 2.510831594467163,
      "learning_rate": 8.648376063127765e-05,
      "loss": 2.5164,
      "step": 11840
    },
    {
      "epoch": 0.719621060302423,
      "grad_norm": 1.6286613941192627,
      "learning_rate": 8.646201076174608e-05,
      "loss": 2.1187,
      "step": 11850
    },
    {
      "epoch": 0.7202283354587964,
      "grad_norm": 1.5965510606765747,
      "learning_rate": 8.644024614639665e-05,
      "loss": 2.2467,
      "step": 11860
    },
    {
      "epoch": 0.7208356106151698,
      "grad_norm": 2.3933534622192383,
      "learning_rate": 8.641846679403131e-05,
      "loss": 2.6519,
      "step": 11870
    },
    {
      "epoch": 0.7214428857715431,
      "grad_norm": 2.3515665531158447,
      "learning_rate": 8.639667271345798e-05,
      "loss": 2.5174,
      "step": 11880
    },
    {
      "epoch": 0.7220501609279164,
      "grad_norm": 3.7553038597106934,
      "learning_rate": 8.637486391349055e-05,
      "loss": 2.5086,
      "step": 11890
    },
    {
      "epoch": 0.7226574360842898,
      "grad_norm": 2.82210636138916,
      "learning_rate": 8.635304040294883e-05,
      "loss": 2.6263,
      "step": 11900
    },
    {
      "epoch": 0.7232647112406632,
      "grad_norm": 5.5038228034973145,
      "learning_rate": 8.63312021906586e-05,
      "loss": 2.5174,
      "step": 11910
    },
    {
      "epoch": 0.7238719863970365,
      "grad_norm": 2.410322427749634,
      "learning_rate": 8.630934928545156e-05,
      "loss": 2.7475,
      "step": 11920
    },
    {
      "epoch": 0.7244792615534098,
      "grad_norm": 4.554124355316162,
      "learning_rate": 8.62874816961654e-05,
      "loss": 2.549,
      "step": 11930
    },
    {
      "epoch": 0.7250865367097832,
      "grad_norm": 2.554035186767578,
      "learning_rate": 8.626559943164371e-05,
      "loss": 2.2746,
      "step": 11940
    },
    {
      "epoch": 0.7256938118661566,
      "grad_norm": 2.4692511558532715,
      "learning_rate": 8.624370250073606e-05,
      "loss": 2.4697,
      "step": 11950
    },
    {
      "epoch": 0.7263010870225299,
      "grad_norm": 4.0665388107299805,
      "learning_rate": 8.622179091229785e-05,
      "loss": 2.9763,
      "step": 11960
    },
    {
      "epoch": 0.7269083621789033,
      "grad_norm": 3.6282260417938232,
      "learning_rate": 8.619986467519052e-05,
      "loss": 2.6488,
      "step": 11970
    },
    {
      "epoch": 0.7275156373352766,
      "grad_norm": 2.9815902709960938,
      "learning_rate": 8.61779237982814e-05,
      "loss": 2.5356,
      "step": 11980
    },
    {
      "epoch": 0.72812291249165,
      "grad_norm": 2.5579867362976074,
      "learning_rate": 8.615596829044371e-05,
      "loss": 2.5631,
      "step": 11990
    },
    {
      "epoch": 0.7287301876480233,
      "grad_norm": 2.2907872200012207,
      "learning_rate": 8.613399816055658e-05,
      "loss": 2.7221,
      "step": 12000
    },
    {
      "epoch": 0.7293374628043967,
      "grad_norm": 4.127490520477295,
      "learning_rate": 8.611201341750514e-05,
      "loss": 2.4702,
      "step": 12010
    },
    {
      "epoch": 0.72994473796077,
      "grad_norm": 2.083357572555542,
      "learning_rate": 8.609001407018032e-05,
      "loss": 2.301,
      "step": 12020
    },
    {
      "epoch": 0.7305520131171434,
      "grad_norm": 2.482703924179077,
      "learning_rate": 8.606800012747904e-05,
      "loss": 2.3263,
      "step": 12030
    },
    {
      "epoch": 0.7311592882735167,
      "grad_norm": 2.9380152225494385,
      "learning_rate": 8.604597159830407e-05,
      "loss": 2.3825,
      "step": 12040
    },
    {
      "epoch": 0.7317665634298901,
      "grad_norm": 3.2425944805145264,
      "learning_rate": 8.60239284915641e-05,
      "loss": 2.5301,
      "step": 12050
    },
    {
      "epoch": 0.7323738385862635,
      "grad_norm": 2.3856546878814697,
      "learning_rate": 8.600187081617372e-05,
      "loss": 2.3959,
      "step": 12060
    },
    {
      "epoch": 0.7329811137426367,
      "grad_norm": 1.9480715990066528,
      "learning_rate": 8.59797985810534e-05,
      "loss": 2.5315,
      "step": 12070
    },
    {
      "epoch": 0.7335883888990101,
      "grad_norm": 4.169482707977295,
      "learning_rate": 8.59577117951295e-05,
      "loss": 2.5322,
      "step": 12080
    },
    {
      "epoch": 0.7341956640553835,
      "grad_norm": 3.9851677417755127,
      "learning_rate": 8.593561046733429e-05,
      "loss": 2.905,
      "step": 12090
    },
    {
      "epoch": 0.7348029392117569,
      "grad_norm": 2.3589980602264404,
      "learning_rate": 8.591349460660586e-05,
      "loss": 2.7986,
      "step": 12100
    },
    {
      "epoch": 0.7354102143681303,
      "grad_norm": 4.960108280181885,
      "learning_rate": 8.589136422188825e-05,
      "loss": 2.9707,
      "step": 12110
    },
    {
      "epoch": 0.7360174895245035,
      "grad_norm": 3.9063751697540283,
      "learning_rate": 8.586921932213133e-05,
      "loss": 2.3679,
      "step": 12120
    },
    {
      "epoch": 0.7366247646808769,
      "grad_norm": 2.3410685062408447,
      "learning_rate": 8.584705991629085e-05,
      "loss": 2.2307,
      "step": 12130
    },
    {
      "epoch": 0.7372320398372503,
      "grad_norm": 3.320397138595581,
      "learning_rate": 8.582488601332842e-05,
      "loss": 2.7699,
      "step": 12140
    },
    {
      "epoch": 0.7378393149936237,
      "grad_norm": 3.449104070663452,
      "learning_rate": 8.580269762221152e-05,
      "loss": 2.9865,
      "step": 12150
    },
    {
      "epoch": 0.7384465901499969,
      "grad_norm": 4.642445087432861,
      "learning_rate": 8.578049475191349e-05,
      "loss": 2.7348,
      "step": 12160
    },
    {
      "epoch": 0.7390538653063703,
      "grad_norm": 2.631683826446533,
      "learning_rate": 8.575827741141356e-05,
      "loss": 2.3757,
      "step": 12170
    },
    {
      "epoch": 0.7396611404627437,
      "grad_norm": 3.2988593578338623,
      "learning_rate": 8.573604560969672e-05,
      "loss": 2.7262,
      "step": 12180
    },
    {
      "epoch": 0.740268415619117,
      "grad_norm": 3.3551158905029297,
      "learning_rate": 8.571379935575388e-05,
      "loss": 2.5552,
      "step": 12190
    },
    {
      "epoch": 0.7408756907754904,
      "grad_norm": 2.020345687866211,
      "learning_rate": 8.56915386585818e-05,
      "loss": 2.1725,
      "step": 12200
    },
    {
      "epoch": 0.7414829659318637,
      "grad_norm": 2.195716142654419,
      "learning_rate": 8.566926352718305e-05,
      "loss": 2.5121,
      "step": 12210
    },
    {
      "epoch": 0.7420902410882371,
      "grad_norm": 2.804353952407837,
      "learning_rate": 8.564697397056605e-05,
      "loss": 2.8907,
      "step": 12220
    },
    {
      "epoch": 0.7426975162446104,
      "grad_norm": 3.9870293140411377,
      "learning_rate": 8.562466999774503e-05,
      "loss": 2.6383,
      "step": 12230
    },
    {
      "epoch": 0.7433047914009838,
      "grad_norm": 2.4484710693359375,
      "learning_rate": 8.56023516177401e-05,
      "loss": 2.8208,
      "step": 12240
    },
    {
      "epoch": 0.7439120665573571,
      "grad_norm": 2.685448169708252,
      "learning_rate": 8.558001883957717e-05,
      "loss": 2.7254,
      "step": 12250
    },
    {
      "epoch": 0.7445193417137305,
      "grad_norm": 2.2661547660827637,
      "learning_rate": 8.555767167228796e-05,
      "loss": 2.4185,
      "step": 12260
    },
    {
      "epoch": 0.7451266168701038,
      "grad_norm": 2.602473258972168,
      "learning_rate": 8.553531012491e-05,
      "loss": 2.5444,
      "step": 12270
    },
    {
      "epoch": 0.7457338920264772,
      "grad_norm": 2.4099934101104736,
      "learning_rate": 8.55129342064867e-05,
      "loss": 2.1561,
      "step": 12280
    },
    {
      "epoch": 0.7463411671828506,
      "grad_norm": 1.9889905452728271,
      "learning_rate": 8.549054392606719e-05,
      "loss": 2.2029,
      "step": 12290
    },
    {
      "epoch": 0.7469484423392239,
      "grad_norm": 1.427160620689392,
      "learning_rate": 8.54681392927065e-05,
      "loss": 2.2326,
      "step": 12300
    },
    {
      "epoch": 0.7475557174955972,
      "grad_norm": 1.6829954385757446,
      "learning_rate": 8.544572031546539e-05,
      "loss": 2.1735,
      "step": 12310
    },
    {
      "epoch": 0.7481629926519706,
      "grad_norm": 2.298722505569458,
      "learning_rate": 8.542328700341046e-05,
      "loss": 2.3039,
      "step": 12320
    },
    {
      "epoch": 0.748770267808344,
      "grad_norm": 2.367161989212036,
      "learning_rate": 8.54008393656141e-05,
      "loss": 2.4927,
      "step": 12330
    },
    {
      "epoch": 0.7493775429647173,
      "grad_norm": 1.292523741722107,
      "learning_rate": 8.537837741115449e-05,
      "loss": 2.3177,
      "step": 12340
    },
    {
      "epoch": 0.7499848181210906,
      "grad_norm": 1.629451870918274,
      "learning_rate": 8.535590114911561e-05,
      "loss": 2.0671,
      "step": 12350
    },
    {
      "epoch": 0.750592093277464,
      "grad_norm": 1.9883320331573486,
      "learning_rate": 8.533341058858721e-05,
      "loss": 2.3407,
      "step": 12360
    },
    {
      "epoch": 0.7511993684338374,
      "grad_norm": 1.7368723154067993,
      "learning_rate": 8.531090573866485e-05,
      "loss": 2.2881,
      "step": 12370
    },
    {
      "epoch": 0.7518066435902108,
      "grad_norm": 1.8307616710662842,
      "learning_rate": 8.528838660844982e-05,
      "loss": 2.5488,
      "step": 12380
    },
    {
      "epoch": 0.752413918746584,
      "grad_norm": 2.095053195953369,
      "learning_rate": 8.526585320704923e-05,
      "loss": 2.0862,
      "step": 12390
    },
    {
      "epoch": 0.7530211939029574,
      "grad_norm": 1.9407196044921875,
      "learning_rate": 8.524330554357594e-05,
      "loss": 2.4531,
      "step": 12400
    },
    {
      "epoch": 0.7536284690593308,
      "grad_norm": 2.0376124382019043,
      "learning_rate": 8.522074362714859e-05,
      "loss": 2.4825,
      "step": 12410
    },
    {
      "epoch": 0.7542357442157042,
      "grad_norm": 2.9444565773010254,
      "learning_rate": 8.519816746689157e-05,
      "loss": 2.6911,
      "step": 12420
    },
    {
      "epoch": 0.7548430193720775,
      "grad_norm": 3.3410415649414062,
      "learning_rate": 8.517557707193506e-05,
      "loss": 2.5068,
      "step": 12430
    },
    {
      "epoch": 0.7554502945284508,
      "grad_norm": 2.4086990356445312,
      "learning_rate": 8.515297245141497e-05,
      "loss": 2.6127,
      "step": 12440
    },
    {
      "epoch": 0.7560575696848242,
      "grad_norm": 2.7177772521972656,
      "learning_rate": 8.513035361447294e-05,
      "loss": 2.7316,
      "step": 12450
    },
    {
      "epoch": 0.7566648448411976,
      "grad_norm": 1.6825549602508545,
      "learning_rate": 8.510772057025643e-05,
      "loss": 2.1345,
      "step": 12460
    },
    {
      "epoch": 0.7572721199975709,
      "grad_norm": 1.9317388534545898,
      "learning_rate": 8.508507332791857e-05,
      "loss": 2.4682,
      "step": 12470
    },
    {
      "epoch": 0.7578793951539442,
      "grad_norm": 3.309476137161255,
      "learning_rate": 8.506241189661827e-05,
      "loss": 2.3302,
      "step": 12480
    },
    {
      "epoch": 0.7584866703103176,
      "grad_norm": 1.859839677810669,
      "learning_rate": 8.50397362855202e-05,
      "loss": 1.9408,
      "step": 12490
    },
    {
      "epoch": 0.759093945466691,
      "grad_norm": 1.459124207496643,
      "learning_rate": 8.50170465037947e-05,
      "loss": 2.1485,
      "step": 12500
    },
    {
      "epoch": 0.7597012206230643,
      "grad_norm": 1.0410512685775757,
      "learning_rate": 8.49943425606179e-05,
      "loss": 2.0481,
      "step": 12510
    },
    {
      "epoch": 0.7603084957794377,
      "grad_norm": 1.0356611013412476,
      "learning_rate": 8.497162446517164e-05,
      "loss": 1.8196,
      "step": 12520
    },
    {
      "epoch": 0.760915770935811,
      "grad_norm": 2.2021400928497314,
      "learning_rate": 8.494889222664348e-05,
      "loss": 2.4323,
      "step": 12530
    },
    {
      "epoch": 0.7615230460921844,
      "grad_norm": 2.3735203742980957,
      "learning_rate": 8.492614585422669e-05,
      "loss": 2.8658,
      "step": 12540
    },
    {
      "epoch": 0.7621303212485577,
      "grad_norm": 3.0863265991210938,
      "learning_rate": 8.490338535712026e-05,
      "loss": 2.3499,
      "step": 12550
    },
    {
      "epoch": 0.7627375964049311,
      "grad_norm": 2.730555772781372,
      "learning_rate": 8.488061074452891e-05,
      "loss": 2.414,
      "step": 12560
    },
    {
      "epoch": 0.7633448715613044,
      "grad_norm": 3.928560733795166,
      "learning_rate": 8.485782202566306e-05,
      "loss": 2.6671,
      "step": 12570
    },
    {
      "epoch": 0.7639521467176777,
      "grad_norm": 6.450390338897705,
      "learning_rate": 8.483501920973882e-05,
      "loss": 2.548,
      "step": 12580
    },
    {
      "epoch": 0.7645594218740511,
      "grad_norm": 3.4004361629486084,
      "learning_rate": 8.481220230597801e-05,
      "loss": 2.5146,
      "step": 12590
    },
    {
      "epoch": 0.7651666970304245,
      "grad_norm": 1.7330594062805176,
      "learning_rate": 8.478937132360816e-05,
      "loss": 2.3994,
      "step": 12600
    },
    {
      "epoch": 0.7657739721867979,
      "grad_norm": 3.00627064704895,
      "learning_rate": 8.476652627186248e-05,
      "loss": 2.5771,
      "step": 12610
    },
    {
      "epoch": 0.7663812473431711,
      "grad_norm": 7.128241539001465,
      "learning_rate": 8.474366715997986e-05,
      "loss": 2.6336,
      "step": 12620
    },
    {
      "epoch": 0.7669885224995445,
      "grad_norm": 3.4288289546966553,
      "learning_rate": 8.47207939972049e-05,
      "loss": 3.089,
      "step": 12630
    },
    {
      "epoch": 0.7675957976559179,
      "grad_norm": 4.700195789337158,
      "learning_rate": 8.469790679278789e-05,
      "loss": 2.8137,
      "step": 12640
    },
    {
      "epoch": 0.7682030728122913,
      "grad_norm": 3.2817933559417725,
      "learning_rate": 8.467500555598473e-05,
      "loss": 2.466,
      "step": 12650
    },
    {
      "epoch": 0.7688103479686647,
      "grad_norm": 3.635267496109009,
      "learning_rate": 8.465209029605709e-05,
      "loss": 2.7356,
      "step": 12660
    },
    {
      "epoch": 0.7694176231250379,
      "grad_norm": 2.351529359817505,
      "learning_rate": 8.462916102227226e-05,
      "loss": 2.3176,
      "step": 12670
    },
    {
      "epoch": 0.7700248982814113,
      "grad_norm": 1.876102089881897,
      "learning_rate": 8.460621774390319e-05,
      "loss": 2.3651,
      "step": 12680
    },
    {
      "epoch": 0.7706321734377847,
      "grad_norm": 1.9611821174621582,
      "learning_rate": 8.458326047022852e-05,
      "loss": 2.2849,
      "step": 12690
    },
    {
      "epoch": 0.771239448594158,
      "grad_norm": 2.0260138511657715,
      "learning_rate": 8.45602892105325e-05,
      "loss": 3.1043,
      "step": 12700
    },
    {
      "epoch": 0.7718467237505313,
      "grad_norm": 3.062626361846924,
      "learning_rate": 8.453730397410512e-05,
      "loss": 2.6477,
      "step": 12710
    },
    {
      "epoch": 0.7724539989069047,
      "grad_norm": 2.758742570877075,
      "learning_rate": 8.451430477024196e-05,
      "loss": 2.4825,
      "step": 12720
    },
    {
      "epoch": 0.7730612740632781,
      "grad_norm": 2.8642354011535645,
      "learning_rate": 8.449129160824427e-05,
      "loss": 2.4363,
      "step": 12730
    },
    {
      "epoch": 0.7736685492196514,
      "grad_norm": 2.541844606399536,
      "learning_rate": 8.446826449741891e-05,
      "loss": 2.5599,
      "step": 12740
    },
    {
      "epoch": 0.7742758243760248,
      "grad_norm": 5.834018230438232,
      "learning_rate": 8.444522344707844e-05,
      "loss": 2.6359,
      "step": 12750
    },
    {
      "epoch": 0.7748830995323981,
      "grad_norm": 3.472008466720581,
      "learning_rate": 8.442216846654102e-05,
      "loss": 2.3415,
      "step": 12760
    },
    {
      "epoch": 0.7754903746887715,
      "grad_norm": 2.1439666748046875,
      "learning_rate": 8.439909956513046e-05,
      "loss": 2.135,
      "step": 12770
    },
    {
      "epoch": 0.7760976498451448,
      "grad_norm": 3.7304089069366455,
      "learning_rate": 8.437601675217616e-05,
      "loss": 2.5596,
      "step": 12780
    },
    {
      "epoch": 0.7767049250015182,
      "grad_norm": 1.5036545991897583,
      "learning_rate": 8.435292003701323e-05,
      "loss": 2.2213,
      "step": 12790
    },
    {
      "epoch": 0.7773122001578915,
      "grad_norm": 2.005058765411377,
      "learning_rate": 8.43298094289823e-05,
      "loss": 2.2357,
      "step": 12800
    },
    {
      "epoch": 0.7779194753142649,
      "grad_norm": 2.5616648197174072,
      "learning_rate": 8.43066849374297e-05,
      "loss": 2.4601,
      "step": 12810
    },
    {
      "epoch": 0.7785267504706382,
      "grad_norm": 2.1473047733306885,
      "learning_rate": 8.42835465717073e-05,
      "loss": 2.0917,
      "step": 12820
    },
    {
      "epoch": 0.7791340256270116,
      "grad_norm": 2.136707067489624,
      "learning_rate": 8.426039434117268e-05,
      "loss": 2.6254,
      "step": 12830
    },
    {
      "epoch": 0.779741300783385,
      "grad_norm": 2.4741482734680176,
      "learning_rate": 8.423722825518893e-05,
      "loss": 2.4613,
      "step": 12840
    },
    {
      "epoch": 0.7803485759397583,
      "grad_norm": 2.643094301223755,
      "learning_rate": 8.421404832312482e-05,
      "loss": 2.338,
      "step": 12850
    },
    {
      "epoch": 0.7809558510961316,
      "grad_norm": 2.091867685317993,
      "learning_rate": 8.419085455435465e-05,
      "loss": 2.1584,
      "step": 12860
    },
    {
      "epoch": 0.781563126252505,
      "grad_norm": 1.9694041013717651,
      "learning_rate": 8.416764695825835e-05,
      "loss": 2.3327,
      "step": 12870
    },
    {
      "epoch": 0.7821704014088784,
      "grad_norm": 1.7031898498535156,
      "learning_rate": 8.414442554422146e-05,
      "loss": 2.731,
      "step": 12880
    },
    {
      "epoch": 0.7827776765652518,
      "grad_norm": 2.5709309577941895,
      "learning_rate": 8.412119032163508e-05,
      "loss": 2.4037,
      "step": 12890
    },
    {
      "epoch": 0.783384951721625,
      "grad_norm": 2.479111433029175,
      "learning_rate": 8.40979412998959e-05,
      "loss": 2.8875,
      "step": 12900
    },
    {
      "epoch": 0.7839922268779984,
      "grad_norm": 3.3739397525787354,
      "learning_rate": 8.407467848840621e-05,
      "loss": 2.6635,
      "step": 12910
    },
    {
      "epoch": 0.7845995020343718,
      "grad_norm": 2.9242236614227295,
      "learning_rate": 8.405140189657385e-05,
      "loss": 2.7224,
      "step": 12920
    },
    {
      "epoch": 0.7852067771907452,
      "grad_norm": 2.5716285705566406,
      "learning_rate": 8.402811153381224e-05,
      "loss": 2.6009,
      "step": 12930
    },
    {
      "epoch": 0.7858140523471184,
      "grad_norm": 2.881065845489502,
      "learning_rate": 8.40048074095404e-05,
      "loss": 2.3889,
      "step": 12940
    },
    {
      "epoch": 0.7864213275034918,
      "grad_norm": 3.3019120693206787,
      "learning_rate": 8.398148953318285e-05,
      "loss": 2.5101,
      "step": 12950
    },
    {
      "epoch": 0.7870286026598652,
      "grad_norm": 2.1673166751861572,
      "learning_rate": 8.395815791416975e-05,
      "loss": 2.4984,
      "step": 12960
    },
    {
      "epoch": 0.7876358778162386,
      "grad_norm": 2.124929666519165,
      "learning_rate": 8.393481256193674e-05,
      "loss": 2.5633,
      "step": 12970
    },
    {
      "epoch": 0.7882431529726119,
      "grad_norm": 2.5952885150909424,
      "learning_rate": 8.391145348592506e-05,
      "loss": 2.1786,
      "step": 12980
    },
    {
      "epoch": 0.7888504281289852,
      "grad_norm": 2.5406508445739746,
      "learning_rate": 8.388808069558153e-05,
      "loss": 2.0521,
      "step": 12990
    },
    {
      "epoch": 0.7894577032853586,
      "grad_norm": 2.3589870929718018,
      "learning_rate": 8.386469420035845e-05,
      "loss": 2.5765,
      "step": 13000
    },
    {
      "epoch": 0.790064978441732,
      "grad_norm": 2.960855484008789,
      "learning_rate": 8.384129400971368e-05,
      "loss": 2.1828,
      "step": 13010
    },
    {
      "epoch": 0.7906722535981053,
      "grad_norm": 2.806478261947632,
      "learning_rate": 8.381788013311065e-05,
      "loss": 2.5048,
      "step": 13020
    },
    {
      "epoch": 0.7912795287544786,
      "grad_norm": 2.5773706436157227,
      "learning_rate": 8.379445258001828e-05,
      "loss": 2.473,
      "step": 13030
    },
    {
      "epoch": 0.791886803910852,
      "grad_norm": 2.9111194610595703,
      "learning_rate": 8.377101135991108e-05,
      "loss": 2.629,
      "step": 13040
    },
    {
      "epoch": 0.7924940790672254,
      "grad_norm": 3.5611307621002197,
      "learning_rate": 8.374755648226903e-05,
      "loss": 2.7101,
      "step": 13050
    },
    {
      "epoch": 0.7931013542235987,
      "grad_norm": 2.315772533416748,
      "learning_rate": 8.372408795657766e-05,
      "loss": 2.2226,
      "step": 13060
    },
    {
      "epoch": 0.7937086293799721,
      "grad_norm": 1.711525559425354,
      "learning_rate": 8.370060579232802e-05,
      "loss": 2.4281,
      "step": 13070
    },
    {
      "epoch": 0.7943159045363454,
      "grad_norm": 2.324127435684204,
      "learning_rate": 8.367710999901667e-05,
      "loss": 2.0477,
      "step": 13080
    },
    {
      "epoch": 0.7949231796927188,
      "grad_norm": 1.2778127193450928,
      "learning_rate": 8.365360058614567e-05,
      "loss": 1.8017,
      "step": 13090
    },
    {
      "epoch": 0.7955304548490921,
      "grad_norm": 1.9032167196273804,
      "learning_rate": 8.363007756322263e-05,
      "loss": 2.9376,
      "step": 13100
    },
    {
      "epoch": 0.7961377300054655,
      "grad_norm": 2.874929189682007,
      "learning_rate": 8.360654093976061e-05,
      "loss": 2.6261,
      "step": 13110
    },
    {
      "epoch": 0.7967450051618389,
      "grad_norm": 4.557077407836914,
      "learning_rate": 8.35829907252782e-05,
      "loss": 2.477,
      "step": 13120
    },
    {
      "epoch": 0.7973522803182121,
      "grad_norm": 4.758883953094482,
      "learning_rate": 8.355942692929948e-05,
      "loss": 2.4263,
      "step": 13130
    },
    {
      "epoch": 0.7979595554745855,
      "grad_norm": 2.719919443130493,
      "learning_rate": 8.353584956135405e-05,
      "loss": 2.4558,
      "step": 13140
    },
    {
      "epoch": 0.7985668306309589,
      "grad_norm": 2.0837626457214355,
      "learning_rate": 8.351225863097693e-05,
      "loss": 2.6519,
      "step": 13150
    },
    {
      "epoch": 0.7991741057873323,
      "grad_norm": 1.9154090881347656,
      "learning_rate": 8.34886541477087e-05,
      "loss": 2.3142,
      "step": 13160
    },
    {
      "epoch": 0.7997813809437055,
      "grad_norm": 2.6755120754241943,
      "learning_rate": 8.346503612109537e-05,
      "loss": 2.3519,
      "step": 13170
    },
    {
      "epoch": 0.8003886561000789,
      "grad_norm": 2.5423855781555176,
      "learning_rate": 8.344140456068846e-05,
      "loss": 2.3536,
      "step": 13180
    },
    {
      "epoch": 0.8009959312564523,
      "grad_norm": 2.4292235374450684,
      "learning_rate": 8.341775947604495e-05,
      "loss": 2.3336,
      "step": 13190
    },
    {
      "epoch": 0.8016032064128257,
      "grad_norm": 1.8967987298965454,
      "learning_rate": 8.339410087672727e-05,
      "loss": 2.2914,
      "step": 13200
    },
    {
      "epoch": 0.802210481569199,
      "grad_norm": 2.057884931564331,
      "learning_rate": 8.337042877230337e-05,
      "loss": 2.9064,
      "step": 13210
    },
    {
      "epoch": 0.8028177567255723,
      "grad_norm": 2.6186962127685547,
      "learning_rate": 8.334674317234659e-05,
      "loss": 2.7126,
      "step": 13220
    },
    {
      "epoch": 0.8034250318819457,
      "grad_norm": 3.4090452194213867,
      "learning_rate": 8.332304408643577e-05,
      "loss": 3.0608,
      "step": 13230
    },
    {
      "epoch": 0.8040323070383191,
      "grad_norm": 3.7857513427734375,
      "learning_rate": 8.32993315241552e-05,
      "loss": 2.6376,
      "step": 13240
    },
    {
      "epoch": 0.8046395821946924,
      "grad_norm": 2.4018149375915527,
      "learning_rate": 8.327560549509465e-05,
      "loss": 2.3394,
      "step": 13250
    },
    {
      "epoch": 0.8052468573510657,
      "grad_norm": 1.6329115629196167,
      "learning_rate": 8.325186600884926e-05,
      "loss": 2.3094,
      "step": 13260
    },
    {
      "epoch": 0.8058541325074391,
      "grad_norm": 2.6282739639282227,
      "learning_rate": 8.322811307501968e-05,
      "loss": 2.6324,
      "step": 13270
    },
    {
      "epoch": 0.8064614076638125,
      "grad_norm": 3.3258767127990723,
      "learning_rate": 8.320434670321196e-05,
      "loss": 2.4961,
      "step": 13280
    },
    {
      "epoch": 0.8070686828201858,
      "grad_norm": 2.4998958110809326,
      "learning_rate": 8.318056690303762e-05,
      "loss": 2.6439,
      "step": 13290
    },
    {
      "epoch": 0.8076759579765592,
      "grad_norm": 5.0452494621276855,
      "learning_rate": 8.315677368411356e-05,
      "loss": 2.5144,
      "step": 13300
    },
    {
      "epoch": 0.8082832331329325,
      "grad_norm": 2.682633876800537,
      "learning_rate": 8.313296705606217e-05,
      "loss": 2.4193,
      "step": 13310
    },
    {
      "epoch": 0.8088905082893059,
      "grad_norm": 1.8968080282211304,
      "learning_rate": 8.31091470285112e-05,
      "loss": 2.6738,
      "step": 13320
    },
    {
      "epoch": 0.8094977834456792,
      "grad_norm": 3.1589014530181885,
      "learning_rate": 8.308531361109389e-05,
      "loss": 3.0168,
      "step": 13330
    },
    {
      "epoch": 0.8101050586020526,
      "grad_norm": 4.218478202819824,
      "learning_rate": 8.30614668134488e-05,
      "loss": 2.9548,
      "step": 13340
    },
    {
      "epoch": 0.810712333758426,
      "grad_norm": 4.336192607879639,
      "learning_rate": 8.303760664521998e-05,
      "loss": 3.0472,
      "step": 13350
    },
    {
      "epoch": 0.8113196089147993,
      "grad_norm": 3.2313005924224854,
      "learning_rate": 8.301373311605687e-05,
      "loss": 2.6008,
      "step": 13360
    },
    {
      "epoch": 0.8119268840711726,
      "grad_norm": 2.6195454597473145,
      "learning_rate": 8.29898462356143e-05,
      "loss": 2.4358,
      "step": 13370
    },
    {
      "epoch": 0.812534159227546,
      "grad_norm": 2.825472593307495,
      "learning_rate": 8.29659460135525e-05,
      "loss": 2.6959,
      "step": 13380
    },
    {
      "epoch": 0.8131414343839194,
      "grad_norm": 1.622677206993103,
      "learning_rate": 8.294203245953709e-05,
      "loss": 2.2056,
      "step": 13390
    },
    {
      "epoch": 0.8137487095402927,
      "grad_norm": 1.7132394313812256,
      "learning_rate": 8.291810558323911e-05,
      "loss": 2.2276,
      "step": 13400
    },
    {
      "epoch": 0.814355984696666,
      "grad_norm": 1.8683552742004395,
      "learning_rate": 8.289416539433498e-05,
      "loss": 2.1035,
      "step": 13410
    },
    {
      "epoch": 0.8149632598530394,
      "grad_norm": 2.5628504753112793,
      "learning_rate": 8.287021190250647e-05,
      "loss": 2.3154,
      "step": 13420
    },
    {
      "epoch": 0.8155705350094128,
      "grad_norm": 2.6364221572875977,
      "learning_rate": 8.284624511744076e-05,
      "loss": 2.474,
      "step": 13430
    },
    {
      "epoch": 0.8161778101657862,
      "grad_norm": 2.844710350036621,
      "learning_rate": 8.282226504883042e-05,
      "loss": 2.1227,
      "step": 13440
    },
    {
      "epoch": 0.8167850853221594,
      "grad_norm": 2.473994493484497,
      "learning_rate": 8.279827170637333e-05,
      "loss": 2.386,
      "step": 13450
    },
    {
      "epoch": 0.8173923604785328,
      "grad_norm": 2.2195279598236084,
      "learning_rate": 8.277426509977281e-05,
      "loss": 2.8405,
      "step": 13460
    },
    {
      "epoch": 0.8179996356349062,
      "grad_norm": 3.744962215423584,
      "learning_rate": 8.275024523873753e-05,
      "loss": 2.6105,
      "step": 13470
    },
    {
      "epoch": 0.8186069107912796,
      "grad_norm": 1.6180806159973145,
      "learning_rate": 8.272621213298146e-05,
      "loss": 2.302,
      "step": 13480
    },
    {
      "epoch": 0.8192141859476528,
      "grad_norm": 2.8582286834716797,
      "learning_rate": 8.270216579222398e-05,
      "loss": 2.1161,
      "step": 13490
    },
    {
      "epoch": 0.8198214611040262,
      "grad_norm": 2.333277463912964,
      "learning_rate": 8.267810622618986e-05,
      "loss": 2.4821,
      "step": 13500
    },
    {
      "epoch": 0.8204287362603996,
      "grad_norm": 1.9967674016952515,
      "learning_rate": 8.265403344460911e-05,
      "loss": 2.5312,
      "step": 13510
    },
    {
      "epoch": 0.821036011416773,
      "grad_norm": 2.542839765548706,
      "learning_rate": 8.26299474572172e-05,
      "loss": 2.3433,
      "step": 13520
    },
    {
      "epoch": 0.8216432865731463,
      "grad_norm": 3.862605333328247,
      "learning_rate": 8.260584827375484e-05,
      "loss": 2.2608,
      "step": 13530
    },
    {
      "epoch": 0.8222505617295196,
      "grad_norm": 1.4928942918777466,
      "learning_rate": 8.258173590396814e-05,
      "loss": 2.6259,
      "step": 13540
    },
    {
      "epoch": 0.822857836885893,
      "grad_norm": 2.0023536682128906,
      "learning_rate": 8.255761035760853e-05,
      "loss": 2.2278,
      "step": 13550
    },
    {
      "epoch": 0.8234651120422664,
      "grad_norm": 3.051513195037842,
      "learning_rate": 8.25334716444328e-05,
      "loss": 2.2445,
      "step": 13560
    },
    {
      "epoch": 0.8240723871986397,
      "grad_norm": 1.7742516994476318,
      "learning_rate": 8.250931977420296e-05,
      "loss": 1.996,
      "step": 13570
    },
    {
      "epoch": 0.8246796623550131,
      "grad_norm": 1.3505935668945312,
      "learning_rate": 8.248515475668645e-05,
      "loss": 2.2175,
      "step": 13580
    },
    {
      "epoch": 0.8252869375113864,
      "grad_norm": 2.2673656940460205,
      "learning_rate": 8.2460976601656e-05,
      "loss": 2.2615,
      "step": 13590
    },
    {
      "epoch": 0.8258942126677598,
      "grad_norm": 3.0157172679901123,
      "learning_rate": 8.243678531888962e-05,
      "loss": 2.3413,
      "step": 13600
    },
    {
      "epoch": 0.8265014878241331,
      "grad_norm": 2.931645154953003,
      "learning_rate": 8.241258091817066e-05,
      "loss": 2.4041,
      "step": 13610
    },
    {
      "epoch": 0.8271087629805065,
      "grad_norm": 2.87677001953125,
      "learning_rate": 8.238836340928775e-05,
      "loss": 2.6186,
      "step": 13620
    },
    {
      "epoch": 0.8277160381368798,
      "grad_norm": 3.2362022399902344,
      "learning_rate": 8.236413280203486e-05,
      "loss": 2.3927,
      "step": 13630
    },
    {
      "epoch": 0.8283233132932531,
      "grad_norm": 3.1075081825256348,
      "learning_rate": 8.233988910621121e-05,
      "loss": 2.7662,
      "step": 13640
    },
    {
      "epoch": 0.8289305884496265,
      "grad_norm": 2.1110453605651855,
      "learning_rate": 8.231563233162134e-05,
      "loss": 2.4271,
      "step": 13650
    },
    {
      "epoch": 0.8295378636059999,
      "grad_norm": 1.6828699111938477,
      "learning_rate": 8.22913624880751e-05,
      "loss": 2.246,
      "step": 13660
    },
    {
      "epoch": 0.8301451387623733,
      "grad_norm": 1.2686536312103271,
      "learning_rate": 8.226707958538757e-05,
      "loss": 2.4472,
      "step": 13670
    },
    {
      "epoch": 0.8307524139187465,
      "grad_norm": 3.2288880348205566,
      "learning_rate": 8.224278363337916e-05,
      "loss": 2.2723,
      "step": 13680
    },
    {
      "epoch": 0.8313596890751199,
      "grad_norm": 2.7412126064300537,
      "learning_rate": 8.221847464187556e-05,
      "loss": 2.5299,
      "step": 13690
    },
    {
      "epoch": 0.8319669642314933,
      "grad_norm": 4.39555549621582,
      "learning_rate": 8.219415262070766e-05,
      "loss": 2.5416,
      "step": 13700
    },
    {
      "epoch": 0.8325742393878667,
      "grad_norm": 2.048339605331421,
      "learning_rate": 8.216981757971173e-05,
      "loss": 2.3627,
      "step": 13710
    },
    {
      "epoch": 0.8331815145442399,
      "grad_norm": 2.835141181945801,
      "learning_rate": 8.21454695287292e-05,
      "loss": 2.3699,
      "step": 13720
    },
    {
      "epoch": 0.8337887897006133,
      "grad_norm": 1.7608753442764282,
      "learning_rate": 8.212110847760684e-05,
      "loss": 2.4842,
      "step": 13730
    },
    {
      "epoch": 0.8343960648569867,
      "grad_norm": 4.060382843017578,
      "learning_rate": 8.209673443619664e-05,
      "loss": 2.0942,
      "step": 13740
    },
    {
      "epoch": 0.8350033400133601,
      "grad_norm": 3.5765583515167236,
      "learning_rate": 8.207234741435585e-05,
      "loss": 2.5347,
      "step": 13750
    },
    {
      "epoch": 0.8356106151697335,
      "grad_norm": 2.8071422576904297,
      "learning_rate": 8.204794742194698e-05,
      "loss": 2.4106,
      "step": 13760
    },
    {
      "epoch": 0.8362178903261067,
      "grad_norm": 3.2632009983062744,
      "learning_rate": 8.202353446883773e-05,
      "loss": 2.5065,
      "step": 13770
    },
    {
      "epoch": 0.8368251654824801,
      "grad_norm": 1.4529907703399658,
      "learning_rate": 8.199910856490114e-05,
      "loss": 2.6212,
      "step": 13780
    },
    {
      "epoch": 0.8374324406388535,
      "grad_norm": 3.3968262672424316,
      "learning_rate": 8.197466972001542e-05,
      "loss": 2.4654,
      "step": 13790
    },
    {
      "epoch": 0.8380397157952268,
      "grad_norm": 2.1333422660827637,
      "learning_rate": 8.195021794406401e-05,
      "loss": 2.1672,
      "step": 13800
    },
    {
      "epoch": 0.8386469909516002,
      "grad_norm": 2.8348331451416016,
      "learning_rate": 8.192575324693564e-05,
      "loss": 2.6651,
      "step": 13810
    },
    {
      "epoch": 0.8392542661079735,
      "grad_norm": 4.144172668457031,
      "learning_rate": 8.190127563852417e-05,
      "loss": 2.8167,
      "step": 13820
    },
    {
      "epoch": 0.8398615412643469,
      "grad_norm": 2.990058422088623,
      "learning_rate": 8.187678512872875e-05,
      "loss": 2.4382,
      "step": 13830
    },
    {
      "epoch": 0.8404688164207202,
      "grad_norm": 2.9836502075195312,
      "learning_rate": 8.185228172745376e-05,
      "loss": 2.114,
      "step": 13840
    },
    {
      "epoch": 0.8410760915770936,
      "grad_norm": 2.6517345905303955,
      "learning_rate": 8.182776544460875e-05,
      "loss": 2.2129,
      "step": 13850
    },
    {
      "epoch": 0.8416833667334669,
      "grad_norm": 2.177889585494995,
      "learning_rate": 8.180323629010848e-05,
      "loss": 2.402,
      "step": 13860
    },
    {
      "epoch": 0.8422906418898403,
      "grad_norm": 2.4541287422180176,
      "learning_rate": 8.177869427387296e-05,
      "loss": 2.1343,
      "step": 13870
    },
    {
      "epoch": 0.8428979170462136,
      "grad_norm": 1.1903932094573975,
      "learning_rate": 8.175413940582734e-05,
      "loss": 2.027,
      "step": 13880
    },
    {
      "epoch": 0.843505192202587,
      "grad_norm": 2.189837694168091,
      "learning_rate": 8.172957169590202e-05,
      "loss": 2.2826,
      "step": 13890
    },
    {
      "epoch": 0.8441124673589604,
      "grad_norm": 2.6319968700408936,
      "learning_rate": 8.170499115403256e-05,
      "loss": 2.5356,
      "step": 13900
    },
    {
      "epoch": 0.8447197425153337,
      "grad_norm": 3.175452470779419,
      "learning_rate": 8.168039779015974e-05,
      "loss": 2.4329,
      "step": 13910
    },
    {
      "epoch": 0.845327017671707,
      "grad_norm": 2.3415842056274414,
      "learning_rate": 8.16557916142295e-05,
      "loss": 2.6731,
      "step": 13920
    },
    {
      "epoch": 0.8459342928280804,
      "grad_norm": 1.3133344650268555,
      "learning_rate": 8.163117263619297e-05,
      "loss": 2.2982,
      "step": 13930
    },
    {
      "epoch": 0.8465415679844538,
      "grad_norm": 1.575184941291809,
      "learning_rate": 8.160654086600646e-05,
      "loss": 2.1976,
      "step": 13940
    },
    {
      "epoch": 0.8471488431408271,
      "grad_norm": 2.800210475921631,
      "learning_rate": 8.158189631363142e-05,
      "loss": 2.4166,
      "step": 13950
    },
    {
      "epoch": 0.8477561182972004,
      "grad_norm": 2.47287917137146,
      "learning_rate": 8.155723898903455e-05,
      "loss": 2.3407,
      "step": 13960
    },
    {
      "epoch": 0.8483633934535738,
      "grad_norm": 1.5428630113601685,
      "learning_rate": 8.153256890218764e-05,
      "loss": 2.4305,
      "step": 13970
    },
    {
      "epoch": 0.8489706686099472,
      "grad_norm": 1.8768900632858276,
      "learning_rate": 8.150788606306765e-05,
      "loss": 2.4644,
      "step": 13980
    },
    {
      "epoch": 0.8495779437663206,
      "grad_norm": 2.8523519039154053,
      "learning_rate": 8.148319048165674e-05,
      "loss": 2.5282,
      "step": 13990
    },
    {
      "epoch": 0.8501852189226938,
      "grad_norm": 2.360447645187378,
      "learning_rate": 8.145848216794219e-05,
      "loss": 2.6921,
      "step": 14000
    },
    {
      "epoch": 0.8507924940790672,
      "grad_norm": 3.7220401763916016,
      "learning_rate": 8.143376113191641e-05,
      "loss": 2.6553,
      "step": 14010
    },
    {
      "epoch": 0.8513997692354406,
      "grad_norm": 3.6995880603790283,
      "learning_rate": 8.140902738357702e-05,
      "loss": 2.1895,
      "step": 14020
    },
    {
      "epoch": 0.852007044391814,
      "grad_norm": 2.649040937423706,
      "learning_rate": 8.138428093292672e-05,
      "loss": 2.1989,
      "step": 14030
    },
    {
      "epoch": 0.8526143195481873,
      "grad_norm": 2.2280173301696777,
      "learning_rate": 8.135952178997338e-05,
      "loss": 2.2253,
      "step": 14040
    },
    {
      "epoch": 0.8532215947045606,
      "grad_norm": 2.132962465286255,
      "learning_rate": 8.133474996472996e-05,
      "loss": 2.4112,
      "step": 14050
    },
    {
      "epoch": 0.853828869860934,
      "grad_norm": 2.3102171421051025,
      "learning_rate": 8.130996546721462e-05,
      "loss": 2.1681,
      "step": 14060
    },
    {
      "epoch": 0.8544361450173074,
      "grad_norm": 2.109811544418335,
      "learning_rate": 8.128516830745058e-05,
      "loss": 2.3382,
      "step": 14070
    },
    {
      "epoch": 0.8550434201736807,
      "grad_norm": 2.0714871883392334,
      "learning_rate": 8.126035849546623e-05,
      "loss": 2.5596,
      "step": 14080
    },
    {
      "epoch": 0.855650695330054,
      "grad_norm": 3.6590170860290527,
      "learning_rate": 8.123553604129504e-05,
      "loss": 2.3262,
      "step": 14090
    },
    {
      "epoch": 0.8562579704864274,
      "grad_norm": 2.3014512062072754,
      "learning_rate": 8.12107009549756e-05,
      "loss": 2.0562,
      "step": 14100
    },
    {
      "epoch": 0.8568652456428008,
      "grad_norm": 3.3457398414611816,
      "learning_rate": 8.118585324655161e-05,
      "loss": 2.3507,
      "step": 14110
    },
    {
      "epoch": 0.8574725207991741,
      "grad_norm": 2.981548547744751,
      "learning_rate": 8.116099292607189e-05,
      "loss": 2.6772,
      "step": 14120
    },
    {
      "epoch": 0.8580797959555475,
      "grad_norm": 1.813355565071106,
      "learning_rate": 8.113612000359036e-05,
      "loss": 2.2055,
      "step": 14130
    },
    {
      "epoch": 0.8586870711119208,
      "grad_norm": 1.5853006839752197,
      "learning_rate": 8.111123448916602e-05,
      "loss": 2.6819,
      "step": 14140
    },
    {
      "epoch": 0.8592943462682942,
      "grad_norm": 2.876767873764038,
      "learning_rate": 8.108633639286294e-05,
      "loss": 2.5329,
      "step": 14150
    },
    {
      "epoch": 0.8599016214246675,
      "grad_norm": 1.3732216358184814,
      "learning_rate": 8.106142572475035e-05,
      "loss": 1.9446,
      "step": 14160
    },
    {
      "epoch": 0.8605088965810409,
      "grad_norm": 1.814455270767212,
      "learning_rate": 8.103650249490249e-05,
      "loss": 2.1733,
      "step": 14170
    },
    {
      "epoch": 0.8611161717374142,
      "grad_norm": 2.2597944736480713,
      "learning_rate": 8.101156671339873e-05,
      "loss": 2.5973,
      "step": 14180
    },
    {
      "epoch": 0.8617234468937875,
      "grad_norm": 1.9215071201324463,
      "learning_rate": 8.098661839032349e-05,
      "loss": 2.3681,
      "step": 14190
    },
    {
      "epoch": 0.8623307220501609,
      "grad_norm": 2.8064026832580566,
      "learning_rate": 8.096165753576625e-05,
      "loss": 2.3254,
      "step": 14200
    },
    {
      "epoch": 0.8629379972065343,
      "grad_norm": 1.8613202571868896,
      "learning_rate": 8.093668415982161e-05,
      "loss": 2.4581,
      "step": 14210
    },
    {
      "epoch": 0.8635452723629077,
      "grad_norm": 3.495070457458496,
      "learning_rate": 8.091169827258918e-05,
      "loss": 2.871,
      "step": 14220
    },
    {
      "epoch": 0.864152547519281,
      "grad_norm": 4.918758869171143,
      "learning_rate": 8.088669988417366e-05,
      "loss": 2.6349,
      "step": 14230
    },
    {
      "epoch": 0.8647598226756543,
      "grad_norm": 2.881579875946045,
      "learning_rate": 8.086168900468478e-05,
      "loss": 3.1681,
      "step": 14240
    },
    {
      "epoch": 0.8653670978320277,
      "grad_norm": 3.4151830673217773,
      "learning_rate": 8.083666564423736e-05,
      "loss": 2.7448,
      "step": 14250
    },
    {
      "epoch": 0.8659743729884011,
      "grad_norm": 2.0789084434509277,
      "learning_rate": 8.081162981295123e-05,
      "loss": 2.2524,
      "step": 14260
    },
    {
      "epoch": 0.8665816481447745,
      "grad_norm": 2.2574095726013184,
      "learning_rate": 8.07865815209513e-05,
      "loss": 2.3894,
      "step": 14270
    },
    {
      "epoch": 0.8671889233011477,
      "grad_norm": 2.1313610076904297,
      "learning_rate": 8.076152077836747e-05,
      "loss": 2.559,
      "step": 14280
    },
    {
      "epoch": 0.8677961984575211,
      "grad_norm": 2.239670753479004,
      "learning_rate": 8.073644759533473e-05,
      "loss": 2.5135,
      "step": 14290
    },
    {
      "epoch": 0.8684034736138945,
      "grad_norm": 3.7109780311584473,
      "learning_rate": 8.071136198199305e-05,
      "loss": 2.1408,
      "step": 14300
    },
    {
      "epoch": 0.8690107487702678,
      "grad_norm": 1.7204209566116333,
      "learning_rate": 8.068626394848748e-05,
      "loss": 2.447,
      "step": 14310
    },
    {
      "epoch": 0.8696180239266411,
      "grad_norm": 2.8504374027252197,
      "learning_rate": 8.066115350496802e-05,
      "loss": 2.6088,
      "step": 14320
    },
    {
      "epoch": 0.8702252990830145,
      "grad_norm": 1.808363676071167,
      "learning_rate": 8.063603066158978e-05,
      "loss": 2.4599,
      "step": 14330
    },
    {
      "epoch": 0.8708325742393879,
      "grad_norm": 1.5743000507354736,
      "learning_rate": 8.06108954285128e-05,
      "loss": 2.4164,
      "step": 14340
    },
    {
      "epoch": 0.8714398493957612,
      "grad_norm": 2.0879335403442383,
      "learning_rate": 8.058574781590221e-05,
      "loss": 2.4542,
      "step": 14350
    },
    {
      "epoch": 0.8720471245521346,
      "grad_norm": 1.8936529159545898,
      "learning_rate": 8.056058783392807e-05,
      "loss": 2.3614,
      "step": 14360
    },
    {
      "epoch": 0.8726543997085079,
      "grad_norm": 2.3419861793518066,
      "learning_rate": 8.053541549276549e-05,
      "loss": 2.4959,
      "step": 14370
    },
    {
      "epoch": 0.8732616748648813,
      "grad_norm": 3.9795079231262207,
      "learning_rate": 8.051023080259459e-05,
      "loss": 2.6176,
      "step": 14380
    },
    {
      "epoch": 0.8738689500212546,
      "grad_norm": 3.4352641105651855,
      "learning_rate": 8.04850337736004e-05,
      "loss": 2.4473,
      "step": 14390
    },
    {
      "epoch": 0.874476225177628,
      "grad_norm": 2.3498103618621826,
      "learning_rate": 8.045982441597307e-05,
      "loss": 2.3904,
      "step": 14400
    },
    {
      "epoch": 0.8750835003340013,
      "grad_norm": 2.1589627265930176,
      "learning_rate": 8.04346027399076e-05,
      "loss": 2.2676,
      "step": 14410
    },
    {
      "epoch": 0.8756907754903747,
      "grad_norm": 3.1897470951080322,
      "learning_rate": 8.040936875560408e-05,
      "loss": 2.8094,
      "step": 14420
    },
    {
      "epoch": 0.876298050646748,
      "grad_norm": 2.512303590774536,
      "learning_rate": 8.038412247326752e-05,
      "loss": 2.6731,
      "step": 14430
    },
    {
      "epoch": 0.8769053258031214,
      "grad_norm": 4.151480674743652,
      "learning_rate": 8.035886390310792e-05,
      "loss": 2.8509,
      "step": 14440
    },
    {
      "epoch": 0.8775126009594948,
      "grad_norm": 3.727976083755493,
      "learning_rate": 8.033359305534025e-05,
      "loss": 2.4852,
      "step": 14450
    },
    {
      "epoch": 0.8781198761158681,
      "grad_norm": 2.1578946113586426,
      "learning_rate": 8.030830994018446e-05,
      "loss": 2.1491,
      "step": 14460
    },
    {
      "epoch": 0.8787271512722414,
      "grad_norm": 1.788666009902954,
      "learning_rate": 8.02830145678654e-05,
      "loss": 2.0136,
      "step": 14470
    },
    {
      "epoch": 0.8793344264286148,
      "grad_norm": 1.789258360862732,
      "learning_rate": 8.025770694861299e-05,
      "loss": 2.1495,
      "step": 14480
    },
    {
      "epoch": 0.8799417015849882,
      "grad_norm": 2.4654128551483154,
      "learning_rate": 8.023238709266196e-05,
      "loss": 2.5395,
      "step": 14490
    },
    {
      "epoch": 0.8805489767413615,
      "grad_norm": 2.7974510192871094,
      "learning_rate": 8.02070550102521e-05,
      "loss": 2.0708,
      "step": 14500
    },
    {
      "epoch": 0.8811562518977348,
      "grad_norm": 1.6571016311645508,
      "learning_rate": 8.018171071162812e-05,
      "loss": 2.1512,
      "step": 14510
    },
    {
      "epoch": 0.8817635270541082,
      "grad_norm": 2.8650360107421875,
      "learning_rate": 8.015635420703963e-05,
      "loss": 2.3669,
      "step": 14520
    },
    {
      "epoch": 0.8823708022104816,
      "grad_norm": 3.1920647621154785,
      "learning_rate": 8.013098550674122e-05,
      "loss": 2.7443,
      "step": 14530
    },
    {
      "epoch": 0.882978077366855,
      "grad_norm": 4.367587089538574,
      "learning_rate": 8.010560462099238e-05,
      "loss": 2.9754,
      "step": 14540
    },
    {
      "epoch": 0.8835853525232282,
      "grad_norm": 4.049655914306641,
      "learning_rate": 8.008021156005758e-05,
      "loss": 2.3614,
      "step": 14550
    },
    {
      "epoch": 0.8841926276796016,
      "grad_norm": 2.5558342933654785,
      "learning_rate": 8.005480633420614e-05,
      "loss": 2.8934,
      "step": 14560
    },
    {
      "epoch": 0.884799902835975,
      "grad_norm": 2.371664524078369,
      "learning_rate": 8.002938895371237e-05,
      "loss": 3.0711,
      "step": 14570
    },
    {
      "epoch": 0.8854071779923484,
      "grad_norm": 1.7377221584320068,
      "learning_rate": 8.000395942885543e-05,
      "loss": 2.296,
      "step": 14580
    },
    {
      "epoch": 0.8860144531487217,
      "grad_norm": 1.8490080833435059,
      "learning_rate": 7.997851776991945e-05,
      "loss": 2.2931,
      "step": 14590
    },
    {
      "epoch": 0.886621728305095,
      "grad_norm": 1.832748532295227,
      "learning_rate": 7.995306398719342e-05,
      "loss": 2.2782,
      "step": 14600
    },
    {
      "epoch": 0.8872290034614684,
      "grad_norm": 2.516735315322876,
      "learning_rate": 7.992759809097128e-05,
      "loss": 2.6955,
      "step": 14610
    },
    {
      "epoch": 0.8878362786178418,
      "grad_norm": 2.9285168647766113,
      "learning_rate": 7.990212009155184e-05,
      "loss": 2.4884,
      "step": 14620
    },
    {
      "epoch": 0.8884435537742151,
      "grad_norm": 2.779439687728882,
      "learning_rate": 7.987662999923879e-05,
      "loss": 2.8564,
      "step": 14630
    },
    {
      "epoch": 0.8890508289305884,
      "grad_norm": 3.486205816268921,
      "learning_rate": 7.985112782434074e-05,
      "loss": 2.375,
      "step": 14640
    },
    {
      "epoch": 0.8896581040869618,
      "grad_norm": 3.814786672592163,
      "learning_rate": 7.98256135771712e-05,
      "loss": 2.278,
      "step": 14650
    },
    {
      "epoch": 0.8902653792433352,
      "grad_norm": 3.5412020683288574,
      "learning_rate": 7.980008726804849e-05,
      "loss": 2.3876,
      "step": 14660
    },
    {
      "epoch": 0.8908726543997085,
      "grad_norm": 3.5556771755218506,
      "learning_rate": 7.97745489072959e-05,
      "loss": 2.3736,
      "step": 14670
    },
    {
      "epoch": 0.8914799295560819,
      "grad_norm": 2.403332233428955,
      "learning_rate": 7.974899850524151e-05,
      "loss": 2.5403,
      "step": 14680
    },
    {
      "epoch": 0.8920872047124552,
      "grad_norm": 3.4171156883239746,
      "learning_rate": 7.972343607221836e-05,
      "loss": 2.4093,
      "step": 14690
    },
    {
      "epoch": 0.8926944798688285,
      "grad_norm": 2.9310600757598877,
      "learning_rate": 7.969786161856424e-05,
      "loss": 2.0482,
      "step": 14700
    },
    {
      "epoch": 0.8933017550252019,
      "grad_norm": 1.4467486143112183,
      "learning_rate": 7.967227515462191e-05,
      "loss": 2.284,
      "step": 14710
    },
    {
      "epoch": 0.8939090301815753,
      "grad_norm": 2.8390300273895264,
      "learning_rate": 7.964667669073894e-05,
      "loss": 2.2462,
      "step": 14720
    },
    {
      "epoch": 0.8945163053379486,
      "grad_norm": 3.9782779216766357,
      "learning_rate": 7.962106623726775e-05,
      "loss": 2.29,
      "step": 14730
    },
    {
      "epoch": 0.895123580494322,
      "grad_norm": 1.9151089191436768,
      "learning_rate": 7.959544380456563e-05,
      "loss": 2.0701,
      "step": 14740
    },
    {
      "epoch": 0.8957308556506953,
      "grad_norm": 2.6230833530426025,
      "learning_rate": 7.956980940299467e-05,
      "loss": 2.5214,
      "step": 14750
    },
    {
      "epoch": 0.8963381308070687,
      "grad_norm": 4.862583160400391,
      "learning_rate": 7.954416304292185e-05,
      "loss": 2.5188,
      "step": 14760
    },
    {
      "epoch": 0.8969454059634421,
      "grad_norm": 2.775786876678467,
      "learning_rate": 7.951850473471897e-05,
      "loss": 2.0076,
      "step": 14770
    },
    {
      "epoch": 0.8975526811198153,
      "grad_norm": 3.1819043159484863,
      "learning_rate": 7.949283448876264e-05,
      "loss": 2.6531,
      "step": 14780
    },
    {
      "epoch": 0.8981599562761887,
      "grad_norm": 4.957727909088135,
      "learning_rate": 7.946715231543434e-05,
      "loss": 2.6867,
      "step": 14790
    },
    {
      "epoch": 0.8987672314325621,
      "grad_norm": 3.7349653244018555,
      "learning_rate": 7.944145822512034e-05,
      "loss": 2.6197,
      "step": 14800
    },
    {
      "epoch": 0.8993745065889355,
      "grad_norm": 2.166565179824829,
      "learning_rate": 7.941575222821171e-05,
      "loss": 2.32,
      "step": 14810
    },
    {
      "epoch": 0.8999817817453089,
      "grad_norm": 1.8019078969955444,
      "learning_rate": 7.939003433510442e-05,
      "loss": 1.9526,
      "step": 14820
    },
    {
      "epoch": 0.9005890569016821,
      "grad_norm": 1.717832326889038,
      "learning_rate": 7.936430455619917e-05,
      "loss": 2.2671,
      "step": 14830
    },
    {
      "epoch": 0.9011963320580555,
      "grad_norm": 2.6785995960235596,
      "learning_rate": 7.933856290190149e-05,
      "loss": 2.3013,
      "step": 14840
    },
    {
      "epoch": 0.9018036072144289,
      "grad_norm": 2.3188323974609375,
      "learning_rate": 7.931280938262169e-05,
      "loss": 2.2861,
      "step": 14850
    },
    {
      "epoch": 0.9024108823708022,
      "grad_norm": 1.9764552116394043,
      "learning_rate": 7.928704400877495e-05,
      "loss": 2.2652,
      "step": 14860
    },
    {
      "epoch": 0.9030181575271755,
      "grad_norm": 3.1202142238616943,
      "learning_rate": 7.926126679078116e-05,
      "loss": 2.3702,
      "step": 14870
    },
    {
      "epoch": 0.9036254326835489,
      "grad_norm": 2.226175308227539,
      "learning_rate": 7.923547773906507e-05,
      "loss": 2.4236,
      "step": 14880
    },
    {
      "epoch": 0.9042327078399223,
      "grad_norm": 2.6553168296813965,
      "learning_rate": 7.920967686405616e-05,
      "loss": 2.7328,
      "step": 14890
    },
    {
      "epoch": 0.9048399829962956,
      "grad_norm": 3.43151593208313,
      "learning_rate": 7.918386417618872e-05,
      "loss": 2.5852,
      "step": 14900
    },
    {
      "epoch": 0.905447258152669,
      "grad_norm": 1.8898957967758179,
      "learning_rate": 7.915803968590181e-05,
      "loss": 2.5824,
      "step": 14910
    },
    {
      "epoch": 0.9060545333090423,
      "grad_norm": 2.6855783462524414,
      "learning_rate": 7.913220340363927e-05,
      "loss": 2.2231,
      "step": 14920
    },
    {
      "epoch": 0.9066618084654157,
      "grad_norm": 2.775252103805542,
      "learning_rate": 7.91063553398497e-05,
      "loss": 2.3945,
      "step": 14930
    },
    {
      "epoch": 0.907269083621789,
      "grad_norm": 2.539144992828369,
      "learning_rate": 7.908049550498648e-05,
      "loss": 2.6163,
      "step": 14940
    },
    {
      "epoch": 0.9078763587781624,
      "grad_norm": 1.9513890743255615,
      "learning_rate": 7.905462390950772e-05,
      "loss": 2.3562,
      "step": 14950
    },
    {
      "epoch": 0.9084836339345357,
      "grad_norm": 2.3795104026794434,
      "learning_rate": 7.902874056387633e-05,
      "loss": 2.4214,
      "step": 14960
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.6715095043182373,
      "learning_rate": 7.900284547855991e-05,
      "loss": 2.7066,
      "step": 14970
    },
    {
      "epoch": 0.9096981842472824,
      "grad_norm": 3.3640217781066895,
      "learning_rate": 7.897693866403089e-05,
      "loss": 2.6013,
      "step": 14980
    },
    {
      "epoch": 0.9103054594036558,
      "grad_norm": 2.447582721710205,
      "learning_rate": 7.895102013076637e-05,
      "loss": 2.5466,
      "step": 14990
    },
    {
      "epoch": 0.9109127345600292,
      "grad_norm": 2.505021572113037,
      "learning_rate": 7.892508988924822e-05,
      "loss": 2.3578,
      "step": 15000
    },
    {
      "epoch": 0.9110949171069411,
      "eval_loss": 4.4830098152160645,
      "eval_runtime": 2322.7101,
      "eval_samples_per_second": 7.09,
      "eval_steps_per_second": 1.772,
      "step": 15003
    },
    {
      "epoch": 0.9115200097164025,
      "grad_norm": 3.3838343620300293,
      "learning_rate": 7.889914794996304e-05,
      "loss": 3.4559,
      "step": 15010
    },
    {
      "epoch": 0.9121272848727758,
      "grad_norm": 3.776482105255127,
      "learning_rate": 7.887319432340218e-05,
      "loss": 3.5904,
      "step": 15020
    },
    {
      "epoch": 0.9127345600291492,
      "grad_norm": 3.785726308822632,
      "learning_rate": 7.884722902006168e-05,
      "loss": 2.6964,
      "step": 15030
    },
    {
      "epoch": 0.9133418351855226,
      "grad_norm": 2.918492078781128,
      "learning_rate": 7.882125205044234e-05,
      "loss": 2.2085,
      "step": 15040
    },
    {
      "epoch": 0.913949110341896,
      "grad_norm": 3.503875732421875,
      "learning_rate": 7.879526342504964e-05,
      "loss": 2.6518,
      "step": 15050
    },
    {
      "epoch": 0.9145563854982692,
      "grad_norm": 3.1826727390289307,
      "learning_rate": 7.876926315439382e-05,
      "loss": 2.2895,
      "step": 15060
    },
    {
      "epoch": 0.9151636606546426,
      "grad_norm": 3.940772771835327,
      "learning_rate": 7.874325124898978e-05,
      "loss": 2.1768,
      "step": 15070
    },
    {
      "epoch": 0.915770935811016,
      "grad_norm": 1.7419800758361816,
      "learning_rate": 7.871722771935716e-05,
      "loss": 2.0965,
      "step": 15080
    },
    {
      "epoch": 0.9163782109673894,
      "grad_norm": 2.1959474086761475,
      "learning_rate": 7.86911925760203e-05,
      "loss": 2.0993,
      "step": 15090
    },
    {
      "epoch": 0.9169854861237626,
      "grad_norm": 2.3109028339385986,
      "learning_rate": 7.86651458295082e-05,
      "loss": 2.3778,
      "step": 15100
    },
    {
      "epoch": 0.917592761280136,
      "grad_norm": 2.8039638996124268,
      "learning_rate": 7.86390874903546e-05,
      "loss": 2.3352,
      "step": 15110
    },
    {
      "epoch": 0.9182000364365094,
      "grad_norm": 3.0424575805664062,
      "learning_rate": 7.861301756909791e-05,
      "loss": 2.2549,
      "step": 15120
    },
    {
      "epoch": 0.9188073115928828,
      "grad_norm": 1.7120468616485596,
      "learning_rate": 7.858693607628121e-05,
      "loss": 2.5041,
      "step": 15130
    },
    {
      "epoch": 0.9194145867492561,
      "grad_norm": 3.7013604640960693,
      "learning_rate": 7.856084302245225e-05,
      "loss": 2.3236,
      "step": 15140
    },
    {
      "epoch": 0.9200218619056294,
      "grad_norm": 2.9843320846557617,
      "learning_rate": 7.853473841816353e-05,
      "loss": 2.1654,
      "step": 15150
    },
    {
      "epoch": 0.9206291370620028,
      "grad_norm": 1.5397008657455444,
      "learning_rate": 7.850862227397213e-05,
      "loss": 2.1939,
      "step": 15160
    },
    {
      "epoch": 0.9212364122183762,
      "grad_norm": 2.3865952491760254,
      "learning_rate": 7.848249460043984e-05,
      "loss": 2.6093,
      "step": 15170
    },
    {
      "epoch": 0.9218436873747495,
      "grad_norm": 3.7049291133880615,
      "learning_rate": 7.84563554081331e-05,
      "loss": 2.7605,
      "step": 15180
    },
    {
      "epoch": 0.9224509625311228,
      "grad_norm": 3.143036365509033,
      "learning_rate": 7.843020470762305e-05,
      "loss": 2.2811,
      "step": 15190
    },
    {
      "epoch": 0.9230582376874962,
      "grad_norm": 2.728450059890747,
      "learning_rate": 7.84040425094854e-05,
      "loss": 2.442,
      "step": 15200
    },
    {
      "epoch": 0.9236655128438696,
      "grad_norm": 2.566380739212036,
      "learning_rate": 7.83778688243006e-05,
      "loss": 2.5174,
      "step": 15210
    },
    {
      "epoch": 0.9242727880002429,
      "grad_norm": 3.0204076766967773,
      "learning_rate": 7.835168366265368e-05,
      "loss": 2.4993,
      "step": 15220
    },
    {
      "epoch": 0.9248800631566163,
      "grad_norm": 2.527573823928833,
      "learning_rate": 7.832548703513436e-05,
      "loss": 2.5463,
      "step": 15230
    },
    {
      "epoch": 0.9254873383129896,
      "grad_norm": 3.2340967655181885,
      "learning_rate": 7.829927895233695e-05,
      "loss": 2.4937,
      "step": 15240
    },
    {
      "epoch": 0.926094613469363,
      "grad_norm": 2.4668309688568115,
      "learning_rate": 7.827305942486043e-05,
      "loss": 2.4797,
      "step": 15250
    },
    {
      "epoch": 0.9267018886257363,
      "grad_norm": 3.8804962635040283,
      "learning_rate": 7.824682846330837e-05,
      "loss": 2.7585,
      "step": 15260
    },
    {
      "epoch": 0.9273091637821097,
      "grad_norm": 7.703159332275391,
      "learning_rate": 7.822058607828904e-05,
      "loss": 2.8662,
      "step": 15270
    },
    {
      "epoch": 0.9279164389384831,
      "grad_norm": 3.452092170715332,
      "learning_rate": 7.819433228041524e-05,
      "loss": 2.5161,
      "step": 15280
    },
    {
      "epoch": 0.9285237140948563,
      "grad_norm": 4.544465065002441,
      "learning_rate": 7.816806708030442e-05,
      "loss": 2.2135,
      "step": 15290
    },
    {
      "epoch": 0.9291309892512297,
      "grad_norm": 3.4999186992645264,
      "learning_rate": 7.814179048857866e-05,
      "loss": 2.3259,
      "step": 15300
    },
    {
      "epoch": 0.9297382644076031,
      "grad_norm": 2.0343661308288574,
      "learning_rate": 7.811550251586463e-05,
      "loss": 2.2386,
      "step": 15310
    },
    {
      "epoch": 0.9303455395639765,
      "grad_norm": 3.3276662826538086,
      "learning_rate": 7.808920317279362e-05,
      "loss": 2.3886,
      "step": 15320
    },
    {
      "epoch": 0.9309528147203497,
      "grad_norm": 2.4290812015533447,
      "learning_rate": 7.806289247000146e-05,
      "loss": 2.4292,
      "step": 15330
    },
    {
      "epoch": 0.9315600898767231,
      "grad_norm": 2.563664197921753,
      "learning_rate": 7.803657041812866e-05,
      "loss": 2.7228,
      "step": 15340
    },
    {
      "epoch": 0.9321673650330965,
      "grad_norm": 2.0519354343414307,
      "learning_rate": 7.801023702782027e-05,
      "loss": 2.3366,
      "step": 15350
    },
    {
      "epoch": 0.9327746401894699,
      "grad_norm": 1.7212198972702026,
      "learning_rate": 7.798389230972592e-05,
      "loss": 2.274,
      "step": 15360
    },
    {
      "epoch": 0.9333819153458432,
      "grad_norm": 1.9410597085952759,
      "learning_rate": 7.795753627449982e-05,
      "loss": 2.1796,
      "step": 15370
    },
    {
      "epoch": 0.9339891905022165,
      "grad_norm": 1.310122013092041,
      "learning_rate": 7.79311689328008e-05,
      "loss": 1.9421,
      "step": 15380
    },
    {
      "epoch": 0.9345964656585899,
      "grad_norm": 2.0357577800750732,
      "learning_rate": 7.790479029529221e-05,
      "loss": 2.7292,
      "step": 15390
    },
    {
      "epoch": 0.9352037408149633,
      "grad_norm": 2.90539288520813,
      "learning_rate": 7.787840037264202e-05,
      "loss": 2.5699,
      "step": 15400
    },
    {
      "epoch": 0.9358110159713366,
      "grad_norm": 2.64540958404541,
      "learning_rate": 7.78519991755227e-05,
      "loss": 2.4895,
      "step": 15410
    },
    {
      "epoch": 0.9364182911277099,
      "grad_norm": 2.8549838066101074,
      "learning_rate": 7.782558671461134e-05,
      "loss": 2.2363,
      "step": 15420
    },
    {
      "epoch": 0.9370255662840833,
      "grad_norm": 2.1893811225891113,
      "learning_rate": 7.779916300058955e-05,
      "loss": 2.6483,
      "step": 15430
    },
    {
      "epoch": 0.9376328414404567,
      "grad_norm": 2.7163429260253906,
      "learning_rate": 7.77727280441435e-05,
      "loss": 2.451,
      "step": 15440
    },
    {
      "epoch": 0.93824011659683,
      "grad_norm": 2.248129367828369,
      "learning_rate": 7.774628185596391e-05,
      "loss": 2.4031,
      "step": 15450
    },
    {
      "epoch": 0.9388473917532034,
      "grad_norm": 2.3666789531707764,
      "learning_rate": 7.771982444674604e-05,
      "loss": 2.5534,
      "step": 15460
    },
    {
      "epoch": 0.9394546669095767,
      "grad_norm": 2.247781753540039,
      "learning_rate": 7.769335582718968e-05,
      "loss": 2.1649,
      "step": 15470
    },
    {
      "epoch": 0.9400619420659501,
      "grad_norm": 1.2278811931610107,
      "learning_rate": 7.766687600799918e-05,
      "loss": 2.1256,
      "step": 15480
    },
    {
      "epoch": 0.9406692172223234,
      "grad_norm": 1.8245683908462524,
      "learning_rate": 7.764038499988338e-05,
      "loss": 1.9464,
      "step": 15490
    },
    {
      "epoch": 0.9412764923786968,
      "grad_norm": 3.1644599437713623,
      "learning_rate": 7.761388281355569e-05,
      "loss": 2.1037,
      "step": 15500
    },
    {
      "epoch": 0.9418837675350702,
      "grad_norm": 4.17237663269043,
      "learning_rate": 7.7587369459734e-05,
      "loss": 2.4975,
      "step": 15510
    },
    {
      "epoch": 0.9424910426914435,
      "grad_norm": 2.1713597774505615,
      "learning_rate": 7.75608449491407e-05,
      "loss": 2.5448,
      "step": 15520
    },
    {
      "epoch": 0.9430983178478168,
      "grad_norm": 2.276399850845337,
      "learning_rate": 7.753430929250278e-05,
      "loss": 2.5464,
      "step": 15530
    },
    {
      "epoch": 0.9437055930041902,
      "grad_norm": 2.668902635574341,
      "learning_rate": 7.750776250055165e-05,
      "loss": 2.7589,
      "step": 15540
    },
    {
      "epoch": 0.9443128681605636,
      "grad_norm": 2.7473864555358887,
      "learning_rate": 7.748120458402328e-05,
      "loss": 2.4082,
      "step": 15550
    },
    {
      "epoch": 0.9449201433169369,
      "grad_norm": 2.3531837463378906,
      "learning_rate": 7.745463555365809e-05,
      "loss": 2.2309,
      "step": 15560
    },
    {
      "epoch": 0.9455274184733102,
      "grad_norm": 2.7354495525360107,
      "learning_rate": 7.742805542020104e-05,
      "loss": 2.8973,
      "step": 15570
    },
    {
      "epoch": 0.9461346936296836,
      "grad_norm": 2.0404837131500244,
      "learning_rate": 7.740146419440153e-05,
      "loss": 2.5625,
      "step": 15580
    },
    {
      "epoch": 0.946741968786057,
      "grad_norm": 2.584423303604126,
      "learning_rate": 7.737486188701351e-05,
      "loss": 2.3129,
      "step": 15590
    },
    {
      "epoch": 0.9473492439424304,
      "grad_norm": 2.092221260070801,
      "learning_rate": 7.734824850879535e-05,
      "loss": 2.2979,
      "step": 15600
    },
    {
      "epoch": 0.9479565190988036,
      "grad_norm": 2.8912882804870605,
      "learning_rate": 7.732162407050994e-05,
      "loss": 2.3391,
      "step": 15610
    },
    {
      "epoch": 0.948563794255177,
      "grad_norm": 2.735154390335083,
      "learning_rate": 7.729498858292461e-05,
      "loss": 2.7176,
      "step": 15620
    },
    {
      "epoch": 0.9491710694115504,
      "grad_norm": 4.048547744750977,
      "learning_rate": 7.72683420568112e-05,
      "loss": 2.6865,
      "step": 15630
    },
    {
      "epoch": 0.9497783445679238,
      "grad_norm": 2.854902505874634,
      "learning_rate": 7.724168450294597e-05,
      "loss": 2.3698,
      "step": 15640
    },
    {
      "epoch": 0.950385619724297,
      "grad_norm": 2.97930908203125,
      "learning_rate": 7.721501593210967e-05,
      "loss": 2.4847,
      "step": 15650
    },
    {
      "epoch": 0.9509928948806704,
      "grad_norm": 3.2094507217407227,
      "learning_rate": 7.718833635508748e-05,
      "loss": 2.5584,
      "step": 15660
    },
    {
      "epoch": 0.9516001700370438,
      "grad_norm": 3.1259214878082275,
      "learning_rate": 7.716164578266907e-05,
      "loss": 2.4315,
      "step": 15670
    },
    {
      "epoch": 0.9522074451934172,
      "grad_norm": 2.5143697261810303,
      "learning_rate": 7.71349442256485e-05,
      "loss": 2.2106,
      "step": 15680
    },
    {
      "epoch": 0.9528147203497905,
      "grad_norm": 2.8077034950256348,
      "learning_rate": 7.710823169482433e-05,
      "loss": 2.4839,
      "step": 15690
    },
    {
      "epoch": 0.9534219955061638,
      "grad_norm": 3.8490984439849854,
      "learning_rate": 7.708150820099953e-05,
      "loss": 2.5395,
      "step": 15700
    },
    {
      "epoch": 0.9540292706625372,
      "grad_norm": 3.9497735500335693,
      "learning_rate": 7.705477375498148e-05,
      "loss": 2.9362,
      "step": 15710
    },
    {
      "epoch": 0.9546365458189106,
      "grad_norm": 3.260486602783203,
      "learning_rate": 7.702802836758207e-05,
      "loss": 2.8801,
      "step": 15720
    },
    {
      "epoch": 0.9552438209752839,
      "grad_norm": 3.297589063644409,
      "learning_rate": 7.700127204961752e-05,
      "loss": 2.3747,
      "step": 15730
    },
    {
      "epoch": 0.9558510961316573,
      "grad_norm": 2.6584463119506836,
      "learning_rate": 7.697450481190851e-05,
      "loss": 3.041,
      "step": 15740
    },
    {
      "epoch": 0.9564583712880306,
      "grad_norm": 5.243325233459473,
      "learning_rate": 7.694772666528016e-05,
      "loss": 2.5621,
      "step": 15750
    },
    {
      "epoch": 0.957065646444404,
      "grad_norm": 3.3690779209136963,
      "learning_rate": 7.692093762056196e-05,
      "loss": 2.6608,
      "step": 15760
    },
    {
      "epoch": 0.9576729216007773,
      "grad_norm": 4.844656944274902,
      "learning_rate": 7.689413768858783e-05,
      "loss": 3.0542,
      "step": 15770
    },
    {
      "epoch": 0.9582801967571507,
      "grad_norm": 2.8305485248565674,
      "learning_rate": 7.686732688019611e-05,
      "loss": 2.2368,
      "step": 15780
    },
    {
      "epoch": 0.958887471913524,
      "grad_norm": 3.448009490966797,
      "learning_rate": 7.684050520622947e-05,
      "loss": 2.2497,
      "step": 15790
    },
    {
      "epoch": 0.9594947470698973,
      "grad_norm": 2.821291923522949,
      "learning_rate": 7.681367267753508e-05,
      "loss": 2.5204,
      "step": 15800
    },
    {
      "epoch": 0.9601020222262707,
      "grad_norm": 3.2434916496276855,
      "learning_rate": 7.678682930496439e-05,
      "loss": 2.4907,
      "step": 15810
    },
    {
      "epoch": 0.9607092973826441,
      "grad_norm": 3.2554771900177,
      "learning_rate": 7.675997509937333e-05,
      "loss": 2.6828,
      "step": 15820
    },
    {
      "epoch": 0.9613165725390175,
      "grad_norm": 1.8367259502410889,
      "learning_rate": 7.673311007162214e-05,
      "loss": 2.3076,
      "step": 15830
    },
    {
      "epoch": 0.9619238476953907,
      "grad_norm": 2.811612129211426,
      "learning_rate": 7.670623423257548e-05,
      "loss": 2.3695,
      "step": 15840
    },
    {
      "epoch": 0.9625311228517641,
      "grad_norm": 1.5722495317459106,
      "learning_rate": 7.667934759310236e-05,
      "loss": 2.0114,
      "step": 15850
    },
    {
      "epoch": 0.9631383980081375,
      "grad_norm": 1.2013311386108398,
      "learning_rate": 7.665245016407617e-05,
      "loss": 2.0139,
      "step": 15860
    },
    {
      "epoch": 0.9637456731645109,
      "grad_norm": 2.53019380569458,
      "learning_rate": 7.662554195637464e-05,
      "loss": 2.5398,
      "step": 15870
    },
    {
      "epoch": 0.9643529483208841,
      "grad_norm": 3.4593803882598877,
      "learning_rate": 7.659862298087991e-05,
      "loss": 2.7475,
      "step": 15880
    },
    {
      "epoch": 0.9649602234772575,
      "grad_norm": 4.647335529327393,
      "learning_rate": 7.657169324847842e-05,
      "loss": 2.5941,
      "step": 15890
    },
    {
      "epoch": 0.9655674986336309,
      "grad_norm": 3.3269615173339844,
      "learning_rate": 7.654475277006098e-05,
      "loss": 2.5818,
      "step": 15900
    },
    {
      "epoch": 0.9661747737900043,
      "grad_norm": 2.4165642261505127,
      "learning_rate": 7.651780155652277e-05,
      "loss": 2.2582,
      "step": 15910
    },
    {
      "epoch": 0.9667820489463776,
      "grad_norm": 1.7330423593521118,
      "learning_rate": 7.649083961876329e-05,
      "loss": 2.1781,
      "step": 15920
    },
    {
      "epoch": 0.9673893241027509,
      "grad_norm": 4.600907802581787,
      "learning_rate": 7.646386696768635e-05,
      "loss": 2.3722,
      "step": 15930
    },
    {
      "epoch": 0.9679965992591243,
      "grad_norm": 2.917179584503174,
      "learning_rate": 7.643688361420014e-05,
      "loss": 2.694,
      "step": 15940
    },
    {
      "epoch": 0.9686038744154977,
      "grad_norm": 3.6189873218536377,
      "learning_rate": 7.640988956921715e-05,
      "loss": 2.4481,
      "step": 15950
    },
    {
      "epoch": 0.969211149571871,
      "grad_norm": 3.0618910789489746,
      "learning_rate": 7.63828848436542e-05,
      "loss": 2.913,
      "step": 15960
    },
    {
      "epoch": 0.9698184247282444,
      "grad_norm": 2.347747564315796,
      "learning_rate": 7.635586944843242e-05,
      "loss": 2.5606,
      "step": 15970
    },
    {
      "epoch": 0.9704256998846177,
      "grad_norm": 1.7678221464157104,
      "learning_rate": 7.63288433944773e-05,
      "loss": 2.2814,
      "step": 15980
    },
    {
      "epoch": 0.9710329750409911,
      "grad_norm": 2.215529680252075,
      "learning_rate": 7.630180669271859e-05,
      "loss": 2.2218,
      "step": 15990
    },
    {
      "epoch": 0.9716402501973644,
      "grad_norm": 2.7217302322387695,
      "learning_rate": 7.627475935409034e-05,
      "loss": 2.5395,
      "step": 16000
    },
    {
      "epoch": 0.9722475253537378,
      "grad_norm": 2.435736656188965,
      "learning_rate": 7.624770138953097e-05,
      "loss": 2.4066,
      "step": 16010
    },
    {
      "epoch": 0.9728548005101111,
      "grad_norm": 3.1278278827667236,
      "learning_rate": 7.622063280998311e-05,
      "loss": 2.9669,
      "step": 16020
    },
    {
      "epoch": 0.9734620756664845,
      "grad_norm": 2.8675901889801025,
      "learning_rate": 7.619355362639376e-05,
      "loss": 2.5631,
      "step": 16030
    },
    {
      "epoch": 0.9740693508228578,
      "grad_norm": 1.772070288658142,
      "learning_rate": 7.616646384971413e-05,
      "loss": 2.4004,
      "step": 16040
    },
    {
      "epoch": 0.9746766259792312,
      "grad_norm": 1.9250037670135498,
      "learning_rate": 7.613936349089981e-05,
      "loss": 2.043,
      "step": 16050
    },
    {
      "epoch": 0.9752839011356046,
      "grad_norm": 1.4289679527282715,
      "learning_rate": 7.611225256091057e-05,
      "loss": 2.1008,
      "step": 16060
    },
    {
      "epoch": 0.9758911762919779,
      "grad_norm": 2.1516525745391846,
      "learning_rate": 7.608513107071052e-05,
      "loss": 2.4704,
      "step": 16070
    },
    {
      "epoch": 0.9764984514483512,
      "grad_norm": 2.682755470275879,
      "learning_rate": 7.605799903126802e-05,
      "loss": 2.3809,
      "step": 16080
    },
    {
      "epoch": 0.9771057266047246,
      "grad_norm": 2.487910747528076,
      "learning_rate": 7.603085645355571e-05,
      "loss": 2.6309,
      "step": 16090
    },
    {
      "epoch": 0.977713001761098,
      "grad_norm": 2.2326955795288086,
      "learning_rate": 7.600370334855047e-05,
      "loss": 2.4418,
      "step": 16100
    },
    {
      "epoch": 0.9783202769174713,
      "grad_norm": 3.2007806301116943,
      "learning_rate": 7.597653972723344e-05,
      "loss": 2.2777,
      "step": 16110
    },
    {
      "epoch": 0.9789275520738446,
      "grad_norm": 2.117856740951538,
      "learning_rate": 7.594936560059005e-05,
      "loss": 2.1677,
      "step": 16120
    },
    {
      "epoch": 0.979534827230218,
      "grad_norm": 2.542632818222046,
      "learning_rate": 7.592218097960993e-05,
      "loss": 2.5105,
      "step": 16130
    },
    {
      "epoch": 0.9801421023865914,
      "grad_norm": 2.7980542182922363,
      "learning_rate": 7.589498587528697e-05,
      "loss": 2.3792,
      "step": 16140
    },
    {
      "epoch": 0.9807493775429648,
      "grad_norm": 2.7859060764312744,
      "learning_rate": 7.58677802986193e-05,
      "loss": 2.041,
      "step": 16150
    },
    {
      "epoch": 0.981356652699338,
      "grad_norm": 2.773711919784546,
      "learning_rate": 7.58405642606093e-05,
      "loss": 2.2397,
      "step": 16160
    },
    {
      "epoch": 0.9819639278557114,
      "grad_norm": 3.502168893814087,
      "learning_rate": 7.581333777226356e-05,
      "loss": 2.2907,
      "step": 16170
    },
    {
      "epoch": 0.9825712030120848,
      "grad_norm": 3.4150657653808594,
      "learning_rate": 7.578610084459293e-05,
      "loss": 2.4584,
      "step": 16180
    },
    {
      "epoch": 0.9831784781684582,
      "grad_norm": 2.371636390686035,
      "learning_rate": 7.575885348861241e-05,
      "loss": 2.664,
      "step": 16190
    },
    {
      "epoch": 0.9837857533248315,
      "grad_norm": 2.261765480041504,
      "learning_rate": 7.573159571534132e-05,
      "loss": 2.7039,
      "step": 16200
    },
    {
      "epoch": 0.9843930284812048,
      "grad_norm": 3.641040086746216,
      "learning_rate": 7.57043275358031e-05,
      "loss": 2.8969,
      "step": 16210
    },
    {
      "epoch": 0.9850003036375782,
      "grad_norm": 2.7097256183624268,
      "learning_rate": 7.567704896102545e-05,
      "loss": 2.7073,
      "step": 16220
    },
    {
      "epoch": 0.9856075787939516,
      "grad_norm": 2.5248239040374756,
      "learning_rate": 7.564976000204024e-05,
      "loss": 2.384,
      "step": 16230
    },
    {
      "epoch": 0.9862148539503249,
      "grad_norm": 0.8192515969276428,
      "learning_rate": 7.562246066988362e-05,
      "loss": 2.1092,
      "step": 16240
    },
    {
      "epoch": 0.9868221291066982,
      "grad_norm": 2.010883092880249,
      "learning_rate": 7.559515097559579e-05,
      "loss": 2.4439,
      "step": 16250
    },
    {
      "epoch": 0.9874294042630716,
      "grad_norm": 2.6490373611450195,
      "learning_rate": 7.556783093022128e-05,
      "loss": 2.4242,
      "step": 16260
    },
    {
      "epoch": 0.988036679419445,
      "grad_norm": 3.6926238536834717,
      "learning_rate": 7.554050054480875e-05,
      "loss": 2.6437,
      "step": 16270
    },
    {
      "epoch": 0.9886439545758183,
      "grad_norm": 3.7239480018615723,
      "learning_rate": 7.551315983041101e-05,
      "loss": 3.0772,
      "step": 16280
    },
    {
      "epoch": 0.9892512297321917,
      "grad_norm": 3.9250261783599854,
      "learning_rate": 7.548580879808512e-05,
      "loss": 2.5929,
      "step": 16290
    },
    {
      "epoch": 0.989858504888565,
      "grad_norm": 3.3257052898406982,
      "learning_rate": 7.545844745889225e-05,
      "loss": 2.8752,
      "step": 16300
    },
    {
      "epoch": 0.9904657800449383,
      "grad_norm": 3.0915865898132324,
      "learning_rate": 7.543107582389776e-05,
      "loss": 2.5074,
      "step": 16310
    },
    {
      "epoch": 0.9910730552013117,
      "grad_norm": 4.811978340148926,
      "learning_rate": 7.540369390417119e-05,
      "loss": 2.6398,
      "step": 16320
    },
    {
      "epoch": 0.9916803303576851,
      "grad_norm": 3.4291813373565674,
      "learning_rate": 7.53763017107862e-05,
      "loss": 2.4704,
      "step": 16330
    },
    {
      "epoch": 0.9922876055140584,
      "grad_norm": 4.447512149810791,
      "learning_rate": 7.534889925482067e-05,
      "loss": 2.4475,
      "step": 16340
    },
    {
      "epoch": 0.9928948806704317,
      "grad_norm": 2.8854727745056152,
      "learning_rate": 7.532148654735653e-05,
      "loss": 2.5026,
      "step": 16350
    },
    {
      "epoch": 0.9935021558268051,
      "grad_norm": 2.551222324371338,
      "learning_rate": 7.529406359947999e-05,
      "loss": 2.5345,
      "step": 16360
    },
    {
      "epoch": 0.9941094309831785,
      "grad_norm": 3.1360068321228027,
      "learning_rate": 7.526663042228125e-05,
      "loss": 2.5454,
      "step": 16370
    },
    {
      "epoch": 0.9947167061395519,
      "grad_norm": 2.3546061515808105,
      "learning_rate": 7.52391870268548e-05,
      "loss": 2.3976,
      "step": 16380
    },
    {
      "epoch": 0.9953239812959251,
      "grad_norm": 2.1330440044403076,
      "learning_rate": 7.521173342429911e-05,
      "loss": 2.5301,
      "step": 16390
    },
    {
      "epoch": 0.9959312564522985,
      "grad_norm": 1.6910419464111328,
      "learning_rate": 7.518426962571691e-05,
      "loss": 2.0857,
      "step": 16400
    },
    {
      "epoch": 0.9965385316086719,
      "grad_norm": 2.6917760372161865,
      "learning_rate": 7.515679564221498e-05,
      "loss": 2.2011,
      "step": 16410
    },
    {
      "epoch": 0.9971458067650453,
      "grad_norm": 1.8819271326065063,
      "learning_rate": 7.512931148490423e-05,
      "loss": 2.5022,
      "step": 16420
    },
    {
      "epoch": 0.9977530819214186,
      "grad_norm": 1.388978362083435,
      "learning_rate": 7.51018171648997e-05,
      "loss": 2.0969,
      "step": 16430
    },
    {
      "epoch": 0.9983603570777919,
      "grad_norm": 2.672764301300049,
      "learning_rate": 7.507431269332053e-05,
      "loss": 2.346,
      "step": 16440
    },
    {
      "epoch": 0.9989676322341653,
      "grad_norm": 1.9770917892456055,
      "learning_rate": 7.504679808128995e-05,
      "loss": 2.3567,
      "step": 16450
    },
    {
      "epoch": 0.9995749073905387,
      "grad_norm": 1.213008999824524,
      "learning_rate": 7.501927333993533e-05,
      "loss": 1.8997,
      "step": 16460
    },
    {
      "epoch": 1.000182182546912,
      "grad_norm": 2.2300033569335938,
      "learning_rate": 7.49917384803881e-05,
      "loss": 2.1452,
      "step": 16470
    },
    {
      "epoch": 1.0007894577032854,
      "grad_norm": 3.3467369079589844,
      "learning_rate": 7.49641935137838e-05,
      "loss": 2.3975,
      "step": 16480
    },
    {
      "epoch": 1.0013967328596587,
      "grad_norm": 5.290434837341309,
      "learning_rate": 7.493663845126205e-05,
      "loss": 2.3926,
      "step": 16490
    },
    {
      "epoch": 1.002004008016032,
      "grad_norm": 2.518239736557007,
      "learning_rate": 7.490907330396657e-05,
      "loss": 2.5354,
      "step": 16500
    },
    {
      "epoch": 1.0026112831724054,
      "grad_norm": 3.5240888595581055,
      "learning_rate": 7.488149808304514e-05,
      "loss": 2.6042,
      "step": 16510
    },
    {
      "epoch": 1.0032185583287787,
      "grad_norm": 2.704362392425537,
      "learning_rate": 7.485391279964958e-05,
      "loss": 2.3841,
      "step": 16520
    },
    {
      "epoch": 1.0038258334851522,
      "grad_norm": 2.8501522541046143,
      "learning_rate": 7.482631746493588e-05,
      "loss": 2.3176,
      "step": 16530
    },
    {
      "epoch": 1.0044331086415255,
      "grad_norm": 2.5881896018981934,
      "learning_rate": 7.4798712090064e-05,
      "loss": 2.1438,
      "step": 16540
    },
    {
      "epoch": 1.0050403837978987,
      "grad_norm": 2.2903072834014893,
      "learning_rate": 7.4771096686198e-05,
      "loss": 2.5394,
      "step": 16550
    },
    {
      "epoch": 1.0056476589542722,
      "grad_norm": 2.6375346183776855,
      "learning_rate": 7.474347126450596e-05,
      "loss": 2.5032,
      "step": 16560
    },
    {
      "epoch": 1.0062549341106455,
      "grad_norm": 2.406923532485962,
      "learning_rate": 7.47158358361601e-05,
      "loss": 2.6325,
      "step": 16570
    },
    {
      "epoch": 1.006862209267019,
      "grad_norm": 1.662270426750183,
      "learning_rate": 7.468819041233656e-05,
      "loss": 2.3184,
      "step": 16580
    },
    {
      "epoch": 1.0074694844233922,
      "grad_norm": 1.1426562070846558,
      "learning_rate": 7.466053500421565e-05,
      "loss": 1.9151,
      "step": 16590
    },
    {
      "epoch": 1.0080767595797655,
      "grad_norm": 1.600846767425537,
      "learning_rate": 7.463286962298162e-05,
      "loss": 1.9988,
      "step": 16600
    },
    {
      "epoch": 1.008684034736139,
      "grad_norm": 2.5900344848632812,
      "learning_rate": 7.460519427982281e-05,
      "loss": 2.2235,
      "step": 16610
    },
    {
      "epoch": 1.0092913098925123,
      "grad_norm": 3.4321486949920654,
      "learning_rate": 7.457750898593154e-05,
      "loss": 2.7333,
      "step": 16620
    },
    {
      "epoch": 1.0098985850488857,
      "grad_norm": 3.397627115249634,
      "learning_rate": 7.454981375250423e-05,
      "loss": 2.4097,
      "step": 16630
    },
    {
      "epoch": 1.010505860205259,
      "grad_norm": 2.3231072425842285,
      "learning_rate": 7.452210859074123e-05,
      "loss": 2.0868,
      "step": 16640
    },
    {
      "epoch": 1.0111131353616323,
      "grad_norm": 1.5357143878936768,
      "learning_rate": 7.449439351184698e-05,
      "loss": 1.9642,
      "step": 16650
    },
    {
      "epoch": 1.0117204105180058,
      "grad_norm": 3.628856658935547,
      "learning_rate": 7.446666852702987e-05,
      "loss": 2.2392,
      "step": 16660
    },
    {
      "epoch": 1.012327685674379,
      "grad_norm": 1.516406774520874,
      "learning_rate": 7.443893364750236e-05,
      "loss": 2.1896,
      "step": 16670
    },
    {
      "epoch": 1.0129349608307525,
      "grad_norm": 1.6754002571105957,
      "learning_rate": 7.441118888448085e-05,
      "loss": 2.1542,
      "step": 16680
    },
    {
      "epoch": 1.0135422359871258,
      "grad_norm": 2.5542776584625244,
      "learning_rate": 7.438343424918577e-05,
      "loss": 2.4731,
      "step": 16690
    },
    {
      "epoch": 1.014149511143499,
      "grad_norm": 2.69513201713562,
      "learning_rate": 7.435566975284154e-05,
      "loss": 2.5893,
      "step": 16700
    },
    {
      "epoch": 1.0147567862998725,
      "grad_norm": 1.657680869102478,
      "learning_rate": 7.432789540667657e-05,
      "loss": 2.7283,
      "step": 16710
    },
    {
      "epoch": 1.0153640614562458,
      "grad_norm": 2.659292459487915,
      "learning_rate": 7.430011122192324e-05,
      "loss": 2.5297,
      "step": 16720
    },
    {
      "epoch": 1.015971336612619,
      "grad_norm": 2.805443048477173,
      "learning_rate": 7.427231720981791e-05,
      "loss": 2.1248,
      "step": 16730
    },
    {
      "epoch": 1.0165786117689926,
      "grad_norm": 2.2397613525390625,
      "learning_rate": 7.424451338160095e-05,
      "loss": 2.228,
      "step": 16740
    },
    {
      "epoch": 1.0171858869253658,
      "grad_norm": 2.0699470043182373,
      "learning_rate": 7.421669974851663e-05,
      "loss": 2.28,
      "step": 16750
    },
    {
      "epoch": 1.0177931620817393,
      "grad_norm": 2.5967466831207275,
      "learning_rate": 7.418887632181327e-05,
      "loss": 2.3678,
      "step": 16760
    },
    {
      "epoch": 1.0184004372381126,
      "grad_norm": 2.30029559135437,
      "learning_rate": 7.416104311274304e-05,
      "loss": 2.3529,
      "step": 16770
    },
    {
      "epoch": 1.0190077123944858,
      "grad_norm": 2.5348081588745117,
      "learning_rate": 7.413320013256222e-05,
      "loss": 2.2753,
      "step": 16780
    },
    {
      "epoch": 1.0196149875508593,
      "grad_norm": 4.367463111877441,
      "learning_rate": 7.410534739253089e-05,
      "loss": 2.7004,
      "step": 16790
    },
    {
      "epoch": 1.0202222627072326,
      "grad_norm": 5.433869361877441,
      "learning_rate": 7.407748490391318e-05,
      "loss": 2.5066,
      "step": 16800
    },
    {
      "epoch": 1.020829537863606,
      "grad_norm": 1.9731565713882446,
      "learning_rate": 7.404961267797709e-05,
      "loss": 2.2259,
      "step": 16810
    },
    {
      "epoch": 1.0214368130199794,
      "grad_norm": 3.612797737121582,
      "learning_rate": 7.402173072599462e-05,
      "loss": 2.732,
      "step": 16820
    },
    {
      "epoch": 1.0220440881763526,
      "grad_norm": 3.1052961349487305,
      "learning_rate": 7.399383905924165e-05,
      "loss": 2.6153,
      "step": 16830
    },
    {
      "epoch": 1.022651363332726,
      "grad_norm": 3.7769901752471924,
      "learning_rate": 7.396593768899805e-05,
      "loss": 2.5594,
      "step": 16840
    },
    {
      "epoch": 1.0232586384890994,
      "grad_norm": 1.557926893234253,
      "learning_rate": 7.393802662654755e-05,
      "loss": 2.1483,
      "step": 16850
    },
    {
      "epoch": 1.0238659136454729,
      "grad_norm": 1.9986075162887573,
      "learning_rate": 7.391010588317782e-05,
      "loss": 1.9952,
      "step": 16860
    },
    {
      "epoch": 1.0244731888018461,
      "grad_norm": 1.7630712985992432,
      "learning_rate": 7.388217547018048e-05,
      "loss": 2.6562,
      "step": 16870
    },
    {
      "epoch": 1.0250804639582194,
      "grad_norm": 2.211613416671753,
      "learning_rate": 7.385423539885103e-05,
      "loss": 2.5195,
      "step": 16880
    },
    {
      "epoch": 1.0256877391145929,
      "grad_norm": 1.735293984413147,
      "learning_rate": 7.382628568048886e-05,
      "loss": 2.1464,
      "step": 16890
    },
    {
      "epoch": 1.0262950142709661,
      "grad_norm": 3.015000104904175,
      "learning_rate": 7.379832632639729e-05,
      "loss": 2.2392,
      "step": 16900
    },
    {
      "epoch": 1.0269022894273396,
      "grad_norm": 2.083535671234131,
      "learning_rate": 7.377035734788355e-05,
      "loss": 2.2766,
      "step": 16910
    },
    {
      "epoch": 1.027509564583713,
      "grad_norm": 1.6180250644683838,
      "learning_rate": 7.374237875625872e-05,
      "loss": 2.1796,
      "step": 16920
    },
    {
      "epoch": 1.0281168397400862,
      "grad_norm": 1.5626914501190186,
      "learning_rate": 7.371439056283779e-05,
      "loss": 2.0453,
      "step": 16930
    },
    {
      "epoch": 1.0287241148964597,
      "grad_norm": 4.286826133728027,
      "learning_rate": 7.368639277893962e-05,
      "loss": 2.7555,
      "step": 16940
    },
    {
      "epoch": 1.029331390052833,
      "grad_norm": 3.8016269207000732,
      "learning_rate": 7.365838541588698e-05,
      "loss": 2.5435,
      "step": 16950
    },
    {
      "epoch": 1.0299386652092062,
      "grad_norm": 2.7585020065307617,
      "learning_rate": 7.363036848500649e-05,
      "loss": 2.5844,
      "step": 16960
    },
    {
      "epoch": 1.0305459403655797,
      "grad_norm": 2.0620741844177246,
      "learning_rate": 7.360234199762863e-05,
      "loss": 2.1843,
      "step": 16970
    },
    {
      "epoch": 1.031153215521953,
      "grad_norm": 2.0876951217651367,
      "learning_rate": 7.357430596508776e-05,
      "loss": 2.4249,
      "step": 16980
    },
    {
      "epoch": 1.0317604906783264,
      "grad_norm": 1.526053547859192,
      "learning_rate": 7.354626039872212e-05,
      "loss": 2.3907,
      "step": 16990
    },
    {
      "epoch": 1.0323677658346997,
      "grad_norm": 1.7861580848693848,
      "learning_rate": 7.351820530987374e-05,
      "loss": 2.2635,
      "step": 17000
    },
    {
      "epoch": 1.032975040991073,
      "grad_norm": 3.07169771194458,
      "learning_rate": 7.34901407098886e-05,
      "loss": 2.5001,
      "step": 17010
    },
    {
      "epoch": 1.0335823161474464,
      "grad_norm": 2.5938785076141357,
      "learning_rate": 7.346206661011643e-05,
      "loss": 2.3975,
      "step": 17020
    },
    {
      "epoch": 1.0341895913038197,
      "grad_norm": 2.2124712467193604,
      "learning_rate": 7.343398302191087e-05,
      "loss": 2.3181,
      "step": 17030
    },
    {
      "epoch": 1.0347968664601932,
      "grad_norm": 3.1348679065704346,
      "learning_rate": 7.340588995662934e-05,
      "loss": 2.3007,
      "step": 17040
    },
    {
      "epoch": 1.0354041416165665,
      "grad_norm": 2.991563081741333,
      "learning_rate": 7.337778742563315e-05,
      "loss": 2.5878,
      "step": 17050
    },
    {
      "epoch": 1.0360114167729397,
      "grad_norm": 2.443864345550537,
      "learning_rate": 7.33496754402874e-05,
      "loss": 2.208,
      "step": 17060
    },
    {
      "epoch": 1.0366186919293132,
      "grad_norm": 2.11344838142395,
      "learning_rate": 7.332155401196102e-05,
      "loss": 2.3553,
      "step": 17070
    },
    {
      "epoch": 1.0372259670856865,
      "grad_norm": 3.1701388359069824,
      "learning_rate": 7.329342315202677e-05,
      "loss": 2.2957,
      "step": 17080
    },
    {
      "epoch": 1.03783324224206,
      "grad_norm": 2.6667873859405518,
      "learning_rate": 7.326528287186123e-05,
      "loss": 2.3625,
      "step": 17090
    },
    {
      "epoch": 1.0384405173984332,
      "grad_norm": 2.2123160362243652,
      "learning_rate": 7.323713318284475e-05,
      "loss": 2.338,
      "step": 17100
    },
    {
      "epoch": 1.0390477925548065,
      "grad_norm": 3.1107263565063477,
      "learning_rate": 7.320897409636154e-05,
      "loss": 2.4046,
      "step": 17110
    },
    {
      "epoch": 1.03965506771118,
      "grad_norm": 2.4386892318725586,
      "learning_rate": 7.318080562379956e-05,
      "loss": 2.5344,
      "step": 17120
    },
    {
      "epoch": 1.0402623428675533,
      "grad_norm": 2.5597832202911377,
      "learning_rate": 7.31526277765506e-05,
      "loss": 2.4588,
      "step": 17130
    },
    {
      "epoch": 1.0408696180239267,
      "grad_norm": 2.4506444931030273,
      "learning_rate": 7.312444056601023e-05,
      "loss": 2.3172,
      "step": 17140
    },
    {
      "epoch": 1.0414768931803,
      "grad_norm": 3.075181245803833,
      "learning_rate": 7.30962440035778e-05,
      "loss": 2.5365,
      "step": 17150
    },
    {
      "epoch": 1.0420841683366733,
      "grad_norm": 2.3315627574920654,
      "learning_rate": 7.306803810065647e-05,
      "loss": 2.3661,
      "step": 17160
    },
    {
      "epoch": 1.0426914434930468,
      "grad_norm": 4.148343086242676,
      "learning_rate": 7.303982286865312e-05,
      "loss": 2.6693,
      "step": 17170
    },
    {
      "epoch": 1.04329871864942,
      "grad_norm": 2.160168170928955,
      "learning_rate": 7.301159831897848e-05,
      "loss": 2.5704,
      "step": 17180
    },
    {
      "epoch": 1.0439059938057933,
      "grad_norm": 2.1055846214294434,
      "learning_rate": 7.298336446304698e-05,
      "loss": 2.5914,
      "step": 17190
    },
    {
      "epoch": 1.0445132689621668,
      "grad_norm": 1.5767991542816162,
      "learning_rate": 7.295512131227686e-05,
      "loss": 2.3457,
      "step": 17200
    },
    {
      "epoch": 1.04512054411854,
      "grad_norm": 2.03265118598938,
      "learning_rate": 7.292686887809005e-05,
      "loss": 2.2389,
      "step": 17210
    },
    {
      "epoch": 1.0457278192749135,
      "grad_norm": 3.292701244354248,
      "learning_rate": 7.289860717191236e-05,
      "loss": 2.3349,
      "step": 17220
    },
    {
      "epoch": 1.0463350944312868,
      "grad_norm": 3.2978053092956543,
      "learning_rate": 7.287033620517324e-05,
      "loss": 2.6287,
      "step": 17230
    },
    {
      "epoch": 1.04694236958766,
      "grad_norm": 2.667999744415283,
      "learning_rate": 7.284205598930592e-05,
      "loss": 2.6148,
      "step": 17240
    },
    {
      "epoch": 1.0475496447440336,
      "grad_norm": 3.850508213043213,
      "learning_rate": 7.281376653574733e-05,
      "loss": 2.5477,
      "step": 17250
    },
    {
      "epoch": 1.0481569199004068,
      "grad_norm": 1.9151253700256348,
      "learning_rate": 7.278546785593825e-05,
      "loss": 2.204,
      "step": 17260
    },
    {
      "epoch": 1.0487641950567803,
      "grad_norm": 2.0719821453094482,
      "learning_rate": 7.275715996132304e-05,
      "loss": 2.069,
      "step": 17270
    },
    {
      "epoch": 1.0493714702131536,
      "grad_norm": 1.993503212928772,
      "learning_rate": 7.272884286334993e-05,
      "loss": 2.1921,
      "step": 17280
    },
    {
      "epoch": 1.0499787453695268,
      "grad_norm": 2.012428045272827,
      "learning_rate": 7.270051657347077e-05,
      "loss": 2.3321,
      "step": 17290
    },
    {
      "epoch": 1.0505860205259003,
      "grad_norm": 2.7659106254577637,
      "learning_rate": 7.267218110314118e-05,
      "loss": 2.4467,
      "step": 17300
    },
    {
      "epoch": 1.0511932956822736,
      "grad_norm": 2.1645145416259766,
      "learning_rate": 7.264383646382045e-05,
      "loss": 2.3165,
      "step": 17310
    },
    {
      "epoch": 1.051800570838647,
      "grad_norm": 2.242777109146118,
      "learning_rate": 7.261548266697164e-05,
      "loss": 2.2726,
      "step": 17320
    },
    {
      "epoch": 1.0524078459950204,
      "grad_norm": 2.396130084991455,
      "learning_rate": 7.258711972406144e-05,
      "loss": 2.0525,
      "step": 17330
    },
    {
      "epoch": 1.0530151211513936,
      "grad_norm": 2.8314902782440186,
      "learning_rate": 7.25587476465603e-05,
      "loss": 2.5715,
      "step": 17340
    },
    {
      "epoch": 1.053622396307767,
      "grad_norm": 2.9886624813079834,
      "learning_rate": 7.253036644594231e-05,
      "loss": 2.7267,
      "step": 17350
    },
    {
      "epoch": 1.0542296714641404,
      "grad_norm": 3.1845173835754395,
      "learning_rate": 7.250197613368532e-05,
      "loss": 2.4879,
      "step": 17360
    },
    {
      "epoch": 1.0548369466205139,
      "grad_norm": 1.6536493301391602,
      "learning_rate": 7.247357672127082e-05,
      "loss": 1.7757,
      "step": 17370
    },
    {
      "epoch": 1.0554442217768871,
      "grad_norm": 1.9831606149673462,
      "learning_rate": 7.244516822018395e-05,
      "loss": 2.332,
      "step": 17380
    },
    {
      "epoch": 1.0560514969332604,
      "grad_norm": 2.4726805686950684,
      "learning_rate": 7.241675064191358e-05,
      "loss": 2.2589,
      "step": 17390
    },
    {
      "epoch": 1.0566587720896339,
      "grad_norm": 2.03825306892395,
      "learning_rate": 7.238832399795223e-05,
      "loss": 2.4388,
      "step": 17400
    },
    {
      "epoch": 1.0572660472460071,
      "grad_norm": 3.404695510864258,
      "learning_rate": 7.235988829979611e-05,
      "loss": 2.7064,
      "step": 17410
    },
    {
      "epoch": 1.0578733224023804,
      "grad_norm": 2.7635347843170166,
      "learning_rate": 7.233144355894504e-05,
      "loss": 2.2536,
      "step": 17420
    },
    {
      "epoch": 1.058480597558754,
      "grad_norm": 2.7069482803344727,
      "learning_rate": 7.230298978690253e-05,
      "loss": 2.5136,
      "step": 17430
    },
    {
      "epoch": 1.0590878727151272,
      "grad_norm": 2.46732234954834,
      "learning_rate": 7.227452699517574e-05,
      "loss": 2.2714,
      "step": 17440
    },
    {
      "epoch": 1.0596951478715007,
      "grad_norm": 1.7095192670822144,
      "learning_rate": 7.22460551952755e-05,
      "loss": 2.0856,
      "step": 17450
    },
    {
      "epoch": 1.060302423027874,
      "grad_norm": 2.1652474403381348,
      "learning_rate": 7.221757439871623e-05,
      "loss": 1.9963,
      "step": 17460
    },
    {
      "epoch": 1.0609096981842472,
      "grad_norm": 2.515026330947876,
      "learning_rate": 7.218908461701603e-05,
      "loss": 2.6972,
      "step": 17470
    },
    {
      "epoch": 1.0615169733406207,
      "grad_norm": 3.132565498352051,
      "learning_rate": 7.21605858616966e-05,
      "loss": 2.2959,
      "step": 17480
    },
    {
      "epoch": 1.062124248496994,
      "grad_norm": 2.6249091625213623,
      "learning_rate": 7.213207814428331e-05,
      "loss": 2.4558,
      "step": 17490
    },
    {
      "epoch": 1.0627315236533674,
      "grad_norm": 2.1649296283721924,
      "learning_rate": 7.210356147630514e-05,
      "loss": 2.5722,
      "step": 17500
    },
    {
      "epoch": 1.0633387988097407,
      "grad_norm": 3.2538537979125977,
      "learning_rate": 7.207503586929471e-05,
      "loss": 2.2366,
      "step": 17510
    },
    {
      "epoch": 1.063946073966114,
      "grad_norm": 3.028743267059326,
      "learning_rate": 7.204650133478816e-05,
      "loss": 2.3156,
      "step": 17520
    },
    {
      "epoch": 1.0645533491224874,
      "grad_norm": 3.471747875213623,
      "learning_rate": 7.201795788432536e-05,
      "loss": 2.5884,
      "step": 17530
    },
    {
      "epoch": 1.0651606242788607,
      "grad_norm": 3.050185441970825,
      "learning_rate": 7.198940552944973e-05,
      "loss": 2.5617,
      "step": 17540
    },
    {
      "epoch": 1.0657678994352342,
      "grad_norm": 2.3764829635620117,
      "learning_rate": 7.196084428170829e-05,
      "loss": 2.7765,
      "step": 17550
    },
    {
      "epoch": 1.0663751745916075,
      "grad_norm": 3.3025760650634766,
      "learning_rate": 7.193227415265166e-05,
      "loss": 2.4641,
      "step": 17560
    },
    {
      "epoch": 1.0669824497479807,
      "grad_norm": 2.290771961212158,
      "learning_rate": 7.190369515383407e-05,
      "loss": 2.2031,
      "step": 17570
    },
    {
      "epoch": 1.0675897249043542,
      "grad_norm": 2.440744638442993,
      "learning_rate": 7.18751072968133e-05,
      "loss": 2.4843,
      "step": 17580
    },
    {
      "epoch": 1.0681970000607275,
      "grad_norm": 3.835413694381714,
      "learning_rate": 7.184651059315076e-05,
      "loss": 2.3867,
      "step": 17590
    },
    {
      "epoch": 1.0688042752171008,
      "grad_norm": 2.6975388526916504,
      "learning_rate": 7.18179050544114e-05,
      "loss": 2.2958,
      "step": 17600
    },
    {
      "epoch": 1.0694115503734742,
      "grad_norm": 2.4078989028930664,
      "learning_rate": 7.178929069216374e-05,
      "loss": 2.3968,
      "step": 17610
    },
    {
      "epoch": 1.0700188255298475,
      "grad_norm": 2.003875255584717,
      "learning_rate": 7.17606675179799e-05,
      "loss": 2.4285,
      "step": 17620
    },
    {
      "epoch": 1.070626100686221,
      "grad_norm": 3.2006776332855225,
      "learning_rate": 7.173203554343556e-05,
      "loss": 2.1646,
      "step": 17630
    },
    {
      "epoch": 1.0712333758425943,
      "grad_norm": 4.076930046081543,
      "learning_rate": 7.170339478010993e-05,
      "loss": 2.6377,
      "step": 17640
    },
    {
      "epoch": 1.0718406509989675,
      "grad_norm": 3.5961062908172607,
      "learning_rate": 7.16747452395858e-05,
      "loss": 2.7714,
      "step": 17650
    },
    {
      "epoch": 1.072447926155341,
      "grad_norm": 3.2631499767303467,
      "learning_rate": 7.164608693344948e-05,
      "loss": 2.6866,
      "step": 17660
    },
    {
      "epoch": 1.0730552013117143,
      "grad_norm": 4.963009834289551,
      "learning_rate": 7.161741987329087e-05,
      "loss": 2.8381,
      "step": 17670
    },
    {
      "epoch": 1.0736624764680878,
      "grad_norm": 4.370872974395752,
      "learning_rate": 7.158874407070337e-05,
      "loss": 2.4321,
      "step": 17680
    },
    {
      "epoch": 1.074269751624461,
      "grad_norm": 3.0816726684570312,
      "learning_rate": 7.156005953728393e-05,
      "loss": 2.2906,
      "step": 17690
    },
    {
      "epoch": 1.0748770267808343,
      "grad_norm": 3.784862756729126,
      "learning_rate": 7.153136628463307e-05,
      "loss": 2.5163,
      "step": 17700
    },
    {
      "epoch": 1.0754843019372078,
      "grad_norm": 2.2109744548797607,
      "learning_rate": 7.150266432435473e-05,
      "loss": 2.3719,
      "step": 17710
    },
    {
      "epoch": 1.076091577093581,
      "grad_norm": 1.96806800365448,
      "learning_rate": 7.14739536680565e-05,
      "loss": 2.4814,
      "step": 17720
    },
    {
      "epoch": 1.0766988522499545,
      "grad_norm": 2.428907871246338,
      "learning_rate": 7.144523432734939e-05,
      "loss": 2.6049,
      "step": 17730
    },
    {
      "epoch": 1.0773061274063278,
      "grad_norm": 2.6250367164611816,
      "learning_rate": 7.141650631384798e-05,
      "loss": 2.5568,
      "step": 17740
    },
    {
      "epoch": 1.077913402562701,
      "grad_norm": 2.2730112075805664,
      "learning_rate": 7.138776963917033e-05,
      "loss": 2.2654,
      "step": 17750
    },
    {
      "epoch": 1.0785206777190746,
      "grad_norm": 3.6953976154327393,
      "learning_rate": 7.135902431493801e-05,
      "loss": 2.4943,
      "step": 17760
    },
    {
      "epoch": 1.0791279528754478,
      "grad_norm": 3.002258539199829,
      "learning_rate": 7.133027035277606e-05,
      "loss": 2.3069,
      "step": 17770
    },
    {
      "epoch": 1.0797352280318213,
      "grad_norm": 3.5109145641326904,
      "learning_rate": 7.13015077643131e-05,
      "loss": 2.6437,
      "step": 17780
    },
    {
      "epoch": 1.0803425031881946,
      "grad_norm": 3.3632192611694336,
      "learning_rate": 7.127273656118112e-05,
      "loss": 2.6754,
      "step": 17790
    },
    {
      "epoch": 1.0809497783445678,
      "grad_norm": 3.290048360824585,
      "learning_rate": 7.124395675501569e-05,
      "loss": 2.6871,
      "step": 17800
    },
    {
      "epoch": 1.0815570535009413,
      "grad_norm": 3.6958205699920654,
      "learning_rate": 7.121516835745581e-05,
      "loss": 2.5053,
      "step": 17810
    },
    {
      "epoch": 1.0821643286573146,
      "grad_norm": 2.907341957092285,
      "learning_rate": 7.118637138014396e-05,
      "loss": 2.5705,
      "step": 17820
    },
    {
      "epoch": 1.082771603813688,
      "grad_norm": 3.3818359375,
      "learning_rate": 7.11575658347261e-05,
      "loss": 2.1885,
      "step": 17830
    },
    {
      "epoch": 1.0833788789700614,
      "grad_norm": 1.5095068216323853,
      "learning_rate": 7.112875173285165e-05,
      "loss": 2.0798,
      "step": 17840
    },
    {
      "epoch": 1.0839861541264346,
      "grad_norm": 1.7554160356521606,
      "learning_rate": 7.10999290861735e-05,
      "loss": 2.2165,
      "step": 17850
    },
    {
      "epoch": 1.084593429282808,
      "grad_norm": 3.0815536975860596,
      "learning_rate": 7.107109790634797e-05,
      "loss": 2.3861,
      "step": 17860
    },
    {
      "epoch": 1.0852007044391814,
      "grad_norm": 2.1938962936401367,
      "learning_rate": 7.104225820503487e-05,
      "loss": 2.1797,
      "step": 17870
    },
    {
      "epoch": 1.0858079795955549,
      "grad_norm": 2.123170852661133,
      "learning_rate": 7.101340999389743e-05,
      "loss": 2.4769,
      "step": 17880
    },
    {
      "epoch": 1.0864152547519281,
      "grad_norm": 2.3277432918548584,
      "learning_rate": 7.098455328460232e-05,
      "loss": 2.2343,
      "step": 17890
    },
    {
      "epoch": 1.0870225299083014,
      "grad_norm": 2.529197931289673,
      "learning_rate": 7.095568808881963e-05,
      "loss": 2.3643,
      "step": 17900
    },
    {
      "epoch": 1.0876298050646749,
      "grad_norm": 2.8931045532226562,
      "learning_rate": 7.092681441822296e-05,
      "loss": 2.2728,
      "step": 17910
    },
    {
      "epoch": 1.0882370802210481,
      "grad_norm": 1.9567911624908447,
      "learning_rate": 7.089793228448923e-05,
      "loss": 2.0258,
      "step": 17920
    },
    {
      "epoch": 1.0888443553774214,
      "grad_norm": 3.5477726459503174,
      "learning_rate": 7.086904169929887e-05,
      "loss": 2.2938,
      "step": 17930
    },
    {
      "epoch": 1.089451630533795,
      "grad_norm": 1.776611089706421,
      "learning_rate": 7.084014267433565e-05,
      "loss": 2.2723,
      "step": 17940
    },
    {
      "epoch": 1.0900589056901682,
      "grad_norm": 4.325263500213623,
      "learning_rate": 7.081123522128686e-05,
      "loss": 2.4294,
      "step": 17950
    },
    {
      "epoch": 1.0906661808465417,
      "grad_norm": 2.167304515838623,
      "learning_rate": 7.078231935184308e-05,
      "loss": 2.2174,
      "step": 17960
    },
    {
      "epoch": 1.091273456002915,
      "grad_norm": 4.140560626983643,
      "learning_rate": 7.075339507769837e-05,
      "loss": 2.6945,
      "step": 17970
    },
    {
      "epoch": 1.0918807311592882,
      "grad_norm": 2.8254878520965576,
      "learning_rate": 7.072446241055017e-05,
      "loss": 3.0171,
      "step": 17980
    },
    {
      "epoch": 1.0924880063156617,
      "grad_norm": 4.306948661804199,
      "learning_rate": 7.06955213620993e-05,
      "loss": 2.8437,
      "step": 17990
    },
    {
      "epoch": 1.093095281472035,
      "grad_norm": 4.6931633949279785,
      "learning_rate": 7.066657194404996e-05,
      "loss": 2.6552,
      "step": 18000
    },
    {
      "epoch": 1.0937025566284084,
      "grad_norm": 2.7818961143493652,
      "learning_rate": 7.063761416810981e-05,
      "loss": 2.6102,
      "step": 18010
    },
    {
      "epoch": 1.0943098317847817,
      "grad_norm": 3.5526061058044434,
      "learning_rate": 7.060864804598981e-05,
      "loss": 2.6681,
      "step": 18020
    },
    {
      "epoch": 1.094917106941155,
      "grad_norm": 2.034482717514038,
      "learning_rate": 7.057967358940431e-05,
      "loss": 2.6017,
      "step": 18030
    },
    {
      "epoch": 1.0955243820975284,
      "grad_norm": 1.9662270545959473,
      "learning_rate": 7.055069081007105e-05,
      "loss": 2.5877,
      "step": 18040
    },
    {
      "epoch": 1.0961316572539017,
      "grad_norm": 3.3585944175720215,
      "learning_rate": 7.052169971971113e-05,
      "loss": 2.6235,
      "step": 18050
    },
    {
      "epoch": 1.096738932410275,
      "grad_norm": 1.844831943511963,
      "learning_rate": 7.049270033004901e-05,
      "loss": 2.1789,
      "step": 18060
    },
    {
      "epoch": 1.0973462075666485,
      "grad_norm": 2.7599000930786133,
      "learning_rate": 7.046369265281249e-05,
      "loss": 2.46,
      "step": 18070
    },
    {
      "epoch": 1.0979534827230217,
      "grad_norm": 2.087520122528076,
      "learning_rate": 7.043467669973276e-05,
      "loss": 2.3094,
      "step": 18080
    },
    {
      "epoch": 1.0985607578793952,
      "grad_norm": 2.5824668407440186,
      "learning_rate": 7.040565248254431e-05,
      "loss": 2.4668,
      "step": 18090
    },
    {
      "epoch": 1.0991680330357685,
      "grad_norm": 2.5614445209503174,
      "learning_rate": 7.037662001298502e-05,
      "loss": 2.4704,
      "step": 18100
    },
    {
      "epoch": 1.0997753081921418,
      "grad_norm": 2.926394462585449,
      "learning_rate": 7.034757930279606e-05,
      "loss": 2.1564,
      "step": 18110
    },
    {
      "epoch": 1.1003825833485152,
      "grad_norm": 3.578590154647827,
      "learning_rate": 7.031853036372197e-05,
      "loss": 2.5152,
      "step": 18120
    },
    {
      "epoch": 1.1009898585048885,
      "grad_norm": 3.162367105484009,
      "learning_rate": 7.02894732075106e-05,
      "loss": 2.3732,
      "step": 18130
    },
    {
      "epoch": 1.101597133661262,
      "grad_norm": 2.8187801837921143,
      "learning_rate": 7.026040784591315e-05,
      "loss": 2.2416,
      "step": 18140
    },
    {
      "epoch": 1.1022044088176353,
      "grad_norm": 2.5920989513397217,
      "learning_rate": 7.023133429068405e-05,
      "loss": 2.6732,
      "step": 18150
    },
    {
      "epoch": 1.1028116839740085,
      "grad_norm": 2.1687629222869873,
      "learning_rate": 7.020225255358119e-05,
      "loss": 2.3453,
      "step": 18160
    },
    {
      "epoch": 1.103418959130382,
      "grad_norm": 2.6781344413757324,
      "learning_rate": 7.017316264636562e-05,
      "loss": 2.2556,
      "step": 18170
    },
    {
      "epoch": 1.1040262342867553,
      "grad_norm": 3.7132835388183594,
      "learning_rate": 7.014406458080179e-05,
      "loss": 2.5432,
      "step": 18180
    },
    {
      "epoch": 1.1046335094431288,
      "grad_norm": 4.225434303283691,
      "learning_rate": 7.011495836865744e-05,
      "loss": 2.8072,
      "step": 18190
    },
    {
      "epoch": 1.105240784599502,
      "grad_norm": 2.468221426010132,
      "learning_rate": 7.008584402170357e-05,
      "loss": 2.3718,
      "step": 18200
    },
    {
      "epoch": 1.1058480597558753,
      "grad_norm": 2.6589255332946777,
      "learning_rate": 7.005672155171445e-05,
      "loss": 2.174,
      "step": 18210
    },
    {
      "epoch": 1.1064553349122488,
      "grad_norm": 2.4699761867523193,
      "learning_rate": 7.002759097046772e-05,
      "loss": 2.1493,
      "step": 18220
    },
    {
      "epoch": 1.107062610068622,
      "grad_norm": 2.301473617553711,
      "learning_rate": 6.999845228974425e-05,
      "loss": 2.0019,
      "step": 18230
    },
    {
      "epoch": 1.1076698852249955,
      "grad_norm": 2.1494948863983154,
      "learning_rate": 6.996930552132816e-05,
      "loss": 2.1396,
      "step": 18240
    },
    {
      "epoch": 1.1082771603813688,
      "grad_norm": 2.0816538333892822,
      "learning_rate": 6.994015067700685e-05,
      "loss": 2.3651,
      "step": 18250
    },
    {
      "epoch": 1.108884435537742,
      "grad_norm": 1.6994779109954834,
      "learning_rate": 6.991098776857108e-05,
      "loss": 2.4958,
      "step": 18260
    },
    {
      "epoch": 1.1094917106941156,
      "grad_norm": 2.2734012603759766,
      "learning_rate": 6.988181680781472e-05,
      "loss": 2.3844,
      "step": 18270
    },
    {
      "epoch": 1.1100989858504888,
      "grad_norm": 2.5590672492980957,
      "learning_rate": 6.985263780653498e-05,
      "loss": 2.4228,
      "step": 18280
    },
    {
      "epoch": 1.1107062610068623,
      "grad_norm": 4.199822425842285,
      "learning_rate": 6.982345077653237e-05,
      "loss": 2.7628,
      "step": 18290
    },
    {
      "epoch": 1.1113135361632356,
      "grad_norm": 3.7434990406036377,
      "learning_rate": 6.979425572961052e-05,
      "loss": 2.6336,
      "step": 18300
    },
    {
      "epoch": 1.1119208113196088,
      "grad_norm": 3.1064627170562744,
      "learning_rate": 6.976505267757642e-05,
      "loss": 2.4336,
      "step": 18310
    },
    {
      "epoch": 1.1125280864759823,
      "grad_norm": 2.321207284927368,
      "learning_rate": 6.973584163224021e-05,
      "loss": 2.5084,
      "step": 18320
    },
    {
      "epoch": 1.1131353616323556,
      "grad_norm": 3.6964850425720215,
      "learning_rate": 6.970662260541534e-05,
      "loss": 2.7803,
      "step": 18330
    },
    {
      "epoch": 1.113742636788729,
      "grad_norm": 3.663141965866089,
      "learning_rate": 6.967739560891843e-05,
      "loss": 2.6555,
      "step": 18340
    },
    {
      "epoch": 1.1143499119451024,
      "grad_norm": 2.3603875637054443,
      "learning_rate": 6.964816065456934e-05,
      "loss": 2.2181,
      "step": 18350
    },
    {
      "epoch": 1.1149571871014756,
      "grad_norm": 2.6038146018981934,
      "learning_rate": 6.961891775419116e-05,
      "loss": 2.3021,
      "step": 18360
    },
    {
      "epoch": 1.115564462257849,
      "grad_norm": 2.6114649772644043,
      "learning_rate": 6.958966691961018e-05,
      "loss": 2.2818,
      "step": 18370
    },
    {
      "epoch": 1.1161717374142224,
      "grad_norm": 3.1024765968322754,
      "learning_rate": 6.956040816265591e-05,
      "loss": 2.7919,
      "step": 18380
    },
    {
      "epoch": 1.1167790125705956,
      "grad_norm": 3.3284683227539062,
      "learning_rate": 6.953114149516104e-05,
      "loss": 2.6273,
      "step": 18390
    },
    {
      "epoch": 1.1173862877269691,
      "grad_norm": 2.9466545581817627,
      "learning_rate": 6.950186692896152e-05,
      "loss": 2.4017,
      "step": 18400
    },
    {
      "epoch": 1.1179935628833424,
      "grad_norm": 4.198663711547852,
      "learning_rate": 6.947258447589638e-05,
      "loss": 2.5993,
      "step": 18410
    },
    {
      "epoch": 1.1186008380397159,
      "grad_norm": 3.2275149822235107,
      "learning_rate": 6.944329414780796e-05,
      "loss": 2.2194,
      "step": 18420
    },
    {
      "epoch": 1.1192081131960891,
      "grad_norm": 4.475086688995361,
      "learning_rate": 6.941399595654176e-05,
      "loss": 2.6274,
      "step": 18430
    },
    {
      "epoch": 1.1198153883524624,
      "grad_norm": 4.616150856018066,
      "learning_rate": 6.938468991394635e-05,
      "loss": 2.7131,
      "step": 18440
    },
    {
      "epoch": 1.120422663508836,
      "grad_norm": 2.4226646423339844,
      "learning_rate": 6.935537603187364e-05,
      "loss": 2.4868,
      "step": 18450
    },
    {
      "epoch": 1.1210299386652092,
      "grad_norm": 2.2366232872009277,
      "learning_rate": 6.93260543221786e-05,
      "loss": 2.2148,
      "step": 18460
    },
    {
      "epoch": 1.1216372138215827,
      "grad_norm": 3.8599143028259277,
      "learning_rate": 6.92967247967194e-05,
      "loss": 2.1958,
      "step": 18470
    },
    {
      "epoch": 1.122244488977956,
      "grad_norm": 2.061711549758911,
      "learning_rate": 6.926738746735734e-05,
      "loss": 2.346,
      "step": 18480
    },
    {
      "epoch": 1.1228517641343292,
      "grad_norm": 2.9126832485198975,
      "learning_rate": 6.923804234595694e-05,
      "loss": 2.2726,
      "step": 18490
    },
    {
      "epoch": 1.1234590392907027,
      "grad_norm": 2.271416425704956,
      "learning_rate": 6.920868944438584e-05,
      "loss": 2.5328,
      "step": 18500
    },
    {
      "epoch": 1.124066314447076,
      "grad_norm": 4.3769612312316895,
      "learning_rate": 6.917932877451478e-05,
      "loss": 2.3165,
      "step": 18510
    },
    {
      "epoch": 1.1246735896034492,
      "grad_norm": 1.8770166635513306,
      "learning_rate": 6.914996034821771e-05,
      "loss": 2.0299,
      "step": 18520
    },
    {
      "epoch": 1.1252808647598227,
      "grad_norm": 2.4236555099487305,
      "learning_rate": 6.91205841773717e-05,
      "loss": 2.4096,
      "step": 18530
    },
    {
      "epoch": 1.125888139916196,
      "grad_norm": 3.348996162414551,
      "learning_rate": 6.909120027385691e-05,
      "loss": 2.7248,
      "step": 18540
    },
    {
      "epoch": 1.1264954150725695,
      "grad_norm": 3.5312857627868652,
      "learning_rate": 6.906180864955668e-05,
      "loss": 2.2876,
      "step": 18550
    },
    {
      "epoch": 1.1271026902289427,
      "grad_norm": 1.7119580507278442,
      "learning_rate": 6.903240931635746e-05,
      "loss": 1.9837,
      "step": 18560
    },
    {
      "epoch": 1.127709965385316,
      "grad_norm": 1.8722522258758545,
      "learning_rate": 6.900300228614877e-05,
      "loss": 2.0551,
      "step": 18570
    },
    {
      "epoch": 1.1283172405416895,
      "grad_norm": 2.655419111251831,
      "learning_rate": 6.897358757082333e-05,
      "loss": 2.3438,
      "step": 18580
    },
    {
      "epoch": 1.1289245156980627,
      "grad_norm": 3.7276358604431152,
      "learning_rate": 6.894416518227688e-05,
      "loss": 2.5342,
      "step": 18590
    },
    {
      "epoch": 1.1295317908544362,
      "grad_norm": 2.860828399658203,
      "learning_rate": 6.891473513240834e-05,
      "loss": 2.2216,
      "step": 18600
    },
    {
      "epoch": 1.1301390660108095,
      "grad_norm": 3.4164884090423584,
      "learning_rate": 6.888529743311964e-05,
      "loss": 2.4464,
      "step": 18610
    },
    {
      "epoch": 1.1307463411671828,
      "grad_norm": 2.769961357116699,
      "learning_rate": 6.88558520963159e-05,
      "loss": 2.8872,
      "step": 18620
    },
    {
      "epoch": 1.1313536163235562,
      "grad_norm": 2.6031572818756104,
      "learning_rate": 6.882639913390526e-05,
      "loss": 2.6294,
      "step": 18630
    },
    {
      "epoch": 1.1319608914799295,
      "grad_norm": 3.9916574954986572,
      "learning_rate": 6.879693855779898e-05,
      "loss": 2.5122,
      "step": 18640
    },
    {
      "epoch": 1.132568166636303,
      "grad_norm": 2.7151050567626953,
      "learning_rate": 6.876747037991137e-05,
      "loss": 2.1869,
      "step": 18650
    },
    {
      "epoch": 1.1331754417926763,
      "grad_norm": 2.7498247623443604,
      "learning_rate": 6.873799461215983e-05,
      "loss": 2.4352,
      "step": 18660
    },
    {
      "epoch": 1.1337827169490495,
      "grad_norm": 3.8694522380828857,
      "learning_rate": 6.870851126646487e-05,
      "loss": 2.3501,
      "step": 18670
    },
    {
      "epoch": 1.134389992105423,
      "grad_norm": 2.694795846939087,
      "learning_rate": 6.867902035474997e-05,
      "loss": 2.3276,
      "step": 18680
    },
    {
      "epoch": 1.1349972672617963,
      "grad_norm": 2.1703503131866455,
      "learning_rate": 6.864952188894176e-05,
      "loss": 2.2638,
      "step": 18690
    },
    {
      "epoch": 1.1356045424181698,
      "grad_norm": 2.3731460571289062,
      "learning_rate": 6.86200158809699e-05,
      "loss": 2.095,
      "step": 18700
    },
    {
      "epoch": 1.136211817574543,
      "grad_norm": 2.3541500568389893,
      "learning_rate": 6.859050234276703e-05,
      "loss": 2.1089,
      "step": 18710
    },
    {
      "epoch": 1.1368190927309163,
      "grad_norm": 3.27152156829834,
      "learning_rate": 6.856098128626895e-05,
      "loss": 2.3199,
      "step": 18720
    },
    {
      "epoch": 1.1374263678872898,
      "grad_norm": 2.832827091217041,
      "learning_rate": 6.853145272341442e-05,
      "loss": 2.8177,
      "step": 18730
    },
    {
      "epoch": 1.138033643043663,
      "grad_norm": 2.348188877105713,
      "learning_rate": 6.850191666614529e-05,
      "loss": 2.5094,
      "step": 18740
    },
    {
      "epoch": 1.1386409182000365,
      "grad_norm": 4.097106456756592,
      "learning_rate": 6.847237312640636e-05,
      "loss": 2.4069,
      "step": 18750
    },
    {
      "epoch": 1.1392481933564098,
      "grad_norm": 3.111391067504883,
      "learning_rate": 6.844282211614555e-05,
      "loss": 2.7987,
      "step": 18760
    },
    {
      "epoch": 1.139855468512783,
      "grad_norm": 6.735995292663574,
      "learning_rate": 6.841326364731378e-05,
      "loss": 2.705,
      "step": 18770
    },
    {
      "epoch": 1.1404627436691566,
      "grad_norm": 3.8487162590026855,
      "learning_rate": 6.838369773186489e-05,
      "loss": 2.306,
      "step": 18780
    },
    {
      "epoch": 1.1410700188255298,
      "grad_norm": 3.056180238723755,
      "learning_rate": 6.835412438175587e-05,
      "loss": 2.7027,
      "step": 18790
    },
    {
      "epoch": 1.1416772939819033,
      "grad_norm": 2.910888195037842,
      "learning_rate": 6.832454360894664e-05,
      "loss": 2.7615,
      "step": 18800
    },
    {
      "epoch": 1.1422845691382766,
      "grad_norm": 4.765516757965088,
      "learning_rate": 6.829495542540013e-05,
      "loss": 2.247,
      "step": 18810
    },
    {
      "epoch": 1.1428918442946499,
      "grad_norm": 2.2994484901428223,
      "learning_rate": 6.826535984308226e-05,
      "loss": 2.4006,
      "step": 18820
    },
    {
      "epoch": 1.1434991194510233,
      "grad_norm": 2.649948835372925,
      "learning_rate": 6.823575687396197e-05,
      "loss": 2.3554,
      "step": 18830
    },
    {
      "epoch": 1.1441063946073966,
      "grad_norm": 2.3305816650390625,
      "learning_rate": 6.820614653001118e-05,
      "loss": 2.4663,
      "step": 18840
    },
    {
      "epoch": 1.1447136697637699,
      "grad_norm": 3.1927902698516846,
      "learning_rate": 6.817652882320478e-05,
      "loss": 2.6676,
      "step": 18850
    },
    {
      "epoch": 1.1453209449201434,
      "grad_norm": 2.243199348449707,
      "learning_rate": 6.814690376552061e-05,
      "loss": 2.3397,
      "step": 18860
    },
    {
      "epoch": 1.1459282200765166,
      "grad_norm": 3.043459415435791,
      "learning_rate": 6.811727136893958e-05,
      "loss": 2.5502,
      "step": 18870
    },
    {
      "epoch": 1.1465354952328901,
      "grad_norm": 3.2281124591827393,
      "learning_rate": 6.808763164544545e-05,
      "loss": 2.4608,
      "step": 18880
    },
    {
      "epoch": 1.1471427703892634,
      "grad_norm": 3.0011210441589355,
      "learning_rate": 6.805798460702501e-05,
      "loss": 2.2084,
      "step": 18890
    },
    {
      "epoch": 1.1477500455456366,
      "grad_norm": 2.247419595718384,
      "learning_rate": 6.802833026566798e-05,
      "loss": 2.3844,
      "step": 18900
    },
    {
      "epoch": 1.1483573207020101,
      "grad_norm": 1.883453607559204,
      "learning_rate": 6.799866863336709e-05,
      "loss": 2.364,
      "step": 18910
    },
    {
      "epoch": 1.1489645958583834,
      "grad_norm": 2.82995867729187,
      "learning_rate": 6.796899972211793e-05,
      "loss": 2.4779,
      "step": 18920
    },
    {
      "epoch": 1.1495718710147567,
      "grad_norm": 3.0754127502441406,
      "learning_rate": 6.793932354391912e-05,
      "loss": 2.3115,
      "step": 18930
    },
    {
      "epoch": 1.1501791461711302,
      "grad_norm": 2.0828487873077393,
      "learning_rate": 6.790964011077214e-05,
      "loss": 2.1516,
      "step": 18940
    },
    {
      "epoch": 1.1507864213275034,
      "grad_norm": 3.556767702102661,
      "learning_rate": 6.787994943468147e-05,
      "loss": 2.9544,
      "step": 18950
    },
    {
      "epoch": 1.151393696483877,
      "grad_norm": 2.645665168762207,
      "learning_rate": 6.78502515276545e-05,
      "loss": 2.6358,
      "step": 18960
    },
    {
      "epoch": 1.1520009716402502,
      "grad_norm": 2.5021779537200928,
      "learning_rate": 6.78205464017015e-05,
      "loss": 2.3462,
      "step": 18970
    },
    {
      "epoch": 1.1526082467966234,
      "grad_norm": 2.5442354679107666,
      "learning_rate": 6.779083406883574e-05,
      "loss": 2.8537,
      "step": 18980
    },
    {
      "epoch": 1.153215521952997,
      "grad_norm": 3.1113553047180176,
      "learning_rate": 6.776111454107333e-05,
      "loss": 2.7237,
      "step": 18990
    },
    {
      "epoch": 1.1538227971093702,
      "grad_norm": 2.215763807296753,
      "learning_rate": 6.773138783043332e-05,
      "loss": 2.208,
      "step": 19000
    },
    {
      "epoch": 1.1544300722657437,
      "grad_norm": 2.042402982711792,
      "learning_rate": 6.770165394893768e-05,
      "loss": 2.0141,
      "step": 19010
    },
    {
      "epoch": 1.155037347422117,
      "grad_norm": 2.5737273693084717,
      "learning_rate": 6.767191290861125e-05,
      "loss": 2.1559,
      "step": 19020
    },
    {
      "epoch": 1.1556446225784902,
      "grad_norm": 1.8469605445861816,
      "learning_rate": 6.76421647214818e-05,
      "loss": 2.2013,
      "step": 19030
    },
    {
      "epoch": 1.1562518977348637,
      "grad_norm": 2.4117088317871094,
      "learning_rate": 6.761240939957997e-05,
      "loss": 2.5387,
      "step": 19040
    },
    {
      "epoch": 1.156859172891237,
      "grad_norm": 2.1572091579437256,
      "learning_rate": 6.758264695493926e-05,
      "loss": 2.8709,
      "step": 19050
    },
    {
      "epoch": 1.1574664480476105,
      "grad_norm": 1.634026288986206,
      "learning_rate": 6.75528773995961e-05,
      "loss": 2.2436,
      "step": 19060
    },
    {
      "epoch": 1.1580737232039837,
      "grad_norm": 2.7082316875457764,
      "learning_rate": 6.752310074558976e-05,
      "loss": 2.3875,
      "step": 19070
    },
    {
      "epoch": 1.158680998360357,
      "grad_norm": 1.5312445163726807,
      "learning_rate": 6.749331700496241e-05,
      "loss": 2.1211,
      "step": 19080
    },
    {
      "epoch": 1.1592882735167305,
      "grad_norm": 3.339411735534668,
      "learning_rate": 6.746352618975906e-05,
      "loss": 2.1365,
      "step": 19090
    },
    {
      "epoch": 1.1598955486731037,
      "grad_norm": 2.728017568588257,
      "learning_rate": 6.743372831202758e-05,
      "loss": 2.2754,
      "step": 19100
    },
    {
      "epoch": 1.1605028238294772,
      "grad_norm": 4.138726711273193,
      "learning_rate": 6.74039233838187e-05,
      "loss": 2.6994,
      "step": 19110
    },
    {
      "epoch": 1.1611100989858505,
      "grad_norm": 4.697002410888672,
      "learning_rate": 6.737411141718605e-05,
      "loss": 2.691,
      "step": 19120
    },
    {
      "epoch": 1.1617173741422238,
      "grad_norm": 3.259801149368286,
      "learning_rate": 6.7344292424186e-05,
      "loss": 2.9862,
      "step": 19130
    },
    {
      "epoch": 1.1623246492985972,
      "grad_norm": 2.1272943019866943,
      "learning_rate": 6.73144664168779e-05,
      "loss": 2.3499,
      "step": 19140
    },
    {
      "epoch": 1.1629319244549705,
      "grad_norm": 2.077991485595703,
      "learning_rate": 6.72846334073238e-05,
      "loss": 1.9394,
      "step": 19150
    },
    {
      "epoch": 1.163539199611344,
      "grad_norm": 1.717759132385254,
      "learning_rate": 6.72547934075887e-05,
      "loss": 1.9782,
      "step": 19160
    },
    {
      "epoch": 1.1641464747677173,
      "grad_norm": 2.046048641204834,
      "learning_rate": 6.722494642974029e-05,
      "loss": 1.9744,
      "step": 19170
    },
    {
      "epoch": 1.1647537499240905,
      "grad_norm": 2.3756556510925293,
      "learning_rate": 6.719509248584923e-05,
      "loss": 1.9983,
      "step": 19180
    },
    {
      "epoch": 1.165361025080464,
      "grad_norm": 1.8267862796783447,
      "learning_rate": 6.716523158798894e-05,
      "loss": 2.0939,
      "step": 19190
    },
    {
      "epoch": 1.1659683002368373,
      "grad_norm": 1.7978153228759766,
      "learning_rate": 6.71353637482356e-05,
      "loss": 2.0668,
      "step": 19200
    },
    {
      "epoch": 1.1665755753932108,
      "grad_norm": 2.799247980117798,
      "learning_rate": 6.710548897866823e-05,
      "loss": 2.71,
      "step": 19210
    },
    {
      "epoch": 1.167182850549584,
      "grad_norm": 2.4897730350494385,
      "learning_rate": 6.70756072913687e-05,
      "loss": 2.4996,
      "step": 19220
    },
    {
      "epoch": 1.1677901257059573,
      "grad_norm": 3.4474568367004395,
      "learning_rate": 6.704571869842165e-05,
      "loss": 2.4484,
      "step": 19230
    },
    {
      "epoch": 1.1683974008623308,
      "grad_norm": 2.8229691982269287,
      "learning_rate": 6.701582321191447e-05,
      "loss": 2.3743,
      "step": 19240
    },
    {
      "epoch": 1.169004676018704,
      "grad_norm": 4.013847827911377,
      "learning_rate": 6.698592084393737e-05,
      "loss": 2.304,
      "step": 19250
    },
    {
      "epoch": 1.1696119511750775,
      "grad_norm": 1.6807793378829956,
      "learning_rate": 6.695601160658337e-05,
      "loss": 2.1298,
      "step": 19260
    },
    {
      "epoch": 1.1702192263314508,
      "grad_norm": 1.0202378034591675,
      "learning_rate": 6.692609551194823e-05,
      "loss": 1.8635,
      "step": 19270
    },
    {
      "epoch": 1.170826501487824,
      "grad_norm": 2.241604804992676,
      "learning_rate": 6.689617257213047e-05,
      "loss": 2.2417,
      "step": 19280
    },
    {
      "epoch": 1.1714337766441976,
      "grad_norm": 2.018838882446289,
      "learning_rate": 6.686624279923146e-05,
      "loss": 2.4542,
      "step": 19290
    },
    {
      "epoch": 1.1720410518005708,
      "grad_norm": 2.9798336029052734,
      "learning_rate": 6.683630620535523e-05,
      "loss": 2.3167,
      "step": 19300
    },
    {
      "epoch": 1.172648326956944,
      "grad_norm": 2.1564815044403076,
      "learning_rate": 6.680636280260862e-05,
      "loss": 2.2836,
      "step": 19310
    },
    {
      "epoch": 1.1732556021133176,
      "grad_norm": 2.962285041809082,
      "learning_rate": 6.677641260310123e-05,
      "loss": 2.6551,
      "step": 19320
    },
    {
      "epoch": 1.1738628772696909,
      "grad_norm": 4.71966552734375,
      "learning_rate": 6.674645561894542e-05,
      "loss": 2.3207,
      "step": 19330
    },
    {
      "epoch": 1.1744701524260643,
      "grad_norm": 5.055599689483643,
      "learning_rate": 6.671649186225621e-05,
      "loss": 2.5167,
      "step": 19340
    },
    {
      "epoch": 1.1750774275824376,
      "grad_norm": 2.463620662689209,
      "learning_rate": 6.668652134515148e-05,
      "loss": 2.4795,
      "step": 19350
    },
    {
      "epoch": 1.1756847027388109,
      "grad_norm": 1.6541861295700073,
      "learning_rate": 6.665654407975175e-05,
      "loss": 2.1974,
      "step": 19360
    },
    {
      "epoch": 1.1762919778951844,
      "grad_norm": 3.926755666732788,
      "learning_rate": 6.662656007818034e-05,
      "loss": 2.5402,
      "step": 19370
    },
    {
      "epoch": 1.1768992530515576,
      "grad_norm": 2.1880502700805664,
      "learning_rate": 6.65965693525632e-05,
      "loss": 2.4917,
      "step": 19380
    },
    {
      "epoch": 1.177506528207931,
      "grad_norm": 2.785290241241455,
      "learning_rate": 6.65665719150291e-05,
      "loss": 2.4878,
      "step": 19390
    },
    {
      "epoch": 1.1781138033643044,
      "grad_norm": 1.778069257736206,
      "learning_rate": 6.653656777770948e-05,
      "loss": 2.3131,
      "step": 19400
    },
    {
      "epoch": 1.1787210785206776,
      "grad_norm": 3.025174617767334,
      "learning_rate": 6.650655695273849e-05,
      "loss": 2.4697,
      "step": 19410
    },
    {
      "epoch": 1.1793283536770511,
      "grad_norm": 2.2291042804718018,
      "learning_rate": 6.647653945225295e-05,
      "loss": 2.3843,
      "step": 19420
    },
    {
      "epoch": 1.1799356288334244,
      "grad_norm": 2.6017277240753174,
      "learning_rate": 6.644651528839247e-05,
      "loss": 2.4984,
      "step": 19430
    },
    {
      "epoch": 1.1805429039897977,
      "grad_norm": 3.2134957313537598,
      "learning_rate": 6.641648447329927e-05,
      "loss": 2.3092,
      "step": 19440
    },
    {
      "epoch": 1.1811501791461712,
      "grad_norm": 2.8550803661346436,
      "learning_rate": 6.638644701911827e-05,
      "loss": 2.2616,
      "step": 19450
    },
    {
      "epoch": 1.1817574543025444,
      "grad_norm": 3.168085813522339,
      "learning_rate": 6.635640293799715e-05,
      "loss": 2.3094,
      "step": 19460
    },
    {
      "epoch": 1.182364729458918,
      "grad_norm": 3.5752127170562744,
      "learning_rate": 6.632635224208617e-05,
      "loss": 2.5736,
      "step": 19470
    },
    {
      "epoch": 1.1829720046152912,
      "grad_norm": 3.1829679012298584,
      "learning_rate": 6.629629494353832e-05,
      "loss": 2.513,
      "step": 19480
    },
    {
      "epoch": 1.1835792797716644,
      "grad_norm": 2.6652944087982178,
      "learning_rate": 6.626623105450926e-05,
      "loss": 2.3842,
      "step": 19490
    },
    {
      "epoch": 1.184186554928038,
      "grad_norm": 2.514465093612671,
      "learning_rate": 6.62361605871573e-05,
      "loss": 2.1306,
      "step": 19500
    },
    {
      "epoch": 1.1847938300844112,
      "grad_norm": 1.7573884725570679,
      "learning_rate": 6.62060835536434e-05,
      "loss": 2.0284,
      "step": 19510
    },
    {
      "epoch": 1.1854011052407847,
      "grad_norm": 2.1102399826049805,
      "learning_rate": 6.617599996613122e-05,
      "loss": 2.0962,
      "step": 19520
    },
    {
      "epoch": 1.186008380397158,
      "grad_norm": 1.6184896230697632,
      "learning_rate": 6.614590983678702e-05,
      "loss": 2.275,
      "step": 19530
    },
    {
      "epoch": 1.1866156555535312,
      "grad_norm": 1.9662317037582397,
      "learning_rate": 6.611581317777975e-05,
      "loss": 2.292,
      "step": 19540
    },
    {
      "epoch": 1.1872229307099047,
      "grad_norm": 2.9671401977539062,
      "learning_rate": 6.608571000128094e-05,
      "loss": 2.4009,
      "step": 19550
    },
    {
      "epoch": 1.187830205866278,
      "grad_norm": 2.6418159008026123,
      "learning_rate": 6.605560031946484e-05,
      "loss": 2.4078,
      "step": 19560
    },
    {
      "epoch": 1.1884374810226515,
      "grad_norm": 2.0991711616516113,
      "learning_rate": 6.602548414450825e-05,
      "loss": 2.4053,
      "step": 19570
    },
    {
      "epoch": 1.1890447561790247,
      "grad_norm": 1.9914486408233643,
      "learning_rate": 6.599536148859066e-05,
      "loss": 2.1321,
      "step": 19580
    },
    {
      "epoch": 1.189652031335398,
      "grad_norm": 1.9773077964782715,
      "learning_rate": 6.596523236389411e-05,
      "loss": 2.1386,
      "step": 19590
    },
    {
      "epoch": 1.1902593064917715,
      "grad_norm": 2.2715020179748535,
      "learning_rate": 6.593509678260336e-05,
      "loss": 2.3282,
      "step": 19600
    },
    {
      "epoch": 1.1908665816481447,
      "grad_norm": 2.6383769512176514,
      "learning_rate": 6.590495475690564e-05,
      "loss": 2.3297,
      "step": 19610
    },
    {
      "epoch": 1.1914738568045182,
      "grad_norm": 2.5933868885040283,
      "learning_rate": 6.587480629899094e-05,
      "loss": 2.1064,
      "step": 19620
    },
    {
      "epoch": 1.1920811319608915,
      "grad_norm": 3.02703857421875,
      "learning_rate": 6.584465142105175e-05,
      "loss": 2.4066,
      "step": 19630
    },
    {
      "epoch": 1.1926884071172648,
      "grad_norm": 3.452075958251953,
      "learning_rate": 6.581449013528315e-05,
      "loss": 2.5307,
      "step": 19640
    },
    {
      "epoch": 1.1932956822736382,
      "grad_norm": 5.447780609130859,
      "learning_rate": 6.578432245388287e-05,
      "loss": 2.8632,
      "step": 19650
    },
    {
      "epoch": 1.1939029574300115,
      "grad_norm": 4.342750072479248,
      "learning_rate": 6.575414838905122e-05,
      "loss": 2.869,
      "step": 19660
    },
    {
      "epoch": 1.194510232586385,
      "grad_norm": 2.2194347381591797,
      "learning_rate": 6.572396795299106e-05,
      "loss": 2.4295,
      "step": 19670
    },
    {
      "epoch": 1.1951175077427583,
      "grad_norm": 1.8629566431045532,
      "learning_rate": 6.569378115790779e-05,
      "loss": 2.1032,
      "step": 19680
    },
    {
      "epoch": 1.1957247828991315,
      "grad_norm": 1.8023120164871216,
      "learning_rate": 6.566358801600951e-05,
      "loss": 2.1637,
      "step": 19690
    },
    {
      "epoch": 1.196332058055505,
      "grad_norm": 2.004401206970215,
      "learning_rate": 6.563338853950677e-05,
      "loss": 2.3245,
      "step": 19700
    },
    {
      "epoch": 1.1969393332118783,
      "grad_norm": 3.239348888397217,
      "learning_rate": 6.560318274061272e-05,
      "loss": 2.4637,
      "step": 19710
    },
    {
      "epoch": 1.1975466083682518,
      "grad_norm": 1.881117582321167,
      "learning_rate": 6.557297063154306e-05,
      "loss": 2.1292,
      "step": 19720
    },
    {
      "epoch": 1.198153883524625,
      "grad_norm": 2.5135414600372314,
      "learning_rate": 6.554275222451606e-05,
      "loss": 2.1394,
      "step": 19730
    },
    {
      "epoch": 1.1987611586809983,
      "grad_norm": 2.164456605911255,
      "learning_rate": 6.551252753175253e-05,
      "loss": 2.1618,
      "step": 19740
    },
    {
      "epoch": 1.1993684338373718,
      "grad_norm": 1.9394879341125488,
      "learning_rate": 6.548229656547581e-05,
      "loss": 2.2145,
      "step": 19750
    },
    {
      "epoch": 1.199975708993745,
      "grad_norm": 2.0900416374206543,
      "learning_rate": 6.545205933791176e-05,
      "loss": 2.1359,
      "step": 19760
    },
    {
      "epoch": 1.2005829841501183,
      "grad_norm": 2.2837891578674316,
      "learning_rate": 6.542181586128884e-05,
      "loss": 2.1419,
      "step": 19770
    },
    {
      "epoch": 1.2011902593064918,
      "grad_norm": 1.8426152467727661,
      "learning_rate": 6.539156614783795e-05,
      "loss": 2.4014,
      "step": 19780
    },
    {
      "epoch": 1.201797534462865,
      "grad_norm": 3.463104009628296,
      "learning_rate": 6.536131020979259e-05,
      "loss": 2.1268,
      "step": 19790
    },
    {
      "epoch": 1.2024048096192386,
      "grad_norm": 2.6080005168914795,
      "learning_rate": 6.533104805938873e-05,
      "loss": 2.3997,
      "step": 19800
    },
    {
      "epoch": 1.2030120847756118,
      "grad_norm": 3.24334979057312,
      "learning_rate": 6.530077970886487e-05,
      "loss": 3.0041,
      "step": 19810
    },
    {
      "epoch": 1.203619359931985,
      "grad_norm": 3.5619726181030273,
      "learning_rate": 6.5270505170462e-05,
      "loss": 2.7005,
      "step": 19820
    },
    {
      "epoch": 1.2042266350883586,
      "grad_norm": 3.1434569358825684,
      "learning_rate": 6.524022445642364e-05,
      "loss": 2.1971,
      "step": 19830
    },
    {
      "epoch": 1.2048339102447319,
      "grad_norm": 3.1530473232269287,
      "learning_rate": 6.520993757899578e-05,
      "loss": 2.3322,
      "step": 19840
    },
    {
      "epoch": 1.2054411854011051,
      "grad_norm": 2.2699804306030273,
      "learning_rate": 6.517964455042694e-05,
      "loss": 2.1629,
      "step": 19850
    },
    {
      "epoch": 1.2060484605574786,
      "grad_norm": 3.000438690185547,
      "learning_rate": 6.514934538296806e-05,
      "loss": 2.2919,
      "step": 19860
    },
    {
      "epoch": 1.2066557357138519,
      "grad_norm": 2.965790033340454,
      "learning_rate": 6.511904008887266e-05,
      "loss": 2.3972,
      "step": 19870
    },
    {
      "epoch": 1.2072630108702254,
      "grad_norm": 2.2877063751220703,
      "learning_rate": 6.508872868039665e-05,
      "loss": 2.2991,
      "step": 19880
    },
    {
      "epoch": 1.2078702860265986,
      "grad_norm": 2.242526054382324,
      "learning_rate": 6.505841116979844e-05,
      "loss": 1.948,
      "step": 19890
    },
    {
      "epoch": 1.208477561182972,
      "grad_norm": 3.0470337867736816,
      "learning_rate": 6.502808756933893e-05,
      "loss": 2.453,
      "step": 19900
    },
    {
      "epoch": 1.2090848363393454,
      "grad_norm": 3.0683939456939697,
      "learning_rate": 6.499775789128149e-05,
      "loss": 2.4813,
      "step": 19910
    },
    {
      "epoch": 1.2096921114957186,
      "grad_norm": 4.666704177856445,
      "learning_rate": 6.496742214789187e-05,
      "loss": 2.4283,
      "step": 19920
    },
    {
      "epoch": 1.2102993866520921,
      "grad_norm": 3.9280576705932617,
      "learning_rate": 6.493708035143834e-05,
      "loss": 2.4046,
      "step": 19930
    },
    {
      "epoch": 1.2109066618084654,
      "grad_norm": 4.35193395614624,
      "learning_rate": 6.490673251419167e-05,
      "loss": 2.2247,
      "step": 19940
    },
    {
      "epoch": 1.2115139369648387,
      "grad_norm": 3.8240857124328613,
      "learning_rate": 6.487637864842493e-05,
      "loss": 2.5482,
      "step": 19950
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 3.5232722759246826,
      "learning_rate": 6.484601876641375e-05,
      "loss": 2.5588,
      "step": 19960
    },
    {
      "epoch": 1.2127284872775854,
      "grad_norm": 2.937333106994629,
      "learning_rate": 6.481565288043612e-05,
      "loss": 2.572,
      "step": 19970
    },
    {
      "epoch": 1.213335762433959,
      "grad_norm": 3.519711494445801,
      "learning_rate": 6.478528100277252e-05,
      "loss": 2.4732,
      "step": 19980
    },
    {
      "epoch": 1.2139430375903322,
      "grad_norm": 3.371105432510376,
      "learning_rate": 6.47549031457058e-05,
      "loss": 2.4404,
      "step": 19990
    },
    {
      "epoch": 1.2145503127467054,
      "grad_norm": 4.351044178009033,
      "learning_rate": 6.472451932152126e-05,
      "loss": 2.571,
      "step": 20000
    },
    {
      "epoch": 1.2147932228092548,
      "eval_loss": 4.162310600280762,
      "eval_runtime": 2329.5052,
      "eval_samples_per_second": 7.069,
      "eval_steps_per_second": 1.767,
      "step": 20004
    },
    {
      "epoch": 1.215157587903079,
      "grad_norm": 4.368370056152344,
      "learning_rate": 6.469412954250659e-05,
      "loss": 3.4092,
      "step": 20010
    },
    {
      "epoch": 1.2157648630594522,
      "grad_norm": 3.740152597427368,
      "learning_rate": 6.466373382095193e-05,
      "loss": 3.1892,
      "step": 20020
    },
    {
      "epoch": 1.2163721382158257,
      "grad_norm": 3.6566150188446045,
      "learning_rate": 6.463333216914975e-05,
      "loss": 2.4401,
      "step": 20030
    },
    {
      "epoch": 1.216979413372199,
      "grad_norm": 4.966261863708496,
      "learning_rate": 6.460292459939499e-05,
      "loss": 2.7731,
      "step": 20040
    },
    {
      "epoch": 1.2175866885285722,
      "grad_norm": 2.232680082321167,
      "learning_rate": 6.457251112398495e-05,
      "loss": 2.2906,
      "step": 20050
    },
    {
      "epoch": 1.2181939636849457,
      "grad_norm": 3.4218928813934326,
      "learning_rate": 6.454209175521933e-05,
      "loss": 2.3777,
      "step": 20060
    },
    {
      "epoch": 1.218801238841319,
      "grad_norm": 3.48014235496521,
      "learning_rate": 6.45116665054002e-05,
      "loss": 2.5603,
      "step": 20070
    },
    {
      "epoch": 1.2194085139976925,
      "grad_norm": 3.01875901222229,
      "learning_rate": 6.448123538683202e-05,
      "loss": 2.4533,
      "step": 20080
    },
    {
      "epoch": 1.2200157891540657,
      "grad_norm": 3.0395419597625732,
      "learning_rate": 6.44507984118216e-05,
      "loss": 2.4731,
      "step": 20090
    },
    {
      "epoch": 1.220623064310439,
      "grad_norm": 2.155229330062866,
      "learning_rate": 6.442035559267816e-05,
      "loss": 2.4059,
      "step": 20100
    },
    {
      "epoch": 1.2212303394668125,
      "grad_norm": 1.5809919834136963,
      "learning_rate": 6.438990694171324e-05,
      "loss": 2.191,
      "step": 20110
    },
    {
      "epoch": 1.2218376146231857,
      "grad_norm": 2.7062299251556396,
      "learning_rate": 6.43594524712408e-05,
      "loss": 2.6442,
      "step": 20120
    },
    {
      "epoch": 1.2224448897795592,
      "grad_norm": 1.6754319667816162,
      "learning_rate": 6.432899219357707e-05,
      "loss": 2.3319,
      "step": 20130
    },
    {
      "epoch": 1.2230521649359325,
      "grad_norm": 3.7216687202453613,
      "learning_rate": 6.429852612104068e-05,
      "loss": 2.2426,
      "step": 20140
    },
    {
      "epoch": 1.2236594400923058,
      "grad_norm": 2.5162081718444824,
      "learning_rate": 6.426805426595264e-05,
      "loss": 2.46,
      "step": 20150
    },
    {
      "epoch": 1.2242667152486792,
      "grad_norm": 1.6102948188781738,
      "learning_rate": 6.423757664063618e-05,
      "loss": 2.1686,
      "step": 20160
    },
    {
      "epoch": 1.2248739904050525,
      "grad_norm": 3.131232261657715,
      "learning_rate": 6.420709325741698e-05,
      "loss": 2.6657,
      "step": 20170
    },
    {
      "epoch": 1.225481265561426,
      "grad_norm": 4.303732872009277,
      "learning_rate": 6.417660412862301e-05,
      "loss": 2.245,
      "step": 20180
    },
    {
      "epoch": 1.2260885407177993,
      "grad_norm": 3.50286865234375,
      "learning_rate": 6.414610926658454e-05,
      "loss": 2.411,
      "step": 20190
    },
    {
      "epoch": 1.2266958158741725,
      "grad_norm": 2.9019265174865723,
      "learning_rate": 6.411560868363418e-05,
      "loss": 2.7373,
      "step": 20200
    },
    {
      "epoch": 1.227303091030546,
      "grad_norm": 4.640141487121582,
      "learning_rate": 6.408510239210687e-05,
      "loss": 2.8756,
      "step": 20210
    },
    {
      "epoch": 1.2279103661869193,
      "grad_norm": 3.6521177291870117,
      "learning_rate": 6.405459040433982e-05,
      "loss": 2.9614,
      "step": 20220
    },
    {
      "epoch": 1.2285176413432926,
      "grad_norm": 4.944643020629883,
      "learning_rate": 6.402407273267258e-05,
      "loss": 2.7652,
      "step": 20230
    },
    {
      "epoch": 1.229124916499666,
      "grad_norm": 3.7496140003204346,
      "learning_rate": 6.399354938944695e-05,
      "loss": 2.2099,
      "step": 20240
    },
    {
      "epoch": 1.2297321916560393,
      "grad_norm": 2.3872177600860596,
      "learning_rate": 6.39630203870071e-05,
      "loss": 2.0187,
      "step": 20250
    },
    {
      "epoch": 1.2303394668124128,
      "grad_norm": 3.6654014587402344,
      "learning_rate": 6.393248573769944e-05,
      "loss": 2.1953,
      "step": 20260
    },
    {
      "epoch": 1.230946741968786,
      "grad_norm": 2.6895158290863037,
      "learning_rate": 6.390194545387262e-05,
      "loss": 2.347,
      "step": 20270
    },
    {
      "epoch": 1.2315540171251593,
      "grad_norm": 2.813509941101074,
      "learning_rate": 6.387139954787766e-05,
      "loss": 2.4594,
      "step": 20280
    },
    {
      "epoch": 1.2321612922815328,
      "grad_norm": 4.872365474700928,
      "learning_rate": 6.384084803206783e-05,
      "loss": 2.5017,
      "step": 20290
    },
    {
      "epoch": 1.232768567437906,
      "grad_norm": 4.186225414276123,
      "learning_rate": 6.381029091879859e-05,
      "loss": 2.7866,
      "step": 20300
    },
    {
      "epoch": 1.2333758425942793,
      "grad_norm": 5.370710849761963,
      "learning_rate": 6.377972822042777e-05,
      "loss": 2.6767,
      "step": 20310
    },
    {
      "epoch": 1.2339831177506528,
      "grad_norm": 3.780998468399048,
      "learning_rate": 6.374915994931538e-05,
      "loss": 2.5763,
      "step": 20320
    },
    {
      "epoch": 1.234590392907026,
      "grad_norm": 3.3811795711517334,
      "learning_rate": 6.371858611782375e-05,
      "loss": 2.5565,
      "step": 20330
    },
    {
      "epoch": 1.2351976680633996,
      "grad_norm": 3.4764044284820557,
      "learning_rate": 6.368800673831739e-05,
      "loss": 2.3935,
      "step": 20340
    },
    {
      "epoch": 1.2358049432197729,
      "grad_norm": 4.896260738372803,
      "learning_rate": 6.365742182316311e-05,
      "loss": 2.3535,
      "step": 20350
    },
    {
      "epoch": 1.2364122183761461,
      "grad_norm": 2.546125888824463,
      "learning_rate": 6.362683138472993e-05,
      "loss": 2.0293,
      "step": 20360
    },
    {
      "epoch": 1.2370194935325196,
      "grad_norm": 2.709432363510132,
      "learning_rate": 6.359623543538911e-05,
      "loss": 2.2944,
      "step": 20370
    },
    {
      "epoch": 1.2376267686888929,
      "grad_norm": 3.0229170322418213,
      "learning_rate": 6.356563398751414e-05,
      "loss": 2.4689,
      "step": 20380
    },
    {
      "epoch": 1.2382340438452664,
      "grad_norm": 2.66573166847229,
      "learning_rate": 6.353502705348076e-05,
      "loss": 2.2113,
      "step": 20390
    },
    {
      "epoch": 1.2388413190016396,
      "grad_norm": 3.3417532444000244,
      "learning_rate": 6.350441464566685e-05,
      "loss": 2.1727,
      "step": 20400
    },
    {
      "epoch": 1.239448594158013,
      "grad_norm": 2.445599317550659,
      "learning_rate": 6.347379677645258e-05,
      "loss": 2.2127,
      "step": 20410
    },
    {
      "epoch": 1.2400558693143864,
      "grad_norm": 4.759430408477783,
      "learning_rate": 6.34431734582203e-05,
      "loss": 2.9285,
      "step": 20420
    },
    {
      "epoch": 1.2406631444707596,
      "grad_norm": 2.9744601249694824,
      "learning_rate": 6.341254470335459e-05,
      "loss": 2.5696,
      "step": 20430
    },
    {
      "epoch": 1.2412704196271331,
      "grad_norm": 3.5168423652648926,
      "learning_rate": 6.338191052424218e-05,
      "loss": 2.3044,
      "step": 20440
    },
    {
      "epoch": 1.2418776947835064,
      "grad_norm": 3.0753023624420166,
      "learning_rate": 6.335127093327202e-05,
      "loss": 2.6618,
      "step": 20450
    },
    {
      "epoch": 1.2424849699398797,
      "grad_norm": 4.222288608551025,
      "learning_rate": 6.332062594283528e-05,
      "loss": 2.727,
      "step": 20460
    },
    {
      "epoch": 1.2430922450962532,
      "grad_norm": 3.4371845722198486,
      "learning_rate": 6.328997556532523e-05,
      "loss": 2.3736,
      "step": 20470
    },
    {
      "epoch": 1.2436995202526264,
      "grad_norm": 3.2194957733154297,
      "learning_rate": 6.325931981313742e-05,
      "loss": 2.1736,
      "step": 20480
    },
    {
      "epoch": 1.244306795409,
      "grad_norm": 2.953640937805176,
      "learning_rate": 6.32286586986695e-05,
      "loss": 2.5242,
      "step": 20490
    },
    {
      "epoch": 1.2449140705653732,
      "grad_norm": 3.61287260055542,
      "learning_rate": 6.319799223432133e-05,
      "loss": 2.4579,
      "step": 20500
    },
    {
      "epoch": 1.2455213457217464,
      "grad_norm": 2.9729888439178467,
      "learning_rate": 6.316732043249487e-05,
      "loss": 2.617,
      "step": 20510
    },
    {
      "epoch": 1.24612862087812,
      "grad_norm": 3.5704915523529053,
      "learning_rate": 6.313664330559434e-05,
      "loss": 2.4398,
      "step": 20520
    },
    {
      "epoch": 1.2467358960344932,
      "grad_norm": 5.7113938331604,
      "learning_rate": 6.310596086602605e-05,
      "loss": 2.5241,
      "step": 20530
    },
    {
      "epoch": 1.2473431711908667,
      "grad_norm": 1.4871479272842407,
      "learning_rate": 6.307527312619844e-05,
      "loss": 1.8933,
      "step": 20540
    },
    {
      "epoch": 1.24795044634724,
      "grad_norm": 2.200803756713867,
      "learning_rate": 6.304458009852213e-05,
      "loss": 2.0594,
      "step": 20550
    },
    {
      "epoch": 1.2485577215036132,
      "grad_norm": 1.8073588609695435,
      "learning_rate": 6.301388179540989e-05,
      "loss": 2.2657,
      "step": 20560
    },
    {
      "epoch": 1.2491649966599867,
      "grad_norm": 3.1596484184265137,
      "learning_rate": 6.298317822927657e-05,
      "loss": 2.2427,
      "step": 20570
    },
    {
      "epoch": 1.24977227181636,
      "grad_norm": 3.148449420928955,
      "learning_rate": 6.295246941253921e-05,
      "loss": 2.101,
      "step": 20580
    },
    {
      "epoch": 1.2503795469727335,
      "grad_norm": 3.455982208251953,
      "learning_rate": 6.292175535761693e-05,
      "loss": 2.5075,
      "step": 20590
    },
    {
      "epoch": 1.2509868221291067,
      "grad_norm": 3.5964205265045166,
      "learning_rate": 6.289103607693098e-05,
      "loss": 2.0786,
      "step": 20600
    },
    {
      "epoch": 1.25159409728548,
      "grad_norm": 4.460583686828613,
      "learning_rate": 6.286031158290475e-05,
      "loss": 2.3954,
      "step": 20610
    },
    {
      "epoch": 1.2522013724418535,
      "grad_norm": 3.6468701362609863,
      "learning_rate": 6.282958188796368e-05,
      "loss": 2.7805,
      "step": 20620
    },
    {
      "epoch": 1.2528086475982267,
      "grad_norm": 3.005331039428711,
      "learning_rate": 6.27988470045354e-05,
      "loss": 2.4683,
      "step": 20630
    },
    {
      "epoch": 1.2534159227546002,
      "grad_norm": 3.222388744354248,
      "learning_rate": 6.276810694504952e-05,
      "loss": 2.3872,
      "step": 20640
    },
    {
      "epoch": 1.2540231979109735,
      "grad_norm": 2.74983811378479,
      "learning_rate": 6.273736172193784e-05,
      "loss": 2.3341,
      "step": 20650
    },
    {
      "epoch": 1.2546304730673468,
      "grad_norm": 4.675693988800049,
      "learning_rate": 6.270661134763422e-05,
      "loss": 2.6249,
      "step": 20660
    },
    {
      "epoch": 1.25523774822372,
      "grad_norm": 2.9846348762512207,
      "learning_rate": 6.267585583457463e-05,
      "loss": 2.3972,
      "step": 20670
    },
    {
      "epoch": 1.2558450233800935,
      "grad_norm": 1.7777889966964722,
      "learning_rate": 6.264509519519702e-05,
      "loss": 2.0348,
      "step": 20680
    },
    {
      "epoch": 1.256452298536467,
      "grad_norm": 1.802843689918518,
      "learning_rate": 6.261432944194155e-05,
      "loss": 2.1342,
      "step": 20690
    },
    {
      "epoch": 1.2570595736928403,
      "grad_norm": 3.8511569499969482,
      "learning_rate": 6.258355858725034e-05,
      "loss": 2.1822,
      "step": 20700
    },
    {
      "epoch": 1.2576668488492135,
      "grad_norm": 3.1331787109375,
      "learning_rate": 6.255278264356764e-05,
      "loss": 2.1123,
      "step": 20710
    },
    {
      "epoch": 1.2582741240055868,
      "grad_norm": 3.270638942718506,
      "learning_rate": 6.252200162333968e-05,
      "loss": 2.45,
      "step": 20720
    },
    {
      "epoch": 1.2588813991619603,
      "grad_norm": 3.1903674602508545,
      "learning_rate": 6.249121553901487e-05,
      "loss": 2.3426,
      "step": 20730
    },
    {
      "epoch": 1.2594886743183336,
      "grad_norm": 2.203763008117676,
      "learning_rate": 6.246042440304353e-05,
      "loss": 2.3533,
      "step": 20740
    },
    {
      "epoch": 1.260095949474707,
      "grad_norm": 1.8558186292648315,
      "learning_rate": 6.242962822787815e-05,
      "loss": 2.1759,
      "step": 20750
    },
    {
      "epoch": 1.2607032246310803,
      "grad_norm": 2.3789663314819336,
      "learning_rate": 6.239882702597312e-05,
      "loss": 2.1166,
      "step": 20760
    },
    {
      "epoch": 1.2613104997874536,
      "grad_norm": 3.708439826965332,
      "learning_rate": 6.2368020809785e-05,
      "loss": 2.2826,
      "step": 20770
    },
    {
      "epoch": 1.261917774943827,
      "grad_norm": 3.1328024864196777,
      "learning_rate": 6.233720959177227e-05,
      "loss": 2.3012,
      "step": 20780
    },
    {
      "epoch": 1.2625250501002003,
      "grad_norm": 3.3836982250213623,
      "learning_rate": 6.230639338439549e-05,
      "loss": 2.2001,
      "step": 20790
    },
    {
      "epoch": 1.2631323252565738,
      "grad_norm": 3.6833784580230713,
      "learning_rate": 6.227557220011724e-05,
      "loss": 2.2822,
      "step": 20800
    },
    {
      "epoch": 1.263739600412947,
      "grad_norm": 1.9579414129257202,
      "learning_rate": 6.224474605140208e-05,
      "loss": 2.2043,
      "step": 20810
    },
    {
      "epoch": 1.2643468755693203,
      "grad_norm": 2.3955938816070557,
      "learning_rate": 6.221391495071659e-05,
      "loss": 2.2402,
      "step": 20820
    },
    {
      "epoch": 1.2649541507256938,
      "grad_norm": 2.838073968887329,
      "learning_rate": 6.21830789105294e-05,
      "loss": 2.39,
      "step": 20830
    },
    {
      "epoch": 1.265561425882067,
      "grad_norm": 2.6042449474334717,
      "learning_rate": 6.215223794331105e-05,
      "loss": 2.4531,
      "step": 20840
    },
    {
      "epoch": 1.2661687010384406,
      "grad_norm": 3.5424656867980957,
      "learning_rate": 6.212139206153411e-05,
      "loss": 2.41,
      "step": 20850
    },
    {
      "epoch": 1.2667759761948139,
      "grad_norm": 1.6916097402572632,
      "learning_rate": 6.209054127767319e-05,
      "loss": 1.9971,
      "step": 20860
    },
    {
      "epoch": 1.2673832513511871,
      "grad_norm": 2.8387691974639893,
      "learning_rate": 6.205968560420483e-05,
      "loss": 2.6174,
      "step": 20870
    },
    {
      "epoch": 1.2679905265075606,
      "grad_norm": 2.5899767875671387,
      "learning_rate": 6.202882505360754e-05,
      "loss": 2.4744,
      "step": 20880
    },
    {
      "epoch": 1.2685978016639339,
      "grad_norm": 2.690826177597046,
      "learning_rate": 6.199795963836181e-05,
      "loss": 2.4768,
      "step": 20890
    },
    {
      "epoch": 1.2692050768203074,
      "grad_norm": 4.464257717132568,
      "learning_rate": 6.196708937095013e-05,
      "loss": 2.8012,
      "step": 20900
    },
    {
      "epoch": 1.2698123519766806,
      "grad_norm": 3.739450693130493,
      "learning_rate": 6.193621426385691e-05,
      "loss": 2.8842,
      "step": 20910
    },
    {
      "epoch": 1.270419627133054,
      "grad_norm": 3.369910955429077,
      "learning_rate": 6.190533432956856e-05,
      "loss": 2.6449,
      "step": 20920
    },
    {
      "epoch": 1.2710269022894274,
      "grad_norm": 2.8310723304748535,
      "learning_rate": 6.187444958057337e-05,
      "loss": 2.5006,
      "step": 20930
    },
    {
      "epoch": 1.2716341774458007,
      "grad_norm": 3.762418270111084,
      "learning_rate": 6.184356002936167e-05,
      "loss": 2.5105,
      "step": 20940
    },
    {
      "epoch": 1.2722414526021741,
      "grad_norm": 3.723334550857544,
      "learning_rate": 6.181266568842566e-05,
      "loss": 2.5549,
      "step": 20950
    },
    {
      "epoch": 1.2728487277585474,
      "grad_norm": 3.457122325897217,
      "learning_rate": 6.178176657025951e-05,
      "loss": 2.7036,
      "step": 20960
    },
    {
      "epoch": 1.2734560029149207,
      "grad_norm": 2.6367974281311035,
      "learning_rate": 6.175086268735932e-05,
      "loss": 2.6504,
      "step": 20970
    },
    {
      "epoch": 1.2740632780712942,
      "grad_norm": 2.8035314083099365,
      "learning_rate": 6.171995405222312e-05,
      "loss": 1.9425,
      "step": 20980
    },
    {
      "epoch": 1.2746705532276674,
      "grad_norm": 2.060360908508301,
      "learning_rate": 6.168904067735082e-05,
      "loss": 1.9615,
      "step": 20990
    },
    {
      "epoch": 1.275277828384041,
      "grad_norm": 1.5526129007339478,
      "learning_rate": 6.165812257524433e-05,
      "loss": 1.9741,
      "step": 21000
    },
    {
      "epoch": 1.2758851035404142,
      "grad_norm": 3.7882192134857178,
      "learning_rate": 6.16271997584074e-05,
      "loss": 2.4976,
      "step": 21010
    },
    {
      "epoch": 1.2764923786967874,
      "grad_norm": 2.266270160675049,
      "learning_rate": 6.159627223934572e-05,
      "loss": 2.2364,
      "step": 21020
    },
    {
      "epoch": 1.277099653853161,
      "grad_norm": 2.8876495361328125,
      "learning_rate": 6.156534003056682e-05,
      "loss": 2.2659,
      "step": 21030
    },
    {
      "epoch": 1.2777069290095342,
      "grad_norm": 2.499871015548706,
      "learning_rate": 6.153440314458023e-05,
      "loss": 2.5493,
      "step": 21040
    },
    {
      "epoch": 1.2783142041659077,
      "grad_norm": 2.220372200012207,
      "learning_rate": 6.150346159389733e-05,
      "loss": 2.036,
      "step": 21050
    },
    {
      "epoch": 1.278921479322281,
      "grad_norm": 3.6237261295318604,
      "learning_rate": 6.147251539103131e-05,
      "loss": 2.4648,
      "step": 21060
    },
    {
      "epoch": 1.2795287544786542,
      "grad_norm": 2.9123125076293945,
      "learning_rate": 6.144156454849738e-05,
      "loss": 2.5029,
      "step": 21070
    },
    {
      "epoch": 1.2801360296350277,
      "grad_norm": 2.480128288269043,
      "learning_rate": 6.141060907881253e-05,
      "loss": 2.1142,
      "step": 21080
    },
    {
      "epoch": 1.280743304791401,
      "grad_norm": 4.863001823425293,
      "learning_rate": 6.137964899449563e-05,
      "loss": 2.4189,
      "step": 21090
    },
    {
      "epoch": 1.2813505799477745,
      "grad_norm": 2.8528621196746826,
      "learning_rate": 6.134868430806744e-05,
      "loss": 2.4076,
      "step": 21100
    },
    {
      "epoch": 1.2819578551041477,
      "grad_norm": 3.7196996212005615,
      "learning_rate": 6.131771503205059e-05,
      "loss": 2.2801,
      "step": 21110
    },
    {
      "epoch": 1.282565130260521,
      "grad_norm": 2.1412792205810547,
      "learning_rate": 6.128674117896949e-05,
      "loss": 2.1478,
      "step": 21120
    },
    {
      "epoch": 1.2831724054168943,
      "grad_norm": 2.1189017295837402,
      "learning_rate": 6.125576276135052e-05,
      "loss": 2.099,
      "step": 21130
    },
    {
      "epoch": 1.2837796805732677,
      "grad_norm": 6.2339043617248535,
      "learning_rate": 6.122477979172182e-05,
      "loss": 2.2644,
      "step": 21140
    },
    {
      "epoch": 1.2843869557296412,
      "grad_norm": 3.354509115219116,
      "learning_rate": 6.119379228261343e-05,
      "loss": 2.4077,
      "step": 21150
    },
    {
      "epoch": 1.2849942308860145,
      "grad_norm": 1.5877773761749268,
      "learning_rate": 6.116280024655713e-05,
      "loss": 1.9716,
      "step": 21160
    },
    {
      "epoch": 1.2856015060423878,
      "grad_norm": 1.6017475128173828,
      "learning_rate": 6.113180369608665e-05,
      "loss": 1.7732,
      "step": 21170
    },
    {
      "epoch": 1.286208781198761,
      "grad_norm": 2.098203182220459,
      "learning_rate": 6.110080264373745e-05,
      "loss": 1.9184,
      "step": 21180
    },
    {
      "epoch": 1.2868160563551345,
      "grad_norm": 2.778541088104248,
      "learning_rate": 6.106979710204688e-05,
      "loss": 2.2711,
      "step": 21190
    },
    {
      "epoch": 1.2874233315115078,
      "grad_norm": 4.054927825927734,
      "learning_rate": 6.103878708355403e-05,
      "loss": 2.2105,
      "step": 21200
    },
    {
      "epoch": 1.2880306066678813,
      "grad_norm": 4.794691562652588,
      "learning_rate": 6.1007772600799906e-05,
      "loss": 2.7729,
      "step": 21210
    },
    {
      "epoch": 1.2886378818242545,
      "grad_norm": 3.120368003845215,
      "learning_rate": 6.097675366632722e-05,
      "loss": 2.4191,
      "step": 21220
    },
    {
      "epoch": 1.2892451569806278,
      "grad_norm": 5.061145782470703,
      "learning_rate": 6.0945730292680536e-05,
      "loss": 2.2191,
      "step": 21230
    },
    {
      "epoch": 1.2898524321370013,
      "grad_norm": 3.5752768516540527,
      "learning_rate": 6.091470249240617e-05,
      "loss": 2.1409,
      "step": 21240
    },
    {
      "epoch": 1.2904597072933746,
      "grad_norm": 3.947521686553955,
      "learning_rate": 6.088367027805232e-05,
      "loss": 2.5472,
      "step": 21250
    },
    {
      "epoch": 1.291066982449748,
      "grad_norm": 4.19317102432251,
      "learning_rate": 6.0852633662168844e-05,
      "loss": 2.6049,
      "step": 21260
    },
    {
      "epoch": 1.2916742576061213,
      "grad_norm": 5.893950939178467,
      "learning_rate": 6.082159265730749e-05,
      "loss": 2.6672,
      "step": 21270
    },
    {
      "epoch": 1.2922815327624946,
      "grad_norm": 4.480019569396973,
      "learning_rate": 6.07905472760217e-05,
      "loss": 2.7284,
      "step": 21280
    },
    {
      "epoch": 1.292888807918868,
      "grad_norm": 3.5382540225982666,
      "learning_rate": 6.075949753086675e-05,
      "loss": 2.6482,
      "step": 21290
    },
    {
      "epoch": 1.2934960830752413,
      "grad_norm": 2.420593023300171,
      "learning_rate": 6.072844343439964e-05,
      "loss": 2.2648,
      "step": 21300
    },
    {
      "epoch": 1.2941033582316148,
      "grad_norm": 2.6590969562530518,
      "learning_rate": 6.069738499917912e-05,
      "loss": 2.2477,
      "step": 21310
    },
    {
      "epoch": 1.294710633387988,
      "grad_norm": 3.0107879638671875,
      "learning_rate": 6.066632223776576e-05,
      "loss": 2.5372,
      "step": 21320
    },
    {
      "epoch": 1.2953179085443614,
      "grad_norm": 1.230377197265625,
      "learning_rate": 6.0635255162721794e-05,
      "loss": 1.9437,
      "step": 21330
    },
    {
      "epoch": 1.2959251837007348,
      "grad_norm": 2.095503330230713,
      "learning_rate": 6.0604183786611267e-05,
      "loss": 2.2518,
      "step": 21340
    },
    {
      "epoch": 1.296532458857108,
      "grad_norm": 2.226091146469116,
      "learning_rate": 6.057310812199993e-05,
      "loss": 2.2169,
      "step": 21350
    },
    {
      "epoch": 1.2971397340134816,
      "grad_norm": 2.1033968925476074,
      "learning_rate": 6.054202818145528e-05,
      "loss": 2.2733,
      "step": 21360
    },
    {
      "epoch": 1.2977470091698549,
      "grad_norm": 2.569420576095581,
      "learning_rate": 6.051094397754653e-05,
      "loss": 2.3125,
      "step": 21370
    },
    {
      "epoch": 1.2983542843262281,
      "grad_norm": 3.6894590854644775,
      "learning_rate": 6.047985552284463e-05,
      "loss": 2.3385,
      "step": 21380
    },
    {
      "epoch": 1.2989615594826016,
      "grad_norm": 1.7332079410552979,
      "learning_rate": 6.0448762829922264e-05,
      "loss": 2.0686,
      "step": 21390
    },
    {
      "epoch": 1.2995688346389749,
      "grad_norm": 3.5595929622650146,
      "learning_rate": 6.041766591135379e-05,
      "loss": 2.5906,
      "step": 21400
    },
    {
      "epoch": 1.3001761097953484,
      "grad_norm": 2.2264814376831055,
      "learning_rate": 6.0386564779715306e-05,
      "loss": 2.1664,
      "step": 21410
    },
    {
      "epoch": 1.3007833849517216,
      "grad_norm": 3.2171645164489746,
      "learning_rate": 6.0355459447584606e-05,
      "loss": 2.2254,
      "step": 21420
    },
    {
      "epoch": 1.301390660108095,
      "grad_norm": 3.019967794418335,
      "learning_rate": 6.032434992754118e-05,
      "loss": 2.4913,
      "step": 21430
    },
    {
      "epoch": 1.3019979352644684,
      "grad_norm": 3.924384832382202,
      "learning_rate": 6.0293236232166214e-05,
      "loss": 2.2663,
      "step": 21440
    },
    {
      "epoch": 1.3026052104208417,
      "grad_norm": 4.742030620574951,
      "learning_rate": 6.026211837404256e-05,
      "loss": 2.8073,
      "step": 21450
    },
    {
      "epoch": 1.3032124855772151,
      "grad_norm": 4.1355509757995605,
      "learning_rate": 6.0230996365754846e-05,
      "loss": 2.4506,
      "step": 21460
    },
    {
      "epoch": 1.3038197607335884,
      "grad_norm": 3.2240164279937744,
      "learning_rate": 6.019987021988922e-05,
      "loss": 2.3704,
      "step": 21470
    },
    {
      "epoch": 1.3044270358899617,
      "grad_norm": 3.011033773422241,
      "learning_rate": 6.0168739949033636e-05,
      "loss": 2.3077,
      "step": 21480
    },
    {
      "epoch": 1.3050343110463352,
      "grad_norm": 2.379554033279419,
      "learning_rate": 6.0137605565777675e-05,
      "loss": 2.2654,
      "step": 21490
    },
    {
      "epoch": 1.3056415862027084,
      "grad_norm": 1.484666109085083,
      "learning_rate": 6.010646708271256e-05,
      "loss": 2.1349,
      "step": 21500
    },
    {
      "epoch": 1.306248861359082,
      "grad_norm": 3.2751975059509277,
      "learning_rate": 6.00753245124312e-05,
      "loss": 2.1946,
      "step": 21510
    },
    {
      "epoch": 1.3068561365154552,
      "grad_norm": 2.4016966819763184,
      "learning_rate": 6.0044177867528137e-05,
      "loss": 2.3436,
      "step": 21520
    },
    {
      "epoch": 1.3074634116718284,
      "grad_norm": 3.1140260696411133,
      "learning_rate": 6.001302716059959e-05,
      "loss": 2.3582,
      "step": 21530
    },
    {
      "epoch": 1.308070686828202,
      "grad_norm": 3.1974754333496094,
      "learning_rate": 5.998187240424337e-05,
      "loss": 2.2366,
      "step": 21540
    },
    {
      "epoch": 1.3086779619845752,
      "grad_norm": 2.722257614135742,
      "learning_rate": 5.9950713611058984e-05,
      "loss": 2.1587,
      "step": 21550
    },
    {
      "epoch": 1.3092852371409487,
      "grad_norm": 4.033088684082031,
      "learning_rate": 5.991955079364754e-05,
      "loss": 2.4571,
      "step": 21560
    },
    {
      "epoch": 1.309892512297322,
      "grad_norm": 3.544597625732422,
      "learning_rate": 5.988838396461176e-05,
      "loss": 2.5468,
      "step": 21570
    },
    {
      "epoch": 1.3104997874536952,
      "grad_norm": 2.8349194526672363,
      "learning_rate": 5.9857213136556025e-05,
      "loss": 2.3559,
      "step": 21580
    },
    {
      "epoch": 1.3111070626100685,
      "grad_norm": 4.329996109008789,
      "learning_rate": 5.982603832208631e-05,
      "loss": 2.6088,
      "step": 21590
    },
    {
      "epoch": 1.311714337766442,
      "grad_norm": 3.3195672035217285,
      "learning_rate": 5.979485953381021e-05,
      "loss": 2.4839,
      "step": 21600
    },
    {
      "epoch": 1.3123216129228155,
      "grad_norm": 2.0134847164154053,
      "learning_rate": 5.976367678433691e-05,
      "loss": 2.189,
      "step": 21610
    },
    {
      "epoch": 1.3129288880791887,
      "grad_norm": 2.7243447303771973,
      "learning_rate": 5.97324900862772e-05,
      "loss": 2.253,
      "step": 21620
    },
    {
      "epoch": 1.313536163235562,
      "grad_norm": 3.706880569458008,
      "learning_rate": 5.970129945224353e-05,
      "loss": 2.5041,
      "step": 21630
    },
    {
      "epoch": 1.3141434383919353,
      "grad_norm": 3.10998272895813,
      "learning_rate": 5.9670104894849835e-05,
      "loss": 2.6734,
      "step": 21640
    },
    {
      "epoch": 1.3147507135483087,
      "grad_norm": 2.4918978214263916,
      "learning_rate": 5.963890642671172e-05,
      "loss": 2.2158,
      "step": 21650
    },
    {
      "epoch": 1.315357988704682,
      "grad_norm": 1.980982780456543,
      "learning_rate": 5.9607704060446324e-05,
      "loss": 2.1399,
      "step": 21660
    },
    {
      "epoch": 1.3159652638610555,
      "grad_norm": 3.0517663955688477,
      "learning_rate": 5.95764978086724e-05,
      "loss": 2.1875,
      "step": 21670
    },
    {
      "epoch": 1.3165725390174288,
      "grad_norm": 3.266446590423584,
      "learning_rate": 5.954528768401022e-05,
      "loss": 2.3722,
      "step": 21680
    },
    {
      "epoch": 1.317179814173802,
      "grad_norm": 3.251305341720581,
      "learning_rate": 5.9514073699081695e-05,
      "loss": 2.5442,
      "step": 21690
    },
    {
      "epoch": 1.3177870893301755,
      "grad_norm": 2.3098225593566895,
      "learning_rate": 5.948285586651024e-05,
      "loss": 2.3301,
      "step": 21700
    },
    {
      "epoch": 1.3183943644865488,
      "grad_norm": 3.07763409614563,
      "learning_rate": 5.945163419892086e-05,
      "loss": 2.0574,
      "step": 21710
    },
    {
      "epoch": 1.3190016396429223,
      "grad_norm": 1.9337173700332642,
      "learning_rate": 5.9420408708940054e-05,
      "loss": 2.264,
      "step": 21720
    },
    {
      "epoch": 1.3196089147992955,
      "grad_norm": 2.9767637252807617,
      "learning_rate": 5.938917940919595e-05,
      "loss": 1.9862,
      "step": 21730
    },
    {
      "epoch": 1.3202161899556688,
      "grad_norm": 5.139755725860596,
      "learning_rate": 5.9357946312318146e-05,
      "loss": 2.449,
      "step": 21740
    },
    {
      "epoch": 1.3208234651120423,
      "grad_norm": 2.191716194152832,
      "learning_rate": 5.9326709430937824e-05,
      "loss": 2.3785,
      "step": 21750
    },
    {
      "epoch": 1.3214307402684156,
      "grad_norm": 2.664109945297241,
      "learning_rate": 5.9295468777687655e-05,
      "loss": 2.085,
      "step": 21760
    },
    {
      "epoch": 1.322038015424789,
      "grad_norm": 3.378018379211426,
      "learning_rate": 5.926422436520188e-05,
      "loss": 2.5381,
      "step": 21770
    },
    {
      "epoch": 1.3226452905811623,
      "grad_norm": 3.5056843757629395,
      "learning_rate": 5.923297620611623e-05,
      "loss": 2.6968,
      "step": 21780
    },
    {
      "epoch": 1.3232525657375356,
      "grad_norm": 1.6795437335968018,
      "learning_rate": 5.920172431306793e-05,
      "loss": 2.4156,
      "step": 21790
    },
    {
      "epoch": 1.323859840893909,
      "grad_norm": 3.842709541320801,
      "learning_rate": 5.9170468698695806e-05,
      "loss": 2.4886,
      "step": 21800
    },
    {
      "epoch": 1.3244671160502823,
      "grad_norm": 2.57081937789917,
      "learning_rate": 5.913920937564006e-05,
      "loss": 2.2717,
      "step": 21810
    },
    {
      "epoch": 1.3250743912066558,
      "grad_norm": 3.2052297592163086,
      "learning_rate": 5.910794635654249e-05,
      "loss": 2.3717,
      "step": 21820
    },
    {
      "epoch": 1.325681666363029,
      "grad_norm": 2.5987281799316406,
      "learning_rate": 5.907667965404636e-05,
      "loss": 2.2815,
      "step": 21830
    },
    {
      "epoch": 1.3262889415194024,
      "grad_norm": 3.805068254470825,
      "learning_rate": 5.904540928079643e-05,
      "loss": 2.1963,
      "step": 21840
    },
    {
      "epoch": 1.3268962166757758,
      "grad_norm": 2.22316575050354,
      "learning_rate": 5.901413524943891e-05,
      "loss": 2.1005,
      "step": 21850
    },
    {
      "epoch": 1.327503491832149,
      "grad_norm": 3.581660747528076,
      "learning_rate": 5.898285757262153e-05,
      "loss": 2.5096,
      "step": 21860
    },
    {
      "epoch": 1.3281107669885226,
      "grad_norm": 3.5823254585266113,
      "learning_rate": 5.89515762629935e-05,
      "loss": 2.5661,
      "step": 21870
    },
    {
      "epoch": 1.3287180421448959,
      "grad_norm": 1.9805468320846558,
      "learning_rate": 5.8920291333205456e-05,
      "loss": 2.0712,
      "step": 21880
    },
    {
      "epoch": 1.3293253173012691,
      "grad_norm": 4.452923774719238,
      "learning_rate": 5.888900279590952e-05,
      "loss": 2.5957,
      "step": 21890
    },
    {
      "epoch": 1.3299325924576426,
      "grad_norm": 3.415898323059082,
      "learning_rate": 5.885771066375929e-05,
      "loss": 2.4259,
      "step": 21900
    },
    {
      "epoch": 1.3305398676140159,
      "grad_norm": 1.2856284379959106,
      "learning_rate": 5.88264149494098e-05,
      "loss": 1.9122,
      "step": 21910
    },
    {
      "epoch": 1.3311471427703894,
      "grad_norm": 2.8114688396453857,
      "learning_rate": 5.879511566551753e-05,
      "loss": 2.1064,
      "step": 21920
    },
    {
      "epoch": 1.3317544179267626,
      "grad_norm": 2.8158249855041504,
      "learning_rate": 5.8763812824740396e-05,
      "loss": 2.1918,
      "step": 21930
    },
    {
      "epoch": 1.332361693083136,
      "grad_norm": 2.7347679138183594,
      "learning_rate": 5.873250643973781e-05,
      "loss": 1.8475,
      "step": 21940
    },
    {
      "epoch": 1.3329689682395094,
      "grad_norm": 4.510173320770264,
      "learning_rate": 5.870119652317051e-05,
      "loss": 2.2573,
      "step": 21950
    },
    {
      "epoch": 1.3335762433958827,
      "grad_norm": 2.2114648818969727,
      "learning_rate": 5.866988308770078e-05,
      "loss": 2.0649,
      "step": 21960
    },
    {
      "epoch": 1.3341835185522561,
      "grad_norm": 3.709468126296997,
      "learning_rate": 5.863856614599223e-05,
      "loss": 2.2503,
      "step": 21970
    },
    {
      "epoch": 1.3347907937086294,
      "grad_norm": 4.378516674041748,
      "learning_rate": 5.860724571070997e-05,
      "loss": 2.4943,
      "step": 21980
    },
    {
      "epoch": 1.3353980688650027,
      "grad_norm": 4.237511157989502,
      "learning_rate": 5.857592179452045e-05,
      "loss": 2.2231,
      "step": 21990
    },
    {
      "epoch": 1.3360053440213762,
      "grad_norm": 2.6773338317871094,
      "learning_rate": 5.854459441009157e-05,
      "loss": 2.7384,
      "step": 22000
    },
    {
      "epoch": 1.3366126191777494,
      "grad_norm": 3.2080142498016357,
      "learning_rate": 5.851326357009264e-05,
      "loss": 2.4331,
      "step": 22010
    },
    {
      "epoch": 1.337219894334123,
      "grad_norm": 1.7223373651504517,
      "learning_rate": 5.848192928719431e-05,
      "loss": 1.8786,
      "step": 22020
    },
    {
      "epoch": 1.3378271694904962,
      "grad_norm": 2.259845018386841,
      "learning_rate": 5.8450591574068714e-05,
      "loss": 2.2391,
      "step": 22030
    },
    {
      "epoch": 1.3384344446468694,
      "grad_norm": 1.6425542831420898,
      "learning_rate": 5.84192504433893e-05,
      "loss": 1.9803,
      "step": 22040
    },
    {
      "epoch": 1.3390417198032427,
      "grad_norm": 1.603104829788208,
      "learning_rate": 5.838790590783091e-05,
      "loss": 1.9957,
      "step": 22050
    },
    {
      "epoch": 1.3396489949596162,
      "grad_norm": 2.435748815536499,
      "learning_rate": 5.835655798006977e-05,
      "loss": 2.2574,
      "step": 22060
    },
    {
      "epoch": 1.3402562701159897,
      "grad_norm": 2.820528745651245,
      "learning_rate": 5.83252066727835e-05,
      "loss": 2.7075,
      "step": 22070
    },
    {
      "epoch": 1.340863545272363,
      "grad_norm": 1.618232250213623,
      "learning_rate": 5.829385199865107e-05,
      "loss": 2.2825,
      "step": 22080
    },
    {
      "epoch": 1.3414708204287362,
      "grad_norm": 3.2974791526794434,
      "learning_rate": 5.826249397035281e-05,
      "loss": 2.5318,
      "step": 22090
    },
    {
      "epoch": 1.3420780955851095,
      "grad_norm": 3.173562526702881,
      "learning_rate": 5.823113260057037e-05,
      "loss": 2.4624,
      "step": 22100
    },
    {
      "epoch": 1.342685370741483,
      "grad_norm": 2.4079930782318115,
      "learning_rate": 5.819976790198683e-05,
      "loss": 2.1063,
      "step": 22110
    },
    {
      "epoch": 1.3432926458978562,
      "grad_norm": 2.9350147247314453,
      "learning_rate": 5.816839988728655e-05,
      "loss": 2.3962,
      "step": 22120
    },
    {
      "epoch": 1.3438999210542297,
      "grad_norm": 3.0651192665100098,
      "learning_rate": 5.813702856915527e-05,
      "loss": 2.3009,
      "step": 22130
    },
    {
      "epoch": 1.344507196210603,
      "grad_norm": 3.067342519760132,
      "learning_rate": 5.8105653960280046e-05,
      "loss": 2.2004,
      "step": 22140
    },
    {
      "epoch": 1.3451144713669763,
      "grad_norm": 3.7049973011016846,
      "learning_rate": 5.807427607334927e-05,
      "loss": 2.3861,
      "step": 22150
    },
    {
      "epoch": 1.3457217465233497,
      "grad_norm": 3.197577476501465,
      "learning_rate": 5.8042894921052625e-05,
      "loss": 2.2366,
      "step": 22160
    },
    {
      "epoch": 1.346329021679723,
      "grad_norm": 3.082277774810791,
      "learning_rate": 5.80115105160812e-05,
      "loss": 2.1875,
      "step": 22170
    },
    {
      "epoch": 1.3469362968360965,
      "grad_norm": 3.222543239593506,
      "learning_rate": 5.798012287112734e-05,
      "loss": 2.4047,
      "step": 22180
    },
    {
      "epoch": 1.3475435719924698,
      "grad_norm": 2.2798984050750732,
      "learning_rate": 5.7948731998884685e-05,
      "loss": 2.1525,
      "step": 22190
    },
    {
      "epoch": 1.348150847148843,
      "grad_norm": 2.4649744033813477,
      "learning_rate": 5.791733791204821e-05,
      "loss": 2.3833,
      "step": 22200
    },
    {
      "epoch": 1.3487581223052165,
      "grad_norm": 3.059943914413452,
      "learning_rate": 5.7885940623314196e-05,
      "loss": 2.3856,
      "step": 22210
    },
    {
      "epoch": 1.3493653974615898,
      "grad_norm": 3.5266942977905273,
      "learning_rate": 5.78545401453802e-05,
      "loss": 2.4647,
      "step": 22220
    },
    {
      "epoch": 1.3499726726179633,
      "grad_norm": 3.85202693939209,
      "learning_rate": 5.7823136490945064e-05,
      "loss": 2.3489,
      "step": 22230
    },
    {
      "epoch": 1.3505799477743365,
      "grad_norm": 2.846860885620117,
      "learning_rate": 5.7791729672708935e-05,
      "loss": 2.3349,
      "step": 22240
    },
    {
      "epoch": 1.3511872229307098,
      "grad_norm": 3.3841311931610107,
      "learning_rate": 5.776031970337325e-05,
      "loss": 2.4393,
      "step": 22250
    },
    {
      "epoch": 1.3517944980870833,
      "grad_norm": 3.8901546001434326,
      "learning_rate": 5.7728906595640675e-05,
      "loss": 2.7178,
      "step": 22260
    },
    {
      "epoch": 1.3524017732434566,
      "grad_norm": 4.4373555183410645,
      "learning_rate": 5.769749036221517e-05,
      "loss": 2.8009,
      "step": 22270
    },
    {
      "epoch": 1.35300904839983,
      "grad_norm": 2.1731340885162354,
      "learning_rate": 5.766607101580199e-05,
      "loss": 2.3019,
      "step": 22280
    },
    {
      "epoch": 1.3536163235562033,
      "grad_norm": 4.146178245544434,
      "learning_rate": 5.7634648569107576e-05,
      "loss": 2.426,
      "step": 22290
    },
    {
      "epoch": 1.3542235987125766,
      "grad_norm": 4.959318161010742,
      "learning_rate": 5.7603223034839706e-05,
      "loss": 2.7656,
      "step": 22300
    },
    {
      "epoch": 1.35483087386895,
      "grad_norm": 4.664664268493652,
      "learning_rate": 5.757179442570733e-05,
      "loss": 2.5179,
      "step": 22310
    },
    {
      "epoch": 1.3554381490253233,
      "grad_norm": 2.6774542331695557,
      "learning_rate": 5.754036275442072e-05,
      "loss": 2.059,
      "step": 22320
    },
    {
      "epoch": 1.3560454241816968,
      "grad_norm": 3.0119709968566895,
      "learning_rate": 5.75089280336913e-05,
      "loss": 2.1651,
      "step": 22330
    },
    {
      "epoch": 1.35665269933807,
      "grad_norm": 2.60307240486145,
      "learning_rate": 5.74774902762318e-05,
      "loss": 2.1035,
      "step": 22340
    },
    {
      "epoch": 1.3572599744944434,
      "grad_norm": 2.472572088241577,
      "learning_rate": 5.744604949475615e-05,
      "loss": 2.2234,
      "step": 22350
    },
    {
      "epoch": 1.3578672496508168,
      "grad_norm": 1.6012425422668457,
      "learning_rate": 5.7414605701979495e-05,
      "loss": 1.9744,
      "step": 22360
    },
    {
      "epoch": 1.35847452480719,
      "grad_norm": 2.770740509033203,
      "learning_rate": 5.738315891061819e-05,
      "loss": 2.1792,
      "step": 22370
    },
    {
      "epoch": 1.3590817999635636,
      "grad_norm": 4.902367115020752,
      "learning_rate": 5.7351709133389854e-05,
      "loss": 2.4041,
      "step": 22380
    },
    {
      "epoch": 1.3596890751199369,
      "grad_norm": 5.200078010559082,
      "learning_rate": 5.732025638301325e-05,
      "loss": 2.6205,
      "step": 22390
    },
    {
      "epoch": 1.3602963502763101,
      "grad_norm": 3.1386916637420654,
      "learning_rate": 5.7288800672208375e-05,
      "loss": 2.2236,
      "step": 22400
    },
    {
      "epoch": 1.3609036254326836,
      "grad_norm": 3.1617023944854736,
      "learning_rate": 5.725734201369643e-05,
      "loss": 2.323,
      "step": 22410
    },
    {
      "epoch": 1.3615109005890569,
      "grad_norm": 3.0101711750030518,
      "learning_rate": 5.722588042019979e-05,
      "loss": 2.2163,
      "step": 22420
    },
    {
      "epoch": 1.3621181757454304,
      "grad_norm": 2.1007239818573,
      "learning_rate": 5.7194415904442025e-05,
      "loss": 2.0412,
      "step": 22430
    },
    {
      "epoch": 1.3627254509018036,
      "grad_norm": 3.534935235977173,
      "learning_rate": 5.716294847914788e-05,
      "loss": 2.3113,
      "step": 22440
    },
    {
      "epoch": 1.363332726058177,
      "grad_norm": 2.4972825050354004,
      "learning_rate": 5.7131478157043296e-05,
      "loss": 2.0182,
      "step": 22450
    },
    {
      "epoch": 1.3639400012145504,
      "grad_norm": 3.243548631668091,
      "learning_rate": 5.710000495085537e-05,
      "loss": 2.3381,
      "step": 22460
    },
    {
      "epoch": 1.3645472763709237,
      "grad_norm": 3.2693309783935547,
      "learning_rate": 5.706852887331237e-05,
      "loss": 2.3588,
      "step": 22470
    },
    {
      "epoch": 1.3651545515272971,
      "grad_norm": 2.987229108810425,
      "learning_rate": 5.70370499371437e-05,
      "loss": 1.9797,
      "step": 22480
    },
    {
      "epoch": 1.3657618266836704,
      "grad_norm": 1.167131781578064,
      "learning_rate": 5.7005568155079984e-05,
      "loss": 1.8933,
      "step": 22490
    },
    {
      "epoch": 1.3663691018400437,
      "grad_norm": 2.786234140396118,
      "learning_rate": 5.69740835398529e-05,
      "loss": 1.955,
      "step": 22500
    },
    {
      "epoch": 1.366976376996417,
      "grad_norm": 2.7088491916656494,
      "learning_rate": 5.694259610419539e-05,
      "loss": 2.5087,
      "step": 22510
    },
    {
      "epoch": 1.3675836521527904,
      "grad_norm": 4.756573677062988,
      "learning_rate": 5.691110586084143e-05,
      "loss": 2.5616,
      "step": 22520
    },
    {
      "epoch": 1.368190927309164,
      "grad_norm": 4.066243648529053,
      "learning_rate": 5.687961282252619e-05,
      "loss": 2.4871,
      "step": 22530
    },
    {
      "epoch": 1.3687982024655372,
      "grad_norm": 4.049471855163574,
      "learning_rate": 5.6848117001985955e-05,
      "loss": 2.5266,
      "step": 22540
    },
    {
      "epoch": 1.3694054776219104,
      "grad_norm": 4.4481730461120605,
      "learning_rate": 5.681661841195816e-05,
      "loss": 2.6489,
      "step": 22550
    },
    {
      "epoch": 1.3700127527782837,
      "grad_norm": 3.300901412963867,
      "learning_rate": 5.678511706518129e-05,
      "loss": 2.0452,
      "step": 22560
    },
    {
      "epoch": 1.3706200279346572,
      "grad_norm": 2.0291078090667725,
      "learning_rate": 5.6753612974395035e-05,
      "loss": 1.8453,
      "step": 22570
    },
    {
      "epoch": 1.3712273030910305,
      "grad_norm": 4.100689888000488,
      "learning_rate": 5.6722106152340116e-05,
      "loss": 2.2458,
      "step": 22580
    },
    {
      "epoch": 1.371834578247404,
      "grad_norm": 4.384496212005615,
      "learning_rate": 5.669059661175842e-05,
      "loss": 2.5363,
      "step": 22590
    },
    {
      "epoch": 1.3724418534037772,
      "grad_norm": 3.139131546020508,
      "learning_rate": 5.66590843653929e-05,
      "loss": 2.3688,
      "step": 22600
    },
    {
      "epoch": 1.3730491285601505,
      "grad_norm": 2.6327219009399414,
      "learning_rate": 5.66275694259876e-05,
      "loss": 2.0707,
      "step": 22610
    },
    {
      "epoch": 1.373656403716524,
      "grad_norm": 3.0339412689208984,
      "learning_rate": 5.6596051806287665e-05,
      "loss": 2.1756,
      "step": 22620
    },
    {
      "epoch": 1.3742636788728972,
      "grad_norm": 2.941539764404297,
      "learning_rate": 5.6564531519039356e-05,
      "loss": 2.1243,
      "step": 22630
    },
    {
      "epoch": 1.3748709540292707,
      "grad_norm": 2.997899293899536,
      "learning_rate": 5.6533008576989924e-05,
      "loss": 2.051,
      "step": 22640
    },
    {
      "epoch": 1.375478229185644,
      "grad_norm": 4.35849666595459,
      "learning_rate": 5.650148299288778e-05,
      "loss": 2.3201,
      "step": 22650
    },
    {
      "epoch": 1.3760855043420173,
      "grad_norm": 4.1714677810668945,
      "learning_rate": 5.646995477948238e-05,
      "loss": 2.397,
      "step": 22660
    },
    {
      "epoch": 1.3766927794983908,
      "grad_norm": 5.225345134735107,
      "learning_rate": 5.643842394952422e-05,
      "loss": 2.5288,
      "step": 22670
    },
    {
      "epoch": 1.377300054654764,
      "grad_norm": 5.564054012298584,
      "learning_rate": 5.640689051576486e-05,
      "loss": 2.4501,
      "step": 22680
    },
    {
      "epoch": 1.3779073298111375,
      "grad_norm": 5.427289009094238,
      "learning_rate": 5.637535449095694e-05,
      "loss": 2.6657,
      "step": 22690
    },
    {
      "epoch": 1.3785146049675108,
      "grad_norm": 5.249153137207031,
      "learning_rate": 5.6343815887854144e-05,
      "loss": 2.5132,
      "step": 22700
    },
    {
      "epoch": 1.379121880123884,
      "grad_norm": 3.8081133365631104,
      "learning_rate": 5.631227471921116e-05,
      "loss": 2.207,
      "step": 22710
    },
    {
      "epoch": 1.3797291552802575,
      "grad_norm": 3.008796453475952,
      "learning_rate": 5.628073099778375e-05,
      "loss": 2.2044,
      "step": 22720
    },
    {
      "epoch": 1.3803364304366308,
      "grad_norm": 3.2154979705810547,
      "learning_rate": 5.624918473632869e-05,
      "loss": 2.0421,
      "step": 22730
    },
    {
      "epoch": 1.3809437055930043,
      "grad_norm": 3.766472578048706,
      "learning_rate": 5.6217635947603796e-05,
      "loss": 2.2218,
      "step": 22740
    },
    {
      "epoch": 1.3815509807493775,
      "grad_norm": 3.916962146759033,
      "learning_rate": 5.61860846443679e-05,
      "loss": 2.4141,
      "step": 22750
    },
    {
      "epoch": 1.3821582559057508,
      "grad_norm": 3.503105401992798,
      "learning_rate": 5.615453083938086e-05,
      "loss": 2.3762,
      "step": 22760
    },
    {
      "epoch": 1.3827655310621243,
      "grad_norm": 2.9742259979248047,
      "learning_rate": 5.612297454540352e-05,
      "loss": 2.391,
      "step": 22770
    },
    {
      "epoch": 1.3833728062184976,
      "grad_norm": 4.634607791900635,
      "learning_rate": 5.609141577519777e-05,
      "loss": 2.7546,
      "step": 22780
    },
    {
      "epoch": 1.383980081374871,
      "grad_norm": 3.0566000938415527,
      "learning_rate": 5.6059854541526435e-05,
      "loss": 2.4472,
      "step": 22790
    },
    {
      "epoch": 1.3845873565312443,
      "grad_norm": 3.6333494186401367,
      "learning_rate": 5.602829085715345e-05,
      "loss": 2.4768,
      "step": 22800
    },
    {
      "epoch": 1.3851946316876176,
      "grad_norm": 3.8535330295562744,
      "learning_rate": 5.599672473484361e-05,
      "loss": 2.438,
      "step": 22810
    },
    {
      "epoch": 1.385801906843991,
      "grad_norm": 5.228507041931152,
      "learning_rate": 5.596515618736279e-05,
      "loss": 2.4586,
      "step": 22820
    },
    {
      "epoch": 1.3864091820003643,
      "grad_norm": 1.9756308794021606,
      "learning_rate": 5.59335852274778e-05,
      "loss": 2.0052,
      "step": 22830
    },
    {
      "epoch": 1.3870164571567378,
      "grad_norm": 3.6773622035980225,
      "learning_rate": 5.590201186795645e-05,
      "loss": 2.3755,
      "step": 22840
    },
    {
      "epoch": 1.387623732313111,
      "grad_norm": 4.633213996887207,
      "learning_rate": 5.5870436121567495e-05,
      "loss": 2.5495,
      "step": 22850
    },
    {
      "epoch": 1.3882310074694844,
      "grad_norm": 2.650087356567383,
      "learning_rate": 5.5838858001080675e-05,
      "loss": 2.2898,
      "step": 22860
    },
    {
      "epoch": 1.3888382826258578,
      "grad_norm": 3.0097007751464844,
      "learning_rate": 5.58072775192667e-05,
      "loss": 2.1601,
      "step": 22870
    },
    {
      "epoch": 1.389445557782231,
      "grad_norm": 4.003410339355469,
      "learning_rate": 5.5775694688897194e-05,
      "loss": 2.3172,
      "step": 22880
    },
    {
      "epoch": 1.3900528329386046,
      "grad_norm": 3.9928176403045654,
      "learning_rate": 5.574410952274477e-05,
      "loss": 2.6252,
      "step": 22890
    },
    {
      "epoch": 1.3906601080949779,
      "grad_norm": 3.7809016704559326,
      "learning_rate": 5.571252203358298e-05,
      "loss": 2.5911,
      "step": 22900
    },
    {
      "epoch": 1.3912673832513511,
      "grad_norm": 6.161967754364014,
      "learning_rate": 5.56809322341863e-05,
      "loss": 2.3676,
      "step": 22910
    },
    {
      "epoch": 1.3918746584077246,
      "grad_norm": 3.634843349456787,
      "learning_rate": 5.564934013733014e-05,
      "loss": 2.1801,
      "step": 22920
    },
    {
      "epoch": 1.3924819335640979,
      "grad_norm": 2.0011699199676514,
      "learning_rate": 5.561774575579086e-05,
      "loss": 2.1113,
      "step": 22930
    },
    {
      "epoch": 1.3930892087204714,
      "grad_norm": 2.961019992828369,
      "learning_rate": 5.558614910234573e-05,
      "loss": 2.1646,
      "step": 22940
    },
    {
      "epoch": 1.3936964838768446,
      "grad_norm": 3.175379991531372,
      "learning_rate": 5.5554550189772936e-05,
      "loss": 2.3461,
      "step": 22950
    },
    {
      "epoch": 1.394303759033218,
      "grad_norm": 2.585653781890869,
      "learning_rate": 5.5522949030851566e-05,
      "loss": 2.1612,
      "step": 22960
    },
    {
      "epoch": 1.3949110341895912,
      "grad_norm": 1.3201640844345093,
      "learning_rate": 5.549134563836167e-05,
      "loss": 2.1213,
      "step": 22970
    },
    {
      "epoch": 1.3955183093459647,
      "grad_norm": 2.028712034225464,
      "learning_rate": 5.545974002508412e-05,
      "loss": 2.2446,
      "step": 22980
    },
    {
      "epoch": 1.3961255845023381,
      "grad_norm": 1.4722450971603394,
      "learning_rate": 5.542813220380076e-05,
      "loss": 2.1715,
      "step": 22990
    },
    {
      "epoch": 1.3967328596587114,
      "grad_norm": 1.6093052625656128,
      "learning_rate": 5.539652218729429e-05,
      "loss": 1.9685,
      "step": 23000
    },
    {
      "epoch": 1.3973401348150847,
      "grad_norm": 1.8005714416503906,
      "learning_rate": 5.53649099883483e-05,
      "loss": 2.005,
      "step": 23010
    },
    {
      "epoch": 1.397947409971458,
      "grad_norm": 2.213197708129883,
      "learning_rate": 5.533329561974725e-05,
      "loss": 2.3081,
      "step": 23020
    },
    {
      "epoch": 1.3985546851278314,
      "grad_norm": 2.5714643001556396,
      "learning_rate": 5.5301679094276524e-05,
      "loss": 2.5309,
      "step": 23030
    },
    {
      "epoch": 1.3991619602842047,
      "grad_norm": 2.3512673377990723,
      "learning_rate": 5.527006042472234e-05,
      "loss": 2.1347,
      "step": 23040
    },
    {
      "epoch": 1.3997692354405782,
      "grad_norm": 1.8336467742919922,
      "learning_rate": 5.5238439623871786e-05,
      "loss": 2.1041,
      "step": 23050
    },
    {
      "epoch": 1.4003765105969515,
      "grad_norm": 2.8192005157470703,
      "learning_rate": 5.52068167045128e-05,
      "loss": 2.3498,
      "step": 23060
    },
    {
      "epoch": 1.4009837857533247,
      "grad_norm": 2.829153299331665,
      "learning_rate": 5.517519167943423e-05,
      "loss": 2.4425,
      "step": 23070
    },
    {
      "epoch": 1.4015910609096982,
      "grad_norm": 4.24854850769043,
      "learning_rate": 5.514356456142572e-05,
      "loss": 2.5874,
      "step": 23080
    },
    {
      "epoch": 1.4021983360660715,
      "grad_norm": 3.533386468887329,
      "learning_rate": 5.511193536327779e-05,
      "loss": 2.3139,
      "step": 23090
    },
    {
      "epoch": 1.402805611222445,
      "grad_norm": 3.213651657104492,
      "learning_rate": 5.508030409778177e-05,
      "loss": 2.2104,
      "step": 23100
    },
    {
      "epoch": 1.4034128863788182,
      "grad_norm": 3.8061575889587402,
      "learning_rate": 5.504867077772988e-05,
      "loss": 2.2537,
      "step": 23110
    },
    {
      "epoch": 1.4040201615351915,
      "grad_norm": 4.207998275756836,
      "learning_rate": 5.50170354159151e-05,
      "loss": 2.1523,
      "step": 23120
    },
    {
      "epoch": 1.404627436691565,
      "grad_norm": 2.545511245727539,
      "learning_rate": 5.498539802513131e-05,
      "loss": 2.3825,
      "step": 23130
    },
    {
      "epoch": 1.4052347118479382,
      "grad_norm": 3.090460777282715,
      "learning_rate": 5.4953758618173157e-05,
      "loss": 2.3559,
      "step": 23140
    },
    {
      "epoch": 1.4058419870043117,
      "grad_norm": 4.374146461486816,
      "learning_rate": 5.492211720783611e-05,
      "loss": 2.4738,
      "step": 23150
    },
    {
      "epoch": 1.406449262160685,
      "grad_norm": 3.1456823348999023,
      "learning_rate": 5.489047380691649e-05,
      "loss": 2.213,
      "step": 23160
    },
    {
      "epoch": 1.4070565373170583,
      "grad_norm": 2.5038788318634033,
      "learning_rate": 5.485882842821136e-05,
      "loss": 2.2693,
      "step": 23170
    },
    {
      "epoch": 1.4076638124734318,
      "grad_norm": 3.2882022857666016,
      "learning_rate": 5.482718108451864e-05,
      "loss": 2.4439,
      "step": 23180
    },
    {
      "epoch": 1.408271087629805,
      "grad_norm": 3.5708651542663574,
      "learning_rate": 5.4795531788637e-05,
      "loss": 2.3965,
      "step": 23190
    },
    {
      "epoch": 1.4088783627861785,
      "grad_norm": 3.8320183753967285,
      "learning_rate": 5.4763880553365945e-05,
      "loss": 2.3193,
      "step": 23200
    },
    {
      "epoch": 1.4094856379425518,
      "grad_norm": 2.804100751876831,
      "learning_rate": 5.473222739150571e-05,
      "loss": 2.1832,
      "step": 23210
    },
    {
      "epoch": 1.410092913098925,
      "grad_norm": 2.4851105213165283,
      "learning_rate": 5.470057231585736e-05,
      "loss": 2.1296,
      "step": 23220
    },
    {
      "epoch": 1.4107001882552985,
      "grad_norm": 3.250296115875244,
      "learning_rate": 5.4668915339222684e-05,
      "loss": 2.4717,
      "step": 23230
    },
    {
      "epoch": 1.4113074634116718,
      "grad_norm": 4.0260539054870605,
      "learning_rate": 5.4637256474404295e-05,
      "loss": 2.3535,
      "step": 23240
    },
    {
      "epoch": 1.4119147385680453,
      "grad_norm": 2.909729480743408,
      "learning_rate": 5.460559573420554e-05,
      "loss": 2.3597,
      "step": 23250
    },
    {
      "epoch": 1.4125220137244185,
      "grad_norm": 2.8652172088623047,
      "learning_rate": 5.4573933131430524e-05,
      "loss": 2.3154,
      "step": 23260
    },
    {
      "epoch": 1.4131292888807918,
      "grad_norm": 3.3744618892669678,
      "learning_rate": 5.454226867888408e-05,
      "loss": 2.4903,
      "step": 23270
    },
    {
      "epoch": 1.4137365640371653,
      "grad_norm": 3.9472386837005615,
      "learning_rate": 5.451060238937187e-05,
      "loss": 2.345,
      "step": 23280
    },
    {
      "epoch": 1.4143438391935386,
      "grad_norm": 3.248817205429077,
      "learning_rate": 5.447893427570019e-05,
      "loss": 2.4283,
      "step": 23290
    },
    {
      "epoch": 1.414951114349912,
      "grad_norm": 4.945276737213135,
      "learning_rate": 5.444726435067618e-05,
      "loss": 2.3108,
      "step": 23300
    },
    {
      "epoch": 1.4155583895062853,
      "grad_norm": 3.2565150260925293,
      "learning_rate": 5.4415592627107625e-05,
      "loss": 2.2678,
      "step": 23310
    },
    {
      "epoch": 1.4161656646626586,
      "grad_norm": 3.598906993865967,
      "learning_rate": 5.4383919117803084e-05,
      "loss": 2.496,
      "step": 23320
    },
    {
      "epoch": 1.416772939819032,
      "grad_norm": 3.2228169441223145,
      "learning_rate": 5.435224383557183e-05,
      "loss": 2.6288,
      "step": 23330
    },
    {
      "epoch": 1.4173802149754053,
      "grad_norm": 3.0945565700531006,
      "learning_rate": 5.432056679322386e-05,
      "loss": 2.4971,
      "step": 23340
    },
    {
      "epoch": 1.4179874901317788,
      "grad_norm": 3.942640781402588,
      "learning_rate": 5.428888800356986e-05,
      "loss": 2.1649,
      "step": 23350
    },
    {
      "epoch": 1.418594765288152,
      "grad_norm": 3.8771657943725586,
      "learning_rate": 5.425720747942126e-05,
      "loss": 2.4583,
      "step": 23360
    },
    {
      "epoch": 1.4192020404445254,
      "grad_norm": 2.4737401008605957,
      "learning_rate": 5.422552523359014e-05,
      "loss": 2.1235,
      "step": 23370
    },
    {
      "epoch": 1.4198093156008988,
      "grad_norm": 4.497203350067139,
      "learning_rate": 5.419384127888931e-05,
      "loss": 2.2021,
      "step": 23380
    },
    {
      "epoch": 1.4204165907572721,
      "grad_norm": 2.743349552154541,
      "learning_rate": 5.4162155628132296e-05,
      "loss": 2.0435,
      "step": 23390
    },
    {
      "epoch": 1.4210238659136456,
      "grad_norm": 4.137497901916504,
      "learning_rate": 5.413046829413324e-05,
      "loss": 2.4165,
      "step": 23400
    },
    {
      "epoch": 1.4216311410700189,
      "grad_norm": 3.769988536834717,
      "learning_rate": 5.409877928970703e-05,
      "loss": 2.7103,
      "step": 23410
    },
    {
      "epoch": 1.4222384162263921,
      "grad_norm": 2.9337234497070312,
      "learning_rate": 5.406708862766919e-05,
      "loss": 2.5319,
      "step": 23420
    },
    {
      "epoch": 1.4228456913827654,
      "grad_norm": 1.591366171836853,
      "learning_rate": 5.403539632083595e-05,
      "loss": 2.0736,
      "step": 23430
    },
    {
      "epoch": 1.4234529665391389,
      "grad_norm": 3.760835886001587,
      "learning_rate": 5.4003702382024156e-05,
      "loss": 2.144,
      "step": 23440
    },
    {
      "epoch": 1.4240602416955124,
      "grad_norm": 4.859288215637207,
      "learning_rate": 5.397200682405137e-05,
      "loss": 2.6493,
      "step": 23450
    },
    {
      "epoch": 1.4246675168518856,
      "grad_norm": 2.5209460258483887,
      "learning_rate": 5.394030965973574e-05,
      "loss": 1.9981,
      "step": 23460
    },
    {
      "epoch": 1.425274792008259,
      "grad_norm": 3.263157606124878,
      "learning_rate": 5.3908610901896154e-05,
      "loss": 2.1022,
      "step": 23470
    },
    {
      "epoch": 1.4258820671646322,
      "grad_norm": 2.9718475341796875,
      "learning_rate": 5.3876910563352045e-05,
      "loss": 2.1781,
      "step": 23480
    },
    {
      "epoch": 1.4264893423210057,
      "grad_norm": 3.223691463470459,
      "learning_rate": 5.384520865692357e-05,
      "loss": 2.2003,
      "step": 23490
    },
    {
      "epoch": 1.427096617477379,
      "grad_norm": 2.665102481842041,
      "learning_rate": 5.3813505195431476e-05,
      "loss": 2.0331,
      "step": 23500
    },
    {
      "epoch": 1.4277038926337524,
      "grad_norm": 2.704996109008789,
      "learning_rate": 5.378180019169714e-05,
      "loss": 2.1369,
      "step": 23510
    },
    {
      "epoch": 1.4283111677901257,
      "grad_norm": 3.693089246749878,
      "learning_rate": 5.375009365854259e-05,
      "loss": 2.8797,
      "step": 23520
    },
    {
      "epoch": 1.428918442946499,
      "grad_norm": 4.736975193023682,
      "learning_rate": 5.371838560879043e-05,
      "loss": 2.5894,
      "step": 23530
    },
    {
      "epoch": 1.4295257181028724,
      "grad_norm": 4.028130531311035,
      "learning_rate": 5.3686676055263895e-05,
      "loss": 2.4104,
      "step": 23540
    },
    {
      "epoch": 1.4301329932592457,
      "grad_norm": 2.1023294925689697,
      "learning_rate": 5.365496501078686e-05,
      "loss": 2.4542,
      "step": 23550
    },
    {
      "epoch": 1.4307402684156192,
      "grad_norm": 3.4731991291046143,
      "learning_rate": 5.3623252488183774e-05,
      "loss": 2.1678,
      "step": 23560
    },
    {
      "epoch": 1.4313475435719925,
      "grad_norm": 1.9503980875015259,
      "learning_rate": 5.359153850027967e-05,
      "loss": 1.8252,
      "step": 23570
    },
    {
      "epoch": 1.4319548187283657,
      "grad_norm": 3.1944639682769775,
      "learning_rate": 5.355982305990018e-05,
      "loss": 2.1205,
      "step": 23580
    },
    {
      "epoch": 1.4325620938847392,
      "grad_norm": 3.925532102584839,
      "learning_rate": 5.352810617987158e-05,
      "loss": 2.2507,
      "step": 23590
    },
    {
      "epoch": 1.4331693690411125,
      "grad_norm": 3.322997808456421,
      "learning_rate": 5.3496387873020626e-05,
      "loss": 2.3407,
      "step": 23600
    },
    {
      "epoch": 1.433776644197486,
      "grad_norm": 3.5070111751556396,
      "learning_rate": 5.346466815217472e-05,
      "loss": 2.2899,
      "step": 23610
    },
    {
      "epoch": 1.4343839193538592,
      "grad_norm": 2.3415913581848145,
      "learning_rate": 5.3432947030161864e-05,
      "loss": 2.1622,
      "step": 23620
    },
    {
      "epoch": 1.4349911945102325,
      "grad_norm": 3.8666341304779053,
      "learning_rate": 5.3401224519810545e-05,
      "loss": 2.2435,
      "step": 23630
    },
    {
      "epoch": 1.435598469666606,
      "grad_norm": 3.204643964767456,
      "learning_rate": 5.336950063394987e-05,
      "loss": 2.1823,
      "step": 23640
    },
    {
      "epoch": 1.4362057448229792,
      "grad_norm": 3.2856478691101074,
      "learning_rate": 5.333777538540945e-05,
      "loss": 2.1682,
      "step": 23650
    },
    {
      "epoch": 1.4368130199793527,
      "grad_norm": 2.899998664855957,
      "learning_rate": 5.330604878701954e-05,
      "loss": 2.3676,
      "step": 23660
    },
    {
      "epoch": 1.437420295135726,
      "grad_norm": 2.9116554260253906,
      "learning_rate": 5.32743208516108e-05,
      "loss": 2.0231,
      "step": 23670
    },
    {
      "epoch": 1.4380275702920993,
      "grad_norm": 3.5163543224334717,
      "learning_rate": 5.32425915920146e-05,
      "loss": 2.4454,
      "step": 23680
    },
    {
      "epoch": 1.4386348454484728,
      "grad_norm": 2.848759174346924,
      "learning_rate": 5.3210861021062696e-05,
      "loss": 2.3232,
      "step": 23690
    },
    {
      "epoch": 1.439242120604846,
      "grad_norm": 2.6630189418792725,
      "learning_rate": 5.317912915158747e-05,
      "loss": 2.3426,
      "step": 23700
    },
    {
      "epoch": 1.4398493957612195,
      "grad_norm": 3.535207986831665,
      "learning_rate": 5.314739599642177e-05,
      "loss": 2.4951,
      "step": 23710
    },
    {
      "epoch": 1.4404566709175928,
      "grad_norm": 3.7419302463531494,
      "learning_rate": 5.311566156839901e-05,
      "loss": 2.2393,
      "step": 23720
    },
    {
      "epoch": 1.441063946073966,
      "grad_norm": 4.187992572784424,
      "learning_rate": 5.30839258803531e-05,
      "loss": 2.3162,
      "step": 23730
    },
    {
      "epoch": 1.4416712212303395,
      "grad_norm": 3.8007662296295166,
      "learning_rate": 5.305218894511844e-05,
      "loss": 2.3944,
      "step": 23740
    },
    {
      "epoch": 1.4422784963867128,
      "grad_norm": 2.9628100395202637,
      "learning_rate": 5.302045077552995e-05,
      "loss": 2.3202,
      "step": 23750
    },
    {
      "epoch": 1.4428857715430863,
      "grad_norm": 3.0144898891448975,
      "learning_rate": 5.298871138442307e-05,
      "loss": 2.3199,
      "step": 23760
    },
    {
      "epoch": 1.4434930466994595,
      "grad_norm": 4.0958051681518555,
      "learning_rate": 5.295697078463371e-05,
      "loss": 2.2266,
      "step": 23770
    },
    {
      "epoch": 1.4441003218558328,
      "grad_norm": 5.839349746704102,
      "learning_rate": 5.2925228988998274e-05,
      "loss": 2.4581,
      "step": 23780
    },
    {
      "epoch": 1.4447075970122063,
      "grad_norm": 4.201521873474121,
      "learning_rate": 5.2893486010353635e-05,
      "loss": 2.5813,
      "step": 23790
    },
    {
      "epoch": 1.4453148721685796,
      "grad_norm": 3.1724300384521484,
      "learning_rate": 5.286174186153718e-05,
      "loss": 2.4944,
      "step": 23800
    },
    {
      "epoch": 1.445922147324953,
      "grad_norm": 3.492673397064209,
      "learning_rate": 5.282999655538673e-05,
      "loss": 2.3346,
      "step": 23810
    },
    {
      "epoch": 1.4465294224813263,
      "grad_norm": 3.0091419219970703,
      "learning_rate": 5.279825010474061e-05,
      "loss": 2.3919,
      "step": 23820
    },
    {
      "epoch": 1.4471366976376996,
      "grad_norm": 2.7945432662963867,
      "learning_rate": 5.276650252243758e-05,
      "loss": 2.3092,
      "step": 23830
    },
    {
      "epoch": 1.447743972794073,
      "grad_norm": 3.6656150817871094,
      "learning_rate": 5.2734753821316864e-05,
      "loss": 2.5402,
      "step": 23840
    },
    {
      "epoch": 1.4483512479504463,
      "grad_norm": 2.873405933380127,
      "learning_rate": 5.270300401421815e-05,
      "loss": 2.3016,
      "step": 23850
    },
    {
      "epoch": 1.4489585231068198,
      "grad_norm": 3.4249565601348877,
      "learning_rate": 5.267125311398157e-05,
      "loss": 2.3602,
      "step": 23860
    },
    {
      "epoch": 1.449565798263193,
      "grad_norm": 3.671112298965454,
      "learning_rate": 5.2639501133447675e-05,
      "loss": 2.0734,
      "step": 23870
    },
    {
      "epoch": 1.4501730734195664,
      "grad_norm": 4.096269607543945,
      "learning_rate": 5.2607748085457485e-05,
      "loss": 2.3887,
      "step": 23880
    },
    {
      "epoch": 1.4507803485759396,
      "grad_norm": 3.4064278602600098,
      "learning_rate": 5.2575993982852444e-05,
      "loss": 2.5075,
      "step": 23890
    },
    {
      "epoch": 1.4513876237323131,
      "grad_norm": 2.991823196411133,
      "learning_rate": 5.254423883847441e-05,
      "loss": 2.28,
      "step": 23900
    },
    {
      "epoch": 1.4519948988886866,
      "grad_norm": 4.440502643585205,
      "learning_rate": 5.2512482665165676e-05,
      "loss": 2.6304,
      "step": 23910
    },
    {
      "epoch": 1.4526021740450599,
      "grad_norm": 4.867480754852295,
      "learning_rate": 5.248072547576892e-05,
      "loss": 2.3617,
      "step": 23920
    },
    {
      "epoch": 1.4532094492014331,
      "grad_norm": 2.511622667312622,
      "learning_rate": 5.244896728312728e-05,
      "loss": 2.3882,
      "step": 23930
    },
    {
      "epoch": 1.4538167243578064,
      "grad_norm": 2.7315211296081543,
      "learning_rate": 5.241720810008428e-05,
      "loss": 2.3367,
      "step": 23940
    },
    {
      "epoch": 1.4544239995141799,
      "grad_norm": 4.903066158294678,
      "learning_rate": 5.238544793948381e-05,
      "loss": 2.4162,
      "step": 23950
    },
    {
      "epoch": 1.4550312746705532,
      "grad_norm": 4.381254196166992,
      "learning_rate": 5.235368681417021e-05,
      "loss": 2.4625,
      "step": 23960
    },
    {
      "epoch": 1.4556385498269266,
      "grad_norm": 1.5554792881011963,
      "learning_rate": 5.232192473698818e-05,
      "loss": 2.0992,
      "step": 23970
    },
    {
      "epoch": 1.4562458249833,
      "grad_norm": 2.546722173690796,
      "learning_rate": 5.229016172078279e-05,
      "loss": 2.16,
      "step": 23980
    },
    {
      "epoch": 1.4568531001396732,
      "grad_norm": 5.589058876037598,
      "learning_rate": 5.225839777839955e-05,
      "loss": 2.4866,
      "step": 23990
    },
    {
      "epoch": 1.4574603752960467,
      "grad_norm": 2.2472877502441406,
      "learning_rate": 5.222663292268427e-05,
      "loss": 2.2643,
      "step": 24000
    },
    {
      "epoch": 1.45806765045242,
      "grad_norm": 3.155989170074463,
      "learning_rate": 5.2194867166483174e-05,
      "loss": 2.4007,
      "step": 24010
    },
    {
      "epoch": 1.4586749256087934,
      "grad_norm": 3.8943655490875244,
      "learning_rate": 5.2163100522642824e-05,
      "loss": 2.4192,
      "step": 24020
    },
    {
      "epoch": 1.4592822007651667,
      "grad_norm": 4.16610860824585,
      "learning_rate": 5.213133300401018e-05,
      "loss": 2.4234,
      "step": 24030
    },
    {
      "epoch": 1.45988947592154,
      "grad_norm": 3.2061538696289062,
      "learning_rate": 5.209956462343254e-05,
      "loss": 2.5069,
      "step": 24040
    },
    {
      "epoch": 1.4604967510779134,
      "grad_norm": 2.3357365131378174,
      "learning_rate": 5.2067795393757526e-05,
      "loss": 2.0335,
      "step": 24050
    },
    {
      "epoch": 1.4611040262342867,
      "grad_norm": 3.548426866531372,
      "learning_rate": 5.203602532783311e-05,
      "loss": 2.0529,
      "step": 24060
    },
    {
      "epoch": 1.4617113013906602,
      "grad_norm": 2.4709293842315674,
      "learning_rate": 5.200425443850763e-05,
      "loss": 2.1436,
      "step": 24070
    },
    {
      "epoch": 1.4623185765470335,
      "grad_norm": 2.9410042762756348,
      "learning_rate": 5.1972482738629727e-05,
      "loss": 2.0811,
      "step": 24080
    },
    {
      "epoch": 1.4629258517034067,
      "grad_norm": 2.8888657093048096,
      "learning_rate": 5.194071024104838e-05,
      "loss": 2.3572,
      "step": 24090
    },
    {
      "epoch": 1.4635331268597802,
      "grad_norm": 3.938457727432251,
      "learning_rate": 5.190893695861293e-05,
      "loss": 2.5264,
      "step": 24100
    },
    {
      "epoch": 1.4641404020161535,
      "grad_norm": 3.780322790145874,
      "learning_rate": 5.187716290417296e-05,
      "loss": 2.4182,
      "step": 24110
    },
    {
      "epoch": 1.464747677172527,
      "grad_norm": 2.5900535583496094,
      "learning_rate": 5.1845388090578414e-05,
      "loss": 2.1418,
      "step": 24120
    },
    {
      "epoch": 1.4653549523289002,
      "grad_norm": 3.268035411834717,
      "learning_rate": 5.181361253067953e-05,
      "loss": 2.2519,
      "step": 24130
    },
    {
      "epoch": 1.4659622274852735,
      "grad_norm": 3.2233705520629883,
      "learning_rate": 5.178183623732687e-05,
      "loss": 2.2538,
      "step": 24140
    },
    {
      "epoch": 1.466569502641647,
      "grad_norm": 2.3499021530151367,
      "learning_rate": 5.175005922337125e-05,
      "loss": 2.4041,
      "step": 24150
    },
    {
      "epoch": 1.4671767777980202,
      "grad_norm": 3.183408498764038,
      "learning_rate": 5.17182815016638e-05,
      "loss": 2.2876,
      "step": 24160
    },
    {
      "epoch": 1.4677840529543937,
      "grad_norm": 3.3242814540863037,
      "learning_rate": 5.168650308505596e-05,
      "loss": 2.3143,
      "step": 24170
    },
    {
      "epoch": 1.468391328110767,
      "grad_norm": 2.1962807178497314,
      "learning_rate": 5.16547239863994e-05,
      "loss": 2.1299,
      "step": 24180
    },
    {
      "epoch": 1.4689986032671403,
      "grad_norm": 2.5734262466430664,
      "learning_rate": 5.16229442185461e-05,
      "loss": 2.2057,
      "step": 24190
    },
    {
      "epoch": 1.4696058784235138,
      "grad_norm": 2.7334518432617188,
      "learning_rate": 5.159116379434833e-05,
      "loss": 2.2665,
      "step": 24200
    },
    {
      "epoch": 1.470213153579887,
      "grad_norm": 4.3080315589904785,
      "learning_rate": 5.1559382726658565e-05,
      "loss": 2.4461,
      "step": 24210
    },
    {
      "epoch": 1.4708204287362605,
      "grad_norm": 3.4066948890686035,
      "learning_rate": 5.15276010283296e-05,
      "loss": 2.5711,
      "step": 24220
    },
    {
      "epoch": 1.4714277038926338,
      "grad_norm": 1.7637747526168823,
      "learning_rate": 5.1495818712214425e-05,
      "loss": 2.1493,
      "step": 24230
    },
    {
      "epoch": 1.472034979049007,
      "grad_norm": 3.13073468208313,
      "learning_rate": 5.1464035791166364e-05,
      "loss": 2.4364,
      "step": 24240
    },
    {
      "epoch": 1.4726422542053805,
      "grad_norm": 3.4898436069488525,
      "learning_rate": 5.143225227803892e-05,
      "loss": 2.3391,
      "step": 24250
    },
    {
      "epoch": 1.4732495293617538,
      "grad_norm": 2.766230583190918,
      "learning_rate": 5.140046818568583e-05,
      "loss": 2.3659,
      "step": 24260
    },
    {
      "epoch": 1.4738568045181273,
      "grad_norm": 4.018533706665039,
      "learning_rate": 5.136868352696109e-05,
      "loss": 2.2992,
      "step": 24270
    },
    {
      "epoch": 1.4744640796745005,
      "grad_norm": 3.46826434135437,
      "learning_rate": 5.133689831471896e-05,
      "loss": 2.4166,
      "step": 24280
    },
    {
      "epoch": 1.4750713548308738,
      "grad_norm": 2.4011824131011963,
      "learning_rate": 5.130511256181384e-05,
      "loss": 2.3447,
      "step": 24290
    },
    {
      "epoch": 1.4756786299872473,
      "grad_norm": 3.970773458480835,
      "learning_rate": 5.1273326281100406e-05,
      "loss": 2.2531,
      "step": 24300
    },
    {
      "epoch": 1.4762859051436206,
      "grad_norm": 2.2447025775909424,
      "learning_rate": 5.124153948543357e-05,
      "loss": 2.4308,
      "step": 24310
    },
    {
      "epoch": 1.476893180299994,
      "grad_norm": 3.2296788692474365,
      "learning_rate": 5.1209752187668384e-05,
      "loss": 2.5197,
      "step": 24320
    },
    {
      "epoch": 1.4775004554563673,
      "grad_norm": 3.9394302368164062,
      "learning_rate": 5.117796440066014e-05,
      "loss": 2.4656,
      "step": 24330
    },
    {
      "epoch": 1.4781077306127406,
      "grad_norm": 4.347395896911621,
      "learning_rate": 5.1146176137264335e-05,
      "loss": 2.7826,
      "step": 24340
    },
    {
      "epoch": 1.4787150057691139,
      "grad_norm": 4.076003551483154,
      "learning_rate": 5.111438741033665e-05,
      "loss": 2.7595,
      "step": 24350
    },
    {
      "epoch": 1.4793222809254873,
      "grad_norm": 6.358520984649658,
      "learning_rate": 5.108259823273294e-05,
      "loss": 2.6954,
      "step": 24360
    },
    {
      "epoch": 1.4799295560818606,
      "grad_norm": 5.426828384399414,
      "learning_rate": 5.105080861730927e-05,
      "loss": 2.5015,
      "step": 24370
    },
    {
      "epoch": 1.480536831238234,
      "grad_norm": 5.2795257568359375,
      "learning_rate": 5.101901857692185e-05,
      "loss": 2.4382,
      "step": 24380
    },
    {
      "epoch": 1.4811441063946074,
      "grad_norm": 3.35461688041687,
      "learning_rate": 5.09872281244271e-05,
      "loss": 2.0868,
      "step": 24390
    },
    {
      "epoch": 1.4817513815509806,
      "grad_norm": 3.2019810676574707,
      "learning_rate": 5.095543727268156e-05,
      "loss": 2.0149,
      "step": 24400
    },
    {
      "epoch": 1.4823586567073541,
      "grad_norm": 5.761817932128906,
      "learning_rate": 5.092364603454197e-05,
      "loss": 2.2173,
      "step": 24410
    },
    {
      "epoch": 1.4829659318637274,
      "grad_norm": 1.808186650276184,
      "learning_rate": 5.089185442286523e-05,
      "loss": 2.0827,
      "step": 24420
    },
    {
      "epoch": 1.4835732070201009,
      "grad_norm": 4.491277694702148,
      "learning_rate": 5.086006245050835e-05,
      "loss": 2.2803,
      "step": 24430
    },
    {
      "epoch": 1.4841804821764741,
      "grad_norm": 3.793854236602783,
      "learning_rate": 5.08282701303285e-05,
      "loss": 2.4954,
      "step": 24440
    },
    {
      "epoch": 1.4847877573328474,
      "grad_norm": 3.477466106414795,
      "learning_rate": 5.0796477475183055e-05,
      "loss": 2.2461,
      "step": 24450
    },
    {
      "epoch": 1.485395032489221,
      "grad_norm": 2.9933533668518066,
      "learning_rate": 5.0764684497929404e-05,
      "loss": 2.1023,
      "step": 24460
    },
    {
      "epoch": 1.4860023076455942,
      "grad_norm": 3.4751675128936768,
      "learning_rate": 5.073289121142517e-05,
      "loss": 2.3127,
      "step": 24470
    },
    {
      "epoch": 1.4866095828019676,
      "grad_norm": 3.534052610397339,
      "learning_rate": 5.070109762852807e-05,
      "loss": 2.3572,
      "step": 24480
    },
    {
      "epoch": 1.487216857958341,
      "grad_norm": 2.957505702972412,
      "learning_rate": 5.066930376209591e-05,
      "loss": 1.9202,
      "step": 24490
    },
    {
      "epoch": 1.4878241331147142,
      "grad_norm": 3.599759817123413,
      "learning_rate": 5.0637509624986644e-05,
      "loss": 2.2241,
      "step": 24500
    },
    {
      "epoch": 1.4884314082710877,
      "grad_norm": 4.38781213760376,
      "learning_rate": 5.0605715230058346e-05,
      "loss": 2.5373,
      "step": 24510
    },
    {
      "epoch": 1.489038683427461,
      "grad_norm": 5.022547721862793,
      "learning_rate": 5.057392059016916e-05,
      "loss": 2.61,
      "step": 24520
    },
    {
      "epoch": 1.4896459585838344,
      "grad_norm": 2.9379634857177734,
      "learning_rate": 5.0542125718177335e-05,
      "loss": 2.3551,
      "step": 24530
    },
    {
      "epoch": 1.4902532337402077,
      "grad_norm": 2.001614809036255,
      "learning_rate": 5.0510330626941247e-05,
      "loss": 2.1226,
      "step": 24540
    },
    {
      "epoch": 1.490860508896581,
      "grad_norm": 3.243945360183716,
      "learning_rate": 5.047853532931932e-05,
      "loss": 2.2858,
      "step": 24550
    },
    {
      "epoch": 1.4914677840529544,
      "grad_norm": 3.6389009952545166,
      "learning_rate": 5.044673983817009e-05,
      "loss": 2.6296,
      "step": 24560
    },
    {
      "epoch": 1.4920750592093277,
      "grad_norm": 3.9672296047210693,
      "learning_rate": 5.041494416635214e-05,
      "loss": 2.6164,
      "step": 24570
    },
    {
      "epoch": 1.4926823343657012,
      "grad_norm": 2.235898971557617,
      "learning_rate": 5.038314832672418e-05,
      "loss": 2.0036,
      "step": 24580
    },
    {
      "epoch": 1.4932896095220745,
      "grad_norm": 6.780132293701172,
      "learning_rate": 5.035135233214493e-05,
      "loss": 2.4129,
      "step": 24590
    },
    {
      "epoch": 1.4938968846784477,
      "grad_norm": 3.6566052436828613,
      "learning_rate": 5.031955619547319e-05,
      "loss": 2.5719,
      "step": 24600
    },
    {
      "epoch": 1.4945041598348212,
      "grad_norm": 3.374255895614624,
      "learning_rate": 5.028775992956785e-05,
      "loss": 2.2309,
      "step": 24610
    },
    {
      "epoch": 1.4951114349911945,
      "grad_norm": 2.846104860305786,
      "learning_rate": 5.025596354728781e-05,
      "loss": 2.1598,
      "step": 24620
    },
    {
      "epoch": 1.495718710147568,
      "grad_norm": 4.10567045211792,
      "learning_rate": 5.022416706149202e-05,
      "loss": 2.3506,
      "step": 24630
    },
    {
      "epoch": 1.4963259853039412,
      "grad_norm": 5.233534812927246,
      "learning_rate": 5.019237048503951e-05,
      "loss": 2.545,
      "step": 24640
    },
    {
      "epoch": 1.4969332604603145,
      "grad_norm": 3.1773734092712402,
      "learning_rate": 5.01605738307893e-05,
      "loss": 2.1674,
      "step": 24650
    },
    {
      "epoch": 1.497540535616688,
      "grad_norm": 4.14766788482666,
      "learning_rate": 5.012877711160051e-05,
      "loss": 2.2595,
      "step": 24660
    },
    {
      "epoch": 1.4981478107730613,
      "grad_norm": 3.9756150245666504,
      "learning_rate": 5.009698034033217e-05,
      "loss": 2.432,
      "step": 24670
    },
    {
      "epoch": 1.4987550859294347,
      "grad_norm": 6.164638042449951,
      "learning_rate": 5.006518352984345e-05,
      "loss": 2.1971,
      "step": 24680
    },
    {
      "epoch": 1.499362361085808,
      "grad_norm": 4.869242191314697,
      "learning_rate": 5.003338669299347e-05,
      "loss": 2.5569,
      "step": 24690
    },
    {
      "epoch": 1.4999696362421813,
      "grad_norm": 3.6111907958984375,
      "learning_rate": 5.000158984264138e-05,
      "loss": 2.1581,
      "step": 24700
    },
    {
      "epoch": 1.5005769113985545,
      "grad_norm": 4.692960739135742,
      "learning_rate": 4.996979299164634e-05,
      "loss": 2.3392,
      "step": 24710
    },
    {
      "epoch": 1.501184186554928,
      "grad_norm": 3.652996301651001,
      "learning_rate": 4.9937996152867476e-05,
      "loss": 2.5144,
      "step": 24720
    },
    {
      "epoch": 1.5017914617113015,
      "grad_norm": 3.5589253902435303,
      "learning_rate": 4.990619933916397e-05,
      "loss": 2.3009,
      "step": 24730
    },
    {
      "epoch": 1.5023987368676748,
      "grad_norm": 3.6435883045196533,
      "learning_rate": 4.987440256339494e-05,
      "loss": 2.3057,
      "step": 24740
    },
    {
      "epoch": 1.503006012024048,
      "grad_norm": 3.919541358947754,
      "learning_rate": 4.984260583841953e-05,
      "loss": 2.3379,
      "step": 24750
    },
    {
      "epoch": 1.5036132871804213,
      "grad_norm": 2.467543840408325,
      "learning_rate": 4.9810809177096794e-05,
      "loss": 2.0555,
      "step": 24760
    },
    {
      "epoch": 1.5042205623367948,
      "grad_norm": 2.6856563091278076,
      "learning_rate": 4.977901259228586e-05,
      "loss": 2.0526,
      "step": 24770
    },
    {
      "epoch": 1.5048278374931683,
      "grad_norm": 4.966968059539795,
      "learning_rate": 4.974721609684576e-05,
      "loss": 2.3839,
      "step": 24780
    },
    {
      "epoch": 1.5054351126495416,
      "grad_norm": 3.4982359409332275,
      "learning_rate": 4.9715419703635474e-05,
      "loss": 2.2621,
      "step": 24790
    },
    {
      "epoch": 1.5060423878059148,
      "grad_norm": 3.847262144088745,
      "learning_rate": 4.968362342551401e-05,
      "loss": 2.4644,
      "step": 24800
    },
    {
      "epoch": 1.506649662962288,
      "grad_norm": 3.3555338382720947,
      "learning_rate": 4.965182727534027e-05,
      "loss": 2.2976,
      "step": 24810
    },
    {
      "epoch": 1.5072569381186616,
      "grad_norm": 3.756687641143799,
      "learning_rate": 4.9620031265973123e-05,
      "loss": 2.2231,
      "step": 24820
    },
    {
      "epoch": 1.507864213275035,
      "grad_norm": 1.9706087112426758,
      "learning_rate": 4.9588235410271375e-05,
      "loss": 2.1744,
      "step": 24830
    },
    {
      "epoch": 1.5084714884314083,
      "grad_norm": 3.2987232208251953,
      "learning_rate": 4.955643972109379e-05,
      "loss": 2.088,
      "step": 24840
    },
    {
      "epoch": 1.5090787635877816,
      "grad_norm": 2.544856548309326,
      "learning_rate": 4.952464421129905e-05,
      "loss": 2.061,
      "step": 24850
    },
    {
      "epoch": 1.5096860387441549,
      "grad_norm": 4.288061618804932,
      "learning_rate": 4.9492848893745763e-05,
      "loss": 2.2459,
      "step": 24860
    },
    {
      "epoch": 1.5102933139005283,
      "grad_norm": 2.6500823497772217,
      "learning_rate": 4.946105378129245e-05,
      "loss": 2.1989,
      "step": 24870
    },
    {
      "epoch": 1.5109005890569018,
      "grad_norm": 4.000594615936279,
      "learning_rate": 4.942925888679758e-05,
      "loss": 2.2493,
      "step": 24880
    },
    {
      "epoch": 1.511507864213275,
      "grad_norm": 2.090508460998535,
      "learning_rate": 4.939746422311951e-05,
      "loss": 1.9818,
      "step": 24890
    },
    {
      "epoch": 1.5121151393696484,
      "grad_norm": 3.1410329341888428,
      "learning_rate": 4.936566980311651e-05,
      "loss": 2.1955,
      "step": 24900
    },
    {
      "epoch": 1.5127224145260216,
      "grad_norm": 3.4627392292022705,
      "learning_rate": 4.933387563964671e-05,
      "loss": 2.2471,
      "step": 24910
    },
    {
      "epoch": 1.5133296896823951,
      "grad_norm": 2.3335952758789062,
      "learning_rate": 4.930208174556824e-05,
      "loss": 2.1082,
      "step": 24920
    },
    {
      "epoch": 1.5139369648387686,
      "grad_norm": 3.115830183029175,
      "learning_rate": 4.9270288133739023e-05,
      "loss": 2.2951,
      "step": 24930
    },
    {
      "epoch": 1.5145442399951419,
      "grad_norm": 1.4780781269073486,
      "learning_rate": 4.9238494817016915e-05,
      "loss": 1.8859,
      "step": 24940
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 4.243436336517334,
      "learning_rate": 4.92067018082596e-05,
      "loss": 2.2341,
      "step": 24950
    },
    {
      "epoch": 1.5157587903078884,
      "grad_norm": 3.5129897594451904,
      "learning_rate": 4.917490912032473e-05,
      "loss": 2.3204,
      "step": 24960
    },
    {
      "epoch": 1.516366065464262,
      "grad_norm": 3.6551589965820312,
      "learning_rate": 4.914311676606977e-05,
      "loss": 2.4937,
      "step": 24970
    },
    {
      "epoch": 1.5169733406206352,
      "grad_norm": 3.844897985458374,
      "learning_rate": 4.911132475835202e-05,
      "loss": 2.5468,
      "step": 24980
    },
    {
      "epoch": 1.5175806157770086,
      "grad_norm": 2.8867027759552,
      "learning_rate": 4.9079533110028665e-05,
      "loss": 2.327,
      "step": 24990
    },
    {
      "epoch": 1.518187890933382,
      "grad_norm": 3.2632970809936523,
      "learning_rate": 4.904774183395682e-05,
      "loss": 2.2626,
      "step": 25000
    },
    {
      "epoch": 1.5184915285115685,
      "eval_loss": 3.631157875061035,
      "eval_runtime": 2261.6799,
      "eval_samples_per_second": 7.281,
      "eval_steps_per_second": 1.82,
      "step": 25005
    },
    {
      "epoch": 1.5187951660897552,
      "grad_norm": 5.677043437957764,
      "learning_rate": 4.9015950942993326e-05,
      "loss": 2.7506,
      "step": 25010
    },
    {
      "epoch": 1.5194024412461287,
      "grad_norm": 4.206907749176025,
      "learning_rate": 4.898416044999493e-05,
      "loss": 3.0807,
      "step": 25020
    },
    {
      "epoch": 1.520009716402502,
      "grad_norm": 3.4926295280456543,
      "learning_rate": 4.895237036781825e-05,
      "loss": 2.2553,
      "step": 25030
    },
    {
      "epoch": 1.5206169915588754,
      "grad_norm": 6.242613315582275,
      "learning_rate": 4.8920580709319673e-05,
      "loss": 2.3022,
      "step": 25040
    },
    {
      "epoch": 1.5212242667152487,
      "grad_norm": 6.09660005569458,
      "learning_rate": 4.888879148735544e-05,
      "loss": 2.4382,
      "step": 25050
    },
    {
      "epoch": 1.521831541871622,
      "grad_norm": 5.296742916107178,
      "learning_rate": 4.885700271478163e-05,
      "loss": 2.4735,
      "step": 25060
    },
    {
      "epoch": 1.5224388170279954,
      "grad_norm": 4.036552906036377,
      "learning_rate": 4.8825214404454126e-05,
      "loss": 2.3631,
      "step": 25070
    },
    {
      "epoch": 1.5230460921843687,
      "grad_norm": 4.302045822143555,
      "learning_rate": 4.8793426569228635e-05,
      "loss": 2.422,
      "step": 25080
    },
    {
      "epoch": 1.5236533673407422,
      "grad_norm": 2.315211772918701,
      "learning_rate": 4.876163922196065e-05,
      "loss": 2.0815,
      "step": 25090
    },
    {
      "epoch": 1.5242606424971155,
      "grad_norm": 2.2487294673919678,
      "learning_rate": 4.872985237550548e-05,
      "loss": 2.051,
      "step": 25100
    },
    {
      "epoch": 1.5248679176534887,
      "grad_norm": 2.377251148223877,
      "learning_rate": 4.8698066042718244e-05,
      "loss": 1.9439,
      "step": 25110
    },
    {
      "epoch": 1.525475192809862,
      "grad_norm": 2.192948341369629,
      "learning_rate": 4.866628023645384e-05,
      "loss": 2.0554,
      "step": 25120
    },
    {
      "epoch": 1.5260824679662355,
      "grad_norm": 4.434778213500977,
      "learning_rate": 4.863449496956694e-05,
      "loss": 2.238,
      "step": 25130
    },
    {
      "epoch": 1.526689743122609,
      "grad_norm": 4.458044052124023,
      "learning_rate": 4.8602710254912026e-05,
      "loss": 2.3921,
      "step": 25140
    },
    {
      "epoch": 1.5272970182789822,
      "grad_norm": 3.7331430912017822,
      "learning_rate": 4.857092610534334e-05,
      "loss": 2.4027,
      "step": 25150
    },
    {
      "epoch": 1.5279042934353555,
      "grad_norm": 5.01262092590332,
      "learning_rate": 4.85391425337149e-05,
      "loss": 2.5515,
      "step": 25160
    },
    {
      "epoch": 1.5285115685917288,
      "grad_norm": 3.601273536682129,
      "learning_rate": 4.8507359552880485e-05,
      "loss": 2.2425,
      "step": 25170
    },
    {
      "epoch": 1.5291188437481023,
      "grad_norm": 2.192354679107666,
      "learning_rate": 4.8475577175693625e-05,
      "loss": 2.0904,
      "step": 25180
    },
    {
      "epoch": 1.5297261189044757,
      "grad_norm": 1.9482290744781494,
      "learning_rate": 4.8443795415007646e-05,
      "loss": 2.0815,
      "step": 25190
    },
    {
      "epoch": 1.530333394060849,
      "grad_norm": 3.9002463817596436,
      "learning_rate": 4.841201428367559e-05,
      "loss": 2.7925,
      "step": 25200
    },
    {
      "epoch": 1.5309406692172223,
      "grad_norm": 4.500323295593262,
      "learning_rate": 4.838023379455025e-05,
      "loss": 2.667,
      "step": 25210
    },
    {
      "epoch": 1.5315479443735955,
      "grad_norm": 2.80924916267395,
      "learning_rate": 4.834845396048414e-05,
      "loss": 2.2804,
      "step": 25220
    },
    {
      "epoch": 1.532155219529969,
      "grad_norm": 3.9796876907348633,
      "learning_rate": 4.831667479432957e-05,
      "loss": 2.3386,
      "step": 25230
    },
    {
      "epoch": 1.5327624946863425,
      "grad_norm": 3.7233753204345703,
      "learning_rate": 4.828489630893852e-05,
      "loss": 2.1324,
      "step": 25240
    },
    {
      "epoch": 1.5333697698427158,
      "grad_norm": 1.9464023113250732,
      "learning_rate": 4.825311851716271e-05,
      "loss": 2.0357,
      "step": 25250
    },
    {
      "epoch": 1.533977044999089,
      "grad_norm": 3.9262475967407227,
      "learning_rate": 4.82213414318536e-05,
      "loss": 2.3675,
      "step": 25260
    },
    {
      "epoch": 1.5345843201554623,
      "grad_norm": 3.6616079807281494,
      "learning_rate": 4.818956506586235e-05,
      "loss": 2.3651,
      "step": 25270
    },
    {
      "epoch": 1.5351915953118358,
      "grad_norm": 3.7712957859039307,
      "learning_rate": 4.8157789432039826e-05,
      "loss": 2.4117,
      "step": 25280
    },
    {
      "epoch": 1.5357988704682093,
      "grad_norm": 2.2377612590789795,
      "learning_rate": 4.8126014543236556e-05,
      "loss": 2.0844,
      "step": 25290
    },
    {
      "epoch": 1.5364061456245826,
      "grad_norm": 2.738987445831299,
      "learning_rate": 4.809424041230288e-05,
      "loss": 1.975,
      "step": 25300
    },
    {
      "epoch": 1.5370134207809558,
      "grad_norm": 2.251220464706421,
      "learning_rate": 4.806246705208875e-05,
      "loss": 2.1349,
      "step": 25310
    },
    {
      "epoch": 1.537620695937329,
      "grad_norm": 2.4442057609558105,
      "learning_rate": 4.8030694475443785e-05,
      "loss": 1.9941,
      "step": 25320
    },
    {
      "epoch": 1.5382279710937026,
      "grad_norm": 4.533000946044922,
      "learning_rate": 4.7998922695217313e-05,
      "loss": 2.6626,
      "step": 25330
    },
    {
      "epoch": 1.538835246250076,
      "grad_norm": 4.141018390655518,
      "learning_rate": 4.7967151724258405e-05,
      "loss": 2.4038,
      "step": 25340
    },
    {
      "epoch": 1.5394425214064493,
      "grad_norm": 2.7732925415039062,
      "learning_rate": 4.793538157541571e-05,
      "loss": 2.1643,
      "step": 25350
    },
    {
      "epoch": 1.5400497965628226,
      "grad_norm": 1.7221349477767944,
      "learning_rate": 4.790361226153759e-05,
      "loss": 2.1273,
      "step": 25360
    },
    {
      "epoch": 1.5406570717191959,
      "grad_norm": 4.298733711242676,
      "learning_rate": 4.787184379547204e-05,
      "loss": 2.2549,
      "step": 25370
    },
    {
      "epoch": 1.5412643468755693,
      "grad_norm": 3.4830613136291504,
      "learning_rate": 4.784007619006677e-05,
      "loss": 2.1117,
      "step": 25380
    },
    {
      "epoch": 1.5418716220319428,
      "grad_norm": 3.20461368560791,
      "learning_rate": 4.780830945816909e-05,
      "loss": 2.1742,
      "step": 25390
    },
    {
      "epoch": 1.542478897188316,
      "grad_norm": 2.1616370677948,
      "learning_rate": 4.777654361262597e-05,
      "loss": 2.238,
      "step": 25400
    },
    {
      "epoch": 1.5430861723446894,
      "grad_norm": 3.4519877433776855,
      "learning_rate": 4.7744778666284015e-05,
      "loss": 2.0556,
      "step": 25410
    },
    {
      "epoch": 1.5436934475010626,
      "grad_norm": 3.5076346397399902,
      "learning_rate": 4.771301463198949e-05,
      "loss": 2.5075,
      "step": 25420
    },
    {
      "epoch": 1.5443007226574361,
      "grad_norm": 2.9276702404022217,
      "learning_rate": 4.768125152258827e-05,
      "loss": 2.3662,
      "step": 25430
    },
    {
      "epoch": 1.5449079978138094,
      "grad_norm": 3.103200674057007,
      "learning_rate": 4.764948935092587e-05,
      "loss": 2.1497,
      "step": 25440
    },
    {
      "epoch": 1.5455152729701829,
      "grad_norm": 3.84061861038208,
      "learning_rate": 4.76177281298474e-05,
      "loss": 2.1342,
      "step": 25450
    },
    {
      "epoch": 1.5461225481265561,
      "grad_norm": 2.3827993869781494,
      "learning_rate": 4.758596787219762e-05,
      "loss": 2.1341,
      "step": 25460
    },
    {
      "epoch": 1.5467298232829294,
      "grad_norm": 3.253601312637329,
      "learning_rate": 4.7554208590820885e-05,
      "loss": 2.164,
      "step": 25470
    },
    {
      "epoch": 1.547337098439303,
      "grad_norm": 4.523049831390381,
      "learning_rate": 4.752245029856114e-05,
      "loss": 2.3142,
      "step": 25480
    },
    {
      "epoch": 1.5479443735956762,
      "grad_norm": 4.057461738586426,
      "learning_rate": 4.7490693008261956e-05,
      "loss": 2.3465,
      "step": 25490
    },
    {
      "epoch": 1.5485516487520496,
      "grad_norm": 3.1157188415527344,
      "learning_rate": 4.745893673276649e-05,
      "loss": 2.0148,
      "step": 25500
    },
    {
      "epoch": 1.549158923908423,
      "grad_norm": 2.4930026531219482,
      "learning_rate": 4.742718148491748e-05,
      "loss": 2.2901,
      "step": 25510
    },
    {
      "epoch": 1.5497661990647962,
      "grad_norm": 2.5340628623962402,
      "learning_rate": 4.739542727755724e-05,
      "loss": 2.324,
      "step": 25520
    },
    {
      "epoch": 1.5503734742211697,
      "grad_norm": 3.0714685916900635,
      "learning_rate": 4.73636741235277e-05,
      "loss": 2.1656,
      "step": 25530
    },
    {
      "epoch": 1.550980749377543,
      "grad_norm": 3.4792540073394775,
      "learning_rate": 4.7331922035670326e-05,
      "loss": 2.5063,
      "step": 25540
    },
    {
      "epoch": 1.5515880245339164,
      "grad_norm": 2.7238657474517822,
      "learning_rate": 4.730017102682618e-05,
      "loss": 2.2722,
      "step": 25550
    },
    {
      "epoch": 1.5521952996902897,
      "grad_norm": 3.5923070907592773,
      "learning_rate": 4.726842110983586e-05,
      "loss": 2.3193,
      "step": 25560
    },
    {
      "epoch": 1.552802574846663,
      "grad_norm": 2.289989709854126,
      "learning_rate": 4.723667229753956e-05,
      "loss": 2.1001,
      "step": 25570
    },
    {
      "epoch": 1.5534098500030362,
      "grad_norm": 4.824666976928711,
      "learning_rate": 4.720492460277699e-05,
      "loss": 2.2878,
      "step": 25580
    },
    {
      "epoch": 1.5540171251594097,
      "grad_norm": 2.2472729682922363,
      "learning_rate": 4.7173178038387436e-05,
      "loss": 2.0632,
      "step": 25590
    },
    {
      "epoch": 1.5546244003157832,
      "grad_norm": 2.834925651550293,
      "learning_rate": 4.714143261720967e-05,
      "loss": 1.6471,
      "step": 25600
    },
    {
      "epoch": 1.5552316754721565,
      "grad_norm": 6.865667343139648,
      "learning_rate": 4.710968835208211e-05,
      "loss": 2.4103,
      "step": 25610
    },
    {
      "epoch": 1.5558389506285297,
      "grad_norm": 3.8219454288482666,
      "learning_rate": 4.707794525584262e-05,
      "loss": 2.5527,
      "step": 25620
    },
    {
      "epoch": 1.556446225784903,
      "grad_norm": 4.355526447296143,
      "learning_rate": 4.704620334132859e-05,
      "loss": 2.2092,
      "step": 25630
    },
    {
      "epoch": 1.5570535009412765,
      "grad_norm": 4.099972248077393,
      "learning_rate": 4.701446262137695e-05,
      "loss": 2.1224,
      "step": 25640
    },
    {
      "epoch": 1.55766077609765,
      "grad_norm": 2.5958571434020996,
      "learning_rate": 4.6982723108824214e-05,
      "loss": 2.143,
      "step": 25650
    },
    {
      "epoch": 1.5582680512540232,
      "grad_norm": 2.5373454093933105,
      "learning_rate": 4.695098481650629e-05,
      "loss": 2.0779,
      "step": 25660
    },
    {
      "epoch": 1.5588753264103965,
      "grad_norm": 3.3599607944488525,
      "learning_rate": 4.691924775725867e-05,
      "loss": 2.102,
      "step": 25670
    },
    {
      "epoch": 1.5594826015667698,
      "grad_norm": 2.8980300426483154,
      "learning_rate": 4.6887511943916296e-05,
      "loss": 2.1464,
      "step": 25680
    },
    {
      "epoch": 1.5600898767231433,
      "grad_norm": 4.001591205596924,
      "learning_rate": 4.685577738931371e-05,
      "loss": 2.1075,
      "step": 25690
    },
    {
      "epoch": 1.5606971518795167,
      "grad_norm": 3.6197919845581055,
      "learning_rate": 4.682404410628479e-05,
      "loss": 2.2584,
      "step": 25700
    },
    {
      "epoch": 1.56130442703589,
      "grad_norm": 1.764965534210205,
      "learning_rate": 4.679231210766301e-05,
      "loss": 1.9807,
      "step": 25710
    },
    {
      "epoch": 1.5619117021922633,
      "grad_norm": 2.8795619010925293,
      "learning_rate": 4.676058140628133e-05,
      "loss": 2.1071,
      "step": 25720
    },
    {
      "epoch": 1.5625189773486365,
      "grad_norm": 2.6765291690826416,
      "learning_rate": 4.672885201497211e-05,
      "loss": 2.2924,
      "step": 25730
    },
    {
      "epoch": 1.56312625250501,
      "grad_norm": 2.0069868564605713,
      "learning_rate": 4.6697123946567227e-05,
      "loss": 1.9234,
      "step": 25740
    },
    {
      "epoch": 1.5637335276613835,
      "grad_norm": 4.044826507568359,
      "learning_rate": 4.666539721389802e-05,
      "loss": 2.2938,
      "step": 25750
    },
    {
      "epoch": 1.5643408028177568,
      "grad_norm": 5.340662956237793,
      "learning_rate": 4.663367182979529e-05,
      "loss": 2.5165,
      "step": 25760
    },
    {
      "epoch": 1.56494807797413,
      "grad_norm": 3.3031060695648193,
      "learning_rate": 4.66019478070893e-05,
      "loss": 2.5975,
      "step": 25770
    },
    {
      "epoch": 1.5655553531305033,
      "grad_norm": 5.586901664733887,
      "learning_rate": 4.657022515860972e-05,
      "loss": 2.436,
      "step": 25780
    },
    {
      "epoch": 1.5661626282868768,
      "grad_norm": 3.646562337875366,
      "learning_rate": 4.653850389718571e-05,
      "loss": 2.3262,
      "step": 25790
    },
    {
      "epoch": 1.5667699034432503,
      "grad_norm": 3.951942205429077,
      "learning_rate": 4.650678403564585e-05,
      "loss": 2.4401,
      "step": 25800
    },
    {
      "epoch": 1.5673771785996236,
      "grad_norm": 3.9524142742156982,
      "learning_rate": 4.647506558681818e-05,
      "loss": 2.1538,
      "step": 25810
    },
    {
      "epoch": 1.5679844537559968,
      "grad_norm": 2.35486102104187,
      "learning_rate": 4.644334856353011e-05,
      "loss": 2.0692,
      "step": 25820
    },
    {
      "epoch": 1.56859172891237,
      "grad_norm": 2.971191644668579,
      "learning_rate": 4.641163297860853e-05,
      "loss": 2.2646,
      "step": 25830
    },
    {
      "epoch": 1.5691990040687436,
      "grad_norm": 3.8894827365875244,
      "learning_rate": 4.6379918844879724e-05,
      "loss": 2.284,
      "step": 25840
    },
    {
      "epoch": 1.569806279225117,
      "grad_norm": 3.1443207263946533,
      "learning_rate": 4.634820617516939e-05,
      "loss": 2.3679,
      "step": 25850
    },
    {
      "epoch": 1.5704135543814903,
      "grad_norm": 3.4714133739471436,
      "learning_rate": 4.6316494982302645e-05,
      "loss": 2.3149,
      "step": 25860
    },
    {
      "epoch": 1.5710208295378636,
      "grad_norm": 2.6031670570373535,
      "learning_rate": 4.6284785279103977e-05,
      "loss": 2.2106,
      "step": 25870
    },
    {
      "epoch": 1.5716281046942369,
      "grad_norm": 2.058868408203125,
      "learning_rate": 4.625307707839733e-05,
      "loss": 1.8532,
      "step": 25880
    },
    {
      "epoch": 1.5722353798506103,
      "grad_norm": 2.9482762813568115,
      "learning_rate": 4.622137039300598e-05,
      "loss": 2.1,
      "step": 25890
    },
    {
      "epoch": 1.5728426550069836,
      "grad_norm": 3.896897077560425,
      "learning_rate": 4.618966523575264e-05,
      "loss": 2.3557,
      "step": 25900
    },
    {
      "epoch": 1.573449930163357,
      "grad_norm": 3.020766019821167,
      "learning_rate": 4.6157961619459325e-05,
      "loss": 2.2324,
      "step": 25910
    },
    {
      "epoch": 1.5740572053197304,
      "grad_norm": 4.377120494842529,
      "learning_rate": 4.612625955694755e-05,
      "loss": 2.3072,
      "step": 25920
    },
    {
      "epoch": 1.5746644804761036,
      "grad_norm": 3.297985792160034,
      "learning_rate": 4.6094559061038114e-05,
      "loss": 2.1321,
      "step": 25930
    },
    {
      "epoch": 1.5752717556324771,
      "grad_norm": 2.0621113777160645,
      "learning_rate": 4.606286014455116e-05,
      "loss": 1.9921,
      "step": 25940
    },
    {
      "epoch": 1.5758790307888504,
      "grad_norm": 2.9575843811035156,
      "learning_rate": 4.603116282030628e-05,
      "loss": 2.0751,
      "step": 25950
    },
    {
      "epoch": 1.5764863059452239,
      "grad_norm": 2.5624496936798096,
      "learning_rate": 4.599946710112237e-05,
      "loss": 2.2905,
      "step": 25960
    },
    {
      "epoch": 1.5770935811015971,
      "grad_norm": 2.6306231021881104,
      "learning_rate": 4.59677729998177e-05,
      "loss": 2.1171,
      "step": 25970
    },
    {
      "epoch": 1.5777008562579704,
      "grad_norm": 4.504848480224609,
      "learning_rate": 4.593608052920981e-05,
      "loss": 2.2749,
      "step": 25980
    },
    {
      "epoch": 1.5783081314143437,
      "grad_norm": 3.369332790374756,
      "learning_rate": 4.590438970211569e-05,
      "loss": 2.1458,
      "step": 25990
    },
    {
      "epoch": 1.5789154065707172,
      "grad_norm": 2.552280902862549,
      "learning_rate": 4.587270053135163e-05,
      "loss": 2.0936,
      "step": 26000
    },
    {
      "epoch": 1.5795226817270906,
      "grad_norm": 3.537106513977051,
      "learning_rate": 4.584101302973318e-05,
      "loss": 2.3679,
      "step": 26010
    },
    {
      "epoch": 1.580129956883464,
      "grad_norm": 2.330673933029175,
      "learning_rate": 4.5809327210075287e-05,
      "loss": 2.2693,
      "step": 26020
    },
    {
      "epoch": 1.5807372320398372,
      "grad_norm": 4.434186935424805,
      "learning_rate": 4.577764308519224e-05,
      "loss": 2.6616,
      "step": 26030
    },
    {
      "epoch": 1.5813445071962104,
      "grad_norm": 3.3003644943237305,
      "learning_rate": 4.574596066789757e-05,
      "loss": 2.4872,
      "step": 26040
    },
    {
      "epoch": 1.581951782352584,
      "grad_norm": 2.499042510986328,
      "learning_rate": 4.5714279971004154e-05,
      "loss": 2.3491,
      "step": 26050
    },
    {
      "epoch": 1.5825590575089574,
      "grad_norm": 3.323343515396118,
      "learning_rate": 4.568260100732416e-05,
      "loss": 2.294,
      "step": 26060
    },
    {
      "epoch": 1.5831663326653307,
      "grad_norm": 2.019209861755371,
      "learning_rate": 4.56509237896691e-05,
      "loss": 2.037,
      "step": 26070
    },
    {
      "epoch": 1.583773607821704,
      "grad_norm": 2.850740432739258,
      "learning_rate": 4.561924833084972e-05,
      "loss": 2.0272,
      "step": 26080
    },
    {
      "epoch": 1.5843808829780772,
      "grad_norm": 2.6471402645111084,
      "learning_rate": 4.5587574643676084e-05,
      "loss": 2.08,
      "step": 26090
    },
    {
      "epoch": 1.5849881581344507,
      "grad_norm": 3.076674222946167,
      "learning_rate": 4.555590274095752e-05,
      "loss": 2.0688,
      "step": 26100
    },
    {
      "epoch": 1.5855954332908242,
      "grad_norm": 2.6632251739501953,
      "learning_rate": 4.5524232635502685e-05,
      "loss": 2.2415,
      "step": 26110
    },
    {
      "epoch": 1.5862027084471975,
      "grad_norm": 2.594578266143799,
      "learning_rate": 4.549256434011945e-05,
      "loss": 2.0658,
      "step": 26120
    },
    {
      "epoch": 1.5868099836035707,
      "grad_norm": 3.7194788455963135,
      "learning_rate": 4.546089786761499e-05,
      "loss": 2.4719,
      "step": 26130
    },
    {
      "epoch": 1.587417258759944,
      "grad_norm": 6.577934265136719,
      "learning_rate": 4.542923323079571e-05,
      "loss": 2.6308,
      "step": 26140
    },
    {
      "epoch": 1.5880245339163175,
      "grad_norm": 4.508389472961426,
      "learning_rate": 4.5397570442467316e-05,
      "loss": 2.6741,
      "step": 26150
    },
    {
      "epoch": 1.588631809072691,
      "grad_norm": 4.4439697265625,
      "learning_rate": 4.536590951543474e-05,
      "loss": 2.299,
      "step": 26160
    },
    {
      "epoch": 1.5892390842290642,
      "grad_norm": 4.969281196594238,
      "learning_rate": 4.5334250462502135e-05,
      "loss": 2.4267,
      "step": 26170
    },
    {
      "epoch": 1.5898463593854375,
      "grad_norm": 3.6140573024749756,
      "learning_rate": 4.530259329647297e-05,
      "loss": 2.3393,
      "step": 26180
    },
    {
      "epoch": 1.5904536345418108,
      "grad_norm": 3.069380760192871,
      "learning_rate": 4.52709380301499e-05,
      "loss": 2.3484,
      "step": 26190
    },
    {
      "epoch": 1.5910609096981843,
      "grad_norm": 2.6896049976348877,
      "learning_rate": 4.523928467633479e-05,
      "loss": 2.1897,
      "step": 26200
    },
    {
      "epoch": 1.5916681848545577,
      "grad_norm": 5.153754711151123,
      "learning_rate": 4.520763324782878e-05,
      "loss": 2.5955,
      "step": 26210
    },
    {
      "epoch": 1.592275460010931,
      "grad_norm": 4.020707607269287,
      "learning_rate": 4.517598375743222e-05,
      "loss": 2.4249,
      "step": 26220
    },
    {
      "epoch": 1.5928827351673043,
      "grad_norm": 4.326119422912598,
      "learning_rate": 4.5144336217944646e-05,
      "loss": 2.5409,
      "step": 26230
    },
    {
      "epoch": 1.5934900103236775,
      "grad_norm": 3.4806933403015137,
      "learning_rate": 4.511269064216485e-05,
      "loss": 2.1453,
      "step": 26240
    },
    {
      "epoch": 1.594097285480051,
      "grad_norm": 5.510883808135986,
      "learning_rate": 4.508104704289079e-05,
      "loss": 2.2539,
      "step": 26250
    },
    {
      "epoch": 1.5947045606364245,
      "grad_norm": 3.876960277557373,
      "learning_rate": 4.504940543291965e-05,
      "loss": 2.4201,
      "step": 26260
    },
    {
      "epoch": 1.5953118357927978,
      "grad_norm": 3.246553659439087,
      "learning_rate": 4.501776582504779e-05,
      "loss": 2.2719,
      "step": 26270
    },
    {
      "epoch": 1.595919110949171,
      "grad_norm": 1.8249144554138184,
      "learning_rate": 4.49861282320708e-05,
      "loss": 2.174,
      "step": 26280
    },
    {
      "epoch": 1.5965263861055443,
      "grad_norm": 3.177182674407959,
      "learning_rate": 4.495449266678337e-05,
      "loss": 2.2731,
      "step": 26290
    },
    {
      "epoch": 1.5971336612619178,
      "grad_norm": 2.163991689682007,
      "learning_rate": 4.4922859141979484e-05,
      "loss": 2.3069,
      "step": 26300
    },
    {
      "epoch": 1.5977409364182913,
      "grad_norm": 4.887270450592041,
      "learning_rate": 4.4891227670452235e-05,
      "loss": 2.1691,
      "step": 26310
    },
    {
      "epoch": 1.5983482115746646,
      "grad_norm": 6.653404712677002,
      "learning_rate": 4.4859598264993866e-05,
      "loss": 2.4865,
      "step": 26320
    },
    {
      "epoch": 1.5989554867310378,
      "grad_norm": 3.7804343700408936,
      "learning_rate": 4.482797093839581e-05,
      "loss": 2.4192,
      "step": 26330
    },
    {
      "epoch": 1.599562761887411,
      "grad_norm": 4.001124858856201,
      "learning_rate": 4.4796345703448706e-05,
      "loss": 2.2179,
      "step": 26340
    },
    {
      "epoch": 1.6001700370437846,
      "grad_norm": 4.597071170806885,
      "learning_rate": 4.476472257294226e-05,
      "loss": 2.6376,
      "step": 26350
    },
    {
      "epoch": 1.6007773122001578,
      "grad_norm": 4.23461389541626,
      "learning_rate": 4.4733101559665395e-05,
      "loss": 2.1533,
      "step": 26360
    },
    {
      "epoch": 1.6013845873565313,
      "grad_norm": 4.42889928817749,
      "learning_rate": 4.4701482676406125e-05,
      "loss": 2.3521,
      "step": 26370
    },
    {
      "epoch": 1.6019918625129046,
      "grad_norm": 3.089557647705078,
      "learning_rate": 4.466986593595167e-05,
      "loss": 2.2734,
      "step": 26380
    },
    {
      "epoch": 1.6025991376692779,
      "grad_norm": 2.3953635692596436,
      "learning_rate": 4.46382513510883e-05,
      "loss": 2.3282,
      "step": 26390
    },
    {
      "epoch": 1.6032064128256514,
      "grad_norm": 4.561232566833496,
      "learning_rate": 4.460663893460147e-05,
      "loss": 2.5286,
      "step": 26400
    },
    {
      "epoch": 1.6038136879820246,
      "grad_norm": 3.722461700439453,
      "learning_rate": 4.457502869927578e-05,
      "loss": 2.2604,
      "step": 26410
    },
    {
      "epoch": 1.604420963138398,
      "grad_norm": 4.673840522766113,
      "learning_rate": 4.4543420657894874e-05,
      "loss": 2.442,
      "step": 26420
    },
    {
      "epoch": 1.6050282382947714,
      "grad_norm": 5.517437934875488,
      "learning_rate": 4.4511814823241555e-05,
      "loss": 2.5986,
      "step": 26430
    },
    {
      "epoch": 1.6056355134511446,
      "grad_norm": 3.7548930644989014,
      "learning_rate": 4.448021120809772e-05,
      "loss": 2.4784,
      "step": 26440
    },
    {
      "epoch": 1.606242788607518,
      "grad_norm": 3.690415620803833,
      "learning_rate": 4.444860982524439e-05,
      "loss": 2.3661,
      "step": 26450
    },
    {
      "epoch": 1.6068500637638914,
      "grad_norm": 3.7297122478485107,
      "learning_rate": 4.441701068746166e-05,
      "loss": 2.2329,
      "step": 26460
    },
    {
      "epoch": 1.6074573389202649,
      "grad_norm": 3.0033247470855713,
      "learning_rate": 4.438541380752873e-05,
      "loss": 2.2215,
      "step": 26470
    },
    {
      "epoch": 1.6080646140766381,
      "grad_norm": 3.5624239444732666,
      "learning_rate": 4.4353819198223864e-05,
      "loss": 2.2132,
      "step": 26480
    },
    {
      "epoch": 1.6086718892330114,
      "grad_norm": 2.9894158840179443,
      "learning_rate": 4.4322226872324455e-05,
      "loss": 2.0714,
      "step": 26490
    },
    {
      "epoch": 1.6092791643893847,
      "grad_norm": 3.3393754959106445,
      "learning_rate": 4.429063684260691e-05,
      "loss": 2.1311,
      "step": 26500
    },
    {
      "epoch": 1.6098864395457582,
      "grad_norm": 2.388704299926758,
      "learning_rate": 4.4259049121846765e-05,
      "loss": 2.144,
      "step": 26510
    },
    {
      "epoch": 1.6104937147021317,
      "grad_norm": 3.769709825515747,
      "learning_rate": 4.422746372281858e-05,
      "loss": 2.2372,
      "step": 26520
    },
    {
      "epoch": 1.611100989858505,
      "grad_norm": 3.4732704162597656,
      "learning_rate": 4.419588065829601e-05,
      "loss": 2.0413,
      "step": 26530
    },
    {
      "epoch": 1.6117082650148782,
      "grad_norm": 2.704742193222046,
      "learning_rate": 4.416429994105173e-05,
      "loss": 1.8897,
      "step": 26540
    },
    {
      "epoch": 1.6123155401712514,
      "grad_norm": 3.846339702606201,
      "learning_rate": 4.413272158385751e-05,
      "loss": 2.0101,
      "step": 26550
    },
    {
      "epoch": 1.612922815327625,
      "grad_norm": 2.9254255294799805,
      "learning_rate": 4.4101145599484106e-05,
      "loss": 2.0098,
      "step": 26560
    },
    {
      "epoch": 1.6135300904839984,
      "grad_norm": 4.069077014923096,
      "learning_rate": 4.406957200070139e-05,
      "loss": 2.1625,
      "step": 26570
    },
    {
      "epoch": 1.6141373656403717,
      "grad_norm": 4.281106472015381,
      "learning_rate": 4.403800080027819e-05,
      "loss": 2.2571,
      "step": 26580
    },
    {
      "epoch": 1.614744640796745,
      "grad_norm": 3.204293966293335,
      "learning_rate": 4.4006432010982446e-05,
      "loss": 2.3868,
      "step": 26590
    },
    {
      "epoch": 1.6153519159531182,
      "grad_norm": 3.828246831893921,
      "learning_rate": 4.397486564558102e-05,
      "loss": 2.2882,
      "step": 26600
    },
    {
      "epoch": 1.6159591911094917,
      "grad_norm": 2.8129425048828125,
      "learning_rate": 4.3943301716839896e-05,
      "loss": 2.328,
      "step": 26610
    },
    {
      "epoch": 1.6165664662658652,
      "grad_norm": 3.2577178478240967,
      "learning_rate": 4.3911740237524035e-05,
      "loss": 2.1554,
      "step": 26620
    },
    {
      "epoch": 1.6171737414222385,
      "grad_norm": 2.476515769958496,
      "learning_rate": 4.388018122039735e-05,
      "loss": 2.1902,
      "step": 26630
    },
    {
      "epoch": 1.6177810165786117,
      "grad_norm": 3.3606667518615723,
      "learning_rate": 4.384862467822288e-05,
      "loss": 2.0,
      "step": 26640
    },
    {
      "epoch": 1.618388291734985,
      "grad_norm": 2.361318826675415,
      "learning_rate": 4.3817070623762566e-05,
      "loss": 1.9712,
      "step": 26650
    },
    {
      "epoch": 1.6189955668913585,
      "grad_norm": 2.2806994915008545,
      "learning_rate": 4.378551906977735e-05,
      "loss": 2.1417,
      "step": 26660
    },
    {
      "epoch": 1.619602842047732,
      "grad_norm": 4.8733415603637695,
      "learning_rate": 4.375397002902717e-05,
      "loss": 2.2239,
      "step": 26670
    },
    {
      "epoch": 1.6202101172041052,
      "grad_norm": 3.5559284687042236,
      "learning_rate": 4.372242351427103e-05,
      "loss": 2.7178,
      "step": 26680
    },
    {
      "epoch": 1.6208173923604785,
      "grad_norm": 2.5152578353881836,
      "learning_rate": 4.369087953826678e-05,
      "loss": 2.0197,
      "step": 26690
    },
    {
      "epoch": 1.6214246675168518,
      "grad_norm": 1.903027892112732,
      "learning_rate": 4.3659338113771325e-05,
      "loss": 1.9504,
      "step": 26700
    },
    {
      "epoch": 1.6220319426732253,
      "grad_norm": 4.658653736114502,
      "learning_rate": 4.36277992535405e-05,
      "loss": 2.1562,
      "step": 26710
    },
    {
      "epoch": 1.6226392178295987,
      "grad_norm": 2.737457275390625,
      "learning_rate": 4.359626297032918e-05,
      "loss": 2.0985,
      "step": 26720
    },
    {
      "epoch": 1.623246492985972,
      "grad_norm": 3.4070565700531006,
      "learning_rate": 4.356472927689109e-05,
      "loss": 2.3338,
      "step": 26730
    },
    {
      "epoch": 1.6238537681423453,
      "grad_norm": 2.8049349784851074,
      "learning_rate": 4.353319818597898e-05,
      "loss": 2.3634,
      "step": 26740
    },
    {
      "epoch": 1.6244610432987185,
      "grad_norm": 2.738621950149536,
      "learning_rate": 4.3501669710344495e-05,
      "loss": 2.101,
      "step": 26750
    },
    {
      "epoch": 1.625068318455092,
      "grad_norm": 3.4784703254699707,
      "learning_rate": 4.347014386273829e-05,
      "loss": 2.2914,
      "step": 26760
    },
    {
      "epoch": 1.6256755936114655,
      "grad_norm": 2.762485980987549,
      "learning_rate": 4.34386206559099e-05,
      "loss": 2.1697,
      "step": 26770
    },
    {
      "epoch": 1.6262828687678388,
      "grad_norm": 2.797910451889038,
      "learning_rate": 4.3407100102607816e-05,
      "loss": 1.8299,
      "step": 26780
    },
    {
      "epoch": 1.626890143924212,
      "grad_norm": 4.301344394683838,
      "learning_rate": 4.337558221557945e-05,
      "loss": 2.1506,
      "step": 26790
    },
    {
      "epoch": 1.6274974190805853,
      "grad_norm": 2.289466381072998,
      "learning_rate": 4.334406700757114e-05,
      "loss": 1.9849,
      "step": 26800
    },
    {
      "epoch": 1.6281046942369588,
      "grad_norm": 2.5366427898406982,
      "learning_rate": 4.331255449132814e-05,
      "loss": 2.3008,
      "step": 26810
    },
    {
      "epoch": 1.628711969393332,
      "grad_norm": 5.856332778930664,
      "learning_rate": 4.3281044679594606e-05,
      "loss": 2.1283,
      "step": 26820
    },
    {
      "epoch": 1.6293192445497056,
      "grad_norm": 5.068434715270996,
      "learning_rate": 4.324953758511361e-05,
      "loss": 2.5012,
      "step": 26830
    },
    {
      "epoch": 1.6299265197060788,
      "grad_norm": 3.561526298522949,
      "learning_rate": 4.321803322062713e-05,
      "loss": 2.1185,
      "step": 26840
    },
    {
      "epoch": 1.630533794862452,
      "grad_norm": 2.4408230781555176,
      "learning_rate": 4.318653159887603e-05,
      "loss": 2.0691,
      "step": 26850
    },
    {
      "epoch": 1.6311410700188256,
      "grad_norm": 2.466989517211914,
      "learning_rate": 4.315503273260005e-05,
      "loss": 2.1594,
      "step": 26860
    },
    {
      "epoch": 1.6317483451751988,
      "grad_norm": 3.874054431915283,
      "learning_rate": 4.312353663453787e-05,
      "loss": 2.0966,
      "step": 26870
    },
    {
      "epoch": 1.6323556203315723,
      "grad_norm": 2.085970878601074,
      "learning_rate": 4.309204331742699e-05,
      "loss": 1.9202,
      "step": 26880
    },
    {
      "epoch": 1.6329628954879456,
      "grad_norm": 3.106464385986328,
      "learning_rate": 4.306055279400381e-05,
      "loss": 2.0484,
      "step": 26890
    },
    {
      "epoch": 1.6335701706443189,
      "grad_norm": 4.023837566375732,
      "learning_rate": 4.3029065077003584e-05,
      "loss": 2.2336,
      "step": 26900
    },
    {
      "epoch": 1.6341774458006921,
      "grad_norm": 4.189576625823975,
      "learning_rate": 4.2997580179160474e-05,
      "loss": 2.4022,
      "step": 26910
    },
    {
      "epoch": 1.6347847209570656,
      "grad_norm": 2.7325379848480225,
      "learning_rate": 4.296609811320747e-05,
      "loss": 2.2471,
      "step": 26920
    },
    {
      "epoch": 1.635391996113439,
      "grad_norm": 2.689321994781494,
      "learning_rate": 4.293461889187641e-05,
      "loss": 2.4614,
      "step": 26930
    },
    {
      "epoch": 1.6359992712698124,
      "grad_norm": 4.354347229003906,
      "learning_rate": 4.2903142527897984e-05,
      "loss": 2.4149,
      "step": 26940
    },
    {
      "epoch": 1.6366065464261856,
      "grad_norm": 3.2907731533050537,
      "learning_rate": 4.287166903400175e-05,
      "loss": 2.0978,
      "step": 26950
    },
    {
      "epoch": 1.637213821582559,
      "grad_norm": 3.87534236907959,
      "learning_rate": 4.284019842291611e-05,
      "loss": 1.896,
      "step": 26960
    },
    {
      "epoch": 1.6378210967389324,
      "grad_norm": 3.3763439655303955,
      "learning_rate": 4.280873070736825e-05,
      "loss": 1.9647,
      "step": 26970
    },
    {
      "epoch": 1.6384283718953059,
      "grad_norm": 3.2866950035095215,
      "learning_rate": 4.27772659000842e-05,
      "loss": 2.1614,
      "step": 26980
    },
    {
      "epoch": 1.6390356470516791,
      "grad_norm": 2.518909454345703,
      "learning_rate": 4.274580401378886e-05,
      "loss": 2.4084,
      "step": 26990
    },
    {
      "epoch": 1.6396429222080524,
      "grad_norm": 3.2075183391571045,
      "learning_rate": 4.271434506120593e-05,
      "loss": 2.2439,
      "step": 27000
    },
    {
      "epoch": 1.6402501973644257,
      "grad_norm": 4.067445755004883,
      "learning_rate": 4.268288905505788e-05,
      "loss": 2.2977,
      "step": 27010
    },
    {
      "epoch": 1.6408574725207992,
      "grad_norm": 3.3796157836914062,
      "learning_rate": 4.265143600806601e-05,
      "loss": 2.4752,
      "step": 27020
    },
    {
      "epoch": 1.6414647476771727,
      "grad_norm": 2.7230732440948486,
      "learning_rate": 4.261998593295049e-05,
      "loss": 2.5079,
      "step": 27030
    },
    {
      "epoch": 1.642072022833546,
      "grad_norm": 2.346426248550415,
      "learning_rate": 4.258853884243018e-05,
      "loss": 2.0513,
      "step": 27040
    },
    {
      "epoch": 1.6426792979899192,
      "grad_norm": 3.6357908248901367,
      "learning_rate": 4.25570947492228e-05,
      "loss": 2.2402,
      "step": 27050
    },
    {
      "epoch": 1.6432865731462925,
      "grad_norm": 4.729530334472656,
      "learning_rate": 4.252565366604484e-05,
      "loss": 2.2901,
      "step": 27060
    },
    {
      "epoch": 1.643893848302666,
      "grad_norm": 3.1725106239318848,
      "learning_rate": 4.2494215605611574e-05,
      "loss": 2.1705,
      "step": 27070
    },
    {
      "epoch": 1.6445011234590394,
      "grad_norm": 2.7790122032165527,
      "learning_rate": 4.246278058063707e-05,
      "loss": 1.9855,
      "step": 27080
    },
    {
      "epoch": 1.6451083986154127,
      "grad_norm": 4.052187919616699,
      "learning_rate": 4.243134860383412e-05,
      "loss": 2.1061,
      "step": 27090
    },
    {
      "epoch": 1.645715673771786,
      "grad_norm": 3.8497374057769775,
      "learning_rate": 4.239991968791435e-05,
      "loss": 2.4273,
      "step": 27100
    },
    {
      "epoch": 1.6463229489281592,
      "grad_norm": 2.690755844116211,
      "learning_rate": 4.2368493845588106e-05,
      "loss": 2.0759,
      "step": 27110
    },
    {
      "epoch": 1.6469302240845327,
      "grad_norm": 2.3787147998809814,
      "learning_rate": 4.2337071089564483e-05,
      "loss": 1.9407,
      "step": 27120
    },
    {
      "epoch": 1.6475374992409062,
      "grad_norm": 2.462157964706421,
      "learning_rate": 4.230565143255135e-05,
      "loss": 2.0971,
      "step": 27130
    },
    {
      "epoch": 1.6481447743972795,
      "grad_norm": 3.808772087097168,
      "learning_rate": 4.227423488725533e-05,
      "loss": 2.2344,
      "step": 27140
    },
    {
      "epoch": 1.6487520495536527,
      "grad_norm": 5.247958660125732,
      "learning_rate": 4.224282146638177e-05,
      "loss": 2.695,
      "step": 27150
    },
    {
      "epoch": 1.649359324710026,
      "grad_norm": 4.501919269561768,
      "learning_rate": 4.221141118263474e-05,
      "loss": 2.5123,
      "step": 27160
    },
    {
      "epoch": 1.6499665998663995,
      "grad_norm": 3.5578627586364746,
      "learning_rate": 4.218000404871707e-05,
      "loss": 2.616,
      "step": 27170
    },
    {
      "epoch": 1.650573875022773,
      "grad_norm": 2.988926410675049,
      "learning_rate": 4.214860007733032e-05,
      "loss": 2.3733,
      "step": 27180
    },
    {
      "epoch": 1.6511811501791462,
      "grad_norm": 2.57308292388916,
      "learning_rate": 4.211719928117474e-05,
      "loss": 2.0996,
      "step": 27190
    },
    {
      "epoch": 1.6517884253355195,
      "grad_norm": 5.333561897277832,
      "learning_rate": 4.208580167294932e-05,
      "loss": 2.0025,
      "step": 27200
    },
    {
      "epoch": 1.6523957004918928,
      "grad_norm": 4.529408931732178,
      "learning_rate": 4.2054407265351733e-05,
      "loss": 2.2802,
      "step": 27210
    },
    {
      "epoch": 1.6530029756482663,
      "grad_norm": 2.177471160888672,
      "learning_rate": 4.20230160710784e-05,
      "loss": 2.0917,
      "step": 27220
    },
    {
      "epoch": 1.6536102508046397,
      "grad_norm": 2.6788153648376465,
      "learning_rate": 4.1991628102824416e-05,
      "loss": 1.9492,
      "step": 27230
    },
    {
      "epoch": 1.654217525961013,
      "grad_norm": 3.8084943294525146,
      "learning_rate": 4.196024337328358e-05,
      "loss": 1.9727,
      "step": 27240
    },
    {
      "epoch": 1.6548248011173863,
      "grad_norm": 3.7247812747955322,
      "learning_rate": 4.192886189514834e-05,
      "loss": 2.3047,
      "step": 27250
    },
    {
      "epoch": 1.6554320762737595,
      "grad_norm": 3.7648584842681885,
      "learning_rate": 4.1897483681109916e-05,
      "loss": 2.1565,
      "step": 27260
    },
    {
      "epoch": 1.656039351430133,
      "grad_norm": 3.8764493465423584,
      "learning_rate": 4.186610874385813e-05,
      "loss": 2.2162,
      "step": 27270
    },
    {
      "epoch": 1.6566466265865063,
      "grad_norm": 4.070901870727539,
      "learning_rate": 4.1834737096081525e-05,
      "loss": 2.2445,
      "step": 27280
    },
    {
      "epoch": 1.6572539017428798,
      "grad_norm": 2.4763662815093994,
      "learning_rate": 4.1803368750467254e-05,
      "loss": 2.0583,
      "step": 27290
    },
    {
      "epoch": 1.657861176899253,
      "grad_norm": 3.447909355163574,
      "learning_rate": 4.1772003719701216e-05,
      "loss": 2.1234,
      "step": 27300
    },
    {
      "epoch": 1.6584684520556263,
      "grad_norm": 4.302926063537598,
      "learning_rate": 4.174064201646793e-05,
      "loss": 2.2827,
      "step": 27310
    },
    {
      "epoch": 1.6590757272119998,
      "grad_norm": 4.013578414916992,
      "learning_rate": 4.1709283653450524e-05,
      "loss": 2.3917,
      "step": 27320
    },
    {
      "epoch": 1.659683002368373,
      "grad_norm": 1.7944291830062866,
      "learning_rate": 4.1677928643330874e-05,
      "loss": 1.953,
      "step": 27330
    },
    {
      "epoch": 1.6602902775247466,
      "grad_norm": 6.379343509674072,
      "learning_rate": 4.1646576998789426e-05,
      "loss": 2.2518,
      "step": 27340
    },
    {
      "epoch": 1.6608975526811198,
      "grad_norm": 3.6170966625213623,
      "learning_rate": 4.1615228732505274e-05,
      "loss": 2.2659,
      "step": 27350
    },
    {
      "epoch": 1.661504827837493,
      "grad_norm": 4.623157978057861,
      "learning_rate": 4.1583883857156136e-05,
      "loss": 2.3112,
      "step": 27360
    },
    {
      "epoch": 1.6621121029938664,
      "grad_norm": 3.7339792251586914,
      "learning_rate": 4.1552542385418437e-05,
      "loss": 2.2779,
      "step": 27370
    },
    {
      "epoch": 1.6627193781502398,
      "grad_norm": 3.977245807647705,
      "learning_rate": 4.152120432996713e-05,
      "loss": 2.345,
      "step": 27380
    },
    {
      "epoch": 1.6633266533066133,
      "grad_norm": 3.8252952098846436,
      "learning_rate": 4.148986970347582e-05,
      "loss": 2.3155,
      "step": 27390
    },
    {
      "epoch": 1.6639339284629866,
      "grad_norm": 4.577213287353516,
      "learning_rate": 4.145853851861673e-05,
      "loss": 2.3207,
      "step": 27400
    },
    {
      "epoch": 1.6645412036193599,
      "grad_norm": 4.767502784729004,
      "learning_rate": 4.1427210788060714e-05,
      "loss": 2.2884,
      "step": 27410
    },
    {
      "epoch": 1.6651484787757331,
      "grad_norm": 3.3456099033355713,
      "learning_rate": 4.1395886524477186e-05,
      "loss": 1.9961,
      "step": 27420
    },
    {
      "epoch": 1.6657557539321066,
      "grad_norm": 4.383159637451172,
      "learning_rate": 4.136456574053418e-05,
      "loss": 2.1449,
      "step": 27430
    },
    {
      "epoch": 1.66636302908848,
      "grad_norm": 3.6748316287994385,
      "learning_rate": 4.13332484488983e-05,
      "loss": 2.3315,
      "step": 27440
    },
    {
      "epoch": 1.6669703042448534,
      "grad_norm": 2.6303677558898926,
      "learning_rate": 4.13019346622348e-05,
      "loss": 2.092,
      "step": 27450
    },
    {
      "epoch": 1.6675775794012266,
      "grad_norm": 6.833557605743408,
      "learning_rate": 4.127062439320745e-05,
      "loss": 2.4984,
      "step": 27460
    },
    {
      "epoch": 1.6681848545576,
      "grad_norm": 3.4555556774139404,
      "learning_rate": 4.123931765447862e-05,
      "loss": 2.4311,
      "step": 27470
    },
    {
      "epoch": 1.6687921297139734,
      "grad_norm": 3.9433107376098633,
      "learning_rate": 4.120801445870924e-05,
      "loss": 2.2888,
      "step": 27480
    },
    {
      "epoch": 1.6693994048703469,
      "grad_norm": 4.8893232345581055,
      "learning_rate": 4.117671481855886e-05,
      "loss": 2.1957,
      "step": 27490
    },
    {
      "epoch": 1.6700066800267201,
      "grad_norm": 3.8000943660736084,
      "learning_rate": 4.114541874668553e-05,
      "loss": 2.0661,
      "step": 27500
    },
    {
      "epoch": 1.6706139551830934,
      "grad_norm": 3.1241443157196045,
      "learning_rate": 4.111412625574587e-05,
      "loss": 1.8891,
      "step": 27510
    },
    {
      "epoch": 1.6712212303394667,
      "grad_norm": 3.30277681350708,
      "learning_rate": 4.108283735839506e-05,
      "loss": 2.1977,
      "step": 27520
    },
    {
      "epoch": 1.6718285054958402,
      "grad_norm": 4.257347106933594,
      "learning_rate": 4.105155206728687e-05,
      "loss": 2.3697,
      "step": 27530
    },
    {
      "epoch": 1.6724357806522137,
      "grad_norm": 3.4088752269744873,
      "learning_rate": 4.102027039507352e-05,
      "loss": 2.1416,
      "step": 27540
    },
    {
      "epoch": 1.673043055808587,
      "grad_norm": 2.9809610843658447,
      "learning_rate": 4.0988992354405844e-05,
      "loss": 2.0785,
      "step": 27550
    },
    {
      "epoch": 1.6736503309649602,
      "grad_norm": 3.9873745441436768,
      "learning_rate": 4.095771795793317e-05,
      "loss": 2.15,
      "step": 27560
    },
    {
      "epoch": 1.6742576061213335,
      "grad_norm": 2.6922147274017334,
      "learning_rate": 4.092644721830337e-05,
      "loss": 1.959,
      "step": 27570
    },
    {
      "epoch": 1.674864881277707,
      "grad_norm": 3.2121455669403076,
      "learning_rate": 4.089518014816283e-05,
      "loss": 2.2017,
      "step": 27580
    },
    {
      "epoch": 1.6754721564340804,
      "grad_norm": 5.513160228729248,
      "learning_rate": 4.086391676015642e-05,
      "loss": 2.4148,
      "step": 27590
    },
    {
      "epoch": 1.6760794315904537,
      "grad_norm": 3.2181107997894287,
      "learning_rate": 4.08326570669276e-05,
      "loss": 2.2001,
      "step": 27600
    },
    {
      "epoch": 1.676686706746827,
      "grad_norm": 2.983344793319702,
      "learning_rate": 4.080140108111826e-05,
      "loss": 2.2003,
      "step": 27610
    },
    {
      "epoch": 1.6772939819032002,
      "grad_norm": 3.272167921066284,
      "learning_rate": 4.077014881536884e-05,
      "loss": 2.1662,
      "step": 27620
    },
    {
      "epoch": 1.6779012570595737,
      "grad_norm": 4.796575546264648,
      "learning_rate": 4.073890028231819e-05,
      "loss": 2.3263,
      "step": 27630
    },
    {
      "epoch": 1.6785085322159472,
      "grad_norm": 2.4000706672668457,
      "learning_rate": 4.0707655494603795e-05,
      "loss": 2.0803,
      "step": 27640
    },
    {
      "epoch": 1.6791158073723205,
      "grad_norm": 2.98374080657959,
      "learning_rate": 4.067641446486152e-05,
      "loss": 2.0085,
      "step": 27650
    },
    {
      "epoch": 1.6797230825286937,
      "grad_norm": 5.076995372772217,
      "learning_rate": 4.064517720572572e-05,
      "loss": 2.2248,
      "step": 27660
    },
    {
      "epoch": 1.680330357685067,
      "grad_norm": 2.9848368167877197,
      "learning_rate": 4.061394372982922e-05,
      "loss": 2.3126,
      "step": 27670
    },
    {
      "epoch": 1.6809376328414405,
      "grad_norm": 1.8197658061981201,
      "learning_rate": 4.0582714049803395e-05,
      "loss": 2.1267,
      "step": 27680
    },
    {
      "epoch": 1.681544907997814,
      "grad_norm": 2.8237950801849365,
      "learning_rate": 4.0551488178277986e-05,
      "loss": 2.2238,
      "step": 27690
    },
    {
      "epoch": 1.6821521831541872,
      "grad_norm": 2.203404426574707,
      "learning_rate": 4.052026612788123e-05,
      "loss": 1.7959,
      "step": 27700
    },
    {
      "epoch": 1.6827594583105605,
      "grad_norm": 3.8355839252471924,
      "learning_rate": 4.048904791123983e-05,
      "loss": 2.1297,
      "step": 27710
    },
    {
      "epoch": 1.6833667334669338,
      "grad_norm": 3.6968791484832764,
      "learning_rate": 4.045783354097893e-05,
      "loss": 2.2862,
      "step": 27720
    },
    {
      "epoch": 1.6839740086233073,
      "grad_norm": 3.906506061553955,
      "learning_rate": 4.042662302972212e-05,
      "loss": 2.181,
      "step": 27730
    },
    {
      "epoch": 1.6845812837796805,
      "grad_norm": 3.0012588500976562,
      "learning_rate": 4.039541639009143e-05,
      "loss": 2.0587,
      "step": 27740
    },
    {
      "epoch": 1.685188558936054,
      "grad_norm": 2.1073598861694336,
      "learning_rate": 4.0364213634707305e-05,
      "loss": 2.0356,
      "step": 27750
    },
    {
      "epoch": 1.6857958340924273,
      "grad_norm": 2.9173829555511475,
      "learning_rate": 4.033301477618866e-05,
      "loss": 2.0354,
      "step": 27760
    },
    {
      "epoch": 1.6864031092488005,
      "grad_norm": 3.3272621631622314,
      "learning_rate": 4.03018198271528e-05,
      "loss": 2.1591,
      "step": 27770
    },
    {
      "epoch": 1.687010384405174,
      "grad_norm": 2.892210006713867,
      "learning_rate": 4.027062880021545e-05,
      "loss": 2.5387,
      "step": 27780
    },
    {
      "epoch": 1.6876176595615473,
      "grad_norm": 4.069476127624512,
      "learning_rate": 4.023944170799078e-05,
      "loss": 2.3692,
      "step": 27790
    },
    {
      "epoch": 1.6882249347179208,
      "grad_norm": 3.706947088241577,
      "learning_rate": 4.0208258563091326e-05,
      "loss": 2.2825,
      "step": 27800
    },
    {
      "epoch": 1.688832209874294,
      "grad_norm": 2.4514005184173584,
      "learning_rate": 4.0177079378128065e-05,
      "loss": 2.3005,
      "step": 27810
    },
    {
      "epoch": 1.6894394850306673,
      "grad_norm": 2.6568636894226074,
      "learning_rate": 4.014590416571033e-05,
      "loss": 2.162,
      "step": 27820
    },
    {
      "epoch": 1.6900467601870406,
      "grad_norm": 4.236302375793457,
      "learning_rate": 4.0114732938445907e-05,
      "loss": 2.4458,
      "step": 27830
    },
    {
      "epoch": 1.690654035343414,
      "grad_norm": 2.3241562843322754,
      "learning_rate": 4.0083565708940916e-05,
      "loss": 2.1142,
      "step": 27840
    },
    {
      "epoch": 1.6912613104997876,
      "grad_norm": 3.609666347503662,
      "learning_rate": 4.005240248979989e-05,
      "loss": 2.0755,
      "step": 27850
    },
    {
      "epoch": 1.6918685856561608,
      "grad_norm": 3.103280782699585,
      "learning_rate": 4.002124329362571e-05,
      "loss": 2.4328,
      "step": 27860
    },
    {
      "epoch": 1.692475860812534,
      "grad_norm": 3.9938199520111084,
      "learning_rate": 3.999008813301968e-05,
      "loss": 2.3631,
      "step": 27870
    },
    {
      "epoch": 1.6930831359689074,
      "grad_norm": 3.1502766609191895,
      "learning_rate": 3.995893702058143e-05,
      "loss": 2.1815,
      "step": 27880
    },
    {
      "epoch": 1.6936904111252808,
      "grad_norm": 2.6168081760406494,
      "learning_rate": 3.992778996890897e-05,
      "loss": 2.2977,
      "step": 27890
    },
    {
      "epoch": 1.6942976862816543,
      "grad_norm": 3.7314867973327637,
      "learning_rate": 3.9896646990598655e-05,
      "loss": 2.3266,
      "step": 27900
    },
    {
      "epoch": 1.6949049614380276,
      "grad_norm": 3.7046327590942383,
      "learning_rate": 3.9865508098245205e-05,
      "loss": 2.1652,
      "step": 27910
    },
    {
      "epoch": 1.6955122365944009,
      "grad_norm": 3.119983196258545,
      "learning_rate": 3.98343733044417e-05,
      "loss": 2.2448,
      "step": 27920
    },
    {
      "epoch": 1.6961195117507741,
      "grad_norm": 4.012642860412598,
      "learning_rate": 3.980324262177953e-05,
      "loss": 2.2102,
      "step": 27930
    },
    {
      "epoch": 1.6967267869071476,
      "grad_norm": 4.386782646179199,
      "learning_rate": 3.977211606284841e-05,
      "loss": 2.3454,
      "step": 27940
    },
    {
      "epoch": 1.697334062063521,
      "grad_norm": 3.63000750541687,
      "learning_rate": 3.974099364023648e-05,
      "loss": 2.0885,
      "step": 27950
    },
    {
      "epoch": 1.6979413372198944,
      "grad_norm": 5.597184181213379,
      "learning_rate": 3.970987536653011e-05,
      "loss": 2.6029,
      "step": 27960
    },
    {
      "epoch": 1.6985486123762676,
      "grad_norm": 4.657301902770996,
      "learning_rate": 3.967876125431402e-05,
      "loss": 2.4755,
      "step": 27970
    },
    {
      "epoch": 1.699155887532641,
      "grad_norm": 3.962918758392334,
      "learning_rate": 3.964765131617124e-05,
      "loss": 2.1453,
      "step": 27980
    },
    {
      "epoch": 1.6997631626890144,
      "grad_norm": 2.153668165206909,
      "learning_rate": 3.961654556468318e-05,
      "loss": 2.0995,
      "step": 27990
    },
    {
      "epoch": 1.7003704378453879,
      "grad_norm": 5.173385143280029,
      "learning_rate": 3.9585444012429445e-05,
      "loss": 2.1885,
      "step": 28000
    },
    {
      "epoch": 1.7009777130017611,
      "grad_norm": 3.585767984390259,
      "learning_rate": 3.9554346671988e-05,
      "loss": 2.3973,
      "step": 28010
    },
    {
      "epoch": 1.7015849881581344,
      "grad_norm": 4.062071323394775,
      "learning_rate": 3.9523253555935146e-05,
      "loss": 2.3519,
      "step": 28020
    },
    {
      "epoch": 1.7021922633145077,
      "grad_norm": 4.014684200286865,
      "learning_rate": 3.9492164676845416e-05,
      "loss": 2.0526,
      "step": 28030
    },
    {
      "epoch": 1.7027995384708812,
      "grad_norm": 3.920415163040161,
      "learning_rate": 3.946108004729164e-05,
      "loss": 2.1318,
      "step": 28040
    },
    {
      "epoch": 1.7034068136272547,
      "grad_norm": 4.359710216522217,
      "learning_rate": 3.942999967984491e-05,
      "loss": 2.1336,
      "step": 28050
    },
    {
      "epoch": 1.704014088783628,
      "grad_norm": 3.1690070629119873,
      "learning_rate": 3.939892358707469e-05,
      "loss": 2.281,
      "step": 28060
    },
    {
      "epoch": 1.7046213639400012,
      "grad_norm": 3.121039867401123,
      "learning_rate": 3.936785178154859e-05,
      "loss": 2.2921,
      "step": 28070
    },
    {
      "epoch": 1.7052286390963745,
      "grad_norm": 4.192025661468506,
      "learning_rate": 3.9336784275832575e-05,
      "loss": 2.3148,
      "step": 28080
    },
    {
      "epoch": 1.705835914252748,
      "grad_norm": 3.7839038372039795,
      "learning_rate": 3.93057210824908e-05,
      "loss": 2.3732,
      "step": 28090
    },
    {
      "epoch": 1.7064431894091214,
      "grad_norm": 3.712007999420166,
      "learning_rate": 3.9274662214085756e-05,
      "loss": 2.4014,
      "step": 28100
    },
    {
      "epoch": 1.7070504645654947,
      "grad_norm": 3.086974859237671,
      "learning_rate": 3.924360768317813e-05,
      "loss": 2.2477,
      "step": 28110
    },
    {
      "epoch": 1.707657739721868,
      "grad_norm": 2.5442192554473877,
      "learning_rate": 3.9212557502326866e-05,
      "loss": 2.2621,
      "step": 28120
    },
    {
      "epoch": 1.7082650148782412,
      "grad_norm": 3.6955668926239014,
      "learning_rate": 3.9181511684089146e-05,
      "loss": 2.1399,
      "step": 28130
    },
    {
      "epoch": 1.7088722900346147,
      "grad_norm": 2.3946049213409424,
      "learning_rate": 3.915047024102041e-05,
      "loss": 2.1176,
      "step": 28140
    },
    {
      "epoch": 1.709479565190988,
      "grad_norm": 3.4081170558929443,
      "learning_rate": 3.9119433185674305e-05,
      "loss": 2.1386,
      "step": 28150
    },
    {
      "epoch": 1.7100868403473615,
      "grad_norm": 2.8230443000793457,
      "learning_rate": 3.9088400530602705e-05,
      "loss": 2.1215,
      "step": 28160
    },
    {
      "epoch": 1.7106941155037347,
      "grad_norm": 2.4077138900756836,
      "learning_rate": 3.90573722883557e-05,
      "loss": 2.013,
      "step": 28170
    },
    {
      "epoch": 1.711301390660108,
      "grad_norm": 4.68579626083374,
      "learning_rate": 3.902634847148163e-05,
      "loss": 2.2633,
      "step": 28180
    },
    {
      "epoch": 1.7119086658164815,
      "grad_norm": 3.848534107208252,
      "learning_rate": 3.899532909252701e-05,
      "loss": 2.3572,
      "step": 28190
    },
    {
      "epoch": 1.7125159409728548,
      "grad_norm": 2.225227117538452,
      "learning_rate": 3.896431416403657e-05,
      "loss": 2.0723,
      "step": 28200
    },
    {
      "epoch": 1.7131232161292282,
      "grad_norm": 2.061765193939209,
      "learning_rate": 3.893330369855323e-05,
      "loss": 2.0324,
      "step": 28210
    },
    {
      "epoch": 1.7137304912856015,
      "grad_norm": 2.3736889362335205,
      "learning_rate": 3.8902297708618135e-05,
      "loss": 1.8966,
      "step": 28220
    },
    {
      "epoch": 1.7143377664419748,
      "grad_norm": 3.5192577838897705,
      "learning_rate": 3.88712962067706e-05,
      "loss": 1.963,
      "step": 28230
    },
    {
      "epoch": 1.7149450415983483,
      "grad_norm": 4.7576727867126465,
      "learning_rate": 3.88402992055481e-05,
      "loss": 2.22,
      "step": 28240
    },
    {
      "epoch": 1.7155523167547215,
      "grad_norm": 3.8506808280944824,
      "learning_rate": 3.880930671748635e-05,
      "loss": 2.3736,
      "step": 28250
    },
    {
      "epoch": 1.716159591911095,
      "grad_norm": 4.0749897956848145,
      "learning_rate": 3.877831875511919e-05,
      "loss": 2.2972,
      "step": 28260
    },
    {
      "epoch": 1.7167668670674683,
      "grad_norm": 2.4438211917877197,
      "learning_rate": 3.8747335330978656e-05,
      "loss": 2.1032,
      "step": 28270
    },
    {
      "epoch": 1.7173741422238415,
      "grad_norm": 4.485044479370117,
      "learning_rate": 3.87163564575949e-05,
      "loss": 2.2399,
      "step": 28280
    },
    {
      "epoch": 1.7179814173802148,
      "grad_norm": 3.236790180206299,
      "learning_rate": 3.868538214749632e-05,
      "loss": 2.1016,
      "step": 28290
    },
    {
      "epoch": 1.7185886925365883,
      "grad_norm": 4.819118022918701,
      "learning_rate": 3.8654412413209404e-05,
      "loss": 2.2669,
      "step": 28300
    },
    {
      "epoch": 1.7191959676929618,
      "grad_norm": 3.686511278152466,
      "learning_rate": 3.8623447267258806e-05,
      "loss": 2.1326,
      "step": 28310
    },
    {
      "epoch": 1.719803242849335,
      "grad_norm": 3.001896619796753,
      "learning_rate": 3.8592486722167296e-05,
      "loss": 2.0536,
      "step": 28320
    },
    {
      "epoch": 1.7204105180057083,
      "grad_norm": 2.889878749847412,
      "learning_rate": 3.856153079045585e-05,
      "loss": 1.9904,
      "step": 28330
    },
    {
      "epoch": 1.7210177931620816,
      "grad_norm": 2.333496332168579,
      "learning_rate": 3.8530579484643544e-05,
      "loss": 2.0809,
      "step": 28340
    },
    {
      "epoch": 1.721625068318455,
      "grad_norm": 3.4279332160949707,
      "learning_rate": 3.849963281724754e-05,
      "loss": 2.1805,
      "step": 28350
    },
    {
      "epoch": 1.7222323434748286,
      "grad_norm": 3.4271457195281982,
      "learning_rate": 3.846869080078317e-05,
      "loss": 2.1171,
      "step": 28360
    },
    {
      "epoch": 1.7228396186312018,
      "grad_norm": 2.649402618408203,
      "learning_rate": 3.843775344776393e-05,
      "loss": 1.902,
      "step": 28370
    },
    {
      "epoch": 1.723446893787575,
      "grad_norm": 2.025508165359497,
      "learning_rate": 3.840682077070132e-05,
      "loss": 1.8303,
      "step": 28380
    },
    {
      "epoch": 1.7240541689439484,
      "grad_norm": 4.258204936981201,
      "learning_rate": 3.837589278210503e-05,
      "loss": 2.053,
      "step": 28390
    },
    {
      "epoch": 1.7246614441003218,
      "grad_norm": 2.471703290939331,
      "learning_rate": 3.834496949448283e-05,
      "loss": 2.1518,
      "step": 28400
    },
    {
      "epoch": 1.7252687192566953,
      "grad_norm": 3.574249267578125,
      "learning_rate": 3.831405092034058e-05,
      "loss": 2.2813,
      "step": 28410
    },
    {
      "epoch": 1.7258759944130686,
      "grad_norm": 2.3300235271453857,
      "learning_rate": 3.8283137072182265e-05,
      "loss": 2.1626,
      "step": 28420
    },
    {
      "epoch": 1.7264832695694419,
      "grad_norm": 3.100419282913208,
      "learning_rate": 3.825222796250993e-05,
      "loss": 2.0675,
      "step": 28430
    },
    {
      "epoch": 1.7270905447258151,
      "grad_norm": 3.0691540241241455,
      "learning_rate": 3.822132360382369e-05,
      "loss": 2.271,
      "step": 28440
    },
    {
      "epoch": 1.7276978198821886,
      "grad_norm": 5.224020004272461,
      "learning_rate": 3.8190424008621784e-05,
      "loss": 2.4269,
      "step": 28450
    },
    {
      "epoch": 1.7283050950385621,
      "grad_norm": 4.5402607917785645,
      "learning_rate": 3.8159529189400496e-05,
      "loss": 2.6732,
      "step": 28460
    },
    {
      "epoch": 1.7289123701949354,
      "grad_norm": 3.7410695552825928,
      "learning_rate": 3.812863915865417e-05,
      "loss": 2.3402,
      "step": 28470
    },
    {
      "epoch": 1.7295196453513086,
      "grad_norm": 3.4132959842681885,
      "learning_rate": 3.809775392887525e-05,
      "loss": 2.4028,
      "step": 28480
    },
    {
      "epoch": 1.730126920507682,
      "grad_norm": 3.51007342338562,
      "learning_rate": 3.806687351255419e-05,
      "loss": 2.2184,
      "step": 28490
    },
    {
      "epoch": 1.7307341956640554,
      "grad_norm": 2.7847867012023926,
      "learning_rate": 3.803599792217955e-05,
      "loss": 2.0577,
      "step": 28500
    },
    {
      "epoch": 1.7313414708204289,
      "grad_norm": 2.3529880046844482,
      "learning_rate": 3.800512717023786e-05,
      "loss": 1.7888,
      "step": 28510
    },
    {
      "epoch": 1.7319487459768022,
      "grad_norm": 2.6601932048797607,
      "learning_rate": 3.797426126921381e-05,
      "loss": 2.0276,
      "step": 28520
    },
    {
      "epoch": 1.7325560211331754,
      "grad_norm": 2.399669647216797,
      "learning_rate": 3.794340023159002e-05,
      "loss": 2.1074,
      "step": 28530
    },
    {
      "epoch": 1.7331632962895487,
      "grad_norm": 2.5610597133636475,
      "learning_rate": 3.79125440698472e-05,
      "loss": 2.1713,
      "step": 28540
    },
    {
      "epoch": 1.7337705714459222,
      "grad_norm": 3.4344072341918945,
      "learning_rate": 3.788169279646405e-05,
      "loss": 2.0915,
      "step": 28550
    },
    {
      "epoch": 1.7343778466022957,
      "grad_norm": 2.3427646160125732,
      "learning_rate": 3.785084642391734e-05,
      "loss": 2.1148,
      "step": 28560
    },
    {
      "epoch": 1.734985121758669,
      "grad_norm": 4.562056541442871,
      "learning_rate": 3.782000496468184e-05,
      "loss": 2.102,
      "step": 28570
    },
    {
      "epoch": 1.7355923969150422,
      "grad_norm": 4.375527858734131,
      "learning_rate": 3.778916843123031e-05,
      "loss": 2.2922,
      "step": 28580
    },
    {
      "epoch": 1.7361996720714155,
      "grad_norm": 3.1473476886749268,
      "learning_rate": 3.775833683603353e-05,
      "loss": 2.2001,
      "step": 28590
    },
    {
      "epoch": 1.736806947227789,
      "grad_norm": 3.639951229095459,
      "learning_rate": 3.772751019156031e-05,
      "loss": 2.2607,
      "step": 28600
    },
    {
      "epoch": 1.7374142223841622,
      "grad_norm": 3.503774404525757,
      "learning_rate": 3.769668851027744e-05,
      "loss": 2.0513,
      "step": 28610
    },
    {
      "epoch": 1.7380214975405357,
      "grad_norm": 4.471086502075195,
      "learning_rate": 3.766587180464967e-05,
      "loss": 1.9947,
      "step": 28620
    },
    {
      "epoch": 1.738628772696909,
      "grad_norm": 3.8541276454925537,
      "learning_rate": 3.763506008713976e-05,
      "loss": 2.1959,
      "step": 28630
    },
    {
      "epoch": 1.7392360478532822,
      "grad_norm": 3.135408639907837,
      "learning_rate": 3.76042533702085e-05,
      "loss": 2.2633,
      "step": 28640
    },
    {
      "epoch": 1.7398433230096557,
      "grad_norm": 3.1705636978149414,
      "learning_rate": 3.7573451666314614e-05,
      "loss": 2.3698,
      "step": 28650
    },
    {
      "epoch": 1.740450598166029,
      "grad_norm": 3.3741672039031982,
      "learning_rate": 3.7542654987914763e-05,
      "loss": 2.0097,
      "step": 28660
    },
    {
      "epoch": 1.7410578733224025,
      "grad_norm": 4.545412540435791,
      "learning_rate": 3.751186334746362e-05,
      "loss": 2.3221,
      "step": 28670
    },
    {
      "epoch": 1.7416651484787757,
      "grad_norm": 4.432215690612793,
      "learning_rate": 3.748107675741386e-05,
      "loss": 2.2535,
      "step": 28680
    },
    {
      "epoch": 1.742272423635149,
      "grad_norm": 4.502221584320068,
      "learning_rate": 3.7450295230216036e-05,
      "loss": 2.4518,
      "step": 28690
    },
    {
      "epoch": 1.7428796987915225,
      "grad_norm": 4.1740264892578125,
      "learning_rate": 3.7419518778318666e-05,
      "loss": 2.2002,
      "step": 28700
    },
    {
      "epoch": 1.7434869739478958,
      "grad_norm": 3.4072484970092773,
      "learning_rate": 3.73887474141683e-05,
      "loss": 2.2111,
      "step": 28710
    },
    {
      "epoch": 1.7440942491042692,
      "grad_norm": 2.915781021118164,
      "learning_rate": 3.735798115020932e-05,
      "loss": 2.2357,
      "step": 28720
    },
    {
      "epoch": 1.7447015242606425,
      "grad_norm": 3.6889283657073975,
      "learning_rate": 3.73272199988841e-05,
      "loss": 2.2313,
      "step": 28730
    },
    {
      "epoch": 1.7453087994170158,
      "grad_norm": 3.3265507221221924,
      "learning_rate": 3.729646397263292e-05,
      "loss": 2.3027,
      "step": 28740
    },
    {
      "epoch": 1.745916074573389,
      "grad_norm": 4.303915500640869,
      "learning_rate": 3.7265713083894074e-05,
      "loss": 2.8313,
      "step": 28750
    },
    {
      "epoch": 1.7465233497297625,
      "grad_norm": 3.146467685699463,
      "learning_rate": 3.723496734510365e-05,
      "loss": 2.5167,
      "step": 28760
    },
    {
      "epoch": 1.747130624886136,
      "grad_norm": 2.8309128284454346,
      "learning_rate": 3.720422676869573e-05,
      "loss": 2.4091,
      "step": 28770
    },
    {
      "epoch": 1.7477379000425093,
      "grad_norm": 4.087080001831055,
      "learning_rate": 3.717349136710229e-05,
      "loss": 2.1591,
      "step": 28780
    },
    {
      "epoch": 1.7483451751988826,
      "grad_norm": 1.845767855644226,
      "learning_rate": 3.714276115275324e-05,
      "loss": 1.8527,
      "step": 28790
    },
    {
      "epoch": 1.7489524503552558,
      "grad_norm": 4.450202465057373,
      "learning_rate": 3.7112036138076336e-05,
      "loss": 2.1846,
      "step": 28800
    },
    {
      "epoch": 1.7495597255116293,
      "grad_norm": 3.234703779220581,
      "learning_rate": 3.708131633549727e-05,
      "loss": 2.4447,
      "step": 28810
    },
    {
      "epoch": 1.7501670006680028,
      "grad_norm": 4.824720859527588,
      "learning_rate": 3.705060175743963e-05,
      "loss": 2.3101,
      "step": 28820
    },
    {
      "epoch": 1.750774275824376,
      "grad_norm": 5.814229488372803,
      "learning_rate": 3.7019892416324885e-05,
      "loss": 2.3441,
      "step": 28830
    },
    {
      "epoch": 1.7513815509807493,
      "grad_norm": 2.0378735065460205,
      "learning_rate": 3.6989188324572375e-05,
      "loss": 1.9773,
      "step": 28840
    },
    {
      "epoch": 1.7519888261371226,
      "grad_norm": 6.027170181274414,
      "learning_rate": 3.695848949459932e-05,
      "loss": 2.1877,
      "step": 28850
    },
    {
      "epoch": 1.752596101293496,
      "grad_norm": 5.188862323760986,
      "learning_rate": 3.69277959388208e-05,
      "loss": 2.2029,
      "step": 28860
    },
    {
      "epoch": 1.7532033764498696,
      "grad_norm": 4.559175491333008,
      "learning_rate": 3.689710766964982e-05,
      "loss": 2.4068,
      "step": 28870
    },
    {
      "epoch": 1.7538106516062428,
      "grad_norm": 4.471987247467041,
      "learning_rate": 3.686642469949717e-05,
      "loss": 2.4036,
      "step": 28880
    },
    {
      "epoch": 1.754417926762616,
      "grad_norm": 4.866244316101074,
      "learning_rate": 3.683574704077154e-05,
      "loss": 2.304,
      "step": 28890
    },
    {
      "epoch": 1.7550252019189894,
      "grad_norm": 3.7201175689697266,
      "learning_rate": 3.680507470587946e-05,
      "loss": 2.335,
      "step": 28900
    },
    {
      "epoch": 1.7556324770753629,
      "grad_norm": 3.877960205078125,
      "learning_rate": 3.6774407707225325e-05,
      "loss": 2.0983,
      "step": 28910
    },
    {
      "epoch": 1.7562397522317363,
      "grad_norm": 4.01998233795166,
      "learning_rate": 3.6743746057211354e-05,
      "loss": 2.1544,
      "step": 28920
    },
    {
      "epoch": 1.7568470273881096,
      "grad_norm": 4.304121971130371,
      "learning_rate": 3.671308976823759e-05,
      "loss": 2.1393,
      "step": 28930
    },
    {
      "epoch": 1.7574543025444829,
      "grad_norm": 4.875927448272705,
      "learning_rate": 3.6682438852701946e-05,
      "loss": 2.2033,
      "step": 28940
    },
    {
      "epoch": 1.7580615777008561,
      "grad_norm": 6.2593889236450195,
      "learning_rate": 3.6651793323000125e-05,
      "loss": 2.3357,
      "step": 28950
    },
    {
      "epoch": 1.7586688528572296,
      "grad_norm": 3.72263765335083,
      "learning_rate": 3.6621153191525695e-05,
      "loss": 2.1942,
      "step": 28960
    },
    {
      "epoch": 1.7592761280136031,
      "grad_norm": 3.9173479080200195,
      "learning_rate": 3.659051847066995e-05,
      "loss": 2.5576,
      "step": 28970
    },
    {
      "epoch": 1.7598834031699764,
      "grad_norm": 3.8829100131988525,
      "learning_rate": 3.655988917282212e-05,
      "loss": 2.4466,
      "step": 28980
    },
    {
      "epoch": 1.7604906783263496,
      "grad_norm": 3.487576961517334,
      "learning_rate": 3.6529265310369176e-05,
      "loss": 2.2167,
      "step": 28990
    },
    {
      "epoch": 1.761097953482723,
      "grad_norm": 3.7619943618774414,
      "learning_rate": 3.649864689569587e-05,
      "loss": 2.4366,
      "step": 29000
    },
    {
      "epoch": 1.7617052286390964,
      "grad_norm": 3.206770658493042,
      "learning_rate": 3.646803394118477e-05,
      "loss": 2.2699,
      "step": 29010
    },
    {
      "epoch": 1.7623125037954699,
      "grad_norm": 2.9066884517669678,
      "learning_rate": 3.643742645921629e-05,
      "loss": 2.0676,
      "step": 29020
    },
    {
      "epoch": 1.7629197789518432,
      "grad_norm": 3.6263394355773926,
      "learning_rate": 3.640682446216854e-05,
      "loss": 1.9556,
      "step": 29030
    },
    {
      "epoch": 1.7635270541082164,
      "grad_norm": 2.492509126663208,
      "learning_rate": 3.637622796241746e-05,
      "loss": 2.1696,
      "step": 29040
    },
    {
      "epoch": 1.7641343292645897,
      "grad_norm": 3.0752084255218506,
      "learning_rate": 3.634563697233676e-05,
      "loss": 2.0399,
      "step": 29050
    },
    {
      "epoch": 1.7647416044209632,
      "grad_norm": 4.694231033325195,
      "learning_rate": 3.6315051504297965e-05,
      "loss": 2.3849,
      "step": 29060
    },
    {
      "epoch": 1.7653488795773364,
      "grad_norm": 4.5262041091918945,
      "learning_rate": 3.628447157067028e-05,
      "loss": 2.5981,
      "step": 29070
    },
    {
      "epoch": 1.76595615473371,
      "grad_norm": 4.383108615875244,
      "learning_rate": 3.6253897183820726e-05,
      "loss": 2.4689,
      "step": 29080
    },
    {
      "epoch": 1.7665634298900832,
      "grad_norm": 5.154463768005371,
      "learning_rate": 3.622332835611407e-05,
      "loss": 2.2462,
      "step": 29090
    },
    {
      "epoch": 1.7671707050464565,
      "grad_norm": 3.9106662273406982,
      "learning_rate": 3.6192765099912857e-05,
      "loss": 2.2403,
      "step": 29100
    },
    {
      "epoch": 1.76777798020283,
      "grad_norm": 3.792994260787964,
      "learning_rate": 3.6162207427577336e-05,
      "loss": 2.1273,
      "step": 29110
    },
    {
      "epoch": 1.7683852553592032,
      "grad_norm": 3.3829798698425293,
      "learning_rate": 3.613165535146551e-05,
      "loss": 2.134,
      "step": 29120
    },
    {
      "epoch": 1.7689925305155767,
      "grad_norm": 4.809392929077148,
      "learning_rate": 3.610110888393312e-05,
      "loss": 2.5105,
      "step": 29130
    },
    {
      "epoch": 1.76959980567195,
      "grad_norm": 3.810703992843628,
      "learning_rate": 3.6070568037333674e-05,
      "loss": 2.4485,
      "step": 29140
    },
    {
      "epoch": 1.7702070808283232,
      "grad_norm": 3.369722604751587,
      "learning_rate": 3.604003282401835e-05,
      "loss": 2.2736,
      "step": 29150
    },
    {
      "epoch": 1.7708143559846967,
      "grad_norm": 1.9948097467422485,
      "learning_rate": 3.6009503256336086e-05,
      "loss": 1.9489,
      "step": 29160
    },
    {
      "epoch": 1.77142163114107,
      "grad_norm": 2.2269747257232666,
      "learning_rate": 3.597897934663353e-05,
      "loss": 1.9552,
      "step": 29170
    },
    {
      "epoch": 1.7720289062974435,
      "grad_norm": 3.1660056114196777,
      "learning_rate": 3.594846110725503e-05,
      "loss": 2.1011,
      "step": 29180
    },
    {
      "epoch": 1.7726361814538167,
      "grad_norm": 3.9340898990631104,
      "learning_rate": 3.5917948550542645e-05,
      "loss": 2.3086,
      "step": 29190
    },
    {
      "epoch": 1.77324345661019,
      "grad_norm": 4.366775035858154,
      "learning_rate": 3.588744168883613e-05,
      "loss": 2.4269,
      "step": 29200
    },
    {
      "epoch": 1.7738507317665633,
      "grad_norm": 3.8838980197906494,
      "learning_rate": 3.585694053447298e-05,
      "loss": 2.091,
      "step": 29210
    },
    {
      "epoch": 1.7744580069229368,
      "grad_norm": 3.663719892501831,
      "learning_rate": 3.582644509978832e-05,
      "loss": 2.3443,
      "step": 29220
    },
    {
      "epoch": 1.7750652820793102,
      "grad_norm": 2.8459408283233643,
      "learning_rate": 3.5795955397114996e-05,
      "loss": 1.9989,
      "step": 29230
    },
    {
      "epoch": 1.7756725572356835,
      "grad_norm": 2.623260021209717,
      "learning_rate": 3.576547143878351e-05,
      "loss": 2.0653,
      "step": 29240
    },
    {
      "epoch": 1.7762798323920568,
      "grad_norm": 3.8766791820526123,
      "learning_rate": 3.57349932371221e-05,
      "loss": 2.315,
      "step": 29250
    },
    {
      "epoch": 1.77688710754843,
      "grad_norm": 3.181497573852539,
      "learning_rate": 3.570452080445661e-05,
      "loss": 2.1819,
      "step": 29260
    },
    {
      "epoch": 1.7774943827048035,
      "grad_norm": 4.672824859619141,
      "learning_rate": 3.5674054153110596e-05,
      "loss": 2.2738,
      "step": 29270
    },
    {
      "epoch": 1.778101657861177,
      "grad_norm": 2.635887861251831,
      "learning_rate": 3.5643593295405206e-05,
      "loss": 1.9797,
      "step": 29280
    },
    {
      "epoch": 1.7787089330175503,
      "grad_norm": 3.249037742614746,
      "learning_rate": 3.561313824365936e-05,
      "loss": 2.0641,
      "step": 29290
    },
    {
      "epoch": 1.7793162081739236,
      "grad_norm": 3.1645348072052,
      "learning_rate": 3.558268901018955e-05,
      "loss": 2.2976,
      "step": 29300
    },
    {
      "epoch": 1.7799234833302968,
      "grad_norm": 3.638662099838257,
      "learning_rate": 3.555224560730991e-05,
      "loss": 2.2285,
      "step": 29310
    },
    {
      "epoch": 1.7805307584866703,
      "grad_norm": 4.620377063751221,
      "learning_rate": 3.552180804733222e-05,
      "loss": 2.3816,
      "step": 29320
    },
    {
      "epoch": 1.7811380336430438,
      "grad_norm": 3.7305662631988525,
      "learning_rate": 3.549137634256596e-05,
      "loss": 2.2888,
      "step": 29330
    },
    {
      "epoch": 1.781745308799417,
      "grad_norm": 2.54982590675354,
      "learning_rate": 3.54609505053182e-05,
      "loss": 2.1194,
      "step": 29340
    },
    {
      "epoch": 1.7823525839557903,
      "grad_norm": 4.497311592102051,
      "learning_rate": 3.543053054789359e-05,
      "loss": 2.184,
      "step": 29350
    },
    {
      "epoch": 1.7829598591121636,
      "grad_norm": 2.544084310531616,
      "learning_rate": 3.540011648259445e-05,
      "loss": 1.9673,
      "step": 29360
    },
    {
      "epoch": 1.783567134268537,
      "grad_norm": 2.8805665969848633,
      "learning_rate": 3.536970832172076e-05,
      "loss": 2.0871,
      "step": 29370
    },
    {
      "epoch": 1.7841744094249106,
      "grad_norm": 2.1003975868225098,
      "learning_rate": 3.533930607757002e-05,
      "loss": 2.0323,
      "step": 29380
    },
    {
      "epoch": 1.7847816845812838,
      "grad_norm": 3.17910099029541,
      "learning_rate": 3.530890976243737e-05,
      "loss": 2.108,
      "step": 29390
    },
    {
      "epoch": 1.785388959737657,
      "grad_norm": 2.6107256412506104,
      "learning_rate": 3.527851938861563e-05,
      "loss": 2.0528,
      "step": 29400
    },
    {
      "epoch": 1.7859962348940304,
      "grad_norm": 3.7466540336608887,
      "learning_rate": 3.524813496839509e-05,
      "loss": 2.2043,
      "step": 29410
    },
    {
      "epoch": 1.7866035100504039,
      "grad_norm": 2.7366459369659424,
      "learning_rate": 3.521775651406371e-05,
      "loss": 2.2158,
      "step": 29420
    },
    {
      "epoch": 1.7872107852067773,
      "grad_norm": 3.135611057281494,
      "learning_rate": 3.518738403790702e-05,
      "loss": 2.1208,
      "step": 29430
    },
    {
      "epoch": 1.7878180603631506,
      "grad_norm": 4.242175102233887,
      "learning_rate": 3.515701755220814e-05,
      "loss": 2.1948,
      "step": 29440
    },
    {
      "epoch": 1.7884253355195239,
      "grad_norm": 3.246206045150757,
      "learning_rate": 3.512665706924776e-05,
      "loss": 2.2455,
      "step": 29450
    },
    {
      "epoch": 1.7890326106758971,
      "grad_norm": 4.138006687164307,
      "learning_rate": 3.5096302601304146e-05,
      "loss": 2.4778,
      "step": 29460
    },
    {
      "epoch": 1.7896398858322706,
      "grad_norm": 4.719090461730957,
      "learning_rate": 3.506595416065311e-05,
      "loss": 2.3553,
      "step": 29470
    },
    {
      "epoch": 1.7902471609886441,
      "grad_norm": 4.51827335357666,
      "learning_rate": 3.5035611759568065e-05,
      "loss": 2.2632,
      "step": 29480
    },
    {
      "epoch": 1.7908544361450174,
      "grad_norm": 3.81646728515625,
      "learning_rate": 3.500527541031995e-05,
      "loss": 2.2461,
      "step": 29490
    },
    {
      "epoch": 1.7914617113013906,
      "grad_norm": 3.2723021507263184,
      "learning_rate": 3.497494512517727e-05,
      "loss": 2.0996,
      "step": 29500
    },
    {
      "epoch": 1.792068986457764,
      "grad_norm": 3.7276010513305664,
      "learning_rate": 3.4944620916406076e-05,
      "loss": 2.1293,
      "step": 29510
    },
    {
      "epoch": 1.7926762616141374,
      "grad_norm": 3.0106630325317383,
      "learning_rate": 3.491430279626996e-05,
      "loss": 2.2975,
      "step": 29520
    },
    {
      "epoch": 1.7932835367705107,
      "grad_norm": 2.8248631954193115,
      "learning_rate": 3.4883990777030065e-05,
      "loss": 2.1391,
      "step": 29530
    },
    {
      "epoch": 1.7938908119268842,
      "grad_norm": 3.4093382358551025,
      "learning_rate": 3.4853684870945044e-05,
      "loss": 2.3276,
      "step": 29540
    },
    {
      "epoch": 1.7944980870832574,
      "grad_norm": 3.8942930698394775,
      "learning_rate": 3.482338509027107e-05,
      "loss": 2.1655,
      "step": 29550
    },
    {
      "epoch": 1.7951053622396307,
      "grad_norm": 2.8302478790283203,
      "learning_rate": 3.479309144726188e-05,
      "loss": 2.172,
      "step": 29560
    },
    {
      "epoch": 1.7957126373960042,
      "grad_norm": 3.947446584701538,
      "learning_rate": 3.476280395416871e-05,
      "loss": 2.2865,
      "step": 29570
    },
    {
      "epoch": 1.7963199125523774,
      "grad_norm": 3.7214839458465576,
      "learning_rate": 3.4732522623240304e-05,
      "loss": 2.1465,
      "step": 29580
    },
    {
      "epoch": 1.796927187708751,
      "grad_norm": 2.47717547416687,
      "learning_rate": 3.470224746672287e-05,
      "loss": 1.9049,
      "step": 29590
    },
    {
      "epoch": 1.7975344628651242,
      "grad_norm": 4.568535804748535,
      "learning_rate": 3.4671978496860226e-05,
      "loss": 2.2104,
      "step": 29600
    },
    {
      "epoch": 1.7981417380214975,
      "grad_norm": 4.712090492248535,
      "learning_rate": 3.464171572589359e-05,
      "loss": 2.3629,
      "step": 29610
    },
    {
      "epoch": 1.798749013177871,
      "grad_norm": 3.1815061569213867,
      "learning_rate": 3.461145916606171e-05,
      "loss": 2.1225,
      "step": 29620
    },
    {
      "epoch": 1.7993562883342442,
      "grad_norm": 4.1395721435546875,
      "learning_rate": 3.458120882960083e-05,
      "loss": 2.0125,
      "step": 29630
    },
    {
      "epoch": 1.7999635634906177,
      "grad_norm": 3.5228750705718994,
      "learning_rate": 3.455096472874467e-05,
      "loss": 2.173,
      "step": 29640
    },
    {
      "epoch": 1.800570838646991,
      "grad_norm": 3.863973617553711,
      "learning_rate": 3.452072687572444e-05,
      "loss": 2.2178,
      "step": 29650
    },
    {
      "epoch": 1.8011781138033642,
      "grad_norm": 2.9899041652679443,
      "learning_rate": 3.4490495282768754e-05,
      "loss": 2.0975,
      "step": 29660
    },
    {
      "epoch": 1.8017853889597375,
      "grad_norm": 4.400702476501465,
      "learning_rate": 3.446026996210381e-05,
      "loss": 2.0866,
      "step": 29670
    },
    {
      "epoch": 1.802392664116111,
      "grad_norm": 4.431752681732178,
      "learning_rate": 3.4430050925953186e-05,
      "loss": 2.3789,
      "step": 29680
    },
    {
      "epoch": 1.8029999392724845,
      "grad_norm": 3.60211443901062,
      "learning_rate": 3.439983818653794e-05,
      "loss": 2.1513,
      "step": 29690
    },
    {
      "epoch": 1.8036072144288577,
      "grad_norm": 2.9967007637023926,
      "learning_rate": 3.436963175607656e-05,
      "loss": 1.9857,
      "step": 29700
    },
    {
      "epoch": 1.804214489585231,
      "grad_norm": 3.9019453525543213,
      "learning_rate": 3.433943164678506e-05,
      "loss": 2.488,
      "step": 29710
    },
    {
      "epoch": 1.8048217647416043,
      "grad_norm": 3.6109819412231445,
      "learning_rate": 3.430923787087682e-05,
      "loss": 2.2422,
      "step": 29720
    },
    {
      "epoch": 1.8054290398979778,
      "grad_norm": 3.909360647201538,
      "learning_rate": 3.427905044056267e-05,
      "loss": 2.1767,
      "step": 29730
    },
    {
      "epoch": 1.8060363150543512,
      "grad_norm": 3.8528621196746826,
      "learning_rate": 3.42488693680509e-05,
      "loss": 2.1185,
      "step": 29740
    },
    {
      "epoch": 1.8066435902107245,
      "grad_norm": 4.439741611480713,
      "learning_rate": 3.421869466554722e-05,
      "loss": 2.2661,
      "step": 29750
    },
    {
      "epoch": 1.8072508653670978,
      "grad_norm": 2.947970390319824,
      "learning_rate": 3.4188526345254754e-05,
      "loss": 2.2671,
      "step": 29760
    },
    {
      "epoch": 1.807858140523471,
      "grad_norm": 2.8109447956085205,
      "learning_rate": 3.4158364419374065e-05,
      "loss": 1.9516,
      "step": 29770
    },
    {
      "epoch": 1.8084654156798445,
      "grad_norm": 2.3424575328826904,
      "learning_rate": 3.412820890010309e-05,
      "loss": 2.315,
      "step": 29780
    },
    {
      "epoch": 1.809072690836218,
      "grad_norm": 2.335627555847168,
      "learning_rate": 3.4098059799637225e-05,
      "loss": 2.1629,
      "step": 29790
    },
    {
      "epoch": 1.8096799659925913,
      "grad_norm": 3.7684993743896484,
      "learning_rate": 3.406791713016924e-05,
      "loss": 2.1967,
      "step": 29800
    },
    {
      "epoch": 1.8102872411489646,
      "grad_norm": 3.31821870803833,
      "learning_rate": 3.40377809038893e-05,
      "loss": 2.2782,
      "step": 29810
    },
    {
      "epoch": 1.8108945163053378,
      "grad_norm": 3.221818685531616,
      "learning_rate": 3.400765113298498e-05,
      "loss": 2.1158,
      "step": 29820
    },
    {
      "epoch": 1.8115017914617113,
      "grad_norm": 4.599002361297607,
      "learning_rate": 3.397752782964125e-05,
      "loss": 2.3333,
      "step": 29830
    },
    {
      "epoch": 1.8121090666180848,
      "grad_norm": 3.389561176300049,
      "learning_rate": 3.394741100604045e-05,
      "loss": 2.4996,
      "step": 29840
    },
    {
      "epoch": 1.812716341774458,
      "grad_norm": 3.5737805366516113,
      "learning_rate": 3.391730067436229e-05,
      "loss": 2.4588,
      "step": 29850
    },
    {
      "epoch": 1.8133236169308313,
      "grad_norm": 4.4566521644592285,
      "learning_rate": 3.388719684678388e-05,
      "loss": 2.558,
      "step": 29860
    },
    {
      "epoch": 1.8139308920872046,
      "grad_norm": 2.25569748878479,
      "learning_rate": 3.385709953547968e-05,
      "loss": 2.1934,
      "step": 29870
    },
    {
      "epoch": 1.814538167243578,
      "grad_norm": 3.1225545406341553,
      "learning_rate": 3.382700875262154e-05,
      "loss": 2.1243,
      "step": 29880
    },
    {
      "epoch": 1.8151454423999516,
      "grad_norm": 2.853156328201294,
      "learning_rate": 3.3796924510378626e-05,
      "loss": 2.1653,
      "step": 29890
    },
    {
      "epoch": 1.8157527175563248,
      "grad_norm": 3.273750066757202,
      "learning_rate": 3.3766846820917494e-05,
      "loss": 2.2636,
      "step": 29900
    },
    {
      "epoch": 1.816359992712698,
      "grad_norm": 2.657320499420166,
      "learning_rate": 3.373677569640206e-05,
      "loss": 2.0813,
      "step": 29910
    },
    {
      "epoch": 1.8169672678690714,
      "grad_norm": 3.1588306427001953,
      "learning_rate": 3.370671114899353e-05,
      "loss": 2.373,
      "step": 29920
    },
    {
      "epoch": 1.8175745430254449,
      "grad_norm": 2.305241346359253,
      "learning_rate": 3.367665319085051e-05,
      "loss": 2.0686,
      "step": 29930
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 4.205333709716797,
      "learning_rate": 3.364660183412892e-05,
      "loss": 2.5142,
      "step": 29940
    },
    {
      "epoch": 1.8187890933381916,
      "grad_norm": 3.8090832233428955,
      "learning_rate": 3.361655709098199e-05,
      "loss": 2.2964,
      "step": 29950
    },
    {
      "epoch": 1.8193963684945649,
      "grad_norm": 4.228847026824951,
      "learning_rate": 3.358651897356032e-05,
      "loss": 2.2871,
      "step": 29960
    },
    {
      "epoch": 1.8200036436509381,
      "grad_norm": 4.725652694702148,
      "learning_rate": 3.3556487494011754e-05,
      "loss": 2.471,
      "step": 29970
    },
    {
      "epoch": 1.8206109188073116,
      "grad_norm": 3.472188711166382,
      "learning_rate": 3.352646266448155e-05,
      "loss": 2.2577,
      "step": 29980
    },
    {
      "epoch": 1.821218193963685,
      "grad_norm": 2.807032823562622,
      "learning_rate": 3.3496444497112226e-05,
      "loss": 2.1989,
      "step": 29990
    },
    {
      "epoch": 1.8218254691200584,
      "grad_norm": 3.7522432804107666,
      "learning_rate": 3.3466433004043565e-05,
      "loss": 2.3802,
      "step": 30000
    },
    {
      "epoch": 1.8221898342138823,
      "eval_loss": 3.1761550903320312,
      "eval_runtime": 2378.328,
      "eval_samples_per_second": 6.924,
      "eval_steps_per_second": 1.731,
      "step": 30006
    },
    {
      "epoch": 1.8224327442764316,
      "grad_norm": 5.951338291168213,
      "learning_rate": 3.3436428197412704e-05,
      "loss": 2.3994,
      "step": 30010
    },
    {
      "epoch": 1.823040019432805,
      "grad_norm": 4.392524719238281,
      "learning_rate": 3.340643008935411e-05,
      "loss": 2.9578,
      "step": 30020
    },
    {
      "epoch": 1.8236472945891784,
      "grad_norm": 3.9585652351379395,
      "learning_rate": 3.337643869199945e-05,
      "loss": 2.5017,
      "step": 30030
    },
    {
      "epoch": 1.8242545697455517,
      "grad_norm": 3.000150442123413,
      "learning_rate": 3.334645401747774e-05,
      "loss": 2.3998,
      "step": 30040
    },
    {
      "epoch": 1.8248618449019252,
      "grad_norm": 3.242999315261841,
      "learning_rate": 3.331647607791525e-05,
      "loss": 1.9752,
      "step": 30050
    },
    {
      "epoch": 1.8254691200582984,
      "grad_norm": 4.20572566986084,
      "learning_rate": 3.328650488543554e-05,
      "loss": 2.1226,
      "step": 30060
    },
    {
      "epoch": 1.8260763952146717,
      "grad_norm": 3.3339908123016357,
      "learning_rate": 3.3256540452159454e-05,
      "loss": 2.2035,
      "step": 30070
    },
    {
      "epoch": 1.826683670371045,
      "grad_norm": 3.355658531188965,
      "learning_rate": 3.322658279020506e-05,
      "loss": 2.3661,
      "step": 30080
    },
    {
      "epoch": 1.8272909455274184,
      "grad_norm": 3.555976629257202,
      "learning_rate": 3.319663191168772e-05,
      "loss": 2.2609,
      "step": 30090
    },
    {
      "epoch": 1.827898220683792,
      "grad_norm": 2.1124346256256104,
      "learning_rate": 3.316668782872007e-05,
      "loss": 2.2117,
      "step": 30100
    },
    {
      "epoch": 1.8285054958401652,
      "grad_norm": 3.7915945053100586,
      "learning_rate": 3.313675055341195e-05,
      "loss": 2.343,
      "step": 30110
    },
    {
      "epoch": 1.8291127709965385,
      "grad_norm": 3.4045987129211426,
      "learning_rate": 3.3106820097870463e-05,
      "loss": 2.3624,
      "step": 30120
    },
    {
      "epoch": 1.8297200461529117,
      "grad_norm": 1.7621084451675415,
      "learning_rate": 3.3076896474199995e-05,
      "loss": 2.036,
      "step": 30130
    },
    {
      "epoch": 1.8303273213092852,
      "grad_norm": 2.999809503555298,
      "learning_rate": 3.3046979694502113e-05,
      "loss": 2.166,
      "step": 30140
    },
    {
      "epoch": 1.8309345964656587,
      "grad_norm": 3.66879940032959,
      "learning_rate": 3.301706977087564e-05,
      "loss": 2.1432,
      "step": 30150
    },
    {
      "epoch": 1.831541871622032,
      "grad_norm": 2.887164354324341,
      "learning_rate": 3.298716671541662e-05,
      "loss": 2.2744,
      "step": 30160
    },
    {
      "epoch": 1.8321491467784052,
      "grad_norm": 3.29803204536438,
      "learning_rate": 3.2957270540218344e-05,
      "loss": 2.194,
      "step": 30170
    },
    {
      "epoch": 1.8327564219347785,
      "grad_norm": 2.7798616886138916,
      "learning_rate": 3.2927381257371294e-05,
      "loss": 2.0991,
      "step": 30180
    },
    {
      "epoch": 1.833363697091152,
      "grad_norm": 3.2272396087646484,
      "learning_rate": 3.2897498878963165e-05,
      "loss": 2.1713,
      "step": 30190
    },
    {
      "epoch": 1.8339709722475255,
      "grad_norm": 2.6412532329559326,
      "learning_rate": 3.286762341707886e-05,
      "loss": 2.1147,
      "step": 30200
    },
    {
      "epoch": 1.8345782474038987,
      "grad_norm": 3.1484925746917725,
      "learning_rate": 3.2837754883800506e-05,
      "loss": 2.1033,
      "step": 30210
    },
    {
      "epoch": 1.835185522560272,
      "grad_norm": 2.423694133758545,
      "learning_rate": 3.280789329120742e-05,
      "loss": 2.2141,
      "step": 30220
    },
    {
      "epoch": 1.8357927977166453,
      "grad_norm": 3.312063455581665,
      "learning_rate": 3.2778038651376096e-05,
      "loss": 2.3731,
      "step": 30230
    },
    {
      "epoch": 1.8364000728730188,
      "grad_norm": 3.2553324699401855,
      "learning_rate": 3.274819097638021e-05,
      "loss": 2.229,
      "step": 30240
    },
    {
      "epoch": 1.8370073480293923,
      "grad_norm": 4.202374458312988,
      "learning_rate": 3.271835027829067e-05,
      "loss": 2.3314,
      "step": 30250
    },
    {
      "epoch": 1.8376146231857655,
      "grad_norm": 4.106795787811279,
      "learning_rate": 3.2688516569175506e-05,
      "loss": 2.2687,
      "step": 30260
    },
    {
      "epoch": 1.8382218983421388,
      "grad_norm": 5.229618072509766,
      "learning_rate": 3.265868986109996e-05,
      "loss": 2.2317,
      "step": 30270
    },
    {
      "epoch": 1.838829173498512,
      "grad_norm": 2.9533872604370117,
      "learning_rate": 3.262887016612639e-05,
      "loss": 1.9284,
      "step": 30280
    },
    {
      "epoch": 1.8394364486548855,
      "grad_norm": 3.06048846244812,
      "learning_rate": 3.25990574963144e-05,
      "loss": 2.1825,
      "step": 30290
    },
    {
      "epoch": 1.840043723811259,
      "grad_norm": 2.650219440460205,
      "learning_rate": 3.256925186372071e-05,
      "loss": 2.034,
      "step": 30300
    },
    {
      "epoch": 1.8406509989676323,
      "grad_norm": 3.7328288555145264,
      "learning_rate": 3.2539453280399165e-05,
      "loss": 2.2825,
      "step": 30310
    },
    {
      "epoch": 1.8412582741240056,
      "grad_norm": 4.737518787384033,
      "learning_rate": 3.250966175840077e-05,
      "loss": 2.2787,
      "step": 30320
    },
    {
      "epoch": 1.8418655492803788,
      "grad_norm": 4.111830234527588,
      "learning_rate": 3.2479877309773756e-05,
      "loss": 2.1159,
      "step": 30330
    },
    {
      "epoch": 1.8424728244367523,
      "grad_norm": 3.5965847969055176,
      "learning_rate": 3.245009994656337e-05,
      "loss": 2.1592,
      "step": 30340
    },
    {
      "epoch": 1.8430800995931258,
      "grad_norm": 3.7265632152557373,
      "learning_rate": 3.242032968081207e-05,
      "loss": 2.1594,
      "step": 30350
    },
    {
      "epoch": 1.843687374749499,
      "grad_norm": 2.6488089561462402,
      "learning_rate": 3.239056652455943e-05,
      "loss": 2.1612,
      "step": 30360
    },
    {
      "epoch": 1.8442946499058723,
      "grad_norm": 2.1498944759368896,
      "learning_rate": 3.2360810489842166e-05,
      "loss": 1.8637,
      "step": 30370
    },
    {
      "epoch": 1.8449019250622456,
      "grad_norm": 2.214458703994751,
      "learning_rate": 3.233106158869405e-05,
      "loss": 2.0294,
      "step": 30380
    },
    {
      "epoch": 1.845509200218619,
      "grad_norm": 4.2916364669799805,
      "learning_rate": 3.230131983314601e-05,
      "loss": 2.2377,
      "step": 30390
    },
    {
      "epoch": 1.8461164753749926,
      "grad_norm": 3.4568045139312744,
      "learning_rate": 3.227158523522614e-05,
      "loss": 2.4051,
      "step": 30400
    },
    {
      "epoch": 1.8467237505313658,
      "grad_norm": 4.32870626449585,
      "learning_rate": 3.2241857806959544e-05,
      "loss": 2.301,
      "step": 30410
    },
    {
      "epoch": 1.847331025687739,
      "grad_norm": 5.24324369430542,
      "learning_rate": 3.221213756036847e-05,
      "loss": 2.5847,
      "step": 30420
    },
    {
      "epoch": 1.8479383008441124,
      "grad_norm": 6.464379787445068,
      "learning_rate": 3.218242450747225e-05,
      "loss": 2.4162,
      "step": 30430
    },
    {
      "epoch": 1.8485455760004859,
      "grad_norm": 3.50740385055542,
      "learning_rate": 3.215271866028734e-05,
      "loss": 2.2454,
      "step": 30440
    },
    {
      "epoch": 1.8491528511568591,
      "grad_norm": 2.73715877532959,
      "learning_rate": 3.212302003082725e-05,
      "loss": 2.0636,
      "step": 30450
    },
    {
      "epoch": 1.8497601263132326,
      "grad_norm": 3.213188409805298,
      "learning_rate": 3.209332863110257e-05,
      "loss": 2.1067,
      "step": 30460
    },
    {
      "epoch": 1.8503674014696059,
      "grad_norm": 3.1894102096557617,
      "learning_rate": 3.2063644473120976e-05,
      "loss": 2.359,
      "step": 30470
    },
    {
      "epoch": 1.8509746766259791,
      "grad_norm": 2.97037672996521,
      "learning_rate": 3.2033967568887224e-05,
      "loss": 2.119,
      "step": 30480
    },
    {
      "epoch": 1.8515819517823526,
      "grad_norm": 3.7037501335144043,
      "learning_rate": 3.200429793040313e-05,
      "loss": 1.8767,
      "step": 30490
    },
    {
      "epoch": 1.852189226938726,
      "grad_norm": 2.2975449562072754,
      "learning_rate": 3.197463556966755e-05,
      "loss": 2.0161,
      "step": 30500
    },
    {
      "epoch": 1.8527965020950994,
      "grad_norm": 1.6323164701461792,
      "learning_rate": 3.1944980498676416e-05,
      "loss": 1.8836,
      "step": 30510
    },
    {
      "epoch": 1.8534037772514727,
      "grad_norm": 5.071223735809326,
      "learning_rate": 3.191533272942273e-05,
      "loss": 2.1654,
      "step": 30520
    },
    {
      "epoch": 1.854011052407846,
      "grad_norm": 5.144079208374023,
      "learning_rate": 3.1885692273896516e-05,
      "loss": 2.6631,
      "step": 30530
    },
    {
      "epoch": 1.8546183275642192,
      "grad_norm": 4.733521461486816,
      "learning_rate": 3.185605914408484e-05,
      "loss": 2.4467,
      "step": 30540
    },
    {
      "epoch": 1.8552256027205927,
      "grad_norm": 4.523679256439209,
      "learning_rate": 3.182643335197181e-05,
      "loss": 2.6169,
      "step": 30550
    },
    {
      "epoch": 1.8558328778769662,
      "grad_norm": 5.064361572265625,
      "learning_rate": 3.1796814909538577e-05,
      "loss": 2.3658,
      "step": 30560
    },
    {
      "epoch": 1.8564401530333394,
      "grad_norm": 3.791386604309082,
      "learning_rate": 3.1767203828763305e-05,
      "loss": 2.1402,
      "step": 30570
    },
    {
      "epoch": 1.8570474281897127,
      "grad_norm": 4.388087272644043,
      "learning_rate": 3.173760012162117e-05,
      "loss": 2.1871,
      "step": 30580
    },
    {
      "epoch": 1.857654703346086,
      "grad_norm": 3.730903148651123,
      "learning_rate": 3.1708003800084415e-05,
      "loss": 2.0625,
      "step": 30590
    },
    {
      "epoch": 1.8582619785024594,
      "grad_norm": 2.6703360080718994,
      "learning_rate": 3.167841487612225e-05,
      "loss": 1.9573,
      "step": 30600
    },
    {
      "epoch": 1.858869253658833,
      "grad_norm": 4.458105087280273,
      "learning_rate": 3.164883336170092e-05,
      "loss": 2.2157,
      "step": 30610
    },
    {
      "epoch": 1.8594765288152062,
      "grad_norm": 4.103026390075684,
      "learning_rate": 3.161925926878362e-05,
      "loss": 2.2725,
      "step": 30620
    },
    {
      "epoch": 1.8600838039715795,
      "grad_norm": 3.637457847595215,
      "learning_rate": 3.1589692609330625e-05,
      "loss": 2.1435,
      "step": 30630
    },
    {
      "epoch": 1.8606910791279527,
      "grad_norm": 3.3638901710510254,
      "learning_rate": 3.156013339529914e-05,
      "loss": 2.122,
      "step": 30640
    },
    {
      "epoch": 1.8612983542843262,
      "grad_norm": 4.262994766235352,
      "learning_rate": 3.1530581638643415e-05,
      "loss": 2.2394,
      "step": 30650
    },
    {
      "epoch": 1.8619056294406997,
      "grad_norm": 4.391603469848633,
      "learning_rate": 3.150103735131459e-05,
      "loss": 2.1681,
      "step": 30660
    },
    {
      "epoch": 1.862512904597073,
      "grad_norm": 3.7662620544433594,
      "learning_rate": 3.1471500545260904e-05,
      "loss": 2.3,
      "step": 30670
    },
    {
      "epoch": 1.8631201797534462,
      "grad_norm": 5.463717937469482,
      "learning_rate": 3.1441971232427496e-05,
      "loss": 2.256,
      "step": 30680
    },
    {
      "epoch": 1.8637274549098195,
      "grad_norm": 4.029974460601807,
      "learning_rate": 3.1412449424756474e-05,
      "loss": 2.1,
      "step": 30690
    },
    {
      "epoch": 1.864334730066193,
      "grad_norm": 3.0946121215820312,
      "learning_rate": 3.138293513418692e-05,
      "loss": 2.1575,
      "step": 30700
    },
    {
      "epoch": 1.8649420052225665,
      "grad_norm": 3.1415934562683105,
      "learning_rate": 3.135342837265494e-05,
      "loss": 2.2382,
      "step": 30710
    },
    {
      "epoch": 1.8655492803789397,
      "grad_norm": 5.358017444610596,
      "learning_rate": 3.132392915209347e-05,
      "loss": 2.2347,
      "step": 30720
    },
    {
      "epoch": 1.866156555535313,
      "grad_norm": 4.149608612060547,
      "learning_rate": 3.1294437484432506e-05,
      "loss": 2.3378,
      "step": 30730
    },
    {
      "epoch": 1.8667638306916863,
      "grad_norm": 3.4977731704711914,
      "learning_rate": 3.126495338159892e-05,
      "loss": 2.1206,
      "step": 30740
    },
    {
      "epoch": 1.8673711058480598,
      "grad_norm": 4.486936569213867,
      "learning_rate": 3.123547685551658e-05,
      "loss": 2.5201,
      "step": 30750
    },
    {
      "epoch": 1.8679783810044333,
      "grad_norm": 4.283632755279541,
      "learning_rate": 3.1206007918106254e-05,
      "loss": 2.3333,
      "step": 30760
    },
    {
      "epoch": 1.8685856561608065,
      "grad_norm": 4.245694160461426,
      "learning_rate": 3.1176546581285645e-05,
      "loss": 2.2668,
      "step": 30770
    },
    {
      "epoch": 1.8691929313171798,
      "grad_norm": 4.553726673126221,
      "learning_rate": 3.114709285696937e-05,
      "loss": 2.268,
      "step": 30780
    },
    {
      "epoch": 1.869800206473553,
      "grad_norm": 5.071352005004883,
      "learning_rate": 3.111764675706903e-05,
      "loss": 2.4927,
      "step": 30790
    },
    {
      "epoch": 1.8704074816299265,
      "grad_norm": 4.153589248657227,
      "learning_rate": 3.108820829349306e-05,
      "loss": 2.2906,
      "step": 30800
    },
    {
      "epoch": 1.8710147567863,
      "grad_norm": 3.59488582611084,
      "learning_rate": 3.105877747814684e-05,
      "loss": 2.5112,
      "step": 30810
    },
    {
      "epoch": 1.8716220319426733,
      "grad_norm": 3.3957157135009766,
      "learning_rate": 3.102935432293268e-05,
      "loss": 2.1485,
      "step": 30820
    },
    {
      "epoch": 1.8722293070990466,
      "grad_norm": 4.460789203643799,
      "learning_rate": 3.099993883974978e-05,
      "loss": 1.9952,
      "step": 30830
    },
    {
      "epoch": 1.8728365822554198,
      "grad_norm": 3.1375515460968018,
      "learning_rate": 3.09705310404942e-05,
      "loss": 2.2052,
      "step": 30840
    },
    {
      "epoch": 1.8734438574117933,
      "grad_norm": 4.603161811828613,
      "learning_rate": 3.094113093705894e-05,
      "loss": 2.3491,
      "step": 30850
    },
    {
      "epoch": 1.8740511325681668,
      "grad_norm": 3.962806224822998,
      "learning_rate": 3.0911738541333866e-05,
      "loss": 2.0953,
      "step": 30860
    },
    {
      "epoch": 1.87465840772454,
      "grad_norm": 4.292742729187012,
      "learning_rate": 3.088235386520575e-05,
      "loss": 2.2707,
      "step": 30870
    },
    {
      "epoch": 1.8752656828809133,
      "grad_norm": 3.746776580810547,
      "learning_rate": 3.08529769205582e-05,
      "loss": 2.0874,
      "step": 30880
    },
    {
      "epoch": 1.8758729580372866,
      "grad_norm": 3.5441153049468994,
      "learning_rate": 3.082360771927171e-05,
      "loss": 2.1367,
      "step": 30890
    },
    {
      "epoch": 1.87648023319366,
      "grad_norm": 3.315150260925293,
      "learning_rate": 3.079424627322368e-05,
      "loss": 2.1861,
      "step": 30900
    },
    {
      "epoch": 1.8770875083500334,
      "grad_norm": 3.047699451446533,
      "learning_rate": 3.0764892594288344e-05,
      "loss": 2.0129,
      "step": 30910
    },
    {
      "epoch": 1.8776947835064068,
      "grad_norm": 3.8293001651763916,
      "learning_rate": 3.073554669433678e-05,
      "loss": 2.1871,
      "step": 30920
    },
    {
      "epoch": 1.87830205866278,
      "grad_norm": 2.885251045227051,
      "learning_rate": 3.070620858523694e-05,
      "loss": 2.1438,
      "step": 30930
    },
    {
      "epoch": 1.8789093338191534,
      "grad_norm": 3.963632345199585,
      "learning_rate": 3.067687827885364e-05,
      "loss": 2.0756,
      "step": 30940
    },
    {
      "epoch": 1.8795166089755269,
      "grad_norm": 4.1029157638549805,
      "learning_rate": 3.0647555787048505e-05,
      "loss": 2.4256,
      "step": 30950
    },
    {
      "epoch": 1.8801238841319001,
      "grad_norm": 3.7131571769714355,
      "learning_rate": 3.061824112168003e-05,
      "loss": 2.4895,
      "step": 30960
    },
    {
      "epoch": 1.8807311592882736,
      "grad_norm": 4.037660598754883,
      "learning_rate": 3.0588934294603515e-05,
      "loss": 2.1882,
      "step": 30970
    },
    {
      "epoch": 1.8813384344446469,
      "grad_norm": 3.3527626991271973,
      "learning_rate": 3.055963531767113e-05,
      "loss": 2.2676,
      "step": 30980
    },
    {
      "epoch": 1.8819457096010201,
      "grad_norm": 5.504415512084961,
      "learning_rate": 3.0530344202731856e-05,
      "loss": 2.2101,
      "step": 30990
    },
    {
      "epoch": 1.8825529847573934,
      "grad_norm": 2.617269277572632,
      "learning_rate": 3.0501060961631446e-05,
      "loss": 2.1771,
      "step": 31000
    },
    {
      "epoch": 1.883160259913767,
      "grad_norm": 5.210195064544678,
      "learning_rate": 3.0471785606212516e-05,
      "loss": 2.094,
      "step": 31010
    },
    {
      "epoch": 1.8837675350701404,
      "grad_norm": 3.2959179878234863,
      "learning_rate": 3.0442518148314532e-05,
      "loss": 2.347,
      "step": 31020
    },
    {
      "epoch": 1.8843748102265137,
      "grad_norm": 4.991263389587402,
      "learning_rate": 3.0413258599773677e-05,
      "loss": 2.3283,
      "step": 31030
    },
    {
      "epoch": 1.884982085382887,
      "grad_norm": 4.0079498291015625,
      "learning_rate": 3.0384006972422974e-05,
      "loss": 2.2489,
      "step": 31040
    },
    {
      "epoch": 1.8855893605392602,
      "grad_norm": 5.048614501953125,
      "learning_rate": 3.0354763278092292e-05,
      "loss": 2.585,
      "step": 31050
    },
    {
      "epoch": 1.8861966356956337,
      "grad_norm": 4.092804908752441,
      "learning_rate": 3.03255275286082e-05,
      "loss": 2.21,
      "step": 31060
    },
    {
      "epoch": 1.8868039108520072,
      "grad_norm": 4.867318153381348,
      "learning_rate": 3.029629973579413e-05,
      "loss": 2.2177,
      "step": 31070
    },
    {
      "epoch": 1.8874111860083804,
      "grad_norm": 4.295666217803955,
      "learning_rate": 3.026707991147023e-05,
      "loss": 2.1486,
      "step": 31080
    },
    {
      "epoch": 1.8880184611647537,
      "grad_norm": 4.112874984741211,
      "learning_rate": 3.0237868067453523e-05,
      "loss": 2.0687,
      "step": 31090
    },
    {
      "epoch": 1.888625736321127,
      "grad_norm": 5.320054531097412,
      "learning_rate": 3.020866421555769e-05,
      "loss": 2.2016,
      "step": 31100
    },
    {
      "epoch": 1.8892330114775004,
      "grad_norm": 4.580479621887207,
      "learning_rate": 3.017946836759326e-05,
      "loss": 2.0297,
      "step": 31110
    },
    {
      "epoch": 1.889840286633874,
      "grad_norm": 3.5777506828308105,
      "learning_rate": 3.015028053536748e-05,
      "loss": 2.1301,
      "step": 31120
    },
    {
      "epoch": 1.8904475617902472,
      "grad_norm": 5.487513065338135,
      "learning_rate": 3.01211007306844e-05,
      "loss": 2.3551,
      "step": 31130
    },
    {
      "epoch": 1.8910548369466205,
      "grad_norm": 3.1449358463287354,
      "learning_rate": 3.0091928965344784e-05,
      "loss": 2.1903,
      "step": 31140
    },
    {
      "epoch": 1.8916621121029937,
      "grad_norm": 6.930731296539307,
      "learning_rate": 3.0062765251146154e-05,
      "loss": 2.2689,
      "step": 31150
    },
    {
      "epoch": 1.8922693872593672,
      "grad_norm": 4.643011569976807,
      "learning_rate": 3.003360959988277e-05,
      "loss": 2.255,
      "step": 31160
    },
    {
      "epoch": 1.8928766624157407,
      "grad_norm": 5.0386528968811035,
      "learning_rate": 3.0004462023345675e-05,
      "loss": 2.2547,
      "step": 31170
    },
    {
      "epoch": 1.893483937572114,
      "grad_norm": 5.136102676391602,
      "learning_rate": 2.997532253332259e-05,
      "loss": 2.3309,
      "step": 31180
    },
    {
      "epoch": 1.8940912127284872,
      "grad_norm": 4.595296859741211,
      "learning_rate": 2.9946191141598e-05,
      "loss": 2.3675,
      "step": 31190
    },
    {
      "epoch": 1.8946984878848605,
      "grad_norm": 4.0680389404296875,
      "learning_rate": 2.9917067859953085e-05,
      "loss": 2.2916,
      "step": 31200
    },
    {
      "epoch": 1.895305763041234,
      "grad_norm": 3.790390968322754,
      "learning_rate": 2.9887952700165777e-05,
      "loss": 2.1277,
      "step": 31210
    },
    {
      "epoch": 1.8959130381976075,
      "grad_norm": 4.535976886749268,
      "learning_rate": 2.9858845674010722e-05,
      "loss": 2.1739,
      "step": 31220
    },
    {
      "epoch": 1.8965203133539807,
      "grad_norm": 1.9681570529937744,
      "learning_rate": 2.982974679325925e-05,
      "loss": 1.942,
      "step": 31230
    },
    {
      "epoch": 1.897127588510354,
      "grad_norm": 3.346897602081299,
      "learning_rate": 2.98006560696794e-05,
      "loss": 2.0459,
      "step": 31240
    },
    {
      "epoch": 1.8977348636667273,
      "grad_norm": 3.817095994949341,
      "learning_rate": 2.9771573515035945e-05,
      "loss": 2.2486,
      "step": 31250
    },
    {
      "epoch": 1.8983421388231008,
      "grad_norm": 3.734403371810913,
      "learning_rate": 2.9742499141090335e-05,
      "loss": 2.1603,
      "step": 31260
    },
    {
      "epoch": 1.8989494139794743,
      "grad_norm": 3.1224279403686523,
      "learning_rate": 2.9713432959600685e-05,
      "loss": 2.2797,
      "step": 31270
    },
    {
      "epoch": 1.8995566891358475,
      "grad_norm": 3.315687417984009,
      "learning_rate": 2.968437498232185e-05,
      "loss": 2.3736,
      "step": 31280
    },
    {
      "epoch": 1.9001639642922208,
      "grad_norm": 4.151516914367676,
      "learning_rate": 2.965532522100533e-05,
      "loss": 2.0785,
      "step": 31290
    },
    {
      "epoch": 1.900771239448594,
      "grad_norm": 4.351778984069824,
      "learning_rate": 2.9626283687399314e-05,
      "loss": 2.3897,
      "step": 31300
    },
    {
      "epoch": 1.9013785146049675,
      "grad_norm": 4.315276622772217,
      "learning_rate": 2.959725039324863e-05,
      "loss": 2.3683,
      "step": 31310
    },
    {
      "epoch": 1.901985789761341,
      "grad_norm": 4.700639724731445,
      "learning_rate": 2.9568225350294844e-05,
      "loss": 2.2078,
      "step": 31320
    },
    {
      "epoch": 1.9025930649177143,
      "grad_norm": 3.7263424396514893,
      "learning_rate": 2.9539208570276145e-05,
      "loss": 2.1858,
      "step": 31330
    },
    {
      "epoch": 1.9032003400740876,
      "grad_norm": 6.768490791320801,
      "learning_rate": 2.951020006492735e-05,
      "loss": 2.2373,
      "step": 31340
    },
    {
      "epoch": 1.9038076152304608,
      "grad_norm": 5.910942554473877,
      "learning_rate": 2.948119984597997e-05,
      "loss": 2.3707,
      "step": 31350
    },
    {
      "epoch": 1.9044148903868343,
      "grad_norm": 3.2954368591308594,
      "learning_rate": 2.9452207925162177e-05,
      "loss": 2.2023,
      "step": 31360
    },
    {
      "epoch": 1.9050221655432076,
      "grad_norm": 4.183809280395508,
      "learning_rate": 2.9423224314198756e-05,
      "loss": 2.123,
      "step": 31370
    },
    {
      "epoch": 1.905629440699581,
      "grad_norm": 4.615235805511475,
      "learning_rate": 2.9394249024811128e-05,
      "loss": 2.293,
      "step": 31380
    },
    {
      "epoch": 1.9062367158559543,
      "grad_norm": 3.845139503479004,
      "learning_rate": 2.9365282068717353e-05,
      "loss": 2.0825,
      "step": 31390
    },
    {
      "epoch": 1.9068439910123276,
      "grad_norm": 3.6072731018066406,
      "learning_rate": 2.9336323457632174e-05,
      "loss": 2.1058,
      "step": 31400
    },
    {
      "epoch": 1.907451266168701,
      "grad_norm": 2.9561357498168945,
      "learning_rate": 2.9307373203266873e-05,
      "loss": 2.0665,
      "step": 31410
    },
    {
      "epoch": 1.9080585413250744,
      "grad_norm": 3.313016176223755,
      "learning_rate": 2.927843131732942e-05,
      "loss": 2.2366,
      "step": 31420
    },
    {
      "epoch": 1.9086658164814478,
      "grad_norm": 2.578730583190918,
      "learning_rate": 2.924949781152434e-05,
      "loss": 2.0406,
      "step": 31430
    },
    {
      "epoch": 1.909273091637821,
      "grad_norm": 3.6256258487701416,
      "learning_rate": 2.9220572697552838e-05,
      "loss": 2.1464,
      "step": 31440
    },
    {
      "epoch": 1.9098803667941944,
      "grad_norm": 4.907168865203857,
      "learning_rate": 2.919165598711269e-05,
      "loss": 2.2041,
      "step": 31450
    },
    {
      "epoch": 1.9104876419505676,
      "grad_norm": 3.7005839347839355,
      "learning_rate": 2.9162747691898253e-05,
      "loss": 2.0853,
      "step": 31460
    },
    {
      "epoch": 1.9110949171069411,
      "grad_norm": 4.149357318878174,
      "learning_rate": 2.913384782360049e-05,
      "loss": 2.4117,
      "step": 31470
    },
    {
      "epoch": 1.9117021922633146,
      "grad_norm": 3.112142562866211,
      "learning_rate": 2.9104956393907014e-05,
      "loss": 2.2169,
      "step": 31480
    },
    {
      "epoch": 1.9123094674196879,
      "grad_norm": 3.4074389934539795,
      "learning_rate": 2.907607341450197e-05,
      "loss": 1.9929,
      "step": 31490
    },
    {
      "epoch": 1.9129167425760611,
      "grad_norm": 3.571885108947754,
      "learning_rate": 2.904719889706604e-05,
      "loss": 2.0768,
      "step": 31500
    },
    {
      "epoch": 1.9135240177324344,
      "grad_norm": 3.740840435028076,
      "learning_rate": 2.9018332853276607e-05,
      "loss": 2.0697,
      "step": 31510
    },
    {
      "epoch": 1.914131292888808,
      "grad_norm": 5.840552806854248,
      "learning_rate": 2.8989475294807523e-05,
      "loss": 2.5702,
      "step": 31520
    },
    {
      "epoch": 1.9147385680451814,
      "grad_norm": 3.9632375240325928,
      "learning_rate": 2.896062623332926e-05,
      "loss": 2.1923,
      "step": 31530
    },
    {
      "epoch": 1.9153458432015547,
      "grad_norm": 2.1000442504882812,
      "learning_rate": 2.8931785680508806e-05,
      "loss": 1.9785,
      "step": 31540
    },
    {
      "epoch": 1.915953118357928,
      "grad_norm": 3.90547776222229,
      "learning_rate": 2.89029536480098e-05,
      "loss": 2.3288,
      "step": 31550
    },
    {
      "epoch": 1.9165603935143012,
      "grad_norm": 4.379929542541504,
      "learning_rate": 2.8874130147492316e-05,
      "loss": 2.4943,
      "step": 31560
    },
    {
      "epoch": 1.9171676686706747,
      "grad_norm": 4.284503936767578,
      "learning_rate": 2.8845315190613066e-05,
      "loss": 2.2744,
      "step": 31570
    },
    {
      "epoch": 1.9177749438270482,
      "grad_norm": 3.2634434700012207,
      "learning_rate": 2.8816508789025247e-05,
      "loss": 2.3124,
      "step": 31580
    },
    {
      "epoch": 1.9183822189834214,
      "grad_norm": 3.1792328357696533,
      "learning_rate": 2.8787710954378676e-05,
      "loss": 2.12,
      "step": 31590
    },
    {
      "epoch": 1.9189894941397947,
      "grad_norm": 3.0401411056518555,
      "learning_rate": 2.8758921698319624e-05,
      "loss": 2.0795,
      "step": 31600
    },
    {
      "epoch": 1.919596769296168,
      "grad_norm": 2.5006916522979736,
      "learning_rate": 2.873014103249093e-05,
      "loss": 1.8497,
      "step": 31610
    },
    {
      "epoch": 1.9202040444525414,
      "grad_norm": 2.8180155754089355,
      "learning_rate": 2.870136896853196e-05,
      "loss": 2.0552,
      "step": 31620
    },
    {
      "epoch": 1.920811319608915,
      "grad_norm": 3.3158326148986816,
      "learning_rate": 2.8672605518078592e-05,
      "loss": 1.8867,
      "step": 31630
    },
    {
      "epoch": 1.9214185947652882,
      "grad_norm": 4.176074028015137,
      "learning_rate": 2.8643850692763218e-05,
      "loss": 2.129,
      "step": 31640
    },
    {
      "epoch": 1.9220258699216615,
      "grad_norm": 2.842195749282837,
      "learning_rate": 2.861510450421475e-05,
      "loss": 2.0986,
      "step": 31650
    },
    {
      "epoch": 1.9226331450780347,
      "grad_norm": 4.3214874267578125,
      "learning_rate": 2.8586366964058587e-05,
      "loss": 1.9712,
      "step": 31660
    },
    {
      "epoch": 1.9232404202344082,
      "grad_norm": 3.3278915882110596,
      "learning_rate": 2.8557638083916682e-05,
      "loss": 2.2623,
      "step": 31670
    },
    {
      "epoch": 1.9238476953907817,
      "grad_norm": 2.5539052486419678,
      "learning_rate": 2.8528917875407433e-05,
      "loss": 2.3174,
      "step": 31680
    },
    {
      "epoch": 1.924454970547155,
      "grad_norm": 2.628838539123535,
      "learning_rate": 2.850020635014576e-05,
      "loss": 2.212,
      "step": 31690
    },
    {
      "epoch": 1.9250622457035282,
      "grad_norm": 2.2411744594573975,
      "learning_rate": 2.8471503519743047e-05,
      "loss": 1.9747,
      "step": 31700
    },
    {
      "epoch": 1.9256695208599015,
      "grad_norm": 2.673079013824463,
      "learning_rate": 2.844280939580719e-05,
      "loss": 1.9188,
      "step": 31710
    },
    {
      "epoch": 1.926276796016275,
      "grad_norm": 2.9993696212768555,
      "learning_rate": 2.8414123989942533e-05,
      "loss": 2.0906,
      "step": 31720
    },
    {
      "epoch": 1.9268840711726485,
      "grad_norm": 3.222721576690674,
      "learning_rate": 2.8385447313749904e-05,
      "loss": 1.9931,
      "step": 31730
    },
    {
      "epoch": 1.9274913463290217,
      "grad_norm": 4.173811435699463,
      "learning_rate": 2.835677937882664e-05,
      "loss": 2.2849,
      "step": 31740
    },
    {
      "epoch": 1.928098621485395,
      "grad_norm": 3.623255729675293,
      "learning_rate": 2.8328120196766494e-05,
      "loss": 2.3621,
      "step": 31750
    },
    {
      "epoch": 1.9287058966417683,
      "grad_norm": 3.2123005390167236,
      "learning_rate": 2.8299469779159703e-05,
      "loss": 2.2798,
      "step": 31760
    },
    {
      "epoch": 1.9293131717981418,
      "grad_norm": 3.54660701751709,
      "learning_rate": 2.827082813759294e-05,
      "loss": 2.143,
      "step": 31770
    },
    {
      "epoch": 1.9299204469545153,
      "grad_norm": 4.781520843505859,
      "learning_rate": 2.8242195283649343e-05,
      "loss": 2.1019,
      "step": 31780
    },
    {
      "epoch": 1.9305277221108885,
      "grad_norm": 3.7427990436553955,
      "learning_rate": 2.8213571228908497e-05,
      "loss": 2.212,
      "step": 31790
    },
    {
      "epoch": 1.9311349972672618,
      "grad_norm": 4.246142387390137,
      "learning_rate": 2.8184955984946427e-05,
      "loss": 2.2765,
      "step": 31800
    },
    {
      "epoch": 1.931742272423635,
      "grad_norm": 4.5087714195251465,
      "learning_rate": 2.8156349563335573e-05,
      "loss": 2.2893,
      "step": 31810
    },
    {
      "epoch": 1.9323495475800085,
      "grad_norm": 2.726388931274414,
      "learning_rate": 2.8127751975644866e-05,
      "loss": 2.1115,
      "step": 31820
    },
    {
      "epoch": 1.9329568227363818,
      "grad_norm": 3.1121339797973633,
      "learning_rate": 2.809916323343963e-05,
      "loss": 2.0496,
      "step": 31830
    },
    {
      "epoch": 1.9335640978927553,
      "grad_norm": 4.077078819274902,
      "learning_rate": 2.8070583348281552e-05,
      "loss": 2.1693,
      "step": 31840
    },
    {
      "epoch": 1.9341713730491286,
      "grad_norm": 4.741729259490967,
      "learning_rate": 2.804201233172882e-05,
      "loss": 2.3791,
      "step": 31850
    },
    {
      "epoch": 1.9347786482055018,
      "grad_norm": 4.088926792144775,
      "learning_rate": 2.801345019533602e-05,
      "loss": 2.2982,
      "step": 31860
    },
    {
      "epoch": 1.9353859233618753,
      "grad_norm": 3.7284858226776123,
      "learning_rate": 2.7984896950654133e-05,
      "loss": 2.0583,
      "step": 31870
    },
    {
      "epoch": 1.9359931985182486,
      "grad_norm": 4.302643299102783,
      "learning_rate": 2.7956352609230542e-05,
      "loss": 2.2112,
      "step": 31880
    },
    {
      "epoch": 1.936600473674622,
      "grad_norm": 4.150589466094971,
      "learning_rate": 2.7927817182609008e-05,
      "loss": 2.3829,
      "step": 31890
    },
    {
      "epoch": 1.9372077488309953,
      "grad_norm": 3.9294824600219727,
      "learning_rate": 2.7899290682329778e-05,
      "loss": 2.2933,
      "step": 31900
    },
    {
      "epoch": 1.9378150239873686,
      "grad_norm": 3.336318254470825,
      "learning_rate": 2.7870773119929362e-05,
      "loss": 2.6526,
      "step": 31910
    },
    {
      "epoch": 1.9384222991437419,
      "grad_norm": 3.5392489433288574,
      "learning_rate": 2.784226450694073e-05,
      "loss": 2.2082,
      "step": 31920
    },
    {
      "epoch": 1.9390295743001154,
      "grad_norm": 2.311295509338379,
      "learning_rate": 2.781376485489321e-05,
      "loss": 2.0003,
      "step": 31930
    },
    {
      "epoch": 1.9396368494564888,
      "grad_norm": 4.7517008781433105,
      "learning_rate": 2.778527417531255e-05,
      "loss": 2.2796,
      "step": 31940
    },
    {
      "epoch": 1.940244124612862,
      "grad_norm": 4.099771976470947,
      "learning_rate": 2.7756792479720806e-05,
      "loss": 2.4534,
      "step": 31950
    },
    {
      "epoch": 1.9408513997692354,
      "grad_norm": 3.105052947998047,
      "learning_rate": 2.7728319779636445e-05,
      "loss": 2.1607,
      "step": 31960
    },
    {
      "epoch": 1.9414586749256086,
      "grad_norm": 2.1766159534454346,
      "learning_rate": 2.769985608657427e-05,
      "loss": 2.0639,
      "step": 31970
    },
    {
      "epoch": 1.9420659500819821,
      "grad_norm": 3.820327043533325,
      "learning_rate": 2.7671401412045456e-05,
      "loss": 2.0928,
      "step": 31980
    },
    {
      "epoch": 1.9426732252383556,
      "grad_norm": 3.4388251304626465,
      "learning_rate": 2.7642955767557523e-05,
      "loss": 2.224,
      "step": 31990
    },
    {
      "epoch": 1.9432805003947289,
      "grad_norm": 3.3137242794036865,
      "learning_rate": 2.7614519164614328e-05,
      "loss": 1.9249,
      "step": 32000
    },
    {
      "epoch": 1.9438877755511021,
      "grad_norm": 3.4970102310180664,
      "learning_rate": 2.758609161471612e-05,
      "loss": 2.2161,
      "step": 32010
    },
    {
      "epoch": 1.9444950507074754,
      "grad_norm": 3.8738584518432617,
      "learning_rate": 2.7557673129359436e-05,
      "loss": 2.2684,
      "step": 32020
    },
    {
      "epoch": 1.945102325863849,
      "grad_norm": 2.842127799987793,
      "learning_rate": 2.752926372003717e-05,
      "loss": 2.1549,
      "step": 32030
    },
    {
      "epoch": 1.9457096010202224,
      "grad_norm": 2.6251060962677,
      "learning_rate": 2.7500863398238525e-05,
      "loss": 2.1737,
      "step": 32040
    },
    {
      "epoch": 1.9463168761765957,
      "grad_norm": 2.2273356914520264,
      "learning_rate": 2.7472472175449053e-05,
      "loss": 1.9549,
      "step": 32050
    },
    {
      "epoch": 1.946924151332969,
      "grad_norm": 4.079366683959961,
      "learning_rate": 2.744409006315062e-05,
      "loss": 2.0646,
      "step": 32060
    },
    {
      "epoch": 1.9475314264893422,
      "grad_norm": 3.9368247985839844,
      "learning_rate": 2.74157170728214e-05,
      "loss": 2.3165,
      "step": 32070
    },
    {
      "epoch": 1.9481387016457157,
      "grad_norm": 1.820165753364563,
      "learning_rate": 2.7387353215935857e-05,
      "loss": 2.2222,
      "step": 32080
    },
    {
      "epoch": 1.9487459768020892,
      "grad_norm": 3.075320243835449,
      "learning_rate": 2.735899850396484e-05,
      "loss": 2.108,
      "step": 32090
    },
    {
      "epoch": 1.9493532519584624,
      "grad_norm": 2.6946959495544434,
      "learning_rate": 2.7330652948375406e-05,
      "loss": 2.1521,
      "step": 32100
    },
    {
      "epoch": 1.9499605271148357,
      "grad_norm": 2.3518450260162354,
      "learning_rate": 2.730231656063099e-05,
      "loss": 2.0218,
      "step": 32110
    },
    {
      "epoch": 1.950567802271209,
      "grad_norm": 3.749661445617676,
      "learning_rate": 2.7273989352191204e-05,
      "loss": 2.0917,
      "step": 32120
    },
    {
      "epoch": 1.9511750774275824,
      "grad_norm": 3.410151958465576,
      "learning_rate": 2.724567133451209e-05,
      "loss": 2.4425,
      "step": 32130
    },
    {
      "epoch": 1.951782352583956,
      "grad_norm": 3.62960147857666,
      "learning_rate": 2.7217362519045887e-05,
      "loss": 2.441,
      "step": 32140
    },
    {
      "epoch": 1.9523896277403292,
      "grad_norm": 3.892002820968628,
      "learning_rate": 2.7189062917241116e-05,
      "loss": 2.1617,
      "step": 32150
    },
    {
      "epoch": 1.9529969028967025,
      "grad_norm": 2.785616159439087,
      "learning_rate": 2.716077254054259e-05,
      "loss": 2.0223,
      "step": 32160
    },
    {
      "epoch": 1.9536041780530757,
      "grad_norm": 3.5987017154693604,
      "learning_rate": 2.7132491400391404e-05,
      "loss": 2.2161,
      "step": 32170
    },
    {
      "epoch": 1.9542114532094492,
      "grad_norm": 2.2534430027008057,
      "learning_rate": 2.710421950822492e-05,
      "loss": 1.8654,
      "step": 32180
    },
    {
      "epoch": 1.9548187283658227,
      "grad_norm": 3.3868212699890137,
      "learning_rate": 2.7075956875476672e-05,
      "loss": 2.1965,
      "step": 32190
    },
    {
      "epoch": 1.955426003522196,
      "grad_norm": 4.926609992980957,
      "learning_rate": 2.7047703513576584e-05,
      "loss": 2.6448,
      "step": 32200
    },
    {
      "epoch": 1.9560332786785692,
      "grad_norm": 3.696992874145508,
      "learning_rate": 2.7019459433950733e-05,
      "loss": 2.3969,
      "step": 32210
    },
    {
      "epoch": 1.9566405538349425,
      "grad_norm": 3.4854624271392822,
      "learning_rate": 2.6991224648021495e-05,
      "loss": 2.11,
      "step": 32220
    },
    {
      "epoch": 1.957247828991316,
      "grad_norm": 3.5815837383270264,
      "learning_rate": 2.696299916720743e-05,
      "loss": 1.9271,
      "step": 32230
    },
    {
      "epoch": 1.9578551041476895,
      "grad_norm": 3.9213032722473145,
      "learning_rate": 2.693478300292344e-05,
      "loss": 2.0835,
      "step": 32240
    },
    {
      "epoch": 1.9584623793040628,
      "grad_norm": 3.9808406829833984,
      "learning_rate": 2.6906576166580534e-05,
      "loss": 2.0058,
      "step": 32250
    },
    {
      "epoch": 1.959069654460436,
      "grad_norm": 2.9281647205352783,
      "learning_rate": 2.6878378669586013e-05,
      "loss": 2.1137,
      "step": 32260
    },
    {
      "epoch": 1.9596769296168093,
      "grad_norm": 2.960824489593506,
      "learning_rate": 2.6850190523343388e-05,
      "loss": 2.0306,
      "step": 32270
    },
    {
      "epoch": 1.9602842047731828,
      "grad_norm": 3.037370204925537,
      "learning_rate": 2.6822011739252428e-05,
      "loss": 1.9608,
      "step": 32280
    },
    {
      "epoch": 1.960891479929556,
      "grad_norm": 2.6557974815368652,
      "learning_rate": 2.679384232870906e-05,
      "loss": 1.9326,
      "step": 32290
    },
    {
      "epoch": 1.9614987550859295,
      "grad_norm": 4.14112663269043,
      "learning_rate": 2.6765682303105442e-05,
      "loss": 2.1031,
      "step": 32300
    },
    {
      "epoch": 1.9621060302423028,
      "grad_norm": 3.956784725189209,
      "learning_rate": 2.6737531673829954e-05,
      "loss": 2.1416,
      "step": 32310
    },
    {
      "epoch": 1.962713305398676,
      "grad_norm": 3.6179897785186768,
      "learning_rate": 2.670939045226713e-05,
      "loss": 2.3094,
      "step": 32320
    },
    {
      "epoch": 1.9633205805550495,
      "grad_norm": 4.172246932983398,
      "learning_rate": 2.6681258649797748e-05,
      "loss": 2.4092,
      "step": 32330
    },
    {
      "epoch": 1.9639278557114228,
      "grad_norm": 3.1463522911071777,
      "learning_rate": 2.6653136277798752e-05,
      "loss": 2.0679,
      "step": 32340
    },
    {
      "epoch": 1.9645351308677963,
      "grad_norm": 3.3282241821289062,
      "learning_rate": 2.6625023347643253e-05,
      "loss": 2.2514,
      "step": 32350
    },
    {
      "epoch": 1.9651424060241696,
      "grad_norm": 5.305061340332031,
      "learning_rate": 2.659691987070061e-05,
      "loss": 2.0714,
      "step": 32360
    },
    {
      "epoch": 1.9657496811805428,
      "grad_norm": 3.963813543319702,
      "learning_rate": 2.6568825858336304e-05,
      "loss": 2.0958,
      "step": 32370
    },
    {
      "epoch": 1.966356956336916,
      "grad_norm": 4.432093620300293,
      "learning_rate": 2.6540741321911995e-05,
      "loss": 2.1467,
      "step": 32380
    },
    {
      "epoch": 1.9669642314932896,
      "grad_norm": 3.838139057159424,
      "learning_rate": 2.6512666272785512e-05,
      "loss": 2.1721,
      "step": 32390
    },
    {
      "epoch": 1.967571506649663,
      "grad_norm": 4.008483409881592,
      "learning_rate": 2.648460072231086e-05,
      "loss": 2.1675,
      "step": 32400
    },
    {
      "epoch": 1.9681787818060363,
      "grad_norm": 6.054288864135742,
      "learning_rate": 2.6456544681838192e-05,
      "loss": 2.395,
      "step": 32410
    },
    {
      "epoch": 1.9687860569624096,
      "grad_norm": 3.656996726989746,
      "learning_rate": 2.64284981627138e-05,
      "loss": 2.1629,
      "step": 32420
    },
    {
      "epoch": 1.9693933321187829,
      "grad_norm": 4.116953372955322,
      "learning_rate": 2.640046117628018e-05,
      "loss": 2.2575,
      "step": 32430
    },
    {
      "epoch": 1.9700006072751564,
      "grad_norm": 4.348620414733887,
      "learning_rate": 2.637243373387591e-05,
      "loss": 2.2,
      "step": 32440
    },
    {
      "epoch": 1.9706078824315298,
      "grad_norm": 4.025592803955078,
      "learning_rate": 2.634441584683574e-05,
      "loss": 2.1218,
      "step": 32450
    },
    {
      "epoch": 1.971215157587903,
      "grad_norm": 3.8564603328704834,
      "learning_rate": 2.6316407526490554e-05,
      "loss": 2.0296,
      "step": 32460
    },
    {
      "epoch": 1.9718224327442764,
      "grad_norm": 3.8048415184020996,
      "learning_rate": 2.6288408784167357e-05,
      "loss": 2.0371,
      "step": 32470
    },
    {
      "epoch": 1.9724297079006496,
      "grad_norm": 3.5565826892852783,
      "learning_rate": 2.6260419631189293e-05,
      "loss": 2.0628,
      "step": 32480
    },
    {
      "epoch": 1.9730369830570231,
      "grad_norm": 5.268736362457275,
      "learning_rate": 2.623244007887561e-05,
      "loss": 2.3018,
      "step": 32490
    },
    {
      "epoch": 1.9736442582133966,
      "grad_norm": 2.9992833137512207,
      "learning_rate": 2.620447013854167e-05,
      "loss": 2.0486,
      "step": 32500
    },
    {
      "epoch": 1.9742515333697699,
      "grad_norm": 4.773982048034668,
      "learning_rate": 2.6176509821499007e-05,
      "loss": 2.1073,
      "step": 32510
    },
    {
      "epoch": 1.9748588085261432,
      "grad_norm": 4.243758201599121,
      "learning_rate": 2.614855913905521e-05,
      "loss": 2.2247,
      "step": 32520
    },
    {
      "epoch": 1.9754660836825164,
      "grad_norm": 3.0065016746520996,
      "learning_rate": 2.612061810251395e-05,
      "loss": 2.0713,
      "step": 32530
    },
    {
      "epoch": 1.97607335883889,
      "grad_norm": 3.8541617393493652,
      "learning_rate": 2.609268672317502e-05,
      "loss": 2.3383,
      "step": 32540
    },
    {
      "epoch": 1.9766806339952634,
      "grad_norm": 3.900207996368408,
      "learning_rate": 2.6064765012334357e-05,
      "loss": 2.3807,
      "step": 32550
    },
    {
      "epoch": 1.9772879091516367,
      "grad_norm": 3.4213788509368896,
      "learning_rate": 2.6036852981283922e-05,
      "loss": 2.2901,
      "step": 32560
    },
    {
      "epoch": 1.97789518430801,
      "grad_norm": 3.573869228363037,
      "learning_rate": 2.600895064131179e-05,
      "loss": 2.1862,
      "step": 32570
    },
    {
      "epoch": 1.9785024594643832,
      "grad_norm": 3.086113452911377,
      "learning_rate": 2.598105800370211e-05,
      "loss": 2.157,
      "step": 32580
    },
    {
      "epoch": 1.9791097346207567,
      "grad_norm": 3.1785290241241455,
      "learning_rate": 2.595317507973511e-05,
      "loss": 2.0873,
      "step": 32590
    },
    {
      "epoch": 1.9797170097771302,
      "grad_norm": 3.1052463054656982,
      "learning_rate": 2.5925301880687088e-05,
      "loss": 2.1579,
      "step": 32600
    },
    {
      "epoch": 1.9803242849335034,
      "grad_norm": 3.281568765640259,
      "learning_rate": 2.5897438417830407e-05,
      "loss": 2.0396,
      "step": 32610
    },
    {
      "epoch": 1.9809315600898767,
      "grad_norm": 5.165091037750244,
      "learning_rate": 2.5869584702433476e-05,
      "loss": 2.2517,
      "step": 32620
    },
    {
      "epoch": 1.98153883524625,
      "grad_norm": 4.204192161560059,
      "learning_rate": 2.5841740745760824e-05,
      "loss": 2.3857,
      "step": 32630
    },
    {
      "epoch": 1.9821461104026235,
      "grad_norm": 3.9750399589538574,
      "learning_rate": 2.5813906559072963e-05,
      "loss": 2.2974,
      "step": 32640
    },
    {
      "epoch": 1.982753385558997,
      "grad_norm": 4.356846332550049,
      "learning_rate": 2.578608215362649e-05,
      "loss": 2.1291,
      "step": 32650
    },
    {
      "epoch": 1.9833606607153702,
      "grad_norm": 3.5166711807250977,
      "learning_rate": 2.5758267540674042e-05,
      "loss": 2.0656,
      "step": 32660
    },
    {
      "epoch": 1.9839679358717435,
      "grad_norm": 2.9157114028930664,
      "learning_rate": 2.5730462731464273e-05,
      "loss": 1.9842,
      "step": 32670
    },
    {
      "epoch": 1.9845752110281167,
      "grad_norm": 5.2818121910095215,
      "learning_rate": 2.5702667737241902e-05,
      "loss": 2.3987,
      "step": 32680
    },
    {
      "epoch": 1.9851824861844902,
      "grad_norm": 3.989781618118286,
      "learning_rate": 2.5674882569247653e-05,
      "loss": 2.3545,
      "step": 32690
    },
    {
      "epoch": 1.9857897613408635,
      "grad_norm": 3.0429625511169434,
      "learning_rate": 2.5647107238718326e-05,
      "loss": 1.8946,
      "step": 32700
    },
    {
      "epoch": 1.986397036497237,
      "grad_norm": 5.019003868103027,
      "learning_rate": 2.5619341756886682e-05,
      "loss": 1.9718,
      "step": 32710
    },
    {
      "epoch": 1.9870043116536102,
      "grad_norm": 5.073563098907471,
      "learning_rate": 2.5591586134981526e-05,
      "loss": 2.2988,
      "step": 32720
    },
    {
      "epoch": 1.9876115868099835,
      "grad_norm": 5.9630126953125,
      "learning_rate": 2.5563840384227677e-05,
      "loss": 2.3159,
      "step": 32730
    },
    {
      "epoch": 1.988218861966357,
      "grad_norm": 6.263295650482178,
      "learning_rate": 2.553610451584596e-05,
      "loss": 2.5403,
      "step": 32740
    },
    {
      "epoch": 1.9888261371227303,
      "grad_norm": 3.3281688690185547,
      "learning_rate": 2.5508378541053196e-05,
      "loss": 2.1809,
      "step": 32750
    },
    {
      "epoch": 1.9894334122791038,
      "grad_norm": 4.116486072540283,
      "learning_rate": 2.5480662471062213e-05,
      "loss": 2.245,
      "step": 32760
    },
    {
      "epoch": 1.990040687435477,
      "grad_norm": 2.721348285675049,
      "learning_rate": 2.545295631708181e-05,
      "loss": 1.9872,
      "step": 32770
    },
    {
      "epoch": 1.9906479625918503,
      "grad_norm": 2.145394802093506,
      "learning_rate": 2.542526009031684e-05,
      "loss": 1.964,
      "step": 32780
    },
    {
      "epoch": 1.9912552377482238,
      "grad_norm": 3.2929039001464844,
      "learning_rate": 2.539757380196808e-05,
      "loss": 2.1601,
      "step": 32790
    },
    {
      "epoch": 1.991862512904597,
      "grad_norm": 2.6689951419830322,
      "learning_rate": 2.5369897463232318e-05,
      "loss": 2.0561,
      "step": 32800
    },
    {
      "epoch": 1.9924697880609705,
      "grad_norm": 3.0971484184265137,
      "learning_rate": 2.534223108530225e-05,
      "loss": 1.9212,
      "step": 32810
    },
    {
      "epoch": 1.9930770632173438,
      "grad_norm": 2.275200843811035,
      "learning_rate": 2.5314574679366664e-05,
      "loss": 1.9357,
      "step": 32820
    },
    {
      "epoch": 1.993684338373717,
      "grad_norm": 1.9126579761505127,
      "learning_rate": 2.528692825661022e-05,
      "loss": 1.9057,
      "step": 32830
    },
    {
      "epoch": 1.9942916135300903,
      "grad_norm": 3.0410122871398926,
      "learning_rate": 2.525929182821359e-05,
      "loss": 2.0067,
      "step": 32840
    },
    {
      "epoch": 1.9948988886864638,
      "grad_norm": 2.989931106567383,
      "learning_rate": 2.5231665405353348e-05,
      "loss": 2.0678,
      "step": 32850
    },
    {
      "epoch": 1.9955061638428373,
      "grad_norm": 3.481858968734741,
      "learning_rate": 2.5204048999202127e-05,
      "loss": 2.2902,
      "step": 32860
    },
    {
      "epoch": 1.9961134389992106,
      "grad_norm": 4.001911163330078,
      "learning_rate": 2.5176442620928388e-05,
      "loss": 2.7299,
      "step": 32870
    },
    {
      "epoch": 1.9967207141555838,
      "grad_norm": 5.158899307250977,
      "learning_rate": 2.5148846281696576e-05,
      "loss": 2.6611,
      "step": 32880
    },
    {
      "epoch": 1.997327989311957,
      "grad_norm": 4.380187034606934,
      "learning_rate": 2.5121259992667134e-05,
      "loss": 2.4585,
      "step": 32890
    },
    {
      "epoch": 1.9979352644683306,
      "grad_norm": 3.6455719470977783,
      "learning_rate": 2.509368376499639e-05,
      "loss": 2.2484,
      "step": 32900
    },
    {
      "epoch": 1.998542539624704,
      "grad_norm": 3.3417861461639404,
      "learning_rate": 2.5066117609836594e-05,
      "loss": 2.1603,
      "step": 32910
    },
    {
      "epoch": 1.9991498147810773,
      "grad_norm": 5.588164329528809,
      "learning_rate": 2.5038561538335924e-05,
      "loss": 2.1909,
      "step": 32920
    },
    {
      "epoch": 1.9997570899374506,
      "grad_norm": 3.9225900173187256,
      "learning_rate": 2.501101556163855e-05,
      "loss": 2.2614,
      "step": 32930
    },
    {
      "epoch": 2.000364365093824,
      "grad_norm": 4.493924140930176,
      "learning_rate": 2.4983479690884448e-05,
      "loss": 2.3723,
      "step": 32940
    },
    {
      "epoch": 2.0009716402501976,
      "grad_norm": 4.046557903289795,
      "learning_rate": 2.4955953937209587e-05,
      "loss": 2.2719,
      "step": 32950
    },
    {
      "epoch": 2.001578915406571,
      "grad_norm": 4.612204074859619,
      "learning_rate": 2.4928438311745795e-05,
      "loss": 2.4757,
      "step": 32960
    },
    {
      "epoch": 2.002186190562944,
      "grad_norm": 3.800156831741333,
      "learning_rate": 2.4900932825620864e-05,
      "loss": 2.2217,
      "step": 32970
    },
    {
      "epoch": 2.0027934657193174,
      "grad_norm": 3.076193332672119,
      "learning_rate": 2.487343748995844e-05,
      "loss": 2.2341,
      "step": 32980
    },
    {
      "epoch": 2.0034007408756906,
      "grad_norm": 2.7436654567718506,
      "learning_rate": 2.4845952315878072e-05,
      "loss": 1.8711,
      "step": 32990
    },
    {
      "epoch": 2.004008016032064,
      "grad_norm": 3.06187105178833,
      "learning_rate": 2.48184773144952e-05,
      "loss": 1.8963,
      "step": 33000
    },
    {
      "epoch": 2.0046152911884376,
      "grad_norm": 4.871078014373779,
      "learning_rate": 2.479101249692115e-05,
      "loss": 2.0874,
      "step": 33010
    },
    {
      "epoch": 2.005222566344811,
      "grad_norm": 4.2056074142456055,
      "learning_rate": 2.476355787426314e-05,
      "loss": 2.1136,
      "step": 33020
    },
    {
      "epoch": 2.005829841501184,
      "grad_norm": 3.1208336353302,
      "learning_rate": 2.4736113457624245e-05,
      "loss": 2.151,
      "step": 33030
    },
    {
      "epoch": 2.0064371166575574,
      "grad_norm": 4.26097297668457,
      "learning_rate": 2.4708679258103416e-05,
      "loss": 2.1148,
      "step": 33040
    },
    {
      "epoch": 2.0070443918139307,
      "grad_norm": 4.1769795417785645,
      "learning_rate": 2.4681255286795514e-05,
      "loss": 2.1972,
      "step": 33050
    },
    {
      "epoch": 2.0076516669703044,
      "grad_norm": 2.6399285793304443,
      "learning_rate": 2.465384155479121e-05,
      "loss": 2.0285,
      "step": 33060
    },
    {
      "epoch": 2.0082589421266777,
      "grad_norm": 2.259222984313965,
      "learning_rate": 2.462643807317705e-05,
      "loss": 1.9867,
      "step": 33070
    },
    {
      "epoch": 2.008866217283051,
      "grad_norm": 2.967489242553711,
      "learning_rate": 2.4599044853035446e-05,
      "loss": 2.1626,
      "step": 33080
    },
    {
      "epoch": 2.009473492439424,
      "grad_norm": 5.107744216918945,
      "learning_rate": 2.4571661905444648e-05,
      "loss": 2.1733,
      "step": 33090
    },
    {
      "epoch": 2.0100807675957975,
      "grad_norm": 4.227643966674805,
      "learning_rate": 2.4544289241478757e-05,
      "loss": 2.0786,
      "step": 33100
    },
    {
      "epoch": 2.010688042752171,
      "grad_norm": 3.22434663772583,
      "learning_rate": 2.4516926872207695e-05,
      "loss": 2.0469,
      "step": 33110
    },
    {
      "epoch": 2.0112953179085444,
      "grad_norm": 2.2119781970977783,
      "learning_rate": 2.4489574808697285e-05,
      "loss": 2.066,
      "step": 33120
    },
    {
      "epoch": 2.0119025930649177,
      "grad_norm": 2.945479393005371,
      "learning_rate": 2.4462233062009105e-05,
      "loss": 1.9365,
      "step": 33130
    },
    {
      "epoch": 2.012509868221291,
      "grad_norm": 2.8987367153167725,
      "learning_rate": 2.443490164320062e-05,
      "loss": 2.186,
      "step": 33140
    },
    {
      "epoch": 2.0131171433776642,
      "grad_norm": 2.9329700469970703,
      "learning_rate": 2.4407580563325033e-05,
      "loss": 2.1913,
      "step": 33150
    },
    {
      "epoch": 2.013724418534038,
      "grad_norm": 3.246819019317627,
      "learning_rate": 2.4380269833431478e-05,
      "loss": 2.1684,
      "step": 33160
    },
    {
      "epoch": 2.014331693690411,
      "grad_norm": 3.805454969406128,
      "learning_rate": 2.435296946456484e-05,
      "loss": 2.3967,
      "step": 33170
    },
    {
      "epoch": 2.0149389688467845,
      "grad_norm": 3.755740165710449,
      "learning_rate": 2.4325679467765806e-05,
      "loss": 2.1538,
      "step": 33180
    },
    {
      "epoch": 2.0155462440031577,
      "grad_norm": 2.7153360843658447,
      "learning_rate": 2.429839985407088e-05,
      "loss": 1.887,
      "step": 33190
    },
    {
      "epoch": 2.016153519159531,
      "grad_norm": 3.4702656269073486,
      "learning_rate": 2.4271130634512405e-05,
      "loss": 2.2501,
      "step": 33200
    },
    {
      "epoch": 2.0167607943159047,
      "grad_norm": 3.0550029277801514,
      "learning_rate": 2.424387182011849e-05,
      "loss": 2.1894,
      "step": 33210
    },
    {
      "epoch": 2.017368069472278,
      "grad_norm": 4.752756595611572,
      "learning_rate": 2.4216623421912994e-05,
      "loss": 2.2959,
      "step": 33220
    },
    {
      "epoch": 2.0179753446286512,
      "grad_norm": 3.823131799697876,
      "learning_rate": 2.4189385450915602e-05,
      "loss": 2.2298,
      "step": 33230
    },
    {
      "epoch": 2.0185826197850245,
      "grad_norm": 4.468879699707031,
      "learning_rate": 2.4162157918141827e-05,
      "loss": 2.351,
      "step": 33240
    },
    {
      "epoch": 2.019189894941398,
      "grad_norm": 5.696530818939209,
      "learning_rate": 2.4134940834602885e-05,
      "loss": 2.3746,
      "step": 33250
    },
    {
      "epoch": 2.0197971700977715,
      "grad_norm": 4.736772537231445,
      "learning_rate": 2.4107734211305815e-05,
      "loss": 2.4363,
      "step": 33260
    },
    {
      "epoch": 2.0204044452541448,
      "grad_norm": 5.2032952308654785,
      "learning_rate": 2.40805380592534e-05,
      "loss": 2.3129,
      "step": 33270
    },
    {
      "epoch": 2.021011720410518,
      "grad_norm": 4.427433013916016,
      "learning_rate": 2.4053352389444195e-05,
      "loss": 2.4535,
      "step": 33280
    },
    {
      "epoch": 2.0216189955668913,
      "grad_norm": 3.935840129852295,
      "learning_rate": 2.402617721287252e-05,
      "loss": 2.0839,
      "step": 33290
    },
    {
      "epoch": 2.0222262707232646,
      "grad_norm": 3.170555830001831,
      "learning_rate": 2.3999012540528452e-05,
      "loss": 2.0812,
      "step": 33300
    },
    {
      "epoch": 2.0228335458796383,
      "grad_norm": 4.266838550567627,
      "learning_rate": 2.3971858383397793e-05,
      "loss": 2.2694,
      "step": 33310
    },
    {
      "epoch": 2.0234408210360115,
      "grad_norm": 3.4229235649108887,
      "learning_rate": 2.3944714752462162e-05,
      "loss": 2.216,
      "step": 33320
    },
    {
      "epoch": 2.024048096192385,
      "grad_norm": 2.116997718811035,
      "learning_rate": 2.3917581658698845e-05,
      "loss": 2.0107,
      "step": 33330
    },
    {
      "epoch": 2.024655371348758,
      "grad_norm": 4.086392402648926,
      "learning_rate": 2.389045911308091e-05,
      "loss": 2.0447,
      "step": 33340
    },
    {
      "epoch": 2.0252626465051313,
      "grad_norm": 4.900223255157471,
      "learning_rate": 2.386334712657714e-05,
      "loss": 2.1349,
      "step": 33350
    },
    {
      "epoch": 2.025869921661505,
      "grad_norm": 3.4885056018829346,
      "learning_rate": 2.383624571015205e-05,
      "loss": 2.1863,
      "step": 33360
    },
    {
      "epoch": 2.0264771968178783,
      "grad_norm": 4.139847278594971,
      "learning_rate": 2.3809154874765893e-05,
      "loss": 2.2435,
      "step": 33370
    },
    {
      "epoch": 2.0270844719742516,
      "grad_norm": 5.6878557205200195,
      "learning_rate": 2.378207463137461e-05,
      "loss": 2.3889,
      "step": 33380
    },
    {
      "epoch": 2.027691747130625,
      "grad_norm": 4.566359043121338,
      "learning_rate": 2.3755004990929923e-05,
      "loss": 2.4928,
      "step": 33390
    },
    {
      "epoch": 2.028299022286998,
      "grad_norm": 4.056695461273193,
      "learning_rate": 2.3727945964379205e-05,
      "loss": 2.4709,
      "step": 33400
    },
    {
      "epoch": 2.028906297443372,
      "grad_norm": 3.4667232036590576,
      "learning_rate": 2.3700897562665563e-05,
      "loss": 2.4154,
      "step": 33410
    },
    {
      "epoch": 2.029513572599745,
      "grad_norm": 4.2017621994018555,
      "learning_rate": 2.3673859796727794e-05,
      "loss": 2.2807,
      "step": 33420
    },
    {
      "epoch": 2.0301208477561183,
      "grad_norm": 2.5142993927001953,
      "learning_rate": 2.3646832677500404e-05,
      "loss": 1.9898,
      "step": 33430
    },
    {
      "epoch": 2.0307281229124916,
      "grad_norm": 3.74411678314209,
      "learning_rate": 2.3619816215913588e-05,
      "loss": 2.2104,
      "step": 33440
    },
    {
      "epoch": 2.031335398068865,
      "grad_norm": 2.653897285461426,
      "learning_rate": 2.3592810422893236e-05,
      "loss": 2.0755,
      "step": 33450
    },
    {
      "epoch": 2.031942673225238,
      "grad_norm": 3.9305784702301025,
      "learning_rate": 2.35658153093609e-05,
      "loss": 2.0221,
      "step": 33460
    },
    {
      "epoch": 2.032549948381612,
      "grad_norm": 4.538702487945557,
      "learning_rate": 2.3538830886233877e-05,
      "loss": 2.1987,
      "step": 33470
    },
    {
      "epoch": 2.033157223537985,
      "grad_norm": 2.3559939861297607,
      "learning_rate": 2.351185716442507e-05,
      "loss": 2.0593,
      "step": 33480
    },
    {
      "epoch": 2.0337644986943584,
      "grad_norm": 2.613159656524658,
      "learning_rate": 2.348489415484311e-05,
      "loss": 1.9356,
      "step": 33490
    },
    {
      "epoch": 2.0343717738507316,
      "grad_norm": 1.886142373085022,
      "learning_rate": 2.3457941868392208e-05,
      "loss": 1.9855,
      "step": 33500
    },
    {
      "epoch": 2.034979049007105,
      "grad_norm": 2.6991965770721436,
      "learning_rate": 2.3431000315972353e-05,
      "loss": 2.0756,
      "step": 33510
    },
    {
      "epoch": 2.0355863241634786,
      "grad_norm": 2.2132885456085205,
      "learning_rate": 2.340406950847911e-05,
      "loss": 1.914,
      "step": 33520
    },
    {
      "epoch": 2.036193599319852,
      "grad_norm": 2.619452714920044,
      "learning_rate": 2.3377149456803744e-05,
      "loss": 2.0614,
      "step": 33530
    },
    {
      "epoch": 2.036800874476225,
      "grad_norm": 3.8908615112304688,
      "learning_rate": 2.335024017183312e-05,
      "loss": 2.1925,
      "step": 33540
    },
    {
      "epoch": 2.0374081496325984,
      "grad_norm": 3.3332879543304443,
      "learning_rate": 2.3323341664449845e-05,
      "loss": 2.1818,
      "step": 33550
    },
    {
      "epoch": 2.0380154247889717,
      "grad_norm": 3.442044973373413,
      "learning_rate": 2.329645394553204e-05,
      "loss": 2.2966,
      "step": 33560
    },
    {
      "epoch": 2.0386226999453454,
      "grad_norm": 5.553603172302246,
      "learning_rate": 2.3269577025953542e-05,
      "loss": 2.4295,
      "step": 33570
    },
    {
      "epoch": 2.0392299751017187,
      "grad_norm": 2.787816286087036,
      "learning_rate": 2.3242710916583826e-05,
      "loss": 2.0142,
      "step": 33580
    },
    {
      "epoch": 2.039837250258092,
      "grad_norm": 3.418137788772583,
      "learning_rate": 2.3215855628287962e-05,
      "loss": 2.3544,
      "step": 33590
    },
    {
      "epoch": 2.040444525414465,
      "grad_norm": 4.833256244659424,
      "learning_rate": 2.3189011171926663e-05,
      "loss": 2.2888,
      "step": 33600
    },
    {
      "epoch": 2.0410518005708385,
      "grad_norm": 3.6937448978424072,
      "learning_rate": 2.3162177558356245e-05,
      "loss": 2.4793,
      "step": 33610
    },
    {
      "epoch": 2.041659075727212,
      "grad_norm": 3.5083508491516113,
      "learning_rate": 2.3135354798428648e-05,
      "loss": 2.0857,
      "step": 33620
    },
    {
      "epoch": 2.0422663508835854,
      "grad_norm": 2.7459962368011475,
      "learning_rate": 2.3108542902991436e-05,
      "loss": 1.9944,
      "step": 33630
    },
    {
      "epoch": 2.0428736260399587,
      "grad_norm": 4.356448650360107,
      "learning_rate": 2.3081741882887752e-05,
      "loss": 2.3473,
      "step": 33640
    },
    {
      "epoch": 2.043480901196332,
      "grad_norm": 4.371840000152588,
      "learning_rate": 2.3054951748956345e-05,
      "loss": 2.3431,
      "step": 33650
    },
    {
      "epoch": 2.0440881763527052,
      "grad_norm": 3.8915984630584717,
      "learning_rate": 2.3028172512031604e-05,
      "loss": 2.0819,
      "step": 33660
    },
    {
      "epoch": 2.044695451509079,
      "grad_norm": 4.8024702072143555,
      "learning_rate": 2.300140418294347e-05,
      "loss": 2.0975,
      "step": 33670
    },
    {
      "epoch": 2.045302726665452,
      "grad_norm": 5.147449016571045,
      "learning_rate": 2.2974646772517468e-05,
      "loss": 2.4235,
      "step": 33680
    },
    {
      "epoch": 2.0459100018218255,
      "grad_norm": 3.7880842685699463,
      "learning_rate": 2.2947900291574732e-05,
      "loss": 2.1378,
      "step": 33690
    },
    {
      "epoch": 2.0465172769781987,
      "grad_norm": 3.346806287765503,
      "learning_rate": 2.2921164750931955e-05,
      "loss": 2.3501,
      "step": 33700
    },
    {
      "epoch": 2.047124552134572,
      "grad_norm": 3.8619723320007324,
      "learning_rate": 2.2894440161401425e-05,
      "loss": 2.2103,
      "step": 33710
    },
    {
      "epoch": 2.0477318272909457,
      "grad_norm": 4.2955851554870605,
      "learning_rate": 2.286772653379099e-05,
      "loss": 2.1392,
      "step": 33720
    },
    {
      "epoch": 2.048339102447319,
      "grad_norm": 3.635930299758911,
      "learning_rate": 2.2841023878904046e-05,
      "loss": 2.1395,
      "step": 33730
    },
    {
      "epoch": 2.0489463776036922,
      "grad_norm": 2.692640542984009,
      "learning_rate": 2.2814332207539603e-05,
      "loss": 2.1872,
      "step": 33740
    },
    {
      "epoch": 2.0495536527600655,
      "grad_norm": 2.744309186935425,
      "learning_rate": 2.278765153049219e-05,
      "loss": 2.0524,
      "step": 33750
    },
    {
      "epoch": 2.050160927916439,
      "grad_norm": 4.889800071716309,
      "learning_rate": 2.276098185855191e-05,
      "loss": 2.5851,
      "step": 33760
    },
    {
      "epoch": 2.0507682030728125,
      "grad_norm": 3.267768144607544,
      "learning_rate": 2.2734323202504347e-05,
      "loss": 2.3113,
      "step": 33770
    },
    {
      "epoch": 2.0513754782291858,
      "grad_norm": 5.066073417663574,
      "learning_rate": 2.2707675573130743e-05,
      "loss": 2.2833,
      "step": 33780
    },
    {
      "epoch": 2.051982753385559,
      "grad_norm": 2.879418134689331,
      "learning_rate": 2.2681038981207807e-05,
      "loss": 2.3088,
      "step": 33790
    },
    {
      "epoch": 2.0525900285419323,
      "grad_norm": 3.747865915298462,
      "learning_rate": 2.265441343750778e-05,
      "loss": 2.1242,
      "step": 33800
    },
    {
      "epoch": 2.0531973036983056,
      "grad_norm": 3.406341791152954,
      "learning_rate": 2.26277989527985e-05,
      "loss": 2.1477,
      "step": 33810
    },
    {
      "epoch": 2.0538045788546793,
      "grad_norm": 3.0617012977600098,
      "learning_rate": 2.260119553784326e-05,
      "loss": 2.2337,
      "step": 33820
    },
    {
      "epoch": 2.0544118540110525,
      "grad_norm": 3.2324986457824707,
      "learning_rate": 2.257460320340093e-05,
      "loss": 2.2721,
      "step": 33830
    },
    {
      "epoch": 2.055019129167426,
      "grad_norm": 3.419144630432129,
      "learning_rate": 2.254802196022581e-05,
      "loss": 2.3361,
      "step": 33840
    },
    {
      "epoch": 2.055626404323799,
      "grad_norm": 3.7078769207000732,
      "learning_rate": 2.252145181906784e-05,
      "loss": 2.2951,
      "step": 33850
    },
    {
      "epoch": 2.0562336794801723,
      "grad_norm": 4.250569820404053,
      "learning_rate": 2.2494892790672397e-05,
      "loss": 2.5031,
      "step": 33860
    },
    {
      "epoch": 2.056840954636546,
      "grad_norm": 4.438331604003906,
      "learning_rate": 2.2468344885780358e-05,
      "loss": 2.3355,
      "step": 33870
    },
    {
      "epoch": 2.0574482297929193,
      "grad_norm": 3.5037758350372314,
      "learning_rate": 2.244180811512811e-05,
      "loss": 2.4067,
      "step": 33880
    },
    {
      "epoch": 2.0580555049492926,
      "grad_norm": 3.5733482837677,
      "learning_rate": 2.2415282489447597e-05,
      "loss": 2.2465,
      "step": 33890
    },
    {
      "epoch": 2.058662780105666,
      "grad_norm": 3.578352689743042,
      "learning_rate": 2.238876801946616e-05,
      "loss": 2.2088,
      "step": 33900
    },
    {
      "epoch": 2.059270055262039,
      "grad_norm": 2.2550265789031982,
      "learning_rate": 2.236226471590668e-05,
      "loss": 2.0564,
      "step": 33910
    },
    {
      "epoch": 2.0598773304184124,
      "grad_norm": 3.017794370651245,
      "learning_rate": 2.2335772589487498e-05,
      "loss": 1.7711,
      "step": 33920
    },
    {
      "epoch": 2.060484605574786,
      "grad_norm": 2.5102639198303223,
      "learning_rate": 2.230929165092249e-05,
      "loss": 2.1558,
      "step": 33930
    },
    {
      "epoch": 2.0610918807311593,
      "grad_norm": 2.867532968521118,
      "learning_rate": 2.2282821910920953e-05,
      "loss": 2.0352,
      "step": 33940
    },
    {
      "epoch": 2.0616991558875326,
      "grad_norm": 3.5497236251831055,
      "learning_rate": 2.2256363380187673e-05,
      "loss": 2.4269,
      "step": 33950
    },
    {
      "epoch": 2.062306431043906,
      "grad_norm": 4.795539379119873,
      "learning_rate": 2.2229916069422897e-05,
      "loss": 2.3781,
      "step": 33960
    },
    {
      "epoch": 2.062913706200279,
      "grad_norm": 2.590458631515503,
      "learning_rate": 2.220347998932234e-05,
      "loss": 2.1222,
      "step": 33970
    },
    {
      "epoch": 2.063520981356653,
      "grad_norm": 2.5600171089172363,
      "learning_rate": 2.2177055150577174e-05,
      "loss": 2.076,
      "step": 33980
    },
    {
      "epoch": 2.064128256513026,
      "grad_norm": 4.214798450469971,
      "learning_rate": 2.215064156387402e-05,
      "loss": 2.1017,
      "step": 33990
    },
    {
      "epoch": 2.0647355316693994,
      "grad_norm": 3.664666175842285,
      "learning_rate": 2.2124239239894944e-05,
      "loss": 2.1954,
      "step": 34000
    },
    {
      "epoch": 2.0653428068257726,
      "grad_norm": 4.526421546936035,
      "learning_rate": 2.20978481893175e-05,
      "loss": 2.362,
      "step": 34010
    },
    {
      "epoch": 2.065950081982146,
      "grad_norm": 5.167747974395752,
      "learning_rate": 2.207146842281462e-05,
      "loss": 2.4489,
      "step": 34020
    },
    {
      "epoch": 2.0665573571385196,
      "grad_norm": 4.770861625671387,
      "learning_rate": 2.204509995105472e-05,
      "loss": 2.2442,
      "step": 34030
    },
    {
      "epoch": 2.067164632294893,
      "grad_norm": 4.360820770263672,
      "learning_rate": 2.2018742784701617e-05,
      "loss": 2.2066,
      "step": 34040
    },
    {
      "epoch": 2.067771907451266,
      "grad_norm": 3.7948384284973145,
      "learning_rate": 2.1992396934414576e-05,
      "loss": 2.0979,
      "step": 34050
    },
    {
      "epoch": 2.0683791826076394,
      "grad_norm": 3.224421262741089,
      "learning_rate": 2.196606241084827e-05,
      "loss": 2.3006,
      "step": 34060
    },
    {
      "epoch": 2.0689864577640127,
      "grad_norm": 3.751512289047241,
      "learning_rate": 2.1939739224652788e-05,
      "loss": 2.1091,
      "step": 34070
    },
    {
      "epoch": 2.0695937329203864,
      "grad_norm": 3.9993228912353516,
      "learning_rate": 2.1913427386473683e-05,
      "loss": 2.2859,
      "step": 34080
    },
    {
      "epoch": 2.0702010080767597,
      "grad_norm": 3.6680614948272705,
      "learning_rate": 2.1887126906951862e-05,
      "loss": 2.1424,
      "step": 34090
    },
    {
      "epoch": 2.070808283233133,
      "grad_norm": 3.4279582500457764,
      "learning_rate": 2.186083779672365e-05,
      "loss": 2.0063,
      "step": 34100
    },
    {
      "epoch": 2.071415558389506,
      "grad_norm": 3.1344704627990723,
      "learning_rate": 2.1834560066420797e-05,
      "loss": 2.0474,
      "step": 34110
    },
    {
      "epoch": 2.0720228335458795,
      "grad_norm": 3.9765360355377197,
      "learning_rate": 2.180829372667042e-05,
      "loss": 2.2858,
      "step": 34120
    },
    {
      "epoch": 2.072630108702253,
      "grad_norm": 3.1762309074401855,
      "learning_rate": 2.1782038788095056e-05,
      "loss": 2.1586,
      "step": 34130
    },
    {
      "epoch": 2.0732373838586264,
      "grad_norm": 4.787160396575928,
      "learning_rate": 2.1755795261312618e-05,
      "loss": 2.0375,
      "step": 34140
    },
    {
      "epoch": 2.0738446590149997,
      "grad_norm": 3.5046255588531494,
      "learning_rate": 2.1729563156936382e-05,
      "loss": 1.8491,
      "step": 34150
    },
    {
      "epoch": 2.074451934171373,
      "grad_norm": 3.6814680099487305,
      "learning_rate": 2.1703342485575072e-05,
      "loss": 2.3556,
      "step": 34160
    },
    {
      "epoch": 2.0750592093277462,
      "grad_norm": 3.8941330909729004,
      "learning_rate": 2.1677133257832743e-05,
      "loss": 2.4055,
      "step": 34170
    },
    {
      "epoch": 2.07566648448412,
      "grad_norm": 4.136829376220703,
      "learning_rate": 2.1650935484308786e-05,
      "loss": 2.3807,
      "step": 34180
    },
    {
      "epoch": 2.076273759640493,
      "grad_norm": 3.5017266273498535,
      "learning_rate": 2.1624749175598e-05,
      "loss": 2.1743,
      "step": 34190
    },
    {
      "epoch": 2.0768810347968665,
      "grad_norm": 4.366950035095215,
      "learning_rate": 2.1598574342290584e-05,
      "loss": 2.2221,
      "step": 34200
    },
    {
      "epoch": 2.0774883099532397,
      "grad_norm": 4.235958576202393,
      "learning_rate": 2.1572410994972037e-05,
      "loss": 2.4702,
      "step": 34210
    },
    {
      "epoch": 2.078095585109613,
      "grad_norm": 4.25294303894043,
      "learning_rate": 2.154625914422323e-05,
      "loss": 2.2345,
      "step": 34220
    },
    {
      "epoch": 2.0787028602659867,
      "grad_norm": 4.164235591888428,
      "learning_rate": 2.152011880062037e-05,
      "loss": 2.3188,
      "step": 34230
    },
    {
      "epoch": 2.07931013542236,
      "grad_norm": 4.558350086212158,
      "learning_rate": 2.149398997473509e-05,
      "loss": 2.3936,
      "step": 34240
    },
    {
      "epoch": 2.0799174105787333,
      "grad_norm": 3.8677031993865967,
      "learning_rate": 2.1467872677134243e-05,
      "loss": 2.1731,
      "step": 34250
    },
    {
      "epoch": 2.0805246857351065,
      "grad_norm": 3.6689937114715576,
      "learning_rate": 2.144176691838008e-05,
      "loss": 1.984,
      "step": 34260
    },
    {
      "epoch": 2.08113196089148,
      "grad_norm": 4.695949554443359,
      "learning_rate": 2.1415672709030225e-05,
      "loss": 2.5246,
      "step": 34270
    },
    {
      "epoch": 2.0817392360478535,
      "grad_norm": 3.414308786392212,
      "learning_rate": 2.1389590059637572e-05,
      "loss": 2.1919,
      "step": 34280
    },
    {
      "epoch": 2.0823465112042268,
      "grad_norm": 4.727762699127197,
      "learning_rate": 2.1363518980750358e-05,
      "loss": 2.4752,
      "step": 34290
    },
    {
      "epoch": 2.0829537863606,
      "grad_norm": 3.2651681900024414,
      "learning_rate": 2.1337459482912143e-05,
      "loss": 2.3788,
      "step": 34300
    },
    {
      "epoch": 2.0835610615169733,
      "grad_norm": 2.9771056175231934,
      "learning_rate": 2.1311411576661807e-05,
      "loss": 2.0149,
      "step": 34310
    },
    {
      "epoch": 2.0841683366733466,
      "grad_norm": 2.222489356994629,
      "learning_rate": 2.1285375272533538e-05,
      "loss": 1.9276,
      "step": 34320
    },
    {
      "epoch": 2.08477561182972,
      "grad_norm": 3.579068183898926,
      "learning_rate": 2.1259350581056824e-05,
      "loss": 2.1187,
      "step": 34330
    },
    {
      "epoch": 2.0853828869860935,
      "grad_norm": 3.863049030303955,
      "learning_rate": 2.1233337512756453e-05,
      "loss": 2.2062,
      "step": 34340
    },
    {
      "epoch": 2.085990162142467,
      "grad_norm": 3.7073984146118164,
      "learning_rate": 2.1207336078152563e-05,
      "loss": 2.3266,
      "step": 34350
    },
    {
      "epoch": 2.08659743729884,
      "grad_norm": 3.5763041973114014,
      "learning_rate": 2.1181346287760517e-05,
      "loss": 2.2611,
      "step": 34360
    },
    {
      "epoch": 2.0872047124552133,
      "grad_norm": 3.704803466796875,
      "learning_rate": 2.115536815209102e-05,
      "loss": 2.5008,
      "step": 34370
    },
    {
      "epoch": 2.0878119876115866,
      "grad_norm": 3.279175043106079,
      "learning_rate": 2.112940168165003e-05,
      "loss": 2.0489,
      "step": 34380
    },
    {
      "epoch": 2.0884192627679603,
      "grad_norm": 2.8037960529327393,
      "learning_rate": 2.1103446886938805e-05,
      "loss": 2.1217,
      "step": 34390
    },
    {
      "epoch": 2.0890265379243336,
      "grad_norm": 2.4539740085601807,
      "learning_rate": 2.107750377845387e-05,
      "loss": 2.0096,
      "step": 34400
    },
    {
      "epoch": 2.089633813080707,
      "grad_norm": 4.126243591308594,
      "learning_rate": 2.105157236668705e-05,
      "loss": 2.2033,
      "step": 34410
    },
    {
      "epoch": 2.09024108823708,
      "grad_norm": 3.5285513401031494,
      "learning_rate": 2.1025652662125383e-05,
      "loss": 2.3464,
      "step": 34420
    },
    {
      "epoch": 2.0908483633934534,
      "grad_norm": 2.8283917903900146,
      "learning_rate": 2.099974467525126e-05,
      "loss": 2.0165,
      "step": 34430
    },
    {
      "epoch": 2.091455638549827,
      "grad_norm": 2.9464480876922607,
      "learning_rate": 2.097384841654226e-05,
      "loss": 1.9928,
      "step": 34440
    },
    {
      "epoch": 2.0920629137062003,
      "grad_norm": 2.5780844688415527,
      "learning_rate": 2.094796389647125e-05,
      "loss": 2.1604,
      "step": 34450
    },
    {
      "epoch": 2.0926701888625736,
      "grad_norm": 3.174715995788574,
      "learning_rate": 2.092209112550631e-05,
      "loss": 2.0804,
      "step": 34460
    },
    {
      "epoch": 2.093277464018947,
      "grad_norm": 2.6908795833587646,
      "learning_rate": 2.089623011411084e-05,
      "loss": 1.9771,
      "step": 34470
    },
    {
      "epoch": 2.09388473917532,
      "grad_norm": 3.9225614070892334,
      "learning_rate": 2.0870380872743427e-05,
      "loss": 2.2819,
      "step": 34480
    },
    {
      "epoch": 2.094492014331694,
      "grad_norm": 3.608710765838623,
      "learning_rate": 2.0844543411857903e-05,
      "loss": 2.1351,
      "step": 34490
    },
    {
      "epoch": 2.095099289488067,
      "grad_norm": 3.421623468399048,
      "learning_rate": 2.0818717741903386e-05,
      "loss": 2.1914,
      "step": 34500
    },
    {
      "epoch": 2.0957065646444404,
      "grad_norm": 3.7887754440307617,
      "learning_rate": 2.0792903873324166e-05,
      "loss": 2.3327,
      "step": 34510
    },
    {
      "epoch": 2.0963138398008136,
      "grad_norm": 3.3267476558685303,
      "learning_rate": 2.0767101816559798e-05,
      "loss": 2.4097,
      "step": 34520
    },
    {
      "epoch": 2.096921114957187,
      "grad_norm": 3.411384105682373,
      "learning_rate": 2.0741311582045e-05,
      "loss": 2.4368,
      "step": 34530
    },
    {
      "epoch": 2.0975283901135606,
      "grad_norm": 3.873483180999756,
      "learning_rate": 2.0715533180209795e-05,
      "loss": 2.4061,
      "step": 34540
    },
    {
      "epoch": 2.098135665269934,
      "grad_norm": 4.113563537597656,
      "learning_rate": 2.0689766621479366e-05,
      "loss": 2.1411,
      "step": 34550
    },
    {
      "epoch": 2.098742940426307,
      "grad_norm": 3.1264398097991943,
      "learning_rate": 2.0664011916274127e-05,
      "loss": 2.3521,
      "step": 34560
    },
    {
      "epoch": 2.0993502155826804,
      "grad_norm": 3.6441688537597656,
      "learning_rate": 2.063826907500967e-05,
      "loss": 2.264,
      "step": 34570
    },
    {
      "epoch": 2.0999574907390537,
      "grad_norm": 4.548835277557373,
      "learning_rate": 2.0612538108096858e-05,
      "loss": 2.3561,
      "step": 34580
    },
    {
      "epoch": 2.1005647658954274,
      "grad_norm": 3.085418224334717,
      "learning_rate": 2.0586819025941657e-05,
      "loss": 2.1091,
      "step": 34590
    },
    {
      "epoch": 2.1011720410518007,
      "grad_norm": 5.381659984588623,
      "learning_rate": 2.0561111838945286e-05,
      "loss": 2.3376,
      "step": 34600
    },
    {
      "epoch": 2.101779316208174,
      "grad_norm": 3.974529266357422,
      "learning_rate": 2.0535416557504123e-05,
      "loss": 2.2082,
      "step": 34610
    },
    {
      "epoch": 2.102386591364547,
      "grad_norm": 4.579303741455078,
      "learning_rate": 2.050973319200979e-05,
      "loss": 2.2934,
      "step": 34620
    },
    {
      "epoch": 2.1029938665209205,
      "grad_norm": 3.4504804611206055,
      "learning_rate": 2.0484061752849025e-05,
      "loss": 2.258,
      "step": 34630
    },
    {
      "epoch": 2.103601141677294,
      "grad_norm": 2.5119755268096924,
      "learning_rate": 2.045840225040377e-05,
      "loss": 2.0515,
      "step": 34640
    },
    {
      "epoch": 2.1042084168336674,
      "grad_norm": 2.9582595825195312,
      "learning_rate": 2.0432754695051136e-05,
      "loss": 1.9495,
      "step": 34650
    },
    {
      "epoch": 2.1048156919900407,
      "grad_norm": 3.219346284866333,
      "learning_rate": 2.040711909716341e-05,
      "loss": 2.0255,
      "step": 34660
    },
    {
      "epoch": 2.105422967146414,
      "grad_norm": 3.2394070625305176,
      "learning_rate": 2.038149546710802e-05,
      "loss": 2.1865,
      "step": 34670
    },
    {
      "epoch": 2.1060302423027872,
      "grad_norm": 2.9071435928344727,
      "learning_rate": 2.0355883815247585e-05,
      "loss": 2.0432,
      "step": 34680
    },
    {
      "epoch": 2.106637517459161,
      "grad_norm": 3.5530612468719482,
      "learning_rate": 2.033028415193984e-05,
      "loss": 2.1458,
      "step": 34690
    },
    {
      "epoch": 2.107244792615534,
      "grad_norm": 3.9906065464019775,
      "learning_rate": 2.030469648753773e-05,
      "loss": 2.1163,
      "step": 34700
    },
    {
      "epoch": 2.1078520677719075,
      "grad_norm": 3.098353385925293,
      "learning_rate": 2.0279120832389305e-05,
      "loss": 2.1455,
      "step": 34710
    },
    {
      "epoch": 2.1084593429282807,
      "grad_norm": 3.675372362136841,
      "learning_rate": 2.0253557196837753e-05,
      "loss": 2.5131,
      "step": 34720
    },
    {
      "epoch": 2.109066618084654,
      "grad_norm": 4.1297526359558105,
      "learning_rate": 2.0228005591221432e-05,
      "loss": 2.4601,
      "step": 34730
    },
    {
      "epoch": 2.1096738932410277,
      "grad_norm": 2.997957468032837,
      "learning_rate": 2.0202466025873813e-05,
      "loss": 2.2772,
      "step": 34740
    },
    {
      "epoch": 2.110281168397401,
      "grad_norm": 2.5639913082122803,
      "learning_rate": 2.0176938511123493e-05,
      "loss": 2.1255,
      "step": 34750
    },
    {
      "epoch": 2.1108884435537743,
      "grad_norm": 2.741783618927002,
      "learning_rate": 2.01514230572942e-05,
      "loss": 2.1308,
      "step": 34760
    },
    {
      "epoch": 2.1114957187101475,
      "grad_norm": 4.587766647338867,
      "learning_rate": 2.0125919674704818e-05,
      "loss": 2.2142,
      "step": 34770
    },
    {
      "epoch": 2.112102993866521,
      "grad_norm": 3.484715700149536,
      "learning_rate": 2.010042837366931e-05,
      "loss": 2.4096,
      "step": 34780
    },
    {
      "epoch": 2.1127102690228945,
      "grad_norm": 3.0198981761932373,
      "learning_rate": 2.007494916449676e-05,
      "loss": 2.1568,
      "step": 34790
    },
    {
      "epoch": 2.1133175441792678,
      "grad_norm": 2.8620758056640625,
      "learning_rate": 2.0049482057491364e-05,
      "loss": 1.877,
      "step": 34800
    },
    {
      "epoch": 2.113924819335641,
      "grad_norm": 4.4031219482421875,
      "learning_rate": 2.0024027062952428e-05,
      "loss": 2.3843,
      "step": 34810
    },
    {
      "epoch": 2.1145320944920143,
      "grad_norm": 2.6187613010406494,
      "learning_rate": 1.999858419117436e-05,
      "loss": 2.3349,
      "step": 34820
    },
    {
      "epoch": 2.1151393696483876,
      "grad_norm": 2.8585777282714844,
      "learning_rate": 1.9973153452446652e-05,
      "loss": 2.2066,
      "step": 34830
    },
    {
      "epoch": 2.115746644804761,
      "grad_norm": 3.800962209701538,
      "learning_rate": 1.9947734857053894e-05,
      "loss": 2.2405,
      "step": 34840
    },
    {
      "epoch": 2.1163539199611345,
      "grad_norm": 4.652849197387695,
      "learning_rate": 1.9922328415275802e-05,
      "loss": 2.2618,
      "step": 34850
    },
    {
      "epoch": 2.116961195117508,
      "grad_norm": 3.55926513671875,
      "learning_rate": 1.9896934137387147e-05,
      "loss": 2.4616,
      "step": 34860
    },
    {
      "epoch": 2.117568470273881,
      "grad_norm": 5.74289083480835,
      "learning_rate": 1.9871552033657742e-05,
      "loss": 2.2494,
      "step": 34870
    },
    {
      "epoch": 2.1181757454302543,
      "grad_norm": 3.605013608932495,
      "learning_rate": 1.9846182114352518e-05,
      "loss": 2.1674,
      "step": 34880
    },
    {
      "epoch": 2.1187830205866276,
      "grad_norm": 4.095517158508301,
      "learning_rate": 1.98208243897315e-05,
      "loss": 2.2868,
      "step": 34890
    },
    {
      "epoch": 2.1193902957430013,
      "grad_norm": 3.45019268989563,
      "learning_rate": 1.979547887004975e-05,
      "loss": 2.095,
      "step": 34900
    },
    {
      "epoch": 2.1199975708993746,
      "grad_norm": 3.4730238914489746,
      "learning_rate": 1.9770145565557392e-05,
      "loss": 2.2588,
      "step": 34910
    },
    {
      "epoch": 2.120604846055748,
      "grad_norm": 4.181178569793701,
      "learning_rate": 1.9744824486499618e-05,
      "loss": 2.2601,
      "step": 34920
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 3.6072998046875,
      "learning_rate": 1.9719515643116674e-05,
      "loss": 2.2519,
      "step": 34930
    },
    {
      "epoch": 2.1218193963684944,
      "grad_norm": 3.759462356567383,
      "learning_rate": 1.9694219045643875e-05,
      "loss": 2.0314,
      "step": 34940
    },
    {
      "epoch": 2.122426671524868,
      "grad_norm": 3.4306905269622803,
      "learning_rate": 1.966893470431153e-05,
      "loss": 1.9546,
      "step": 34950
    },
    {
      "epoch": 2.1230339466812413,
      "grad_norm": 6.0321946144104,
      "learning_rate": 1.9643662629345072e-05,
      "loss": 2.4671,
      "step": 34960
    },
    {
      "epoch": 2.1236412218376146,
      "grad_norm": 4.1044206619262695,
      "learning_rate": 1.9618402830964916e-05,
      "loss": 2.1507,
      "step": 34970
    },
    {
      "epoch": 2.124248496993988,
      "grad_norm": 3.770610809326172,
      "learning_rate": 1.9593155319386523e-05,
      "loss": 2.0808,
      "step": 34980
    },
    {
      "epoch": 2.124855772150361,
      "grad_norm": 3.56752347946167,
      "learning_rate": 1.9567920104820396e-05,
      "loss": 2.1624,
      "step": 34990
    },
    {
      "epoch": 2.125463047306735,
      "grad_norm": 4.780681133270264,
      "learning_rate": 1.9542697197472047e-05,
      "loss": 2.2536,
      "step": 35000
    },
    {
      "epoch": 2.125888139916196,
      "eval_loss": 2.8937482833862305,
      "eval_runtime": 2330.0181,
      "eval_samples_per_second": 7.067,
      "eval_steps_per_second": 1.767,
      "step": 35007
    },
    {
      "epoch": 2.126070322463108,
      "grad_norm": 8.63980484008789,
      "learning_rate": 1.951748660754203e-05,
      "loss": 2.197,
      "step": 35010
    },
    {
      "epoch": 2.1266775976194814,
      "grad_norm": 6.453387260437012,
      "learning_rate": 1.94922883452259e-05,
      "loss": 2.9403,
      "step": 35020
    },
    {
      "epoch": 2.1272848727758547,
      "grad_norm": 3.9849720001220703,
      "learning_rate": 1.946710242071423e-05,
      "loss": 2.7182,
      "step": 35030
    },
    {
      "epoch": 2.127892147932228,
      "grad_norm": 2.974318504333496,
      "learning_rate": 1.9441928844192637e-05,
      "loss": 2.2088,
      "step": 35040
    },
    {
      "epoch": 2.1284994230886016,
      "grad_norm": 3.149263381958008,
      "learning_rate": 1.941676762584169e-05,
      "loss": 2.2028,
      "step": 35050
    },
    {
      "epoch": 2.129106698244975,
      "grad_norm": 3.2111384868621826,
      "learning_rate": 1.9391618775837007e-05,
      "loss": 2.2109,
      "step": 35060
    },
    {
      "epoch": 2.129713973401348,
      "grad_norm": 3.2207345962524414,
      "learning_rate": 1.936648230434917e-05,
      "loss": 2.3037,
      "step": 35070
    },
    {
      "epoch": 2.1303212485577214,
      "grad_norm": 3.6521055698394775,
      "learning_rate": 1.934135822154377e-05,
      "loss": 1.981,
      "step": 35080
    },
    {
      "epoch": 2.1309285237140947,
      "grad_norm": 2.7147216796875,
      "learning_rate": 1.9316246537581385e-05,
      "loss": 1.9218,
      "step": 35090
    },
    {
      "epoch": 2.1315357988704684,
      "grad_norm": 2.9953973293304443,
      "learning_rate": 1.9291147262617582e-05,
      "loss": 2.0173,
      "step": 35100
    },
    {
      "epoch": 2.1321430740268417,
      "grad_norm": 3.250108242034912,
      "learning_rate": 1.9266060406802893e-05,
      "loss": 1.9621,
      "step": 35110
    },
    {
      "epoch": 2.132750349183215,
      "grad_norm": 3.8947441577911377,
      "learning_rate": 1.924098598028287e-05,
      "loss": 2.2242,
      "step": 35120
    },
    {
      "epoch": 2.133357624339588,
      "grad_norm": 3.395357608795166,
      "learning_rate": 1.9215923993198e-05,
      "loss": 2.2919,
      "step": 35130
    },
    {
      "epoch": 2.1339648994959615,
      "grad_norm": 2.588012933731079,
      "learning_rate": 1.9190874455683765e-05,
      "loss": 2.1528,
      "step": 35140
    },
    {
      "epoch": 2.134572174652335,
      "grad_norm": 2.873037099838257,
      "learning_rate": 1.9165837377870542e-05,
      "loss": 2.1656,
      "step": 35150
    },
    {
      "epoch": 2.1351794498087084,
      "grad_norm": 3.792079448699951,
      "learning_rate": 1.9140812769883775e-05,
      "loss": 2.2296,
      "step": 35160
    },
    {
      "epoch": 2.1357867249650817,
      "grad_norm": 4.1810526847839355,
      "learning_rate": 1.9115800641843808e-05,
      "loss": 2.3793,
      "step": 35170
    },
    {
      "epoch": 2.136394000121455,
      "grad_norm": 5.5320725440979,
      "learning_rate": 1.909080100386591e-05,
      "loss": 2.3333,
      "step": 35180
    },
    {
      "epoch": 2.1370012752778282,
      "grad_norm": 3.8908450603485107,
      "learning_rate": 1.906581386606038e-05,
      "loss": 2.25,
      "step": 35190
    },
    {
      "epoch": 2.1376085504342015,
      "grad_norm": 2.466792106628418,
      "learning_rate": 1.9040839238532405e-05,
      "loss": 2.047,
      "step": 35200
    },
    {
      "epoch": 2.138215825590575,
      "grad_norm": 2.4834070205688477,
      "learning_rate": 1.9015877131382097e-05,
      "loss": 2.0078,
      "step": 35210
    },
    {
      "epoch": 2.1388231007469485,
      "grad_norm": 3.079371452331543,
      "learning_rate": 1.8990927554704525e-05,
      "loss": 2.2453,
      "step": 35220
    },
    {
      "epoch": 2.1394303759033217,
      "grad_norm": 3.814452648162842,
      "learning_rate": 1.8965990518589725e-05,
      "loss": 2.2029,
      "step": 35230
    },
    {
      "epoch": 2.140037651059695,
      "grad_norm": 2.69899582862854,
      "learning_rate": 1.8941066033122628e-05,
      "loss": 2.0111,
      "step": 35240
    },
    {
      "epoch": 2.1406449262160683,
      "grad_norm": 4.627962112426758,
      "learning_rate": 1.8916154108383084e-05,
      "loss": 2.3376,
      "step": 35250
    },
    {
      "epoch": 2.141252201372442,
      "grad_norm": 4.15747594833374,
      "learning_rate": 1.8891254754445847e-05,
      "loss": 2.3452,
      "step": 35260
    },
    {
      "epoch": 2.1418594765288153,
      "grad_norm": 3.812822103500366,
      "learning_rate": 1.886636798138068e-05,
      "loss": 2.3093,
      "step": 35270
    },
    {
      "epoch": 2.1424667516851885,
      "grad_norm": 3.4594480991363525,
      "learning_rate": 1.8841493799252124e-05,
      "loss": 2.1609,
      "step": 35280
    },
    {
      "epoch": 2.143074026841562,
      "grad_norm": 3.1697397232055664,
      "learning_rate": 1.881663221811972e-05,
      "loss": 2.2674,
      "step": 35290
    },
    {
      "epoch": 2.143681301997935,
      "grad_norm": 3.8062689304351807,
      "learning_rate": 1.879178324803787e-05,
      "loss": 2.2151,
      "step": 35300
    },
    {
      "epoch": 2.1442885771543088,
      "grad_norm": 5.599492073059082,
      "learning_rate": 1.8766946899055926e-05,
      "loss": 2.3343,
      "step": 35310
    },
    {
      "epoch": 2.144895852310682,
      "grad_norm": 4.088028907775879,
      "learning_rate": 1.8742123181218085e-05,
      "loss": 2.1876,
      "step": 35320
    },
    {
      "epoch": 2.1455031274670553,
      "grad_norm": 5.2918314933776855,
      "learning_rate": 1.8717312104563457e-05,
      "loss": 2.4489,
      "step": 35330
    },
    {
      "epoch": 2.1461104026234286,
      "grad_norm": 4.646781921386719,
      "learning_rate": 1.8692513679126033e-05,
      "loss": 2.4962,
      "step": 35340
    },
    {
      "epoch": 2.146717677779802,
      "grad_norm": 4.22610330581665,
      "learning_rate": 1.8667727914934698e-05,
      "loss": 2.3661,
      "step": 35350
    },
    {
      "epoch": 2.1473249529361755,
      "grad_norm": 2.767798662185669,
      "learning_rate": 1.86429548220132e-05,
      "loss": 2.1321,
      "step": 35360
    },
    {
      "epoch": 2.147932228092549,
      "grad_norm": 3.9183926582336426,
      "learning_rate": 1.8618194410380173e-05,
      "loss": 2.32,
      "step": 35370
    },
    {
      "epoch": 2.148539503248922,
      "grad_norm": 4.54756498336792,
      "learning_rate": 1.8593446690049116e-05,
      "loss": 2.4049,
      "step": 35380
    },
    {
      "epoch": 2.1491467784052953,
      "grad_norm": 4.905688285827637,
      "learning_rate": 1.8568711671028422e-05,
      "loss": 2.2728,
      "step": 35390
    },
    {
      "epoch": 2.1497540535616686,
      "grad_norm": 4.1176934242248535,
      "learning_rate": 1.854398936332132e-05,
      "loss": 2.1219,
      "step": 35400
    },
    {
      "epoch": 2.1503613287180423,
      "grad_norm": 4.606764316558838,
      "learning_rate": 1.8519279776925897e-05,
      "loss": 2.1776,
      "step": 35410
    },
    {
      "epoch": 2.1509686038744156,
      "grad_norm": 4.849236011505127,
      "learning_rate": 1.8494582921835106e-05,
      "loss": 2.1379,
      "step": 35420
    },
    {
      "epoch": 2.151575879030789,
      "grad_norm": 2.7614290714263916,
      "learning_rate": 1.8469898808036752e-05,
      "loss": 1.9573,
      "step": 35430
    },
    {
      "epoch": 2.152183154187162,
      "grad_norm": 3.2503929138183594,
      "learning_rate": 1.844522744551348e-05,
      "loss": 2.0567,
      "step": 35440
    },
    {
      "epoch": 2.1527904293435354,
      "grad_norm": 6.947665691375732,
      "learning_rate": 1.8420568844242763e-05,
      "loss": 2.0975,
      "step": 35450
    },
    {
      "epoch": 2.153397704499909,
      "grad_norm": 7.073882579803467,
      "learning_rate": 1.8395923014196974e-05,
      "loss": 2.0921,
      "step": 35460
    },
    {
      "epoch": 2.1540049796562823,
      "grad_norm": 4.067039966583252,
      "learning_rate": 1.8371289965343258e-05,
      "loss": 2.173,
      "step": 35470
    },
    {
      "epoch": 2.1546122548126556,
      "grad_norm": 4.241763114929199,
      "learning_rate": 1.8346669707643627e-05,
      "loss": 2.1455,
      "step": 35480
    },
    {
      "epoch": 2.155219529969029,
      "grad_norm": 4.266942501068115,
      "learning_rate": 1.8322062251054855e-05,
      "loss": 2.0626,
      "step": 35490
    },
    {
      "epoch": 2.155826805125402,
      "grad_norm": 3.74078106880188,
      "learning_rate": 1.829746760552864e-05,
      "loss": 2.2191,
      "step": 35500
    },
    {
      "epoch": 2.156434080281776,
      "grad_norm": 5.397404193878174,
      "learning_rate": 1.8272885781011433e-05,
      "loss": 2.324,
      "step": 35510
    },
    {
      "epoch": 2.157041355438149,
      "grad_norm": 3.926178216934204,
      "learning_rate": 1.8248316787444515e-05,
      "loss": 2.2479,
      "step": 35520
    },
    {
      "epoch": 2.1576486305945224,
      "grad_norm": 4.162092208862305,
      "learning_rate": 1.822376063476396e-05,
      "loss": 2.2516,
      "step": 35530
    },
    {
      "epoch": 2.1582559057508957,
      "grad_norm": 5.691894054412842,
      "learning_rate": 1.8199217332900703e-05,
      "loss": 2.4577,
      "step": 35540
    },
    {
      "epoch": 2.158863180907269,
      "grad_norm": 4.971480369567871,
      "learning_rate": 1.817468689178045e-05,
      "loss": 2.4357,
      "step": 35550
    },
    {
      "epoch": 2.1594704560636426,
      "grad_norm": 3.6924705505371094,
      "learning_rate": 1.8150169321323656e-05,
      "loss": 2.0784,
      "step": 35560
    },
    {
      "epoch": 2.160077731220016,
      "grad_norm": 2.7004599571228027,
      "learning_rate": 1.8125664631445633e-05,
      "loss": 2.2418,
      "step": 35570
    },
    {
      "epoch": 2.160685006376389,
      "grad_norm": 4.252342700958252,
      "learning_rate": 1.810117283205649e-05,
      "loss": 2.206,
      "step": 35580
    },
    {
      "epoch": 2.1612922815327624,
      "grad_norm": 4.509123802185059,
      "learning_rate": 1.8076693933061083e-05,
      "loss": 2.1128,
      "step": 35590
    },
    {
      "epoch": 2.1618995566891357,
      "grad_norm": 4.110330104827881,
      "learning_rate": 1.805222794435908e-05,
      "loss": 2.2602,
      "step": 35600
    },
    {
      "epoch": 2.1625068318455094,
      "grad_norm": 6.097891330718994,
      "learning_rate": 1.8027774875844904e-05,
      "loss": 2.4371,
      "step": 35610
    },
    {
      "epoch": 2.1631141070018827,
      "grad_norm": 4.8288421630859375,
      "learning_rate": 1.8003334737407772e-05,
      "loss": 2.6159,
      "step": 35620
    },
    {
      "epoch": 2.163721382158256,
      "grad_norm": 4.3128814697265625,
      "learning_rate": 1.797890753893166e-05,
      "loss": 2.5789,
      "step": 35630
    },
    {
      "epoch": 2.164328657314629,
      "grad_norm": 4.033791542053223,
      "learning_rate": 1.795449329029531e-05,
      "loss": 2.4764,
      "step": 35640
    },
    {
      "epoch": 2.1649359324710025,
      "grad_norm": 3.6549878120422363,
      "learning_rate": 1.7930092001372255e-05,
      "loss": 2.2053,
      "step": 35650
    },
    {
      "epoch": 2.165543207627376,
      "grad_norm": 3.4649477005004883,
      "learning_rate": 1.7905703682030755e-05,
      "loss": 2.1788,
      "step": 35660
    },
    {
      "epoch": 2.1661504827837494,
      "grad_norm": 2.8304953575134277,
      "learning_rate": 1.7881328342133834e-05,
      "loss": 2.0748,
      "step": 35670
    },
    {
      "epoch": 2.1667577579401227,
      "grad_norm": 3.2046244144439697,
      "learning_rate": 1.7856965991539265e-05,
      "loss": 1.9994,
      "step": 35680
    },
    {
      "epoch": 2.167365033096496,
      "grad_norm": 3.905546188354492,
      "learning_rate": 1.783261664009957e-05,
      "loss": 1.9799,
      "step": 35690
    },
    {
      "epoch": 2.1679723082528692,
      "grad_norm": 3.165713310241699,
      "learning_rate": 1.7808280297662017e-05,
      "loss": 2.2628,
      "step": 35700
    },
    {
      "epoch": 2.168579583409243,
      "grad_norm": 5.076108455657959,
      "learning_rate": 1.778395697406861e-05,
      "loss": 2.3123,
      "step": 35710
    },
    {
      "epoch": 2.169186858565616,
      "grad_norm": 3.716876268386841,
      "learning_rate": 1.7759646679156073e-05,
      "loss": 2.2066,
      "step": 35720
    },
    {
      "epoch": 2.1697941337219895,
      "grad_norm": 3.4057343006134033,
      "learning_rate": 1.773534942275591e-05,
      "loss": 2.2445,
      "step": 35730
    },
    {
      "epoch": 2.1704014088783627,
      "grad_norm": 3.7117960453033447,
      "learning_rate": 1.7711065214694295e-05,
      "loss": 2.0727,
      "step": 35740
    },
    {
      "epoch": 2.171008684034736,
      "grad_norm": 4.492752552032471,
      "learning_rate": 1.7686794064792157e-05,
      "loss": 2.4532,
      "step": 35750
    },
    {
      "epoch": 2.1716159591911097,
      "grad_norm": 4.190207004547119,
      "learning_rate": 1.766253598286513e-05,
      "loss": 2.1216,
      "step": 35760
    },
    {
      "epoch": 2.172223234347483,
      "grad_norm": 3.8361215591430664,
      "learning_rate": 1.7638290978723575e-05,
      "loss": 2.3514,
      "step": 35770
    },
    {
      "epoch": 2.1728305095038563,
      "grad_norm": 5.490647315979004,
      "learning_rate": 1.761405906217255e-05,
      "loss": 2.3387,
      "step": 35780
    },
    {
      "epoch": 2.1734377846602295,
      "grad_norm": 5.351079940795898,
      "learning_rate": 1.7589840243011833e-05,
      "loss": 2.2604,
      "step": 35790
    },
    {
      "epoch": 2.174045059816603,
      "grad_norm": 3.5725691318511963,
      "learning_rate": 1.7565634531035885e-05,
      "loss": 2.1795,
      "step": 35800
    },
    {
      "epoch": 2.174652334972976,
      "grad_norm": 4.772279739379883,
      "learning_rate": 1.7541441936033908e-05,
      "loss": 2.2008,
      "step": 35810
    },
    {
      "epoch": 2.1752596101293498,
      "grad_norm": 3.627906084060669,
      "learning_rate": 1.7517262467789763e-05,
      "loss": 2.0709,
      "step": 35820
    },
    {
      "epoch": 2.175866885285723,
      "grad_norm": 5.388303279876709,
      "learning_rate": 1.7493096136082022e-05,
      "loss": 2.3089,
      "step": 35830
    },
    {
      "epoch": 2.1764741604420963,
      "grad_norm": 4.647921085357666,
      "learning_rate": 1.746894295068389e-05,
      "loss": 2.1847,
      "step": 35840
    },
    {
      "epoch": 2.1770814355984696,
      "grad_norm": 2.9376626014709473,
      "learning_rate": 1.7444802921363346e-05,
      "loss": 1.9045,
      "step": 35850
    },
    {
      "epoch": 2.177688710754843,
      "grad_norm": 2.9109113216400146,
      "learning_rate": 1.742067605788299e-05,
      "loss": 2.0179,
      "step": 35860
    },
    {
      "epoch": 2.1782959859112165,
      "grad_norm": 5.259521484375,
      "learning_rate": 1.739656237000009e-05,
      "loss": 2.175,
      "step": 35870
    },
    {
      "epoch": 2.17890326106759,
      "grad_norm": 3.909144639968872,
      "learning_rate": 1.7372461867466645e-05,
      "loss": 2.2841,
      "step": 35880
    },
    {
      "epoch": 2.179510536223963,
      "grad_norm": 4.973754405975342,
      "learning_rate": 1.7348374560029267e-05,
      "loss": 2.1199,
      "step": 35890
    },
    {
      "epoch": 2.1801178113803363,
      "grad_norm": 4.268054008483887,
      "learning_rate": 1.7324300457429233e-05,
      "loss": 2.3976,
      "step": 35900
    },
    {
      "epoch": 2.1807250865367096,
      "grad_norm": 4.899366855621338,
      "learning_rate": 1.7300239569402477e-05,
      "loss": 2.3125,
      "step": 35910
    },
    {
      "epoch": 2.1813323616930833,
      "grad_norm": 3.870312452316284,
      "learning_rate": 1.7276191905679646e-05,
      "loss": 2.071,
      "step": 35920
    },
    {
      "epoch": 2.1819396368494566,
      "grad_norm": 4.494777679443359,
      "learning_rate": 1.725215747598598e-05,
      "loss": 2.1807,
      "step": 35930
    },
    {
      "epoch": 2.18254691200583,
      "grad_norm": 4.260366439819336,
      "learning_rate": 1.722813629004139e-05,
      "loss": 2.14,
      "step": 35940
    },
    {
      "epoch": 2.183154187162203,
      "grad_norm": 5.086243629455566,
      "learning_rate": 1.7204128357560417e-05,
      "loss": 2.2742,
      "step": 35950
    },
    {
      "epoch": 2.1837614623185764,
      "grad_norm": 4.1308393478393555,
      "learning_rate": 1.7180133688252266e-05,
      "loss": 2.2369,
      "step": 35960
    },
    {
      "epoch": 2.18436873747495,
      "grad_norm": 4.064233779907227,
      "learning_rate": 1.7156152291820742e-05,
      "loss": 2.1291,
      "step": 35970
    },
    {
      "epoch": 2.1849760126313233,
      "grad_norm": 5.311744213104248,
      "learning_rate": 1.7132184177964324e-05,
      "loss": 2.2987,
      "step": 35980
    },
    {
      "epoch": 2.1855832877876966,
      "grad_norm": 4.550829887390137,
      "learning_rate": 1.7108229356376064e-05,
      "loss": 2.241,
      "step": 35990
    },
    {
      "epoch": 2.18619056294407,
      "grad_norm": 4.840721130371094,
      "learning_rate": 1.708428783674373e-05,
      "loss": 2.2282,
      "step": 36000
    },
    {
      "epoch": 2.186797838100443,
      "grad_norm": 4.154098987579346,
      "learning_rate": 1.706035962874961e-05,
      "loss": 2.473,
      "step": 36010
    },
    {
      "epoch": 2.187405113256817,
      "grad_norm": 5.36306095123291,
      "learning_rate": 1.7036444742070673e-05,
      "loss": 2.0864,
      "step": 36020
    },
    {
      "epoch": 2.18801238841319,
      "grad_norm": 3.870222568511963,
      "learning_rate": 1.7012543186378478e-05,
      "loss": 1.9984,
      "step": 36030
    },
    {
      "epoch": 2.1886196635695634,
      "grad_norm": 2.273092031478882,
      "learning_rate": 1.698865497133918e-05,
      "loss": 2.1547,
      "step": 36040
    },
    {
      "epoch": 2.1892269387259367,
      "grad_norm": 2.676947593688965,
      "learning_rate": 1.696478010661357e-05,
      "loss": 1.9421,
      "step": 36050
    },
    {
      "epoch": 2.18983421388231,
      "grad_norm": 3.614448070526123,
      "learning_rate": 1.6940918601857016e-05,
      "loss": 2.093,
      "step": 36060
    },
    {
      "epoch": 2.1904414890386836,
      "grad_norm": 5.698238849639893,
      "learning_rate": 1.691707046671947e-05,
      "loss": 2.3044,
      "step": 36070
    },
    {
      "epoch": 2.191048764195057,
      "grad_norm": 3.4741532802581787,
      "learning_rate": 1.689323571084554e-05,
      "loss": 1.9602,
      "step": 36080
    },
    {
      "epoch": 2.19165603935143,
      "grad_norm": 3.462704658508301,
      "learning_rate": 1.6869414343874356e-05,
      "loss": 2.2284,
      "step": 36090
    },
    {
      "epoch": 2.1922633145078034,
      "grad_norm": 5.206900596618652,
      "learning_rate": 1.684560637543967e-05,
      "loss": 2.4185,
      "step": 36100
    },
    {
      "epoch": 2.1928705896641767,
      "grad_norm": 4.146042346954346,
      "learning_rate": 1.6821811815169793e-05,
      "loss": 2.3424,
      "step": 36110
    },
    {
      "epoch": 2.19347786482055,
      "grad_norm": 3.472050428390503,
      "learning_rate": 1.6798030672687627e-05,
      "loss": 2.2905,
      "step": 36120
    },
    {
      "epoch": 2.1940851399769237,
      "grad_norm": 4.500948429107666,
      "learning_rate": 1.677426295761064e-05,
      "loss": 2.2909,
      "step": 36130
    },
    {
      "epoch": 2.194692415133297,
      "grad_norm": 3.7024173736572266,
      "learning_rate": 1.6750508679550864e-05,
      "loss": 2.0703,
      "step": 36140
    },
    {
      "epoch": 2.19529969028967,
      "grad_norm": 3.786808729171753,
      "learning_rate": 1.6726767848114944e-05,
      "loss": 2.2532,
      "step": 36150
    },
    {
      "epoch": 2.1959069654460435,
      "grad_norm": 2.982576847076416,
      "learning_rate": 1.6703040472904026e-05,
      "loss": 2.0865,
      "step": 36160
    },
    {
      "epoch": 2.1965142406024167,
      "grad_norm": 3.342329263687134,
      "learning_rate": 1.6679326563513858e-05,
      "loss": 2.2541,
      "step": 36170
    },
    {
      "epoch": 2.1971215157587904,
      "grad_norm": 3.0540077686309814,
      "learning_rate": 1.6655626129534673e-05,
      "loss": 2.1353,
      "step": 36180
    },
    {
      "epoch": 2.1977287909151637,
      "grad_norm": 2.356170892715454,
      "learning_rate": 1.6631939180551353e-05,
      "loss": 2.256,
      "step": 36190
    },
    {
      "epoch": 2.198336066071537,
      "grad_norm": 3.6823530197143555,
      "learning_rate": 1.6608265726143262e-05,
      "loss": 2.149,
      "step": 36200
    },
    {
      "epoch": 2.1989433412279102,
      "grad_norm": 3.7091634273529053,
      "learning_rate": 1.6584605775884325e-05,
      "loss": 2.2473,
      "step": 36210
    },
    {
      "epoch": 2.1995506163842835,
      "grad_norm": 4.558063507080078,
      "learning_rate": 1.656095933934298e-05,
      "loss": 2.3345,
      "step": 36220
    },
    {
      "epoch": 2.200157891540657,
      "grad_norm": 3.5321006774902344,
      "learning_rate": 1.653732642608227e-05,
      "loss": 2.1953,
      "step": 36230
    },
    {
      "epoch": 2.2007651666970305,
      "grad_norm": 3.239922046661377,
      "learning_rate": 1.6513707045659687e-05,
      "loss": 2.1102,
      "step": 36240
    },
    {
      "epoch": 2.2013724418534037,
      "grad_norm": 3.7476258277893066,
      "learning_rate": 1.6490101207627284e-05,
      "loss": 2.2004,
      "step": 36250
    },
    {
      "epoch": 2.201979717009777,
      "grad_norm": 5.695013999938965,
      "learning_rate": 1.6466508921531627e-05,
      "loss": 2.4488,
      "step": 36260
    },
    {
      "epoch": 2.2025869921661503,
      "grad_norm": 4.822564601898193,
      "learning_rate": 1.6442930196913846e-05,
      "loss": 2.2311,
      "step": 36270
    },
    {
      "epoch": 2.203194267322524,
      "grad_norm": 4.237809658050537,
      "learning_rate": 1.641936504330954e-05,
      "loss": 2.5829,
      "step": 36280
    },
    {
      "epoch": 2.2038015424788973,
      "grad_norm": 4.629238128662109,
      "learning_rate": 1.6395813470248832e-05,
      "loss": 2.3065,
      "step": 36290
    },
    {
      "epoch": 2.2044088176352705,
      "grad_norm": 2.9099037647247314,
      "learning_rate": 1.6372275487256344e-05,
      "loss": 2.4424,
      "step": 36300
    },
    {
      "epoch": 2.205016092791644,
      "grad_norm": 6.002307891845703,
      "learning_rate": 1.6348751103851224e-05,
      "loss": 2.4032,
      "step": 36310
    },
    {
      "epoch": 2.205623367948017,
      "grad_norm": 4.129472732543945,
      "learning_rate": 1.63252403295471e-05,
      "loss": 2.1747,
      "step": 36320
    },
    {
      "epoch": 2.2062306431043908,
      "grad_norm": 4.004919528961182,
      "learning_rate": 1.630174317385208e-05,
      "loss": 2.2267,
      "step": 36330
    },
    {
      "epoch": 2.206837918260764,
      "grad_norm": 5.415130138397217,
      "learning_rate": 1.6278259646268833e-05,
      "loss": 2.6094,
      "step": 36340
    },
    {
      "epoch": 2.2074451934171373,
      "grad_norm": 3.8393898010253906,
      "learning_rate": 1.6254789756294454e-05,
      "loss": 2.4702,
      "step": 36350
    },
    {
      "epoch": 2.2080524685735106,
      "grad_norm": 3.5577051639556885,
      "learning_rate": 1.6231333513420532e-05,
      "loss": 2.3265,
      "step": 36360
    },
    {
      "epoch": 2.208659743729884,
      "grad_norm": 5.151214599609375,
      "learning_rate": 1.6207890927133157e-05,
      "loss": 2.2599,
      "step": 36370
    },
    {
      "epoch": 2.2092670188862575,
      "grad_norm": 3.2759287357330322,
      "learning_rate": 1.618446200691287e-05,
      "loss": 2.3601,
      "step": 36380
    },
    {
      "epoch": 2.209874294042631,
      "grad_norm": 4.0052032470703125,
      "learning_rate": 1.6161046762234716e-05,
      "loss": 2.2001,
      "step": 36390
    },
    {
      "epoch": 2.210481569199004,
      "grad_norm": 4.600954055786133,
      "learning_rate": 1.613764520256818e-05,
      "loss": 2.1082,
      "step": 36400
    },
    {
      "epoch": 2.2110888443553773,
      "grad_norm": 3.8600733280181885,
      "learning_rate": 1.611425733737721e-05,
      "loss": 2.2664,
      "step": 36410
    },
    {
      "epoch": 2.2116961195117506,
      "grad_norm": 4.049044609069824,
      "learning_rate": 1.609088317612027e-05,
      "loss": 2.2887,
      "step": 36420
    },
    {
      "epoch": 2.2123033946681243,
      "grad_norm": 4.289280414581299,
      "learning_rate": 1.6067522728250222e-05,
      "loss": 2.1678,
      "step": 36430
    },
    {
      "epoch": 2.2129106698244976,
      "grad_norm": 5.234183311462402,
      "learning_rate": 1.6044176003214408e-05,
      "loss": 2.3717,
      "step": 36440
    },
    {
      "epoch": 2.213517944980871,
      "grad_norm": 4.824693202972412,
      "learning_rate": 1.602084301045461e-05,
      "loss": 2.3525,
      "step": 36450
    },
    {
      "epoch": 2.214125220137244,
      "grad_norm": 5.705555438995361,
      "learning_rate": 1.5997523759407064e-05,
      "loss": 2.3419,
      "step": 36460
    },
    {
      "epoch": 2.2147324952936174,
      "grad_norm": 5.907956600189209,
      "learning_rate": 1.5974218259502448e-05,
      "loss": 2.457,
      "step": 36470
    },
    {
      "epoch": 2.215339770449991,
      "grad_norm": 5.2924981117248535,
      "learning_rate": 1.5950926520165875e-05,
      "loss": 2.1805,
      "step": 36480
    },
    {
      "epoch": 2.2159470456063644,
      "grad_norm": 4.684941291809082,
      "learning_rate": 1.592764855081688e-05,
      "loss": 2.1911,
      "step": 36490
    },
    {
      "epoch": 2.2165543207627376,
      "grad_norm": 3.861600637435913,
      "learning_rate": 1.590438436086948e-05,
      "loss": 2.1243,
      "step": 36500
    },
    {
      "epoch": 2.217161595919111,
      "grad_norm": 4.060206890106201,
      "learning_rate": 1.588113395973208e-05,
      "loss": 2.4024,
      "step": 36510
    },
    {
      "epoch": 2.217768871075484,
      "grad_norm": 4.2036237716674805,
      "learning_rate": 1.5857897356807477e-05,
      "loss": 2.1943,
      "step": 36520
    },
    {
      "epoch": 2.218376146231858,
      "grad_norm": 5.231930255889893,
      "learning_rate": 1.5834674561492935e-05,
      "loss": 2.098,
      "step": 36530
    },
    {
      "epoch": 2.218983421388231,
      "grad_norm": 4.983797550201416,
      "learning_rate": 1.581146558318014e-05,
      "loss": 2.3411,
      "step": 36540
    },
    {
      "epoch": 2.2195906965446044,
      "grad_norm": 3.6353888511657715,
      "learning_rate": 1.5788270431255164e-05,
      "loss": 2.1135,
      "step": 36550
    },
    {
      "epoch": 2.2201979717009777,
      "grad_norm": 3.982391119003296,
      "learning_rate": 1.5765089115098485e-05,
      "loss": 2.0235,
      "step": 36560
    },
    {
      "epoch": 2.220805246857351,
      "grad_norm": 4.592626571655273,
      "learning_rate": 1.574192164408502e-05,
      "loss": 2.1727,
      "step": 36570
    },
    {
      "epoch": 2.2214125220137246,
      "grad_norm": 4.679244041442871,
      "learning_rate": 1.5718768027584063e-05,
      "loss": 2.5154,
      "step": 36580
    },
    {
      "epoch": 2.222019797170098,
      "grad_norm": 5.305424213409424,
      "learning_rate": 1.5695628274959278e-05,
      "loss": 2.5629,
      "step": 36590
    },
    {
      "epoch": 2.222627072326471,
      "grad_norm": 4.802842140197754,
      "learning_rate": 1.567250239556875e-05,
      "loss": 2.4851,
      "step": 36600
    },
    {
      "epoch": 2.2232343474828444,
      "grad_norm": 5.5387654304504395,
      "learning_rate": 1.564939039876498e-05,
      "loss": 2.5038,
      "step": 36610
    },
    {
      "epoch": 2.2238416226392177,
      "grad_norm": 4.230721950531006,
      "learning_rate": 1.5626292293894816e-05,
      "loss": 2.4664,
      "step": 36620
    },
    {
      "epoch": 2.2244488977955914,
      "grad_norm": 5.070956707000732,
      "learning_rate": 1.5603208090299498e-05,
      "loss": 2.686,
      "step": 36630
    },
    {
      "epoch": 2.2250561729519647,
      "grad_norm": 5.131174564361572,
      "learning_rate": 1.5580137797314642e-05,
      "loss": 2.4328,
      "step": 36640
    },
    {
      "epoch": 2.225663448108338,
      "grad_norm": 4.55919075012207,
      "learning_rate": 1.5557081424270247e-05,
      "loss": 2.3006,
      "step": 36650
    },
    {
      "epoch": 2.226270723264711,
      "grad_norm": 5.077537536621094,
      "learning_rate": 1.5534038980490677e-05,
      "loss": 2.3098,
      "step": 36660
    },
    {
      "epoch": 2.2268779984210845,
      "grad_norm": 4.44719123840332,
      "learning_rate": 1.5511010475294664e-05,
      "loss": 2.3502,
      "step": 36670
    },
    {
      "epoch": 2.227485273577458,
      "grad_norm": 4.876386642456055,
      "learning_rate": 1.5487995917995278e-05,
      "loss": 2.3839,
      "step": 36680
    },
    {
      "epoch": 2.2280925487338314,
      "grad_norm": 4.820370197296143,
      "learning_rate": 1.5464995317900015e-05,
      "loss": 2.3475,
      "step": 36690
    },
    {
      "epoch": 2.2286998238902047,
      "grad_norm": 3.394381284713745,
      "learning_rate": 1.5442008684310665e-05,
      "loss": 2.2644,
      "step": 36700
    },
    {
      "epoch": 2.229307099046578,
      "grad_norm": 3.6209051609039307,
      "learning_rate": 1.5419036026523388e-05,
      "loss": 2.3646,
      "step": 36710
    },
    {
      "epoch": 2.2299143742029512,
      "grad_norm": 4.0112690925598145,
      "learning_rate": 1.5396077353828685e-05,
      "loss": 2.49,
      "step": 36720
    },
    {
      "epoch": 2.2305216493593245,
      "grad_norm": 4.674363136291504,
      "learning_rate": 1.537313267551142e-05,
      "loss": 2.3794,
      "step": 36730
    },
    {
      "epoch": 2.231128924515698,
      "grad_norm": 4.011247634887695,
      "learning_rate": 1.5350202000850783e-05,
      "loss": 2.3984,
      "step": 36740
    },
    {
      "epoch": 2.2317361996720715,
      "grad_norm": 4.888919353485107,
      "learning_rate": 1.5327285339120307e-05,
      "loss": 2.4739,
      "step": 36750
    },
    {
      "epoch": 2.2323434748284448,
      "grad_norm": 5.496581554412842,
      "learning_rate": 1.5304382699587828e-05,
      "loss": 2.4516,
      "step": 36760
    },
    {
      "epoch": 2.232950749984818,
      "grad_norm": 3.6972596645355225,
      "learning_rate": 1.528149409151558e-05,
      "loss": 2.3883,
      "step": 36770
    },
    {
      "epoch": 2.2335580251411913,
      "grad_norm": 3.368239402770996,
      "learning_rate": 1.5258619524160066e-05,
      "loss": 2.2428,
      "step": 36780
    },
    {
      "epoch": 2.234165300297565,
      "grad_norm": 3.040630578994751,
      "learning_rate": 1.5235759006772132e-05,
      "loss": 2.0987,
      "step": 36790
    },
    {
      "epoch": 2.2347725754539383,
      "grad_norm": 2.214555025100708,
      "learning_rate": 1.5212912548596896e-05,
      "loss": 2.1117,
      "step": 36800
    },
    {
      "epoch": 2.2353798506103115,
      "grad_norm": 2.5884146690368652,
      "learning_rate": 1.5190080158873876e-05,
      "loss": 2.1012,
      "step": 36810
    },
    {
      "epoch": 2.235987125766685,
      "grad_norm": 4.132891654968262,
      "learning_rate": 1.5167261846836833e-05,
      "loss": 2.1584,
      "step": 36820
    },
    {
      "epoch": 2.236594400923058,
      "grad_norm": 3.766552209854126,
      "learning_rate": 1.5144457621713848e-05,
      "loss": 2.2559,
      "step": 36830
    },
    {
      "epoch": 2.2372016760794318,
      "grad_norm": 3.6453239917755127,
      "learning_rate": 1.5121667492727337e-05,
      "loss": 2.188,
      "step": 36840
    },
    {
      "epoch": 2.237808951235805,
      "grad_norm": 2.857476234436035,
      "learning_rate": 1.5098891469093979e-05,
      "loss": 2.0081,
      "step": 36850
    },
    {
      "epoch": 2.2384162263921783,
      "grad_norm": 4.210121154785156,
      "learning_rate": 1.5076129560024777e-05,
      "loss": 2.2841,
      "step": 36860
    },
    {
      "epoch": 2.2390235015485516,
      "grad_norm": 3.0913991928100586,
      "learning_rate": 1.5053381774724957e-05,
      "loss": 2.1503,
      "step": 36870
    },
    {
      "epoch": 2.239630776704925,
      "grad_norm": 3.6712002754211426,
      "learning_rate": 1.5030648122394136e-05,
      "loss": 2.2632,
      "step": 36880
    },
    {
      "epoch": 2.2402380518612985,
      "grad_norm": 4.283120632171631,
      "learning_rate": 1.5007928612226152e-05,
      "loss": 2.4126,
      "step": 36890
    },
    {
      "epoch": 2.240845327017672,
      "grad_norm": 4.16704797744751,
      "learning_rate": 1.4985223253409137e-05,
      "loss": 2.2667,
      "step": 36900
    },
    {
      "epoch": 2.241452602174045,
      "grad_norm": 2.8955538272857666,
      "learning_rate": 1.4962532055125484e-05,
      "loss": 2.0617,
      "step": 36910
    },
    {
      "epoch": 2.2420598773304183,
      "grad_norm": 3.829211950302124,
      "learning_rate": 1.4939855026551914e-05,
      "loss": 2.3788,
      "step": 36920
    },
    {
      "epoch": 2.2426671524867916,
      "grad_norm": 4.6470255851745605,
      "learning_rate": 1.491719217685934e-05,
      "loss": 2.4545,
      "step": 36930
    },
    {
      "epoch": 2.2432744276431653,
      "grad_norm": 3.3837575912475586,
      "learning_rate": 1.4894543515212995e-05,
      "loss": 2.5208,
      "step": 36940
    },
    {
      "epoch": 2.2438817027995386,
      "grad_norm": 4.694288730621338,
      "learning_rate": 1.4871909050772338e-05,
      "loss": 2.4783,
      "step": 36950
    },
    {
      "epoch": 2.244488977955912,
      "grad_norm": 3.8754658699035645,
      "learning_rate": 1.4849288792691141e-05,
      "loss": 2.4861,
      "step": 36960
    },
    {
      "epoch": 2.245096253112285,
      "grad_norm": 3.4552667140960693,
      "learning_rate": 1.4826682750117376e-05,
      "loss": 2.2706,
      "step": 36970
    },
    {
      "epoch": 2.2457035282686584,
      "grad_norm": 3.7249040603637695,
      "learning_rate": 1.4804090932193298e-05,
      "loss": 2.2507,
      "step": 36980
    },
    {
      "epoch": 2.246310803425032,
      "grad_norm": 4.4185991287231445,
      "learning_rate": 1.4781513348055387e-05,
      "loss": 2.6565,
      "step": 36990
    },
    {
      "epoch": 2.2469180785814054,
      "grad_norm": 4.925186634063721,
      "learning_rate": 1.4758950006834382e-05,
      "loss": 2.503,
      "step": 37000
    },
    {
      "epoch": 2.2475253537377786,
      "grad_norm": 5.647678852081299,
      "learning_rate": 1.4736400917655263e-05,
      "loss": 2.474,
      "step": 37010
    },
    {
      "epoch": 2.248132628894152,
      "grad_norm": 4.669948101043701,
      "learning_rate": 1.471386608963723e-05,
      "loss": 2.4216,
      "step": 37020
    },
    {
      "epoch": 2.248739904050525,
      "grad_norm": 4.019589424133301,
      "learning_rate": 1.4691345531893713e-05,
      "loss": 2.2803,
      "step": 37030
    },
    {
      "epoch": 2.2493471792068984,
      "grad_norm": 6.046123027801514,
      "learning_rate": 1.4668839253532424e-05,
      "loss": 2.3877,
      "step": 37040
    },
    {
      "epoch": 2.249954454363272,
      "grad_norm": 5.133358001708984,
      "learning_rate": 1.4646347263655235e-05,
      "loss": 2.2718,
      "step": 37050
    },
    {
      "epoch": 2.2505617295196454,
      "grad_norm": 4.403951168060303,
      "learning_rate": 1.4623869571358273e-05,
      "loss": 2.0969,
      "step": 37060
    },
    {
      "epoch": 2.2511690046760187,
      "grad_norm": 4.117224216461182,
      "learning_rate": 1.4601406185731869e-05,
      "loss": 2.3659,
      "step": 37070
    },
    {
      "epoch": 2.251776279832392,
      "grad_norm": 4.792459011077881,
      "learning_rate": 1.4578957115860575e-05,
      "loss": 2.1982,
      "step": 37080
    },
    {
      "epoch": 2.252383554988765,
      "grad_norm": 4.907799243927002,
      "learning_rate": 1.455652237082315e-05,
      "loss": 2.2463,
      "step": 37090
    },
    {
      "epoch": 2.252990830145139,
      "grad_norm": 4.9766387939453125,
      "learning_rate": 1.4534101959692554e-05,
      "loss": 2.1962,
      "step": 37100
    },
    {
      "epoch": 2.253598105301512,
      "grad_norm": 4.954466819763184,
      "learning_rate": 1.4511695891535987e-05,
      "loss": 2.2206,
      "step": 37110
    },
    {
      "epoch": 2.2542053804578854,
      "grad_norm": 4.424589157104492,
      "learning_rate": 1.4489304175414797e-05,
      "loss": 2.3332,
      "step": 37120
    },
    {
      "epoch": 2.2548126556142587,
      "grad_norm": 6.668490886688232,
      "learning_rate": 1.4466926820384557e-05,
      "loss": 2.3142,
      "step": 37130
    },
    {
      "epoch": 2.255419930770632,
      "grad_norm": 7.043938636779785,
      "learning_rate": 1.4444563835495028e-05,
      "loss": 2.2181,
      "step": 37140
    },
    {
      "epoch": 2.2560272059270057,
      "grad_norm": 6.468805313110352,
      "learning_rate": 1.4422215229790154e-05,
      "loss": 2.3539,
      "step": 37150
    },
    {
      "epoch": 2.256634481083379,
      "grad_norm": 5.636246204376221,
      "learning_rate": 1.4399881012308063e-05,
      "loss": 2.3086,
      "step": 37160
    },
    {
      "epoch": 2.257241756239752,
      "grad_norm": 4.818166732788086,
      "learning_rate": 1.4377561192081073e-05,
      "loss": 2.5729,
      "step": 37170
    },
    {
      "epoch": 2.2578490313961255,
      "grad_norm": 3.672271728515625,
      "learning_rate": 1.4355255778135662e-05,
      "loss": 2.3359,
      "step": 37180
    },
    {
      "epoch": 2.2584563065524987,
      "grad_norm": 3.844717025756836,
      "learning_rate": 1.433296477949252e-05,
      "loss": 2.1691,
      "step": 37190
    },
    {
      "epoch": 2.2590635817088724,
      "grad_norm": 5.993094444274902,
      "learning_rate": 1.4310688205166483e-05,
      "loss": 2.1056,
      "step": 37200
    },
    {
      "epoch": 2.2596708568652457,
      "grad_norm": 3.6613056659698486,
      "learning_rate": 1.4288426064166527e-05,
      "loss": 2.3639,
      "step": 37210
    },
    {
      "epoch": 2.260278132021619,
      "grad_norm": 4.3517937660217285,
      "learning_rate": 1.4266178365495808e-05,
      "loss": 2.3308,
      "step": 37220
    },
    {
      "epoch": 2.2608854071779922,
      "grad_norm": 4.757228374481201,
      "learning_rate": 1.4243945118151686e-05,
      "loss": 2.1722,
      "step": 37230
    },
    {
      "epoch": 2.2614926823343655,
      "grad_norm": 4.490314483642578,
      "learning_rate": 1.4221726331125624e-05,
      "loss": 2.4344,
      "step": 37240
    },
    {
      "epoch": 2.262099957490739,
      "grad_norm": 4.778746128082275,
      "learning_rate": 1.419952201340325e-05,
      "loss": 2.4978,
      "step": 37250
    },
    {
      "epoch": 2.2627072326471125,
      "grad_norm": 5.319468021392822,
      "learning_rate": 1.4177332173964346e-05,
      "loss": 2.5403,
      "step": 37260
    },
    {
      "epoch": 2.2633145078034858,
      "grad_norm": 4.753818511962891,
      "learning_rate": 1.4155156821782845e-05,
      "loss": 2.3953,
      "step": 37270
    },
    {
      "epoch": 2.263921782959859,
      "grad_norm": 5.578675746917725,
      "learning_rate": 1.4132995965826795e-05,
      "loss": 2.4448,
      "step": 37280
    },
    {
      "epoch": 2.2645290581162323,
      "grad_norm": 3.764521837234497,
      "learning_rate": 1.4110849615058397e-05,
      "loss": 2.3155,
      "step": 37290
    },
    {
      "epoch": 2.265136333272606,
      "grad_norm": 4.90271520614624,
      "learning_rate": 1.4088717778434018e-05,
      "loss": 2.449,
      "step": 37300
    },
    {
      "epoch": 2.2657436084289793,
      "grad_norm": 6.9575886726379395,
      "learning_rate": 1.4066600464904106e-05,
      "loss": 2.2642,
      "step": 37310
    },
    {
      "epoch": 2.2663508835853525,
      "grad_norm": 4.544987678527832,
      "learning_rate": 1.4044497683413249e-05,
      "loss": 2.5594,
      "step": 37320
    },
    {
      "epoch": 2.266958158741726,
      "grad_norm": 5.555137634277344,
      "learning_rate": 1.4022409442900175e-05,
      "loss": 2.3617,
      "step": 37330
    },
    {
      "epoch": 2.267565433898099,
      "grad_norm": 4.100829124450684,
      "learning_rate": 1.4000335752297716e-05,
      "loss": 2.2668,
      "step": 37340
    },
    {
      "epoch": 2.2681727090544728,
      "grad_norm": 6.6997880935668945,
      "learning_rate": 1.3978276620532826e-05,
      "loss": 2.3875,
      "step": 37350
    },
    {
      "epoch": 2.268779984210846,
      "grad_norm": 5.464487552642822,
      "learning_rate": 1.3956232056526564e-05,
      "loss": 2.3966,
      "step": 37360
    },
    {
      "epoch": 2.2693872593672193,
      "grad_norm": 4.702694416046143,
      "learning_rate": 1.3934202069194091e-05,
      "loss": 2.216,
      "step": 37370
    },
    {
      "epoch": 2.2699945345235926,
      "grad_norm": 4.686768054962158,
      "learning_rate": 1.3912186667444716e-05,
      "loss": 2.3909,
      "step": 37380
    },
    {
      "epoch": 2.270601809679966,
      "grad_norm": 6.675116062164307,
      "learning_rate": 1.3890185860181804e-05,
      "loss": 2.4324,
      "step": 37390
    },
    {
      "epoch": 2.2712090848363395,
      "grad_norm": 5.9816975593566895,
      "learning_rate": 1.3868199656302828e-05,
      "loss": 2.5835,
      "step": 37400
    },
    {
      "epoch": 2.271816359992713,
      "grad_norm": 4.9774699211120605,
      "learning_rate": 1.3846228064699358e-05,
      "loss": 2.4056,
      "step": 37410
    },
    {
      "epoch": 2.272423635149086,
      "grad_norm": 3.7527499198913574,
      "learning_rate": 1.3824271094257057e-05,
      "loss": 2.0962,
      "step": 37420
    },
    {
      "epoch": 2.2730309103054593,
      "grad_norm": 4.04290771484375,
      "learning_rate": 1.3802328753855676e-05,
      "loss": 2.174,
      "step": 37430
    },
    {
      "epoch": 2.2736381854618326,
      "grad_norm": 5.8360676765441895,
      "learning_rate": 1.3780401052369046e-05,
      "loss": 2.1943,
      "step": 37440
    },
    {
      "epoch": 2.2742454606182063,
      "grad_norm": 4.40539026260376,
      "learning_rate": 1.3758487998665065e-05,
      "loss": 2.1034,
      "step": 37450
    },
    {
      "epoch": 2.2748527357745796,
      "grad_norm": 5.7447590827941895,
      "learning_rate": 1.3736589601605749e-05,
      "loss": 2.3066,
      "step": 37460
    },
    {
      "epoch": 2.275460010930953,
      "grad_norm": 5.457333087921143,
      "learning_rate": 1.371470587004714e-05,
      "loss": 2.2116,
      "step": 37470
    },
    {
      "epoch": 2.276067286087326,
      "grad_norm": 4.963283538818359,
      "learning_rate": 1.3692836812839388e-05,
      "loss": 2.2672,
      "step": 37480
    },
    {
      "epoch": 2.2766745612436994,
      "grad_norm": 7.273168087005615,
      "learning_rate": 1.367098243882664e-05,
      "loss": 2.7186,
      "step": 37490
    },
    {
      "epoch": 2.277281836400073,
      "grad_norm": 5.165326118469238,
      "learning_rate": 1.3649142756847206e-05,
      "loss": 2.4945,
      "step": 37500
    },
    {
      "epoch": 2.2778891115564464,
      "grad_norm": 5.320772647857666,
      "learning_rate": 1.3627317775733372e-05,
      "loss": 2.3872,
      "step": 37510
    },
    {
      "epoch": 2.2784963867128196,
      "grad_norm": 4.119405746459961,
      "learning_rate": 1.3605507504311499e-05,
      "loss": 2.2779,
      "step": 37520
    },
    {
      "epoch": 2.279103661869193,
      "grad_norm": 4.791367530822754,
      "learning_rate": 1.3583711951402045e-05,
      "loss": 2.3718,
      "step": 37530
    },
    {
      "epoch": 2.279710937025566,
      "grad_norm": 4.264337539672852,
      "learning_rate": 1.3561931125819471e-05,
      "loss": 2.1944,
      "step": 37540
    },
    {
      "epoch": 2.28031821218194,
      "grad_norm": 3.7377121448516846,
      "learning_rate": 1.3540165036372266e-05,
      "loss": 2.1695,
      "step": 37550
    },
    {
      "epoch": 2.280925487338313,
      "grad_norm": 4.20439338684082,
      "learning_rate": 1.3518413691862974e-05,
      "loss": 2.3296,
      "step": 37560
    },
    {
      "epoch": 2.2815327624946864,
      "grad_norm": 4.477372169494629,
      "learning_rate": 1.3496677101088223e-05,
      "loss": 2.2384,
      "step": 37570
    },
    {
      "epoch": 2.2821400376510597,
      "grad_norm": 4.652522087097168,
      "learning_rate": 1.3474955272838619e-05,
      "loss": 2.3006,
      "step": 37580
    },
    {
      "epoch": 2.282747312807433,
      "grad_norm": 5.033714294433594,
      "learning_rate": 1.3453248215898812e-05,
      "loss": 2.2311,
      "step": 37590
    },
    {
      "epoch": 2.2833545879638066,
      "grad_norm": 5.211798191070557,
      "learning_rate": 1.3431555939047468e-05,
      "loss": 2.5637,
      "step": 37600
    },
    {
      "epoch": 2.28396186312018,
      "grad_norm": 5.100250244140625,
      "learning_rate": 1.340987845105734e-05,
      "loss": 2.3632,
      "step": 37610
    },
    {
      "epoch": 2.284569138276553,
      "grad_norm": 4.904970169067383,
      "learning_rate": 1.33882157606951e-05,
      "loss": 2.547,
      "step": 37620
    },
    {
      "epoch": 2.2851764134329264,
      "grad_norm": 3.958563804626465,
      "learning_rate": 1.3366567876721503e-05,
      "loss": 2.2935,
      "step": 37630
    },
    {
      "epoch": 2.2857836885892997,
      "grad_norm": 3.830760955810547,
      "learning_rate": 1.3344934807891273e-05,
      "loss": 2.1492,
      "step": 37640
    },
    {
      "epoch": 2.2863909637456734,
      "grad_norm": 5.176144599914551,
      "learning_rate": 1.3323316562953215e-05,
      "loss": 2.1465,
      "step": 37650
    },
    {
      "epoch": 2.2869982389020467,
      "grad_norm": 4.421435832977295,
      "learning_rate": 1.3301713150650059e-05,
      "loss": 2.2269,
      "step": 37660
    },
    {
      "epoch": 2.28760551405842,
      "grad_norm": 6.865213871002197,
      "learning_rate": 1.3280124579718577e-05,
      "loss": 2.5114,
      "step": 37670
    },
    {
      "epoch": 2.288212789214793,
      "grad_norm": 5.28792142868042,
      "learning_rate": 1.3258550858889535e-05,
      "loss": 2.2128,
      "step": 37680
    },
    {
      "epoch": 2.2888200643711665,
      "grad_norm": 5.979413986206055,
      "learning_rate": 1.3236991996887676e-05,
      "loss": 2.5129,
      "step": 37690
    },
    {
      "epoch": 2.2894273395275397,
      "grad_norm": 6.260182857513428,
      "learning_rate": 1.3215448002431762e-05,
      "loss": 2.5462,
      "step": 37700
    },
    {
      "epoch": 2.2900346146839134,
      "grad_norm": 4.290703296661377,
      "learning_rate": 1.3193918884234524e-05,
      "loss": 2.2291,
      "step": 37710
    },
    {
      "epoch": 2.2906418898402867,
      "grad_norm": 3.7934036254882812,
      "learning_rate": 1.3172404651002657e-05,
      "loss": 2.3757,
      "step": 37720
    },
    {
      "epoch": 2.29124916499666,
      "grad_norm": 5.381631851196289,
      "learning_rate": 1.3150905311436895e-05,
      "loss": 2.2701,
      "step": 37730
    },
    {
      "epoch": 2.2918564401530332,
      "grad_norm": 3.9322168827056885,
      "learning_rate": 1.3129420874231902e-05,
      "loss": 2.2385,
      "step": 37740
    },
    {
      "epoch": 2.2924637153094065,
      "grad_norm": 3.6612486839294434,
      "learning_rate": 1.3107951348076324e-05,
      "loss": 2.1292,
      "step": 37750
    },
    {
      "epoch": 2.2930709904657802,
      "grad_norm": 4.3885698318481445,
      "learning_rate": 1.3086496741652777e-05,
      "loss": 2.3167,
      "step": 37760
    },
    {
      "epoch": 2.2936782656221535,
      "grad_norm": 5.628722190856934,
      "learning_rate": 1.3065057063637841e-05,
      "loss": 2.2712,
      "step": 37770
    },
    {
      "epoch": 2.2942855407785268,
      "grad_norm": 4.2332444190979,
      "learning_rate": 1.3043632322702076e-05,
      "loss": 2.2112,
      "step": 37780
    },
    {
      "epoch": 2.2948928159349,
      "grad_norm": 4.676535606384277,
      "learning_rate": 1.3022222527509963e-05,
      "loss": 2.278,
      "step": 37790
    },
    {
      "epoch": 2.2955000910912733,
      "grad_norm": 4.518399238586426,
      "learning_rate": 1.3000827686720002e-05,
      "loss": 2.2991,
      "step": 37800
    },
    {
      "epoch": 2.296107366247647,
      "grad_norm": 5.290658950805664,
      "learning_rate": 1.2979447808984585e-05,
      "loss": 2.4397,
      "step": 37810
    },
    {
      "epoch": 2.2967146414040203,
      "grad_norm": 5.336484432220459,
      "learning_rate": 1.2958082902950091e-05,
      "loss": 2.4154,
      "step": 37820
    },
    {
      "epoch": 2.2973219165603935,
      "grad_norm": 4.789456367492676,
      "learning_rate": 1.2936732977256787e-05,
      "loss": 2.3744,
      "step": 37830
    },
    {
      "epoch": 2.297929191716767,
      "grad_norm": 4.971426010131836,
      "learning_rate": 1.2915398040538973e-05,
      "loss": 2.2617,
      "step": 37840
    },
    {
      "epoch": 2.29853646687314,
      "grad_norm": 4.683833599090576,
      "learning_rate": 1.2894078101424807e-05,
      "loss": 2.4704,
      "step": 37850
    },
    {
      "epoch": 2.2991437420295133,
      "grad_norm": 3.9912164211273193,
      "learning_rate": 1.2872773168536423e-05,
      "loss": 2.1842,
      "step": 37860
    },
    {
      "epoch": 2.299751017185887,
      "grad_norm": 3.713251829147339,
      "learning_rate": 1.285148325048986e-05,
      "loss": 2.2937,
      "step": 37870
    },
    {
      "epoch": 2.3003582923422603,
      "grad_norm": 5.121769428253174,
      "learning_rate": 1.2830208355895124e-05,
      "loss": 2.2374,
      "step": 37880
    },
    {
      "epoch": 2.3009655674986336,
      "grad_norm": 5.939088821411133,
      "learning_rate": 1.2808948493356127e-05,
      "loss": 2.4571,
      "step": 37890
    },
    {
      "epoch": 2.301572842655007,
      "grad_norm": 4.828867435455322,
      "learning_rate": 1.2787703671470664e-05,
      "loss": 2.3771,
      "step": 37900
    },
    {
      "epoch": 2.30218011781138,
      "grad_norm": 5.296291351318359,
      "learning_rate": 1.2766473898830477e-05,
      "loss": 2.569,
      "step": 37910
    },
    {
      "epoch": 2.302787392967754,
      "grad_norm": 4.929872989654541,
      "learning_rate": 1.2745259184021263e-05,
      "loss": 2.3557,
      "step": 37920
    },
    {
      "epoch": 2.303394668124127,
      "grad_norm": 5.991928577423096,
      "learning_rate": 1.2724059535622562e-05,
      "loss": 2.4868,
      "step": 37930
    },
    {
      "epoch": 2.3040019432805003,
      "grad_norm": 6.0872368812561035,
      "learning_rate": 1.2702874962207861e-05,
      "loss": 2.5892,
      "step": 37940
    },
    {
      "epoch": 2.3046092184368736,
      "grad_norm": 5.451938629150391,
      "learning_rate": 1.268170547234453e-05,
      "loss": 2.5778,
      "step": 37950
    },
    {
      "epoch": 2.305216493593247,
      "grad_norm": 4.982241153717041,
      "learning_rate": 1.2660551074593851e-05,
      "loss": 2.4339,
      "step": 37960
    },
    {
      "epoch": 2.3058237687496206,
      "grad_norm": 5.239492416381836,
      "learning_rate": 1.2639411777511007e-05,
      "loss": 2.3312,
      "step": 37970
    },
    {
      "epoch": 2.306431043905994,
      "grad_norm": 5.031339168548584,
      "learning_rate": 1.2618287589645039e-05,
      "loss": 2.5559,
      "step": 37980
    },
    {
      "epoch": 2.307038319062367,
      "grad_norm": 5.043734073638916,
      "learning_rate": 1.2597178519538944e-05,
      "loss": 2.4016,
      "step": 37990
    },
    {
      "epoch": 2.3076455942187404,
      "grad_norm": 7.264020919799805,
      "learning_rate": 1.2576084575729547e-05,
      "loss": 2.5245,
      "step": 38000
    },
    {
      "epoch": 2.3082528693751136,
      "grad_norm": 4.346323490142822,
      "learning_rate": 1.2555005766747574e-05,
      "loss": 2.632,
      "step": 38010
    },
    {
      "epoch": 2.3088601445314874,
      "grad_norm": 3.7316834926605225,
      "learning_rate": 1.2533942101117635e-05,
      "loss": 2.4543,
      "step": 38020
    },
    {
      "epoch": 2.3094674196878606,
      "grad_norm": 3.997997999191284,
      "learning_rate": 1.251289358735821e-05,
      "loss": 2.2986,
      "step": 38030
    },
    {
      "epoch": 2.310074694844234,
      "grad_norm": 5.055006504058838,
      "learning_rate": 1.2491860233981651e-05,
      "loss": 2.4946,
      "step": 38040
    },
    {
      "epoch": 2.310681970000607,
      "grad_norm": 4.114532947540283,
      "learning_rate": 1.2470842049494186e-05,
      "loss": 2.4065,
      "step": 38050
    },
    {
      "epoch": 2.3112892451569804,
      "grad_norm": 5.32777214050293,
      "learning_rate": 1.2449839042395883e-05,
      "loss": 2.4852,
      "step": 38060
    },
    {
      "epoch": 2.311896520313354,
      "grad_norm": 5.300204753875732,
      "learning_rate": 1.2428851221180726e-05,
      "loss": 2.5156,
      "step": 38070
    },
    {
      "epoch": 2.3125037954697274,
      "grad_norm": 4.157527923583984,
      "learning_rate": 1.2407878594336509e-05,
      "loss": 2.4338,
      "step": 38080
    },
    {
      "epoch": 2.3131110706261007,
      "grad_norm": 4.366896152496338,
      "learning_rate": 1.238692117034489e-05,
      "loss": 2.3395,
      "step": 38090
    },
    {
      "epoch": 2.313718345782474,
      "grad_norm": 5.022538661956787,
      "learning_rate": 1.2365978957681396e-05,
      "loss": 2.4231,
      "step": 38100
    },
    {
      "epoch": 2.314325620938847,
      "grad_norm": 3.85440731048584,
      "learning_rate": 1.2345051964815385e-05,
      "loss": 2.3765,
      "step": 38110
    },
    {
      "epoch": 2.314932896095221,
      "grad_norm": 3.6375410556793213,
      "learning_rate": 1.2324140200210071e-05,
      "loss": 2.0778,
      "step": 38120
    },
    {
      "epoch": 2.315540171251594,
      "grad_norm": 4.241776943206787,
      "learning_rate": 1.2303243672322495e-05,
      "loss": 2.3534,
      "step": 38130
    },
    {
      "epoch": 2.3161474464079674,
      "grad_norm": 4.989594459533691,
      "learning_rate": 1.2282362389603542e-05,
      "loss": 2.3411,
      "step": 38140
    },
    {
      "epoch": 2.3167547215643407,
      "grad_norm": 4.0044941902160645,
      "learning_rate": 1.2261496360497954e-05,
      "loss": 2.5517,
      "step": 38150
    },
    {
      "epoch": 2.317361996720714,
      "grad_norm": 5.012479305267334,
      "learning_rate": 1.2240645593444277e-05,
      "loss": 2.1405,
      "step": 38160
    },
    {
      "epoch": 2.3179692718770877,
      "grad_norm": 4.331122875213623,
      "learning_rate": 1.2219810096874913e-05,
      "loss": 2.4639,
      "step": 38170
    },
    {
      "epoch": 2.318576547033461,
      "grad_norm": 4.246668338775635,
      "learning_rate": 1.2198989879216011e-05,
      "loss": 2.2955,
      "step": 38180
    },
    {
      "epoch": 2.319183822189834,
      "grad_norm": 5.391072750091553,
      "learning_rate": 1.2178184948887656e-05,
      "loss": 2.2784,
      "step": 38190
    },
    {
      "epoch": 2.3197910973462075,
      "grad_norm": 4.701585292816162,
      "learning_rate": 1.2157395314303672e-05,
      "loss": 2.157,
      "step": 38200
    },
    {
      "epoch": 2.3203983725025807,
      "grad_norm": 4.664798259735107,
      "learning_rate": 1.2136620983871706e-05,
      "loss": 2.188,
      "step": 38210
    },
    {
      "epoch": 2.3210056476589545,
      "grad_norm": 4.795843124389648,
      "learning_rate": 1.2115861965993253e-05,
      "loss": 2.3567,
      "step": 38220
    },
    {
      "epoch": 2.3216129228153277,
      "grad_norm": 4.082808017730713,
      "learning_rate": 1.2095118269063593e-05,
      "loss": 2.1797,
      "step": 38230
    },
    {
      "epoch": 2.322220197971701,
      "grad_norm": 3.208822011947632,
      "learning_rate": 1.2074389901471783e-05,
      "loss": 1.9862,
      "step": 38240
    },
    {
      "epoch": 2.3228274731280742,
      "grad_norm": 4.017058372497559,
      "learning_rate": 1.20536768716007e-05,
      "loss": 2.1442,
      "step": 38250
    },
    {
      "epoch": 2.3234347482844475,
      "grad_norm": 4.9464335441589355,
      "learning_rate": 1.2032979187827048e-05,
      "loss": 2.3176,
      "step": 38260
    },
    {
      "epoch": 2.3240420234408212,
      "grad_norm": 5.082799434661865,
      "learning_rate": 1.2012296858521293e-05,
      "loss": 2.2845,
      "step": 38270
    },
    {
      "epoch": 2.3246492985971945,
      "grad_norm": 3.7099175453186035,
      "learning_rate": 1.1991629892047695e-05,
      "loss": 2.3988,
      "step": 38280
    },
    {
      "epoch": 2.3252565737535678,
      "grad_norm": 5.097588539123535,
      "learning_rate": 1.1970978296764313e-05,
      "loss": 2.429,
      "step": 38290
    },
    {
      "epoch": 2.325863848909941,
      "grad_norm": 6.531846046447754,
      "learning_rate": 1.1950342081022963e-05,
      "loss": 2.5151,
      "step": 38300
    },
    {
      "epoch": 2.3264711240663143,
      "grad_norm": 4.643851280212402,
      "learning_rate": 1.1929721253169274e-05,
      "loss": 2.4912,
      "step": 38310
    },
    {
      "epoch": 2.327078399222688,
      "grad_norm": 4.165837287902832,
      "learning_rate": 1.1909115821542633e-05,
      "loss": 2.3288,
      "step": 38320
    },
    {
      "epoch": 2.3276856743790613,
      "grad_norm": 4.654415130615234,
      "learning_rate": 1.1888525794476186e-05,
      "loss": 2.3333,
      "step": 38330
    },
    {
      "epoch": 2.3282929495354345,
      "grad_norm": 3.464583158493042,
      "learning_rate": 1.1867951180296904e-05,
      "loss": 2.1635,
      "step": 38340
    },
    {
      "epoch": 2.328900224691808,
      "grad_norm": 5.381049156188965,
      "learning_rate": 1.1847391987325474e-05,
      "loss": 2.5317,
      "step": 38350
    },
    {
      "epoch": 2.329507499848181,
      "grad_norm": 6.120240211486816,
      "learning_rate": 1.1826848223876352e-05,
      "loss": 2.6014,
      "step": 38360
    },
    {
      "epoch": 2.3301147750045548,
      "grad_norm": 6.710465908050537,
      "learning_rate": 1.1806319898257772e-05,
      "loss": 2.6462,
      "step": 38370
    },
    {
      "epoch": 2.330722050160928,
      "grad_norm": 5.901982307434082,
      "learning_rate": 1.1785807018771711e-05,
      "loss": 2.6207,
      "step": 38380
    },
    {
      "epoch": 2.3313293253173013,
      "grad_norm": 5.049831390380859,
      "learning_rate": 1.1765309593713908e-05,
      "loss": 2.3705,
      "step": 38390
    },
    {
      "epoch": 2.3319366004736746,
      "grad_norm": 4.866934299468994,
      "learning_rate": 1.174482763137385e-05,
      "loss": 2.4689,
      "step": 38400
    },
    {
      "epoch": 2.332543875630048,
      "grad_norm": 6.16029691696167,
      "learning_rate": 1.1724361140034745e-05,
      "loss": 2.5349,
      "step": 38410
    },
    {
      "epoch": 2.3331511507864215,
      "grad_norm": 5.732079982757568,
      "learning_rate": 1.1703910127973606e-05,
      "loss": 2.4332,
      "step": 38420
    },
    {
      "epoch": 2.333758425942795,
      "grad_norm": 4.483534812927246,
      "learning_rate": 1.1683474603461125e-05,
      "loss": 2.3024,
      "step": 38430
    },
    {
      "epoch": 2.334365701099168,
      "grad_norm": 4.706587791442871,
      "learning_rate": 1.1663054574761761e-05,
      "loss": 2.4404,
      "step": 38440
    },
    {
      "epoch": 2.3349729762555413,
      "grad_norm": 4.893857002258301,
      "learning_rate": 1.1642650050133703e-05,
      "loss": 2.4757,
      "step": 38450
    },
    {
      "epoch": 2.3355802514119146,
      "grad_norm": 4.8995184898376465,
      "learning_rate": 1.1622261037828853e-05,
      "loss": 2.3899,
      "step": 38460
    },
    {
      "epoch": 2.3361875265682883,
      "grad_norm": 4.256523609161377,
      "learning_rate": 1.160188754609286e-05,
      "loss": 2.3099,
      "step": 38470
    },
    {
      "epoch": 2.3367948017246616,
      "grad_norm": 6.186554908752441,
      "learning_rate": 1.1581529583165067e-05,
      "loss": 2.5122,
      "step": 38480
    },
    {
      "epoch": 2.337402076881035,
      "grad_norm": 4.666141033172607,
      "learning_rate": 1.1561187157278602e-05,
      "loss": 2.4012,
      "step": 38490
    },
    {
      "epoch": 2.338009352037408,
      "grad_norm": 4.6067094802856445,
      "learning_rate": 1.1540860276660236e-05,
      "loss": 2.463,
      "step": 38500
    },
    {
      "epoch": 2.3386166271937814,
      "grad_norm": 5.559459686279297,
      "learning_rate": 1.1520548949530507e-05,
      "loss": 2.6224,
      "step": 38510
    },
    {
      "epoch": 2.339223902350155,
      "grad_norm": 5.786030292510986,
      "learning_rate": 1.1500253184103592e-05,
      "loss": 2.6193,
      "step": 38520
    },
    {
      "epoch": 2.3398311775065284,
      "grad_norm": 4.920238971710205,
      "learning_rate": 1.1479972988587467e-05,
      "loss": 2.5644,
      "step": 38530
    },
    {
      "epoch": 2.3404384526629016,
      "grad_norm": 4.389969348907471,
      "learning_rate": 1.1459708371183752e-05,
      "loss": 2.4524,
      "step": 38540
    },
    {
      "epoch": 2.341045727819275,
      "grad_norm": 4.925228118896484,
      "learning_rate": 1.1439459340087776e-05,
      "loss": 2.1522,
      "step": 38550
    },
    {
      "epoch": 2.341653002975648,
      "grad_norm": 6.146169662475586,
      "learning_rate": 1.1419225903488561e-05,
      "loss": 2.5131,
      "step": 38560
    },
    {
      "epoch": 2.342260278132022,
      "grad_norm": 4.608455181121826,
      "learning_rate": 1.1399008069568873e-05,
      "loss": 2.3103,
      "step": 38570
    },
    {
      "epoch": 2.342867553288395,
      "grad_norm": 3.8341100215911865,
      "learning_rate": 1.137880584650508e-05,
      "loss": 2.3771,
      "step": 38580
    },
    {
      "epoch": 2.3434748284447684,
      "grad_norm": 5.015522480010986,
      "learning_rate": 1.1358619242467305e-05,
      "loss": 2.1797,
      "step": 38590
    },
    {
      "epoch": 2.3440821036011417,
      "grad_norm": 3.7479500770568848,
      "learning_rate": 1.1338448265619306e-05,
      "loss": 2.2646,
      "step": 38600
    },
    {
      "epoch": 2.344689378757515,
      "grad_norm": 5.197198867797852,
      "learning_rate": 1.1318292924118584e-05,
      "loss": 2.4608,
      "step": 38610
    },
    {
      "epoch": 2.345296653913888,
      "grad_norm": 4.3190765380859375,
      "learning_rate": 1.1298153226116265e-05,
      "loss": 2.2936,
      "step": 38620
    },
    {
      "epoch": 2.345903929070262,
      "grad_norm": 3.283447027206421,
      "learning_rate": 1.127802917975716e-05,
      "loss": 2.2311,
      "step": 38630
    },
    {
      "epoch": 2.346511204226635,
      "grad_norm": 3.531848669052124,
      "learning_rate": 1.125792079317976e-05,
      "loss": 2.3353,
      "step": 38640
    },
    {
      "epoch": 2.3471184793830084,
      "grad_norm": 4.872098445892334,
      "learning_rate": 1.123782807451621e-05,
      "loss": 2.3007,
      "step": 38650
    },
    {
      "epoch": 2.3477257545393817,
      "grad_norm": 3.9844970703125,
      "learning_rate": 1.1217751031892326e-05,
      "loss": 2.4035,
      "step": 38660
    },
    {
      "epoch": 2.348333029695755,
      "grad_norm": 3.9685447216033936,
      "learning_rate": 1.1197689673427581e-05,
      "loss": 2.3589,
      "step": 38670
    },
    {
      "epoch": 2.3489403048521287,
      "grad_norm": 4.310797214508057,
      "learning_rate": 1.1177644007235116e-05,
      "loss": 2.3459,
      "step": 38680
    },
    {
      "epoch": 2.349547580008502,
      "grad_norm": 4.423780918121338,
      "learning_rate": 1.1157614041421722e-05,
      "loss": 2.538,
      "step": 38690
    },
    {
      "epoch": 2.350154855164875,
      "grad_norm": 4.016442775726318,
      "learning_rate": 1.1137599784087827e-05,
      "loss": 2.5702,
      "step": 38700
    },
    {
      "epoch": 2.3507621303212485,
      "grad_norm": 4.231849193572998,
      "learning_rate": 1.1117601243327518e-05,
      "loss": 2.5188,
      "step": 38710
    },
    {
      "epoch": 2.3513694054776217,
      "grad_norm": 3.3169658184051514,
      "learning_rate": 1.1097618427228524e-05,
      "loss": 2.0208,
      "step": 38720
    },
    {
      "epoch": 2.3519766806339955,
      "grad_norm": 2.970088481903076,
      "learning_rate": 1.1077651343872214e-05,
      "loss": 2.1958,
      "step": 38730
    },
    {
      "epoch": 2.3525839557903687,
      "grad_norm": 4.584047317504883,
      "learning_rate": 1.1057700001333588e-05,
      "loss": 2.205,
      "step": 38740
    },
    {
      "epoch": 2.353191230946742,
      "grad_norm": 7.280795574188232,
      "learning_rate": 1.1037764407681284e-05,
      "loss": 2.4952,
      "step": 38750
    },
    {
      "epoch": 2.3537985061031153,
      "grad_norm": 7.5992560386657715,
      "learning_rate": 1.1017844570977592e-05,
      "loss": 2.5132,
      "step": 38760
    },
    {
      "epoch": 2.3544057812594885,
      "grad_norm": 5.799924850463867,
      "learning_rate": 1.0997940499278403e-05,
      "loss": 2.1614,
      "step": 38770
    },
    {
      "epoch": 2.355013056415862,
      "grad_norm": 5.492321968078613,
      "learning_rate": 1.0978052200633238e-05,
      "loss": 2.3707,
      "step": 38780
    },
    {
      "epoch": 2.3556203315722355,
      "grad_norm": 5.123100757598877,
      "learning_rate": 1.095817968308524e-05,
      "loss": 2.5194,
      "step": 38790
    },
    {
      "epoch": 2.3562276067286088,
      "grad_norm": 5.4551239013671875,
      "learning_rate": 1.0938322954671177e-05,
      "loss": 2.4169,
      "step": 38800
    },
    {
      "epoch": 2.356834881884982,
      "grad_norm": 3.8041679859161377,
      "learning_rate": 1.0918482023421416e-05,
      "loss": 2.11,
      "step": 38810
    },
    {
      "epoch": 2.3574421570413553,
      "grad_norm": 5.354506015777588,
      "learning_rate": 1.089865689735996e-05,
      "loss": 2.4656,
      "step": 38820
    },
    {
      "epoch": 2.3580494321977286,
      "grad_norm": 3.716470241546631,
      "learning_rate": 1.087884758450438e-05,
      "loss": 2.4797,
      "step": 38830
    },
    {
      "epoch": 2.3586567073541023,
      "grad_norm": 3.935124158859253,
      "learning_rate": 1.0859054092865917e-05,
      "loss": 2.3529,
      "step": 38840
    },
    {
      "epoch": 2.3592639825104755,
      "grad_norm": 3.8951163291931152,
      "learning_rate": 1.0839276430449358e-05,
      "loss": 2.3468,
      "step": 38850
    },
    {
      "epoch": 2.359871257666849,
      "grad_norm": 3.696293354034424,
      "learning_rate": 1.0819514605253095e-05,
      "loss": 2.1502,
      "step": 38860
    },
    {
      "epoch": 2.360478532823222,
      "grad_norm": 5.195273399353027,
      "learning_rate": 1.0799768625269107e-05,
      "loss": 2.2296,
      "step": 38870
    },
    {
      "epoch": 2.3610858079795953,
      "grad_norm": 5.134791851043701,
      "learning_rate": 1.0780038498483025e-05,
      "loss": 2.5065,
      "step": 38880
    },
    {
      "epoch": 2.361693083135969,
      "grad_norm": 4.8118414878845215,
      "learning_rate": 1.0760324232874002e-05,
      "loss": 2.5803,
      "step": 38890
    },
    {
      "epoch": 2.3623003582923423,
      "grad_norm": 4.911661624908447,
      "learning_rate": 1.0740625836414797e-05,
      "loss": 2.3043,
      "step": 38900
    },
    {
      "epoch": 2.3629076334487156,
      "grad_norm": 4.51185417175293,
      "learning_rate": 1.072094331707178e-05,
      "loss": 2.227,
      "step": 38910
    },
    {
      "epoch": 2.363514908605089,
      "grad_norm": 4.94996452331543,
      "learning_rate": 1.0701276682804868e-05,
      "loss": 2.3552,
      "step": 38920
    },
    {
      "epoch": 2.364122183761462,
      "grad_norm": 5.685305118560791,
      "learning_rate": 1.0681625941567541e-05,
      "loss": 2.3422,
      "step": 38930
    },
    {
      "epoch": 2.364729458917836,
      "grad_norm": 5.38762903213501,
      "learning_rate": 1.0661991101306868e-05,
      "loss": 2.5425,
      "step": 38940
    },
    {
      "epoch": 2.365336734074209,
      "grad_norm": 4.511325836181641,
      "learning_rate": 1.0642372169963522e-05,
      "loss": 2.2471,
      "step": 38950
    },
    {
      "epoch": 2.3659440092305823,
      "grad_norm": 6.247109413146973,
      "learning_rate": 1.0622769155471696e-05,
      "loss": 2.2722,
      "step": 38960
    },
    {
      "epoch": 2.3665512843869556,
      "grad_norm": 6.215561866760254,
      "learning_rate": 1.060318206575916e-05,
      "loss": 2.3628,
      "step": 38970
    },
    {
      "epoch": 2.367158559543329,
      "grad_norm": 5.472112655639648,
      "learning_rate": 1.058361090874725e-05,
      "loss": 2.4312,
      "step": 38980
    },
    {
      "epoch": 2.3677658346997026,
      "grad_norm": 5.708107948303223,
      "learning_rate": 1.0564055692350844e-05,
      "loss": 2.3764,
      "step": 38990
    },
    {
      "epoch": 2.368373109856076,
      "grad_norm": 5.029575824737549,
      "learning_rate": 1.054451642447839e-05,
      "loss": 2.3293,
      "step": 39000
    },
    {
      "epoch": 2.368980385012449,
      "grad_norm": 5.371151924133301,
      "learning_rate": 1.0524993113031877e-05,
      "loss": 2.5231,
      "step": 39010
    },
    {
      "epoch": 2.3695876601688224,
      "grad_norm": 4.507331848144531,
      "learning_rate": 1.0505485765906836e-05,
      "loss": 2.2106,
      "step": 39020
    },
    {
      "epoch": 2.3701949353251957,
      "grad_norm": 5.068617820739746,
      "learning_rate": 1.0485994390992365e-05,
      "loss": 2.163,
      "step": 39030
    },
    {
      "epoch": 2.3708022104815694,
      "grad_norm": 4.753515720367432,
      "learning_rate": 1.0466518996171082e-05,
      "loss": 2.3478,
      "step": 39040
    },
    {
      "epoch": 2.3714094856379426,
      "grad_norm": 4.282208442687988,
      "learning_rate": 1.0447059589319136e-05,
      "loss": 2.3453,
      "step": 39050
    },
    {
      "epoch": 2.372016760794316,
      "grad_norm": 4.070659637451172,
      "learning_rate": 1.0427616178306233e-05,
      "loss": 2.4779,
      "step": 39060
    },
    {
      "epoch": 2.372624035950689,
      "grad_norm": 4.3852925300598145,
      "learning_rate": 1.0408188770995591e-05,
      "loss": 2.5325,
      "step": 39070
    },
    {
      "epoch": 2.3732313111070624,
      "grad_norm": 4.278240203857422,
      "learning_rate": 1.038877737524397e-05,
      "loss": 2.5023,
      "step": 39080
    },
    {
      "epoch": 2.373838586263436,
      "grad_norm": 4.79892110824585,
      "learning_rate": 1.0369381998901639e-05,
      "loss": 2.3774,
      "step": 39090
    },
    {
      "epoch": 2.3744458614198094,
      "grad_norm": 5.516547679901123,
      "learning_rate": 1.0350002649812384e-05,
      "loss": 2.4096,
      "step": 39100
    },
    {
      "epoch": 2.3750531365761827,
      "grad_norm": 6.209362983703613,
      "learning_rate": 1.0330639335813558e-05,
      "loss": 2.5488,
      "step": 39110
    },
    {
      "epoch": 2.375660411732556,
      "grad_norm": 3.8427820205688477,
      "learning_rate": 1.0311292064735972e-05,
      "loss": 2.3554,
      "step": 39120
    },
    {
      "epoch": 2.376267686888929,
      "grad_norm": 3.8235321044921875,
      "learning_rate": 1.0291960844403975e-05,
      "loss": 2.3827,
      "step": 39130
    },
    {
      "epoch": 2.376874962045303,
      "grad_norm": 4.681707382202148,
      "learning_rate": 1.0272645682635418e-05,
      "loss": 2.4762,
      "step": 39140
    },
    {
      "epoch": 2.377482237201676,
      "grad_norm": 5.511081695556641,
      "learning_rate": 1.0253346587241657e-05,
      "loss": 2.5254,
      "step": 39150
    },
    {
      "epoch": 2.3780895123580494,
      "grad_norm": 5.224579334259033,
      "learning_rate": 1.0234063566027563e-05,
      "loss": 2.4703,
      "step": 39160
    },
    {
      "epoch": 2.3786967875144227,
      "grad_norm": 4.162211894989014,
      "learning_rate": 1.0214796626791474e-05,
      "loss": 2.3215,
      "step": 39170
    },
    {
      "epoch": 2.379304062670796,
      "grad_norm": 5.019040107727051,
      "learning_rate": 1.0195545777325282e-05,
      "loss": 2.3672,
      "step": 39180
    },
    {
      "epoch": 2.3799113378271697,
      "grad_norm": 6.499131679534912,
      "learning_rate": 1.0176311025414315e-05,
      "loss": 2.342,
      "step": 39190
    },
    {
      "epoch": 2.380518612983543,
      "grad_norm": 5.565045356750488,
      "learning_rate": 1.015709237883743e-05,
      "loss": 2.689,
      "step": 39200
    },
    {
      "epoch": 2.381125888139916,
      "grad_norm": 6.096498966217041,
      "learning_rate": 1.0137889845366916e-05,
      "loss": 2.5342,
      "step": 39210
    },
    {
      "epoch": 2.3817331632962895,
      "grad_norm": 6.270559787750244,
      "learning_rate": 1.0118703432768623e-05,
      "loss": 2.4678,
      "step": 39220
    },
    {
      "epoch": 2.3823404384526627,
      "grad_norm": 3.7236876487731934,
      "learning_rate": 1.0099533148801826e-05,
      "loss": 2.3131,
      "step": 39230
    },
    {
      "epoch": 2.3829477136090365,
      "grad_norm": 5.163035869598389,
      "learning_rate": 1.0080379001219292e-05,
      "loss": 2.33,
      "step": 39240
    },
    {
      "epoch": 2.3835549887654097,
      "grad_norm": 5.569334983825684,
      "learning_rate": 1.0061240997767257e-05,
      "loss": 2.4561,
      "step": 39250
    },
    {
      "epoch": 2.384162263921783,
      "grad_norm": 4.521908283233643,
      "learning_rate": 1.004211914618547e-05,
      "loss": 2.4001,
      "step": 39260
    },
    {
      "epoch": 2.3847695390781563,
      "grad_norm": 4.549427032470703,
      "learning_rate": 1.002301345420707e-05,
      "loss": 2.2328,
      "step": 39270
    },
    {
      "epoch": 2.3853768142345295,
      "grad_norm": 4.116683483123779,
      "learning_rate": 1.0003923929558717e-05,
      "loss": 2.5628,
      "step": 39280
    },
    {
      "epoch": 2.3859840893909032,
      "grad_norm": 3.9932141304016113,
      "learning_rate": 9.984850579960508e-06,
      "loss": 2.4951,
      "step": 39290
    },
    {
      "epoch": 2.3865913645472765,
      "grad_norm": 3.272641658782959,
      "learning_rate": 9.965793413126035e-06,
      "loss": 2.3327,
      "step": 39300
    },
    {
      "epoch": 2.3871986397036498,
      "grad_norm": 4.217477798461914,
      "learning_rate": 9.946752436762309e-06,
      "loss": 2.432,
      "step": 39310
    },
    {
      "epoch": 2.387805914860023,
      "grad_norm": 3.810577392578125,
      "learning_rate": 9.927727658569796e-06,
      "loss": 2.3333,
      "step": 39320
    },
    {
      "epoch": 2.3884131900163963,
      "grad_norm": 5.284180164337158,
      "learning_rate": 9.908719086242429e-06,
      "loss": 2.4194,
      "step": 39330
    },
    {
      "epoch": 2.38902046517277,
      "grad_norm": 4.3611955642700195,
      "learning_rate": 9.889726727467574e-06,
      "loss": 2.4576,
      "step": 39340
    },
    {
      "epoch": 2.3896277403291433,
      "grad_norm": 4.535001754760742,
      "learning_rate": 9.870750589926042e-06,
      "loss": 2.2824,
      "step": 39350
    },
    {
      "epoch": 2.3902350154855165,
      "grad_norm": 4.690627098083496,
      "learning_rate": 9.851790681292073e-06,
      "loss": 2.2322,
      "step": 39360
    },
    {
      "epoch": 2.39084229064189,
      "grad_norm": 4.737593650817871,
      "learning_rate": 9.832847009233381e-06,
      "loss": 2.5229,
      "step": 39370
    },
    {
      "epoch": 2.391449565798263,
      "grad_norm": 4.750090599060059,
      "learning_rate": 9.813919581411074e-06,
      "loss": 2.228,
      "step": 39380
    },
    {
      "epoch": 2.3920568409546368,
      "grad_norm": 4.645781993865967,
      "learning_rate": 9.79500840547971e-06,
      "loss": 2.4234,
      "step": 39390
    },
    {
      "epoch": 2.39266411611101,
      "grad_norm": 4.827834606170654,
      "learning_rate": 9.776113489087274e-06,
      "loss": 2.3262,
      "step": 39400
    },
    {
      "epoch": 2.3932713912673833,
      "grad_norm": 4.862308025360107,
      "learning_rate": 9.757234839875156e-06,
      "loss": 2.4835,
      "step": 39410
    },
    {
      "epoch": 2.3938786664237566,
      "grad_norm": 3.3741962909698486,
      "learning_rate": 9.738372465478196e-06,
      "loss": 2.3561,
      "step": 39420
    },
    {
      "epoch": 2.39448594158013,
      "grad_norm": 5.092740535736084,
      "learning_rate": 9.719526373524628e-06,
      "loss": 2.2256,
      "step": 39430
    },
    {
      "epoch": 2.3950932167365035,
      "grad_norm": 4.733312129974365,
      "learning_rate": 9.700696571636108e-06,
      "loss": 2.3611,
      "step": 39440
    },
    {
      "epoch": 2.395700491892877,
      "grad_norm": 2.71034574508667,
      "learning_rate": 9.681883067427732e-06,
      "loss": 2.3468,
      "step": 39450
    },
    {
      "epoch": 2.39630776704925,
      "grad_norm": 3.9413692951202393,
      "learning_rate": 9.663085868507965e-06,
      "loss": 2.1187,
      "step": 39460
    },
    {
      "epoch": 2.3969150422056233,
      "grad_norm": 4.160220146179199,
      "learning_rate": 9.644304982478697e-06,
      "loss": 2.3703,
      "step": 39470
    },
    {
      "epoch": 2.3975223173619966,
      "grad_norm": 6.935824394226074,
      "learning_rate": 9.625540416935213e-06,
      "loss": 2.724,
      "step": 39480
    },
    {
      "epoch": 2.3981295925183703,
      "grad_norm": 4.6592302322387695,
      "learning_rate": 9.6067921794662e-06,
      "loss": 2.6441,
      "step": 39490
    },
    {
      "epoch": 2.3987368676747436,
      "grad_norm": 4.1737494468688965,
      "learning_rate": 9.588060277653754e-06,
      "loss": 2.56,
      "step": 39500
    },
    {
      "epoch": 2.399344142831117,
      "grad_norm": 3.924697160720825,
      "learning_rate": 9.569344719073347e-06,
      "loss": 2.2861,
      "step": 39510
    },
    {
      "epoch": 2.39995141798749,
      "grad_norm": 4.642597198486328,
      "learning_rate": 9.550645511293832e-06,
      "loss": 2.3782,
      "step": 39520
    },
    {
      "epoch": 2.4005586931438634,
      "grad_norm": 4.962324142456055,
      "learning_rate": 9.531962661877502e-06,
      "loss": 2.4916,
      "step": 39530
    },
    {
      "epoch": 2.4011659683002367,
      "grad_norm": 4.260626792907715,
      "learning_rate": 9.513296178379999e-06,
      "loss": 2.2745,
      "step": 39540
    },
    {
      "epoch": 2.4017732434566104,
      "grad_norm": 3.6350605487823486,
      "learning_rate": 9.49464606835031e-06,
      "loss": 2.3471,
      "step": 39550
    },
    {
      "epoch": 2.4023805186129836,
      "grad_norm": 4.103041172027588,
      "learning_rate": 9.476012339330841e-06,
      "loss": 2.3778,
      "step": 39560
    },
    {
      "epoch": 2.402987793769357,
      "grad_norm": 4.5218586921691895,
      "learning_rate": 9.457394998857405e-06,
      "loss": 2.4492,
      "step": 39570
    },
    {
      "epoch": 2.40359506892573,
      "grad_norm": 4.577348232269287,
      "learning_rate": 9.438794054459121e-06,
      "loss": 2.4311,
      "step": 39580
    },
    {
      "epoch": 2.4042023440821034,
      "grad_norm": 4.399092197418213,
      "learning_rate": 9.420209513658507e-06,
      "loss": 2.4142,
      "step": 39590
    },
    {
      "epoch": 2.404809619238477,
      "grad_norm": 3.8527352809906006,
      "learning_rate": 9.401641383971477e-06,
      "loss": 2.3532,
      "step": 39600
    },
    {
      "epoch": 2.4054168943948504,
      "grad_norm": 3.8292901515960693,
      "learning_rate": 9.383089672907247e-06,
      "loss": 2.3864,
      "step": 39610
    },
    {
      "epoch": 2.4060241695512237,
      "grad_norm": 4.262669563293457,
      "learning_rate": 9.364554387968433e-06,
      "loss": 2.4179,
      "step": 39620
    },
    {
      "epoch": 2.406631444707597,
      "grad_norm": 4.69713830947876,
      "learning_rate": 9.346035536650987e-06,
      "loss": 2.5326,
      "step": 39630
    },
    {
      "epoch": 2.40723871986397,
      "grad_norm": 4.434315204620361,
      "learning_rate": 9.32753312644426e-06,
      "loss": 2.656,
      "step": 39640
    },
    {
      "epoch": 2.407845995020344,
      "grad_norm": 4.233057498931885,
      "learning_rate": 9.309047164830898e-06,
      "loss": 2.3482,
      "step": 39650
    },
    {
      "epoch": 2.408453270176717,
      "grad_norm": 4.017987251281738,
      "learning_rate": 9.290577659286925e-06,
      "loss": 2.2354,
      "step": 39660
    },
    {
      "epoch": 2.4090605453330904,
      "grad_norm": 3.8586955070495605,
      "learning_rate": 9.272124617281697e-06,
      "loss": 2.4576,
      "step": 39670
    },
    {
      "epoch": 2.4096678204894637,
      "grad_norm": 4.021442413330078,
      "learning_rate": 9.253688046277926e-06,
      "loss": 2.3672,
      "step": 39680
    },
    {
      "epoch": 2.410275095645837,
      "grad_norm": 3.6255393028259277,
      "learning_rate": 9.235267953731652e-06,
      "loss": 2.1967,
      "step": 39690
    },
    {
      "epoch": 2.4108823708022102,
      "grad_norm": 3.952761650085449,
      "learning_rate": 9.216864347092246e-06,
      "loss": 2.3205,
      "step": 39700
    },
    {
      "epoch": 2.411489645958584,
      "grad_norm": 3.9148101806640625,
      "learning_rate": 9.198477233802422e-06,
      "loss": 2.3159,
      "step": 39710
    },
    {
      "epoch": 2.412096921114957,
      "grad_norm": 4.584500312805176,
      "learning_rate": 9.180106621298235e-06,
      "loss": 2.2313,
      "step": 39720
    },
    {
      "epoch": 2.4127041962713305,
      "grad_norm": 4.335254192352295,
      "learning_rate": 9.161752517009048e-06,
      "loss": 2.3748,
      "step": 39730
    },
    {
      "epoch": 2.4133114714277037,
      "grad_norm": 4.743207931518555,
      "learning_rate": 9.14341492835754e-06,
      "loss": 2.33,
      "step": 39740
    },
    {
      "epoch": 2.413918746584077,
      "grad_norm": 4.286795616149902,
      "learning_rate": 9.125093862759743e-06,
      "loss": 2.4312,
      "step": 39750
    },
    {
      "epoch": 2.4145260217404507,
      "grad_norm": 4.320176601409912,
      "learning_rate": 9.106789327624975e-06,
      "loss": 2.2248,
      "step": 39760
    },
    {
      "epoch": 2.415133296896824,
      "grad_norm": 4.660155773162842,
      "learning_rate": 9.088501330355886e-06,
      "loss": 2.358,
      "step": 39770
    },
    {
      "epoch": 2.4157405720531973,
      "grad_norm": 3.4568440914154053,
      "learning_rate": 9.070229878348429e-06,
      "loss": 2.4908,
      "step": 39780
    },
    {
      "epoch": 2.4163478472095705,
      "grad_norm": 4.233647346496582,
      "learning_rate": 9.051974978991862e-06,
      "loss": 2.3451,
      "step": 39790
    },
    {
      "epoch": 2.416955122365944,
      "grad_norm": 3.635788917541504,
      "learning_rate": 9.033736639668777e-06,
      "loss": 2.4285,
      "step": 39800
    },
    {
      "epoch": 2.4175623975223175,
      "grad_norm": 4.33375883102417,
      "learning_rate": 9.015514867755043e-06,
      "loss": 2.4116,
      "step": 39810
    },
    {
      "epoch": 2.4181696726786908,
      "grad_norm": 5.697124004364014,
      "learning_rate": 8.997309670619836e-06,
      "loss": 2.4441,
      "step": 39820
    },
    {
      "epoch": 2.418776947835064,
      "grad_norm": 4.517177104949951,
      "learning_rate": 8.979121055625617e-06,
      "loss": 2.4177,
      "step": 39830
    },
    {
      "epoch": 2.4193842229914373,
      "grad_norm": 5.116489887237549,
      "learning_rate": 8.960949030128162e-06,
      "loss": 2.3709,
      "step": 39840
    },
    {
      "epoch": 2.4199914981478106,
      "grad_norm": 5.071897506713867,
      "learning_rate": 8.942793601476518e-06,
      "loss": 2.4286,
      "step": 39850
    },
    {
      "epoch": 2.4205987733041843,
      "grad_norm": 5.220196723937988,
      "learning_rate": 8.92465477701303e-06,
      "loss": 2.5571,
      "step": 39860
    },
    {
      "epoch": 2.4212060484605575,
      "grad_norm": 6.448292255401611,
      "learning_rate": 8.906532564073339e-06,
      "loss": 2.6516,
      "step": 39870
    },
    {
      "epoch": 2.421813323616931,
      "grad_norm": 5.068519592285156,
      "learning_rate": 8.888426969986368e-06,
      "loss": 2.4272,
      "step": 39880
    },
    {
      "epoch": 2.422420598773304,
      "grad_norm": 3.9991657733917236,
      "learning_rate": 8.870338002074275e-06,
      "loss": 2.2877,
      "step": 39890
    },
    {
      "epoch": 2.4230278739296773,
      "grad_norm": 3.7732632160186768,
      "learning_rate": 8.852265667652525e-06,
      "loss": 2.4505,
      "step": 39900
    },
    {
      "epoch": 2.423635149086051,
      "grad_norm": 3.3529343605041504,
      "learning_rate": 8.834209974029888e-06,
      "loss": 2.1985,
      "step": 39910
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 3.954982042312622,
      "learning_rate": 8.816170928508365e-06,
      "loss": 2.5155,
      "step": 39920
    },
    {
      "epoch": 2.4248496993987976,
      "grad_norm": 3.1552734375,
      "learning_rate": 8.798148538383228e-06,
      "loss": 2.3215,
      "step": 39930
    },
    {
      "epoch": 2.425456974555171,
      "grad_norm": 4.455544471740723,
      "learning_rate": 8.780142810943004e-06,
      "loss": 2.4368,
      "step": 39940
    },
    {
      "epoch": 2.426064249711544,
      "grad_norm": 3.2187678813934326,
      "learning_rate": 8.762153753469537e-06,
      "loss": 2.2536,
      "step": 39950
    },
    {
      "epoch": 2.426671524867918,
      "grad_norm": 4.865078449249268,
      "learning_rate": 8.744181373237848e-06,
      "loss": 2.3566,
      "step": 39960
    },
    {
      "epoch": 2.427278800024291,
      "grad_norm": 5.402032852172852,
      "learning_rate": 8.72622567751627e-06,
      "loss": 2.5018,
      "step": 39970
    },
    {
      "epoch": 2.4278860751806643,
      "grad_norm": 4.063632011413574,
      "learning_rate": 8.708286673566357e-06,
      "loss": 2.6319,
      "step": 39980
    },
    {
      "epoch": 2.4284933503370376,
      "grad_norm": 3.538928508758545,
      "learning_rate": 8.690364368642956e-06,
      "loss": 2.4056,
      "step": 39990
    },
    {
      "epoch": 2.429100625493411,
      "grad_norm": 3.769681453704834,
      "learning_rate": 8.672458769994119e-06,
      "loss": 2.4003,
      "step": 40000
    }
  ],
  "logging_steps": 10,
  "max_steps": 49401,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "total_flos": 8.361345024e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
