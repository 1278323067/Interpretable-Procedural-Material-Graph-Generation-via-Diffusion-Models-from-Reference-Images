{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7327382036800874,
  "eval_steps": 5000,
  "global_step": 45000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006072751563733527,
      "grad_norm": 8.73314380645752,
      "learning_rate": 9.999998988960184e-05,
      "loss": 22.5731,
      "step": 10
    },
    {
      "epoch": 0.0012145503127467055,
      "grad_norm": 7.299468040466309,
      "learning_rate": 9.999995955841146e-05,
      "loss": 5.8327,
      "step": 20
    },
    {
      "epoch": 0.0018218254691200583,
      "grad_norm": 5.63895320892334,
      "learning_rate": 9.99999090064411e-05,
      "loss": 4.0589,
      "step": 30
    },
    {
      "epoch": 0.002429100625493411,
      "grad_norm": 4.273637294769287,
      "learning_rate": 9.999983823371122e-05,
      "loss": 3.7002,
      "step": 40
    },
    {
      "epoch": 0.0030363757818667636,
      "grad_norm": 3.5312461853027344,
      "learning_rate": 9.999974724025045e-05,
      "loss": 3.369,
      "step": 50
    },
    {
      "epoch": 0.0036436509382401167,
      "grad_norm": 3.258755922317505,
      "learning_rate": 9.999963602609556e-05,
      "loss": 3.4307,
      "step": 60
    },
    {
      "epoch": 0.004250926094613469,
      "grad_norm": 5.42689323425293,
      "learning_rate": 9.999950459129158e-05,
      "loss": 3.5532,
      "step": 70
    },
    {
      "epoch": 0.004858201250986822,
      "grad_norm": 3.3213086128234863,
      "learning_rate": 9.99993529358916e-05,
      "loss": 3.8685,
      "step": 80
    },
    {
      "epoch": 0.005465476407360175,
      "grad_norm": 3.1837570667266846,
      "learning_rate": 9.9999181059957e-05,
      "loss": 3.6256,
      "step": 90
    },
    {
      "epoch": 0.006072751563733527,
      "grad_norm": 3.466904640197754,
      "learning_rate": 9.999898896355726e-05,
      "loss": 3.5114,
      "step": 100
    },
    {
      "epoch": 0.006680026720106881,
      "grad_norm": 4.3654656410217285,
      "learning_rate": 9.999877664677009e-05,
      "loss": 3.2516,
      "step": 110
    },
    {
      "epoch": 0.007287301876480233,
      "grad_norm": 5.359692096710205,
      "learning_rate": 9.999854410968134e-05,
      "loss": 3.3049,
      "step": 120
    },
    {
      "epoch": 0.007894577032853586,
      "grad_norm": 4.482663631439209,
      "learning_rate": 9.999829135238505e-05,
      "loss": 3.6974,
      "step": 130
    },
    {
      "epoch": 0.008501852189226939,
      "grad_norm": 3.539574146270752,
      "learning_rate": 9.999801837498346e-05,
      "loss": 3.8387,
      "step": 140
    },
    {
      "epoch": 0.009109127345600291,
      "grad_norm": 4.273318767547607,
      "learning_rate": 9.999772517758694e-05,
      "loss": 3.5155,
      "step": 150
    },
    {
      "epoch": 0.009716402501973644,
      "grad_norm": 5.007519721984863,
      "learning_rate": 9.999741176031408e-05,
      "loss": 3.5392,
      "step": 160
    },
    {
      "epoch": 0.010323677658346997,
      "grad_norm": 3.3709397315979004,
      "learning_rate": 9.999707812329162e-05,
      "loss": 3.4116,
      "step": 170
    },
    {
      "epoch": 0.01093095281472035,
      "grad_norm": 3.1438732147216797,
      "learning_rate": 9.99967242666545e-05,
      "loss": 3.7,
      "step": 180
    },
    {
      "epoch": 0.011538227971093702,
      "grad_norm": 3.8076822757720947,
      "learning_rate": 9.99963501905458e-05,
      "loss": 3.6176,
      "step": 190
    },
    {
      "epoch": 0.012145503127467054,
      "grad_norm": 2.434231758117676,
      "learning_rate": 9.999595589511684e-05,
      "loss": 3.508,
      "step": 200
    },
    {
      "epoch": 0.012752778283840409,
      "grad_norm": 3.3523335456848145,
      "learning_rate": 9.999554138052704e-05,
      "loss": 3.2003,
      "step": 210
    },
    {
      "epoch": 0.013360053440213761,
      "grad_norm": 2.372102737426758,
      "learning_rate": 9.999510664694408e-05,
      "loss": 3.1773,
      "step": 220
    },
    {
      "epoch": 0.013967328596587114,
      "grad_norm": 2.6343319416046143,
      "learning_rate": 9.999465169454374e-05,
      "loss": 3.3748,
      "step": 230
    },
    {
      "epoch": 0.014574603752960467,
      "grad_norm": 2.893259286880493,
      "learning_rate": 9.999417652351002e-05,
      "loss": 3.3261,
      "step": 240
    },
    {
      "epoch": 0.01518187890933382,
      "grad_norm": 3.0276336669921875,
      "learning_rate": 9.999368113403508e-05,
      "loss": 3.3996,
      "step": 250
    },
    {
      "epoch": 0.015789154065707172,
      "grad_norm": 2.975844144821167,
      "learning_rate": 9.999316552631928e-05,
      "loss": 3.5081,
      "step": 260
    },
    {
      "epoch": 0.016396429222080525,
      "grad_norm": 3.4483325481414795,
      "learning_rate": 9.999262970057113e-05,
      "loss": 3.203,
      "step": 270
    },
    {
      "epoch": 0.017003704378453877,
      "grad_norm": 3.643345355987549,
      "learning_rate": 9.999207365700733e-05,
      "loss": 3.2522,
      "step": 280
    },
    {
      "epoch": 0.01761097953482723,
      "grad_norm": 3.3930652141571045,
      "learning_rate": 9.999149739585273e-05,
      "loss": 2.9425,
      "step": 290
    },
    {
      "epoch": 0.018218254691200583,
      "grad_norm": 8.862076759338379,
      "learning_rate": 9.999090091734043e-05,
      "loss": 3.0634,
      "step": 300
    },
    {
      "epoch": 0.018825529847573935,
      "grad_norm": 4.366414546966553,
      "learning_rate": 9.999028422171159e-05,
      "loss": 3.3841,
      "step": 310
    },
    {
      "epoch": 0.019432805003947288,
      "grad_norm": 3.6021599769592285,
      "learning_rate": 9.998964730921568e-05,
      "loss": 2.9756,
      "step": 320
    },
    {
      "epoch": 0.02004008016032064,
      "grad_norm": 4.016651630401611,
      "learning_rate": 9.99889901801102e-05,
      "loss": 3.0616,
      "step": 330
    },
    {
      "epoch": 0.020647355316693993,
      "grad_norm": 5.410763740539551,
      "learning_rate": 9.998831283466099e-05,
      "loss": 2.8264,
      "step": 340
    },
    {
      "epoch": 0.021254630473067346,
      "grad_norm": 2.1410515308380127,
      "learning_rate": 9.998761527314191e-05,
      "loss": 2.6513,
      "step": 350
    },
    {
      "epoch": 0.0218619056294407,
      "grad_norm": 2.7618441581726074,
      "learning_rate": 9.99868974958351e-05,
      "loss": 2.8018,
      "step": 360
    },
    {
      "epoch": 0.02246918078581405,
      "grad_norm": 3.8417913913726807,
      "learning_rate": 9.998615950303083e-05,
      "loss": 3.0578,
      "step": 370
    },
    {
      "epoch": 0.023076455942187404,
      "grad_norm": 4.155792713165283,
      "learning_rate": 9.998540129502756e-05,
      "loss": 3.0438,
      "step": 380
    },
    {
      "epoch": 0.023683731098560756,
      "grad_norm": 3.7576920986175537,
      "learning_rate": 9.998462287213191e-05,
      "loss": 2.9997,
      "step": 390
    },
    {
      "epoch": 0.02429100625493411,
      "grad_norm": 2.9255173206329346,
      "learning_rate": 9.998382423465871e-05,
      "loss": 2.6292,
      "step": 400
    },
    {
      "epoch": 0.024898281411307465,
      "grad_norm": 2.9368913173675537,
      "learning_rate": 9.998300538293091e-05,
      "loss": 2.7257,
      "step": 410
    },
    {
      "epoch": 0.025505556567680818,
      "grad_norm": 2.217425584793091,
      "learning_rate": 9.99821663172797e-05,
      "loss": 2.6305,
      "step": 420
    },
    {
      "epoch": 0.02611283172405417,
      "grad_norm": 2.555161476135254,
      "learning_rate": 9.998130703804438e-05,
      "loss": 2.6514,
      "step": 430
    },
    {
      "epoch": 0.026720106880427523,
      "grad_norm": 3.13724946975708,
      "learning_rate": 9.998042754557249e-05,
      "loss": 2.8582,
      "step": 440
    },
    {
      "epoch": 0.027327382036800876,
      "grad_norm": 4.741809844970703,
      "learning_rate": 9.997952784021967e-05,
      "loss": 2.8164,
      "step": 450
    },
    {
      "epoch": 0.027934657193174228,
      "grad_norm": 2.8679003715515137,
      "learning_rate": 9.997860792234981e-05,
      "loss": 2.7483,
      "step": 460
    },
    {
      "epoch": 0.02854193234954758,
      "grad_norm": 3.5317509174346924,
      "learning_rate": 9.997766779233493e-05,
      "loss": 2.7964,
      "step": 470
    },
    {
      "epoch": 0.029149207505920934,
      "grad_norm": 3.0862157344818115,
      "learning_rate": 9.997670745055522e-05,
      "loss": 2.5837,
      "step": 480
    },
    {
      "epoch": 0.029756482662294286,
      "grad_norm": 2.4865174293518066,
      "learning_rate": 9.997572689739907e-05,
      "loss": 2.9057,
      "step": 490
    },
    {
      "epoch": 0.03036375781866764,
      "grad_norm": 3.4843032360076904,
      "learning_rate": 9.997472613326304e-05,
      "loss": 2.9397,
      "step": 500
    },
    {
      "epoch": 0.03097103297504099,
      "grad_norm": 3.2815189361572266,
      "learning_rate": 9.997370515855182e-05,
      "loss": 2.7854,
      "step": 510
    },
    {
      "epoch": 0.031578308131414344,
      "grad_norm": 3.359445095062256,
      "learning_rate": 9.997266397367836e-05,
      "loss": 2.5884,
      "step": 520
    },
    {
      "epoch": 0.03218558328778769,
      "grad_norm": 2.2165029048919678,
      "learning_rate": 9.99716025790637e-05,
      "loss": 2.5632,
      "step": 530
    },
    {
      "epoch": 0.03279285844416105,
      "grad_norm": 3.029690742492676,
      "learning_rate": 9.997052097513709e-05,
      "loss": 2.8962,
      "step": 540
    },
    {
      "epoch": 0.033400133600534405,
      "grad_norm": 3.5867669582366943,
      "learning_rate": 9.996941916233594e-05,
      "loss": 2.67,
      "step": 550
    },
    {
      "epoch": 0.034007408756907755,
      "grad_norm": 4.172627925872803,
      "learning_rate": 9.996829714110583e-05,
      "loss": 3.0835,
      "step": 560
    },
    {
      "epoch": 0.03461468391328111,
      "grad_norm": 2.922051429748535,
      "learning_rate": 9.996715491190057e-05,
      "loss": 2.9414,
      "step": 570
    },
    {
      "epoch": 0.03522195906965446,
      "grad_norm": 2.5726661682128906,
      "learning_rate": 9.996599247518206e-05,
      "loss": 2.6514,
      "step": 580
    },
    {
      "epoch": 0.035829234226027816,
      "grad_norm": 3.1873505115509033,
      "learning_rate": 9.996480983142041e-05,
      "loss": 2.98,
      "step": 590
    },
    {
      "epoch": 0.036436509382401165,
      "grad_norm": 4.3887038230896,
      "learning_rate": 9.99636069810939e-05,
      "loss": 2.9758,
      "step": 600
    },
    {
      "epoch": 0.03704378453877452,
      "grad_norm": 2.4665658473968506,
      "learning_rate": 9.9962383924689e-05,
      "loss": 2.9414,
      "step": 610
    },
    {
      "epoch": 0.03765105969514787,
      "grad_norm": 4.571435451507568,
      "learning_rate": 9.99611406627003e-05,
      "loss": 3.1792,
      "step": 620
    },
    {
      "epoch": 0.03825833485152123,
      "grad_norm": 4.982296466827393,
      "learning_rate": 9.995987719563062e-05,
      "loss": 2.9067,
      "step": 630
    },
    {
      "epoch": 0.038865610007894576,
      "grad_norm": 2.9362950325012207,
      "learning_rate": 9.995859352399094e-05,
      "loss": 2.8869,
      "step": 640
    },
    {
      "epoch": 0.03947288516426793,
      "grad_norm": 2.0915329456329346,
      "learning_rate": 9.995728964830036e-05,
      "loss": 2.9095,
      "step": 650
    },
    {
      "epoch": 0.04008016032064128,
      "grad_norm": 3.2076094150543213,
      "learning_rate": 9.995596556908622e-05,
      "loss": 2.7181,
      "step": 660
    },
    {
      "epoch": 0.04068743547701464,
      "grad_norm": 2.4879324436187744,
      "learning_rate": 9.995462128688397e-05,
      "loss": 2.7524,
      "step": 670
    },
    {
      "epoch": 0.041294710633387986,
      "grad_norm": 2.552722454071045,
      "learning_rate": 9.995325680223728e-05,
      "loss": 3.0022,
      "step": 680
    },
    {
      "epoch": 0.04190198578976134,
      "grad_norm": 2.6883933544158936,
      "learning_rate": 9.995187211569797e-05,
      "loss": 2.9484,
      "step": 690
    },
    {
      "epoch": 0.04250926094613469,
      "grad_norm": 2.616981267929077,
      "learning_rate": 9.995046722782601e-05,
      "loss": 2.7091,
      "step": 700
    },
    {
      "epoch": 0.04311653610250805,
      "grad_norm": 2.429110050201416,
      "learning_rate": 9.994904213918959e-05,
      "loss": 2.8296,
      "step": 710
    },
    {
      "epoch": 0.0437238112588814,
      "grad_norm": 2.5134122371673584,
      "learning_rate": 9.994759685036501e-05,
      "loss": 2.7634,
      "step": 720
    },
    {
      "epoch": 0.04433108641525475,
      "grad_norm": 3.2414839267730713,
      "learning_rate": 9.994613136193679e-05,
      "loss": 2.719,
      "step": 730
    },
    {
      "epoch": 0.0449383615716281,
      "grad_norm": 2.5579347610473633,
      "learning_rate": 9.994464567449757e-05,
      "loss": 3.2261,
      "step": 740
    },
    {
      "epoch": 0.04554563672800146,
      "grad_norm": 2.47497296333313,
      "learning_rate": 9.99431397886482e-05,
      "loss": 3.1221,
      "step": 750
    },
    {
      "epoch": 0.04615291188437481,
      "grad_norm": 2.8389780521392822,
      "learning_rate": 9.994161370499769e-05,
      "loss": 2.7295,
      "step": 760
    },
    {
      "epoch": 0.04676018704074816,
      "grad_norm": 1.9710752964019775,
      "learning_rate": 9.994006742416321e-05,
      "loss": 2.7401,
      "step": 770
    },
    {
      "epoch": 0.04736746219712151,
      "grad_norm": 2.8304717540740967,
      "learning_rate": 9.99385009467701e-05,
      "loss": 2.7365,
      "step": 780
    },
    {
      "epoch": 0.04797473735349487,
      "grad_norm": 1.6961495876312256,
      "learning_rate": 9.993691427345187e-05,
      "loss": 2.618,
      "step": 790
    },
    {
      "epoch": 0.04858201250986822,
      "grad_norm": 2.002767562866211,
      "learning_rate": 9.993530740485018e-05,
      "loss": 2.8518,
      "step": 800
    },
    {
      "epoch": 0.049189287666241574,
      "grad_norm": 2.8722920417785645,
      "learning_rate": 9.993368034161489e-05,
      "loss": 2.671,
      "step": 810
    },
    {
      "epoch": 0.04979656282261493,
      "grad_norm": 2.291914224624634,
      "learning_rate": 9.9932033084404e-05,
      "loss": 2.9956,
      "step": 820
    },
    {
      "epoch": 0.05040383797898828,
      "grad_norm": 2.7208430767059326,
      "learning_rate": 9.99303656338837e-05,
      "loss": 2.72,
      "step": 830
    },
    {
      "epoch": 0.051011113135361635,
      "grad_norm": 2.1207311153411865,
      "learning_rate": 9.992867799072833e-05,
      "loss": 2.6907,
      "step": 840
    },
    {
      "epoch": 0.051618388291734985,
      "grad_norm": 2.493924617767334,
      "learning_rate": 9.99269701556204e-05,
      "loss": 2.5553,
      "step": 850
    },
    {
      "epoch": 0.05222566344810834,
      "grad_norm": 2.486931324005127,
      "learning_rate": 9.992524212925056e-05,
      "loss": 2.8535,
      "step": 860
    },
    {
      "epoch": 0.05283293860448169,
      "grad_norm": 2.577887535095215,
      "learning_rate": 9.99234939123177e-05,
      "loss": 2.8428,
      "step": 870
    },
    {
      "epoch": 0.053440213760855046,
      "grad_norm": 4.267984390258789,
      "learning_rate": 9.992172550552879e-05,
      "loss": 2.9619,
      "step": 880
    },
    {
      "epoch": 0.054047488917228395,
      "grad_norm": 3.1659934520721436,
      "learning_rate": 9.9919936909599e-05,
      "loss": 2.8851,
      "step": 890
    },
    {
      "epoch": 0.05465476407360175,
      "grad_norm": 2.821790933609009,
      "learning_rate": 9.99181281252517e-05,
      "loss": 3.2501,
      "step": 900
    },
    {
      "epoch": 0.0552620392299751,
      "grad_norm": 3.8878276348114014,
      "learning_rate": 9.991629915321836e-05,
      "loss": 2.8157,
      "step": 910
    },
    {
      "epoch": 0.055869314386348456,
      "grad_norm": 2.886411190032959,
      "learning_rate": 9.991444999423865e-05,
      "loss": 2.9932,
      "step": 920
    },
    {
      "epoch": 0.056476589542721806,
      "grad_norm": 2.860599994659424,
      "learning_rate": 9.991258064906041e-05,
      "loss": 2.8759,
      "step": 930
    },
    {
      "epoch": 0.05708386469909516,
      "grad_norm": 3.4422731399536133,
      "learning_rate": 9.991069111843964e-05,
      "loss": 2.9094,
      "step": 940
    },
    {
      "epoch": 0.05769113985546851,
      "grad_norm": 2.9319090843200684,
      "learning_rate": 9.990878140314047e-05,
      "loss": 2.8307,
      "step": 950
    },
    {
      "epoch": 0.05829841501184187,
      "grad_norm": 2.3495914936065674,
      "learning_rate": 9.990685150393523e-05,
      "loss": 2.4639,
      "step": 960
    },
    {
      "epoch": 0.058905690168215216,
      "grad_norm": 1.9425793886184692,
      "learning_rate": 9.990490142160442e-05,
      "loss": 2.5747,
      "step": 970
    },
    {
      "epoch": 0.05951296532458857,
      "grad_norm": 2.6256930828094482,
      "learning_rate": 9.990293115693667e-05,
      "loss": 2.9018,
      "step": 980
    },
    {
      "epoch": 0.06012024048096192,
      "grad_norm": 2.4946200847625732,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.9638,
      "step": 990
    },
    {
      "epoch": 0.06072751563733528,
      "grad_norm": 4.4017133712768555,
      "learning_rate": 9.989893008378572e-05,
      "loss": 3.11,
      "step": 1000
    },
    {
      "epoch": 0.06133479079370863,
      "grad_norm": 3.213958740234375,
      "learning_rate": 9.989689927692062e-05,
      "loss": 3.3939,
      "step": 1010
    },
    {
      "epoch": 0.06194206595008198,
      "grad_norm": 2.9392549991607666,
      "learning_rate": 9.989484829095478e-05,
      "loss": 2.9621,
      "step": 1020
    },
    {
      "epoch": 0.06254934110645534,
      "grad_norm": 1.9320060014724731,
      "learning_rate": 9.989277712671766e-05,
      "loss": 2.6808,
      "step": 1030
    },
    {
      "epoch": 0.06315661626282869,
      "grad_norm": 2.7876534461975098,
      "learning_rate": 9.989068578504684e-05,
      "loss": 2.7826,
      "step": 1040
    },
    {
      "epoch": 0.06376389141920204,
      "grad_norm": 2.4405105113983154,
      "learning_rate": 9.988857426678811e-05,
      "loss": 2.4446,
      "step": 1050
    },
    {
      "epoch": 0.06437116657557539,
      "grad_norm": 2.4182469844818115,
      "learning_rate": 9.98864425727954e-05,
      "loss": 2.6948,
      "step": 1060
    },
    {
      "epoch": 0.06497844173194875,
      "grad_norm": 1.8279435634613037,
      "learning_rate": 9.98842907039308e-05,
      "loss": 2.536,
      "step": 1070
    },
    {
      "epoch": 0.0655857168883221,
      "grad_norm": 1.8594392538070679,
      "learning_rate": 9.988211866106457e-05,
      "loss": 2.6942,
      "step": 1080
    },
    {
      "epoch": 0.06619299204469545,
      "grad_norm": 1.921242117881775,
      "learning_rate": 9.987992644507511e-05,
      "loss": 2.6993,
      "step": 1090
    },
    {
      "epoch": 0.06680026720106881,
      "grad_norm": 2.9976253509521484,
      "learning_rate": 9.987771405684897e-05,
      "loss": 2.4149,
      "step": 1100
    },
    {
      "epoch": 0.06740754235744216,
      "grad_norm": 1.7830952405929565,
      "learning_rate": 9.987548149728092e-05,
      "loss": 2.3455,
      "step": 1110
    },
    {
      "epoch": 0.06801481751381551,
      "grad_norm": 2.828648090362549,
      "learning_rate": 9.987322876727381e-05,
      "loss": 2.7984,
      "step": 1120
    },
    {
      "epoch": 0.06862209267018886,
      "grad_norm": 2.692549705505371,
      "learning_rate": 9.98709558677387e-05,
      "loss": 3.0832,
      "step": 1130
    },
    {
      "epoch": 0.06922936782656222,
      "grad_norm": 3.2572028636932373,
      "learning_rate": 9.986866279959474e-05,
      "loss": 3.1433,
      "step": 1140
    },
    {
      "epoch": 0.06983664298293557,
      "grad_norm": 2.600918769836426,
      "learning_rate": 9.986634956376932e-05,
      "loss": 2.6048,
      "step": 1150
    },
    {
      "epoch": 0.07044391813930892,
      "grad_norm": 2.7957000732421875,
      "learning_rate": 9.986401616119795e-05,
      "loss": 2.9385,
      "step": 1160
    },
    {
      "epoch": 0.07105119329568227,
      "grad_norm": 1.8874967098236084,
      "learning_rate": 9.98616625928243e-05,
      "loss": 2.6455,
      "step": 1170
    },
    {
      "epoch": 0.07165846845205563,
      "grad_norm": 1.9680075645446777,
      "learning_rate": 9.985928885960019e-05,
      "loss": 2.5289,
      "step": 1180
    },
    {
      "epoch": 0.07226574360842898,
      "grad_norm": 3.384390115737915,
      "learning_rate": 9.985689496248556e-05,
      "loss": 2.4742,
      "step": 1190
    },
    {
      "epoch": 0.07287301876480233,
      "grad_norm": 2.365933418273926,
      "learning_rate": 9.985448090244858e-05,
      "loss": 2.7633,
      "step": 1200
    },
    {
      "epoch": 0.07348029392117568,
      "grad_norm": 2.6769866943359375,
      "learning_rate": 9.985204668046553e-05,
      "loss": 2.8483,
      "step": 1210
    },
    {
      "epoch": 0.07408756907754904,
      "grad_norm": 4.249902725219727,
      "learning_rate": 9.984959229752082e-05,
      "loss": 2.7576,
      "step": 1220
    },
    {
      "epoch": 0.07469484423392239,
      "grad_norm": 2.441829204559326,
      "learning_rate": 9.984711775460707e-05,
      "loss": 3.0064,
      "step": 1230
    },
    {
      "epoch": 0.07530211939029574,
      "grad_norm": 5.364915370941162,
      "learning_rate": 9.9844623052725e-05,
      "loss": 2.8923,
      "step": 1240
    },
    {
      "epoch": 0.07590939454666909,
      "grad_norm": 3.354853391647339,
      "learning_rate": 9.984210819288354e-05,
      "loss": 2.7054,
      "step": 1250
    },
    {
      "epoch": 0.07651666970304245,
      "grad_norm": 3.068939447402954,
      "learning_rate": 9.983957317609971e-05,
      "loss": 2.7741,
      "step": 1260
    },
    {
      "epoch": 0.0771239448594158,
      "grad_norm": 1.6297662258148193,
      "learning_rate": 9.983701800339873e-05,
      "loss": 2.8004,
      "step": 1270
    },
    {
      "epoch": 0.07773122001578915,
      "grad_norm": 2.9537079334259033,
      "learning_rate": 9.983444267581394e-05,
      "loss": 2.7404,
      "step": 1280
    },
    {
      "epoch": 0.0783384951721625,
      "grad_norm": 3.4216933250427246,
      "learning_rate": 9.983184719438687e-05,
      "loss": 2.8271,
      "step": 1290
    },
    {
      "epoch": 0.07894577032853586,
      "grad_norm": 3.1597955226898193,
      "learning_rate": 9.982923156016713e-05,
      "loss": 2.513,
      "step": 1300
    },
    {
      "epoch": 0.07955304548490921,
      "grad_norm": 2.8500096797943115,
      "learning_rate": 9.982659577421255e-05,
      "loss": 2.6858,
      "step": 1310
    },
    {
      "epoch": 0.08016032064128256,
      "grad_norm": 4.05257511138916,
      "learning_rate": 9.982393983758908e-05,
      "loss": 2.9213,
      "step": 1320
    },
    {
      "epoch": 0.08076759579765591,
      "grad_norm": 2.4643449783325195,
      "learning_rate": 9.982126375137083e-05,
      "loss": 2.9492,
      "step": 1330
    },
    {
      "epoch": 0.08137487095402927,
      "grad_norm": 3.050220489501953,
      "learning_rate": 9.981856751664004e-05,
      "loss": 3.0395,
      "step": 1340
    },
    {
      "epoch": 0.08198214611040262,
      "grad_norm": 4.638091564178467,
      "learning_rate": 9.981585113448713e-05,
      "loss": 2.6941,
      "step": 1350
    },
    {
      "epoch": 0.08258942126677597,
      "grad_norm": 1.3575221300125122,
      "learning_rate": 9.981311460601061e-05,
      "loss": 2.384,
      "step": 1360
    },
    {
      "epoch": 0.08319669642314934,
      "grad_norm": 2.282432794570923,
      "learning_rate": 9.981035793231722e-05,
      "loss": 2.7305,
      "step": 1370
    },
    {
      "epoch": 0.08380397157952268,
      "grad_norm": 2.477972984313965,
      "learning_rate": 9.980758111452177e-05,
      "loss": 2.8246,
      "step": 1380
    },
    {
      "epoch": 0.08441124673589603,
      "grad_norm": 3.1577656269073486,
      "learning_rate": 9.980478415374726e-05,
      "loss": 2.8518,
      "step": 1390
    },
    {
      "epoch": 0.08501852189226938,
      "grad_norm": 2.8852007389068604,
      "learning_rate": 9.980196705112484e-05,
      "loss": 3.3468,
      "step": 1400
    },
    {
      "epoch": 0.08562579704864275,
      "grad_norm": 3.554469108581543,
      "learning_rate": 9.979912980779377e-05,
      "loss": 2.9781,
      "step": 1410
    },
    {
      "epoch": 0.0862330722050161,
      "grad_norm": 2.9649083614349365,
      "learning_rate": 9.979627242490148e-05,
      "loss": 2.8333,
      "step": 1420
    },
    {
      "epoch": 0.08684034736138944,
      "grad_norm": 2.8503077030181885,
      "learning_rate": 9.979339490360355e-05,
      "loss": 2.8865,
      "step": 1430
    },
    {
      "epoch": 0.0874476225177628,
      "grad_norm": 2.4087510108947754,
      "learning_rate": 9.979049724506369e-05,
      "loss": 2.8768,
      "step": 1440
    },
    {
      "epoch": 0.08805489767413616,
      "grad_norm": 2.3585808277130127,
      "learning_rate": 9.978757945045378e-05,
      "loss": 2.8741,
      "step": 1450
    },
    {
      "epoch": 0.0886621728305095,
      "grad_norm": 3.1479551792144775,
      "learning_rate": 9.978464152095378e-05,
      "loss": 2.6819,
      "step": 1460
    },
    {
      "epoch": 0.08926944798688285,
      "grad_norm": 3.5354111194610596,
      "learning_rate": 9.978168345775187e-05,
      "loss": 3.0388,
      "step": 1470
    },
    {
      "epoch": 0.0898767231432562,
      "grad_norm": 2.542876958847046,
      "learning_rate": 9.977870526204431e-05,
      "loss": 2.8048,
      "step": 1480
    },
    {
      "epoch": 0.09048399829962957,
      "grad_norm": 2.317054271697998,
      "learning_rate": 9.977570693503557e-05,
      "loss": 2.6436,
      "step": 1490
    },
    {
      "epoch": 0.09109127345600292,
      "grad_norm": 3.0399534702301025,
      "learning_rate": 9.977268847793819e-05,
      "loss": 3.0101,
      "step": 1500
    },
    {
      "epoch": 0.09169854861237627,
      "grad_norm": 2.7465271949768066,
      "learning_rate": 9.976964989197288e-05,
      "loss": 2.6843,
      "step": 1510
    },
    {
      "epoch": 0.09230582376874961,
      "grad_norm": 3.0418500900268555,
      "learning_rate": 9.976659117836851e-05,
      "loss": 2.6886,
      "step": 1520
    },
    {
      "epoch": 0.09291309892512298,
      "grad_norm": 2.56394100189209,
      "learning_rate": 9.976351233836207e-05,
      "loss": 2.9719,
      "step": 1530
    },
    {
      "epoch": 0.09352037408149633,
      "grad_norm": 2.7351760864257812,
      "learning_rate": 9.976041337319867e-05,
      "loss": 2.8551,
      "step": 1540
    },
    {
      "epoch": 0.09412764923786968,
      "grad_norm": 3.8182694911956787,
      "learning_rate": 9.975729428413162e-05,
      "loss": 3.0356,
      "step": 1550
    },
    {
      "epoch": 0.09473492439424303,
      "grad_norm": 3.7572946548461914,
      "learning_rate": 9.975415507242229e-05,
      "loss": 2.8276,
      "step": 1560
    },
    {
      "epoch": 0.09534219955061639,
      "grad_norm": 3.3821988105773926,
      "learning_rate": 9.975099573934026e-05,
      "loss": 2.9426,
      "step": 1570
    },
    {
      "epoch": 0.09594947470698974,
      "grad_norm": 2.6712265014648438,
      "learning_rate": 9.974781628616318e-05,
      "loss": 2.7565,
      "step": 1580
    },
    {
      "epoch": 0.09655674986336309,
      "grad_norm": 2.2046077251434326,
      "learning_rate": 9.974461671417689e-05,
      "loss": 2.8918,
      "step": 1590
    },
    {
      "epoch": 0.09716402501973644,
      "grad_norm": 2.6307032108306885,
      "learning_rate": 9.974139702467538e-05,
      "loss": 2.5935,
      "step": 1600
    },
    {
      "epoch": 0.0977713001761098,
      "grad_norm": 1.7548506259918213,
      "learning_rate": 9.973815721896068e-05,
      "loss": 2.4358,
      "step": 1610
    },
    {
      "epoch": 0.09837857533248315,
      "grad_norm": 1.7997539043426514,
      "learning_rate": 9.973489729834307e-05,
      "loss": 2.5031,
      "step": 1620
    },
    {
      "epoch": 0.0989858504888565,
      "grad_norm": 3.6986019611358643,
      "learning_rate": 9.973161726414088e-05,
      "loss": 2.7718,
      "step": 1630
    },
    {
      "epoch": 0.09959312564522986,
      "grad_norm": 1.95805823802948,
      "learning_rate": 9.972831711768063e-05,
      "loss": 2.7447,
      "step": 1640
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 2.454092502593994,
      "learning_rate": 9.972499686029694e-05,
      "loss": 3.0315,
      "step": 1650
    },
    {
      "epoch": 0.10080767595797656,
      "grad_norm": 5.059553623199463,
      "learning_rate": 9.972165649333259e-05,
      "loss": 2.7288,
      "step": 1660
    },
    {
      "epoch": 0.10141495111434991,
      "grad_norm": 2.813051462173462,
      "learning_rate": 9.971829601813845e-05,
      "loss": 2.715,
      "step": 1670
    },
    {
      "epoch": 0.10202222627072327,
      "grad_norm": 2.9627583026885986,
      "learning_rate": 9.971491543607356e-05,
      "loss": 3.1181,
      "step": 1680
    },
    {
      "epoch": 0.10262950142709662,
      "grad_norm": 3.8409457206726074,
      "learning_rate": 9.971151474850511e-05,
      "loss": 2.899,
      "step": 1690
    },
    {
      "epoch": 0.10323677658346997,
      "grad_norm": 2.38181209564209,
      "learning_rate": 9.970809395680837e-05,
      "loss": 2.6289,
      "step": 1700
    },
    {
      "epoch": 0.10384405173984332,
      "grad_norm": 2.5698463916778564,
      "learning_rate": 9.970465306236676e-05,
      "loss": 2.3416,
      "step": 1710
    },
    {
      "epoch": 0.10445132689621668,
      "grad_norm": 2.4211554527282715,
      "learning_rate": 9.970119206657182e-05,
      "loss": 2.6387,
      "step": 1720
    },
    {
      "epoch": 0.10505860205259003,
      "grad_norm": 2.149939775466919,
      "learning_rate": 9.969771097082326e-05,
      "loss": 2.8649,
      "step": 1730
    },
    {
      "epoch": 0.10566587720896338,
      "grad_norm": 4.14506196975708,
      "learning_rate": 9.969420977652888e-05,
      "loss": 2.7524,
      "step": 1740
    },
    {
      "epoch": 0.10627315236533673,
      "grad_norm": 2.967608690261841,
      "learning_rate": 9.969068848510461e-05,
      "loss": 2.6666,
      "step": 1750
    },
    {
      "epoch": 0.10688042752171009,
      "grad_norm": 2.681540012359619,
      "learning_rate": 9.968714709797453e-05,
      "loss": 2.6866,
      "step": 1760
    },
    {
      "epoch": 0.10748770267808344,
      "grad_norm": 2.9233715534210205,
      "learning_rate": 9.968358561657083e-05,
      "loss": 2.7289,
      "step": 1770
    },
    {
      "epoch": 0.10809497783445679,
      "grad_norm": 2.2403006553649902,
      "learning_rate": 9.968000404233382e-05,
      "loss": 3.2433,
      "step": 1780
    },
    {
      "epoch": 0.10870225299083014,
      "grad_norm": 3.5159912109375,
      "learning_rate": 9.967640237671195e-05,
      "loss": 2.8022,
      "step": 1790
    },
    {
      "epoch": 0.1093095281472035,
      "grad_norm": 4.239684104919434,
      "learning_rate": 9.967278062116179e-05,
      "loss": 2.8403,
      "step": 1800
    },
    {
      "epoch": 0.10991680330357685,
      "grad_norm": 2.3170125484466553,
      "learning_rate": 9.966913877714803e-05,
      "loss": 3.1711,
      "step": 1810
    },
    {
      "epoch": 0.1105240784599502,
      "grad_norm": 3.307117223739624,
      "learning_rate": 9.966547684614352e-05,
      "loss": 2.7114,
      "step": 1820
    },
    {
      "epoch": 0.11113135361632355,
      "grad_norm": 3.7829291820526123,
      "learning_rate": 9.966179482962916e-05,
      "loss": 2.7103,
      "step": 1830
    },
    {
      "epoch": 0.11173862877269691,
      "grad_norm": 2.7745914459228516,
      "learning_rate": 9.965809272909406e-05,
      "loss": 2.681,
      "step": 1840
    },
    {
      "epoch": 0.11234590392907026,
      "grad_norm": 3.5569815635681152,
      "learning_rate": 9.965437054603538e-05,
      "loss": 2.5221,
      "step": 1850
    },
    {
      "epoch": 0.11295317908544361,
      "grad_norm": 2.0007452964782715,
      "learning_rate": 9.965062828195841e-05,
      "loss": 2.7474,
      "step": 1860
    },
    {
      "epoch": 0.11356045424181696,
      "grad_norm": 3.032916307449341,
      "learning_rate": 9.964686593837663e-05,
      "loss": 2.7862,
      "step": 1870
    },
    {
      "epoch": 0.11416772939819032,
      "grad_norm": 3.372737407684326,
      "learning_rate": 9.964308351681157e-05,
      "loss": 2.5999,
      "step": 1880
    },
    {
      "epoch": 0.11477500455456367,
      "grad_norm": 4.012364864349365,
      "learning_rate": 9.96392810187929e-05,
      "loss": 2.4219,
      "step": 1890
    },
    {
      "epoch": 0.11538227971093702,
      "grad_norm": 3.619455575942993,
      "learning_rate": 9.96354584458584e-05,
      "loss": 2.7271,
      "step": 1900
    },
    {
      "epoch": 0.11598955486731038,
      "grad_norm": 3.516681432723999,
      "learning_rate": 9.9631615799554e-05,
      "loss": 2.6901,
      "step": 1910
    },
    {
      "epoch": 0.11659683002368373,
      "grad_norm": 3.2436492443084717,
      "learning_rate": 9.96277530814337e-05,
      "loss": 2.6297,
      "step": 1920
    },
    {
      "epoch": 0.11720410518005708,
      "grad_norm": 3.4480769634246826,
      "learning_rate": 9.962387029305968e-05,
      "loss": 2.9491,
      "step": 1930
    },
    {
      "epoch": 0.11781138033643043,
      "grad_norm": 2.9336230754852295,
      "learning_rate": 9.96199674360022e-05,
      "loss": 2.8411,
      "step": 1940
    },
    {
      "epoch": 0.1184186554928038,
      "grad_norm": 3.0365540981292725,
      "learning_rate": 9.961604451183958e-05,
      "loss": 2.8486,
      "step": 1950
    },
    {
      "epoch": 0.11902593064917714,
      "grad_norm": 4.2890625,
      "learning_rate": 9.961210152215839e-05,
      "loss": 2.6769,
      "step": 1960
    },
    {
      "epoch": 0.1196332058055505,
      "grad_norm": 3.1047985553741455,
      "learning_rate": 9.960813846855318e-05,
      "loss": 2.8853,
      "step": 1970
    },
    {
      "epoch": 0.12024048096192384,
      "grad_norm": 4.234852313995361,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.8959,
      "step": 1980
    },
    {
      "epoch": 0.1208477561182972,
      "grad_norm": 2.035327434539795,
      "learning_rate": 9.96001521759898e-05,
      "loss": 2.5787,
      "step": 1990
    },
    {
      "epoch": 0.12145503127467056,
      "grad_norm": 3.010889768600464,
      "learning_rate": 9.959612894026138e-05,
      "loss": 2.7072,
      "step": 2000
    },
    {
      "epoch": 0.1220623064310439,
      "grad_norm": 3.6868221759796143,
      "learning_rate": 9.959208564706854e-05,
      "loss": 2.9494,
      "step": 2010
    },
    {
      "epoch": 0.12266958158741725,
      "grad_norm": 3.981783390045166,
      "learning_rate": 9.958802229804643e-05,
      "loss": 2.8603,
      "step": 2020
    },
    {
      "epoch": 0.12327685674379062,
      "grad_norm": 3.7650041580200195,
      "learning_rate": 9.958393889483837e-05,
      "loss": 3.0212,
      "step": 2030
    },
    {
      "epoch": 0.12388413190016397,
      "grad_norm": 3.724910020828247,
      "learning_rate": 9.95798354390957e-05,
      "loss": 2.5177,
      "step": 2040
    },
    {
      "epoch": 0.12449140705653731,
      "grad_norm": 2.3840553760528564,
      "learning_rate": 9.957571193247797e-05,
      "loss": 2.8986,
      "step": 2050
    },
    {
      "epoch": 0.12509868221291068,
      "grad_norm": 2.494356632232666,
      "learning_rate": 9.957156837665276e-05,
      "loss": 2.7588,
      "step": 2060
    },
    {
      "epoch": 0.12570595736928403,
      "grad_norm": 2.0523929595947266,
      "learning_rate": 9.95674047732958e-05,
      "loss": 2.4523,
      "step": 2070
    },
    {
      "epoch": 0.12631323252565738,
      "grad_norm": 3.055121898651123,
      "learning_rate": 9.956322112409093e-05,
      "loss": 2.6302,
      "step": 2080
    },
    {
      "epoch": 0.12692050768203073,
      "grad_norm": 2.0229291915893555,
      "learning_rate": 9.955901743073006e-05,
      "loss": 2.8829,
      "step": 2090
    },
    {
      "epoch": 0.12752778283840407,
      "grad_norm": 3.9423272609710693,
      "learning_rate": 9.955479369491326e-05,
      "loss": 3.0913,
      "step": 2100
    },
    {
      "epoch": 0.12813505799477742,
      "grad_norm": 3.903988838195801,
      "learning_rate": 9.955054991834866e-05,
      "loss": 3.0311,
      "step": 2110
    },
    {
      "epoch": 0.12874233315115077,
      "grad_norm": 2.191171884536743,
      "learning_rate": 9.954628610275249e-05,
      "loss": 2.7813,
      "step": 2120
    },
    {
      "epoch": 0.12934960830752415,
      "grad_norm": 1.6979442834854126,
      "learning_rate": 9.954200224984915e-05,
      "loss": 2.5705,
      "step": 2130
    },
    {
      "epoch": 0.1299568834638975,
      "grad_norm": 4.2553181648254395,
      "learning_rate": 9.953769836137106e-05,
      "loss": 3.1209,
      "step": 2140
    },
    {
      "epoch": 0.13056415862027085,
      "grad_norm": 3.8836636543273926,
      "learning_rate": 9.95333744390588e-05,
      "loss": 3.0828,
      "step": 2150
    },
    {
      "epoch": 0.1311714337766442,
      "grad_norm": 3.602637529373169,
      "learning_rate": 9.952903048466104e-05,
      "loss": 2.728,
      "step": 2160
    },
    {
      "epoch": 0.13177870893301755,
      "grad_norm": 3.9693338871002197,
      "learning_rate": 9.952466649993451e-05,
      "loss": 3.1474,
      "step": 2170
    },
    {
      "epoch": 0.1323859840893909,
      "grad_norm": 5.363742351531982,
      "learning_rate": 9.952028248664411e-05,
      "loss": 3.2064,
      "step": 2180
    },
    {
      "epoch": 0.13299325924576424,
      "grad_norm": 4.579226970672607,
      "learning_rate": 9.95158784465628e-05,
      "loss": 2.95,
      "step": 2190
    },
    {
      "epoch": 0.13360053440213762,
      "grad_norm": 3.765742778778076,
      "learning_rate": 9.951145438147162e-05,
      "loss": 2.8311,
      "step": 2200
    },
    {
      "epoch": 0.13420780955851097,
      "grad_norm": 3.4983339309692383,
      "learning_rate": 9.950701029315977e-05,
      "loss": 3.049,
      "step": 2210
    },
    {
      "epoch": 0.13481508471488432,
      "grad_norm": 3.7039794921875,
      "learning_rate": 9.950254618342447e-05,
      "loss": 2.8136,
      "step": 2220
    },
    {
      "epoch": 0.13542235987125767,
      "grad_norm": 1.8091267347335815,
      "learning_rate": 9.949806205407111e-05,
      "loss": 2.8803,
      "step": 2230
    },
    {
      "epoch": 0.13602963502763102,
      "grad_norm": 2.8468713760375977,
      "learning_rate": 9.949355790691311e-05,
      "loss": 2.9479,
      "step": 2240
    },
    {
      "epoch": 0.13663691018400437,
      "grad_norm": 4.1402106285095215,
      "learning_rate": 9.948903374377205e-05,
      "loss": 2.7661,
      "step": 2250
    },
    {
      "epoch": 0.13724418534037772,
      "grad_norm": 6.7314934730529785,
      "learning_rate": 9.948448956647757e-05,
      "loss": 2.9348,
      "step": 2260
    },
    {
      "epoch": 0.13785146049675107,
      "grad_norm": 2.915266513824463,
      "learning_rate": 9.947992537686739e-05,
      "loss": 3.0029,
      "step": 2270
    },
    {
      "epoch": 0.13845873565312444,
      "grad_norm": 5.158283710479736,
      "learning_rate": 9.947534117678735e-05,
      "loss": 2.5912,
      "step": 2280
    },
    {
      "epoch": 0.1390660108094978,
      "grad_norm": 1.9506322145462036,
      "learning_rate": 9.947073696809137e-05,
      "loss": 2.9452,
      "step": 2290
    },
    {
      "epoch": 0.13967328596587114,
      "grad_norm": 2.6737120151519775,
      "learning_rate": 9.946611275264148e-05,
      "loss": 2.6186,
      "step": 2300
    },
    {
      "epoch": 0.1402805611222445,
      "grad_norm": 2.2098734378814697,
      "learning_rate": 9.946146853230777e-05,
      "loss": 2.5265,
      "step": 2310
    },
    {
      "epoch": 0.14088783627861784,
      "grad_norm": 2.5716969966888428,
      "learning_rate": 9.945680430896844e-05,
      "loss": 2.6845,
      "step": 2320
    },
    {
      "epoch": 0.1414951114349912,
      "grad_norm": 6.279703140258789,
      "learning_rate": 9.94521200845098e-05,
      "loss": 2.7842,
      "step": 2330
    },
    {
      "epoch": 0.14210238659136454,
      "grad_norm": 1.47995924949646,
      "learning_rate": 9.944741586082617e-05,
      "loss": 2.4943,
      "step": 2340
    },
    {
      "epoch": 0.1427096617477379,
      "grad_norm": 2.7813518047332764,
      "learning_rate": 9.944269163982007e-05,
      "loss": 2.6753,
      "step": 2350
    },
    {
      "epoch": 0.14331693690411126,
      "grad_norm": 3.297417402267456,
      "learning_rate": 9.943794742340202e-05,
      "loss": 2.655,
      "step": 2360
    },
    {
      "epoch": 0.1439242120604846,
      "grad_norm": 3.5031044483184814,
      "learning_rate": 9.943318321349067e-05,
      "loss": 3.1563,
      "step": 2370
    },
    {
      "epoch": 0.14453148721685796,
      "grad_norm": 3.623807668685913,
      "learning_rate": 9.942839901201272e-05,
      "loss": 2.8813,
      "step": 2380
    },
    {
      "epoch": 0.1451387623732313,
      "grad_norm": 3.1744065284729004,
      "learning_rate": 9.9423594820903e-05,
      "loss": 2.6848,
      "step": 2390
    },
    {
      "epoch": 0.14574603752960466,
      "grad_norm": 1.8180822134017944,
      "learning_rate": 9.94187706421044e-05,
      "loss": 2.8362,
      "step": 2400
    },
    {
      "epoch": 0.146353312685978,
      "grad_norm": 2.4451229572296143,
      "learning_rate": 9.941392647756789e-05,
      "loss": 2.8901,
      "step": 2410
    },
    {
      "epoch": 0.14696058784235136,
      "grad_norm": 3.31856107711792,
      "learning_rate": 9.940906232925251e-05,
      "loss": 2.7521,
      "step": 2420
    },
    {
      "epoch": 0.14756786299872474,
      "grad_norm": 3.4467132091522217,
      "learning_rate": 9.940417819912544e-05,
      "loss": 2.6848,
      "step": 2430
    },
    {
      "epoch": 0.14817513815509809,
      "grad_norm": 1.5821819305419922,
      "learning_rate": 9.939927408916185e-05,
      "loss": 2.678,
      "step": 2440
    },
    {
      "epoch": 0.14878241331147143,
      "grad_norm": 5.235474586486816,
      "learning_rate": 9.939435000134509e-05,
      "loss": 2.8425,
      "step": 2450
    },
    {
      "epoch": 0.14938968846784478,
      "grad_norm": 3.221245288848877,
      "learning_rate": 9.93894059376665e-05,
      "loss": 2.3748,
      "step": 2460
    },
    {
      "epoch": 0.14999696362421813,
      "grad_norm": 2.1303374767303467,
      "learning_rate": 9.938444190012556e-05,
      "loss": 2.0471,
      "step": 2470
    },
    {
      "epoch": 0.15060423878059148,
      "grad_norm": 2.0801103115081787,
      "learning_rate": 9.93794578907298e-05,
      "loss": 2.315,
      "step": 2480
    },
    {
      "epoch": 0.15121151393696483,
      "grad_norm": 2.548487424850464,
      "learning_rate": 9.937445391149483e-05,
      "loss": 2.9818,
      "step": 2490
    },
    {
      "epoch": 0.15181878909333818,
      "grad_norm": 6.218413352966309,
      "learning_rate": 9.936942996444434e-05,
      "loss": 2.4908,
      "step": 2500
    },
    {
      "epoch": 0.15242606424971156,
      "grad_norm": 2.306474447250366,
      "learning_rate": 9.936438605161009e-05,
      "loss": 2.9149,
      "step": 2510
    },
    {
      "epoch": 0.1530333394060849,
      "grad_norm": 2.6594009399414062,
      "learning_rate": 9.935932217503193e-05,
      "loss": 2.9948,
      "step": 2520
    },
    {
      "epoch": 0.15364061456245826,
      "grad_norm": 3.779965400695801,
      "learning_rate": 9.935423833675777e-05,
      "loss": 2.6775,
      "step": 2530
    },
    {
      "epoch": 0.1542478897188316,
      "grad_norm": 3.9439921379089355,
      "learning_rate": 9.934913453884358e-05,
      "loss": 2.4701,
      "step": 2540
    },
    {
      "epoch": 0.15485516487520495,
      "grad_norm": 1.9735569953918457,
      "learning_rate": 9.934401078335342e-05,
      "loss": 2.3808,
      "step": 2550
    },
    {
      "epoch": 0.1554624400315783,
      "grad_norm": 3.4687118530273438,
      "learning_rate": 9.933886707235946e-05,
      "loss": 2.4588,
      "step": 2560
    },
    {
      "epoch": 0.15606971518795165,
      "grad_norm": 3.0585687160491943,
      "learning_rate": 9.933370340794183e-05,
      "loss": 2.8263,
      "step": 2570
    },
    {
      "epoch": 0.156676990344325,
      "grad_norm": 3.8435964584350586,
      "learning_rate": 9.932851979218886e-05,
      "loss": 3.04,
      "step": 2580
    },
    {
      "epoch": 0.15728426550069838,
      "grad_norm": 7.4677653312683105,
      "learning_rate": 9.932331622719685e-05,
      "loss": 3.0039,
      "step": 2590
    },
    {
      "epoch": 0.15789154065707173,
      "grad_norm": 2.7985737323760986,
      "learning_rate": 9.931809271507023e-05,
      "loss": 2.7036,
      "step": 2600
    },
    {
      "epoch": 0.15849881581344508,
      "grad_norm": 3.280261278152466,
      "learning_rate": 9.931284925792142e-05,
      "loss": 2.7521,
      "step": 2610
    },
    {
      "epoch": 0.15910609096981843,
      "grad_norm": 3.4811809062957764,
      "learning_rate": 9.930758585787102e-05,
      "loss": 2.928,
      "step": 2620
    },
    {
      "epoch": 0.15971336612619177,
      "grad_norm": 4.24918270111084,
      "learning_rate": 9.930230251704759e-05,
      "loss": 2.7877,
      "step": 2630
    },
    {
      "epoch": 0.16032064128256512,
      "grad_norm": 3.0246567726135254,
      "learning_rate": 9.929699923758783e-05,
      "loss": 2.4461,
      "step": 2640
    },
    {
      "epoch": 0.16092791643893847,
      "grad_norm": 4.2649312019348145,
      "learning_rate": 9.929167602163647e-05,
      "loss": 2.3786,
      "step": 2650
    },
    {
      "epoch": 0.16153519159531182,
      "grad_norm": 2.0136661529541016,
      "learning_rate": 9.928633287134626e-05,
      "loss": 2.9789,
      "step": 2660
    },
    {
      "epoch": 0.1621424667516852,
      "grad_norm": 2.7881031036376953,
      "learning_rate": 9.928096978887809e-05,
      "loss": 2.833,
      "step": 2670
    },
    {
      "epoch": 0.16274974190805855,
      "grad_norm": 2.0452237129211426,
      "learning_rate": 9.927558677640088e-05,
      "loss": 2.6736,
      "step": 2680
    },
    {
      "epoch": 0.1633570170644319,
      "grad_norm": 3.2745397090911865,
      "learning_rate": 9.927018383609158e-05,
      "loss": 2.6979,
      "step": 2690
    },
    {
      "epoch": 0.16396429222080525,
      "grad_norm": 1.7232918739318848,
      "learning_rate": 9.926476097013524e-05,
      "loss": 2.6615,
      "step": 2700
    },
    {
      "epoch": 0.1645715673771786,
      "grad_norm": 1.462790846824646,
      "learning_rate": 9.925931818072496e-05,
      "loss": 2.592,
      "step": 2710
    },
    {
      "epoch": 0.16517884253355195,
      "grad_norm": 2.3082706928253174,
      "learning_rate": 9.925385547006189e-05,
      "loss": 3.0977,
      "step": 2720
    },
    {
      "epoch": 0.1657861176899253,
      "grad_norm": 2.2669568061828613,
      "learning_rate": 9.924837284035522e-05,
      "loss": 2.8341,
      "step": 2730
    },
    {
      "epoch": 0.16639339284629867,
      "grad_norm": 2.524822473526001,
      "learning_rate": 9.924287029382223e-05,
      "loss": 2.8152,
      "step": 2740
    },
    {
      "epoch": 0.16700066800267202,
      "grad_norm": 2.0612761974334717,
      "learning_rate": 9.923734783268823e-05,
      "loss": 2.6691,
      "step": 2750
    },
    {
      "epoch": 0.16760794315904537,
      "grad_norm": 3.3912651538848877,
      "learning_rate": 9.92318054591866e-05,
      "loss": 2.7544,
      "step": 2760
    },
    {
      "epoch": 0.16821521831541872,
      "grad_norm": 3.6732397079467773,
      "learning_rate": 9.922624317555874e-05,
      "loss": 2.5396,
      "step": 2770
    },
    {
      "epoch": 0.16882249347179207,
      "grad_norm": 3.008756160736084,
      "learning_rate": 9.922066098405415e-05,
      "loss": 2.4443,
      "step": 2780
    },
    {
      "epoch": 0.16942976862816542,
      "grad_norm": 3.3008198738098145,
      "learning_rate": 9.921505888693036e-05,
      "loss": 2.3442,
      "step": 2790
    },
    {
      "epoch": 0.17003704378453877,
      "grad_norm": 2.604207754135132,
      "learning_rate": 9.920943688645292e-05,
      "loss": 2.6282,
      "step": 2800
    },
    {
      "epoch": 0.17064431894091212,
      "grad_norm": 2.4390997886657715,
      "learning_rate": 9.920379498489549e-05,
      "loss": 2.6406,
      "step": 2810
    },
    {
      "epoch": 0.1712515940972855,
      "grad_norm": 2.7473504543304443,
      "learning_rate": 9.919813318453973e-05,
      "loss": 2.7733,
      "step": 2820
    },
    {
      "epoch": 0.17185886925365884,
      "grad_norm": 3.324498414993286,
      "learning_rate": 9.919245148767535e-05,
      "loss": 3.0089,
      "step": 2830
    },
    {
      "epoch": 0.1724661444100322,
      "grad_norm": 2.251988410949707,
      "learning_rate": 9.918674989660013e-05,
      "loss": 2.7805,
      "step": 2840
    },
    {
      "epoch": 0.17307341956640554,
      "grad_norm": 6.7386369705200195,
      "learning_rate": 9.91810284136199e-05,
      "loss": 3.0485,
      "step": 2850
    },
    {
      "epoch": 0.1736806947227789,
      "grad_norm": 3.806328773498535,
      "learning_rate": 9.917528704104848e-05,
      "loss": 2.4614,
      "step": 2860
    },
    {
      "epoch": 0.17428796987915224,
      "grad_norm": 2.314918279647827,
      "learning_rate": 9.916952578120781e-05,
      "loss": 2.5603,
      "step": 2870
    },
    {
      "epoch": 0.1748952450355256,
      "grad_norm": 4.0725016593933105,
      "learning_rate": 9.916374463642781e-05,
      "loss": 2.5102,
      "step": 2880
    },
    {
      "epoch": 0.17550252019189894,
      "grad_norm": 3.8385634422302246,
      "learning_rate": 9.915794360904649e-05,
      "loss": 2.5548,
      "step": 2890
    },
    {
      "epoch": 0.1761097953482723,
      "grad_norm": 1.9582748413085938,
      "learning_rate": 9.915212270140985e-05,
      "loss": 2.7152,
      "step": 2900
    },
    {
      "epoch": 0.17671707050464566,
      "grad_norm": 2.287086248397827,
      "learning_rate": 9.914628191587198e-05,
      "loss": 2.606,
      "step": 2910
    },
    {
      "epoch": 0.177324345661019,
      "grad_norm": 2.6049771308898926,
      "learning_rate": 9.914042125479499e-05,
      "loss": 2.4939,
      "step": 2920
    },
    {
      "epoch": 0.17793162081739236,
      "grad_norm": 2.5738065242767334,
      "learning_rate": 9.913454072054901e-05,
      "loss": 2.3837,
      "step": 2930
    },
    {
      "epoch": 0.1785388959737657,
      "grad_norm": 2.742525339126587,
      "learning_rate": 9.912864031551222e-05,
      "loss": 2.4956,
      "step": 2940
    },
    {
      "epoch": 0.17914617113013906,
      "grad_norm": 3.447512626647949,
      "learning_rate": 9.912272004207085e-05,
      "loss": 2.6777,
      "step": 2950
    },
    {
      "epoch": 0.1797534462865124,
      "grad_norm": 3.2328057289123535,
      "learning_rate": 9.911677990261913e-05,
      "loss": 2.9546,
      "step": 2960
    },
    {
      "epoch": 0.18036072144288579,
      "grad_norm": 3.583386182785034,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.5719,
      "step": 2970
    },
    {
      "epoch": 0.18096799659925913,
      "grad_norm": 1.926430344581604,
      "learning_rate": 9.910484003530192e-05,
      "loss": 2.4778,
      "step": 2980
    },
    {
      "epoch": 0.18157527175563248,
      "grad_norm": 2.0827181339263916,
      "learning_rate": 9.909884031226506e-05,
      "loss": 2.3013,
      "step": 2990
    },
    {
      "epoch": 0.18218254691200583,
      "grad_norm": 1.7040307521820068,
      "learning_rate": 9.909282073287522e-05,
      "loss": 2.4603,
      "step": 3000
    },
    {
      "epoch": 0.18278982206837918,
      "grad_norm": 1.7468299865722656,
      "learning_rate": 9.908678129956681e-05,
      "loss": 2.6785,
      "step": 3010
    },
    {
      "epoch": 0.18339709722475253,
      "grad_norm": 1.826340913772583,
      "learning_rate": 9.908072201478225e-05,
      "loss": 2.796,
      "step": 3020
    },
    {
      "epoch": 0.18400437238112588,
      "grad_norm": 2.2029263973236084,
      "learning_rate": 9.907464288097203e-05,
      "loss": 2.7554,
      "step": 3030
    },
    {
      "epoch": 0.18461164753749923,
      "grad_norm": 1.4329743385314941,
      "learning_rate": 9.906854390059467e-05,
      "loss": 2.5956,
      "step": 3040
    },
    {
      "epoch": 0.1852189226938726,
      "grad_norm": 2.4509642124176025,
      "learning_rate": 9.906242507611665e-05,
      "loss": 2.832,
      "step": 3050
    },
    {
      "epoch": 0.18582619785024596,
      "grad_norm": 6.158661365509033,
      "learning_rate": 9.905628641001255e-05,
      "loss": 2.7845,
      "step": 3060
    },
    {
      "epoch": 0.1864334730066193,
      "grad_norm": 2.9549221992492676,
      "learning_rate": 9.905012790476493e-05,
      "loss": 2.7471,
      "step": 3070
    },
    {
      "epoch": 0.18704074816299265,
      "grad_norm": 4.4936842918396,
      "learning_rate": 9.904394956286441e-05,
      "loss": 2.5968,
      "step": 3080
    },
    {
      "epoch": 0.187648023319366,
      "grad_norm": 2.071228504180908,
      "learning_rate": 9.903775138680956e-05,
      "loss": 2.922,
      "step": 3090
    },
    {
      "epoch": 0.18825529847573935,
      "grad_norm": 3.4392032623291016,
      "learning_rate": 9.903153337910708e-05,
      "loss": 2.7988,
      "step": 3100
    },
    {
      "epoch": 0.1888625736321127,
      "grad_norm": 2.329721689224243,
      "learning_rate": 9.90252955422716e-05,
      "loss": 2.4456,
      "step": 3110
    },
    {
      "epoch": 0.18946984878848605,
      "grad_norm": 1.366805076599121,
      "learning_rate": 9.90190378788258e-05,
      "loss": 2.467,
      "step": 3120
    },
    {
      "epoch": 0.19007712394485943,
      "grad_norm": 3.344682455062866,
      "learning_rate": 9.901276039130039e-05,
      "loss": 2.988,
      "step": 3130
    },
    {
      "epoch": 0.19068439910123278,
      "grad_norm": 3.7272708415985107,
      "learning_rate": 9.900646308223408e-05,
      "loss": 2.7525,
      "step": 3140
    },
    {
      "epoch": 0.19129167425760613,
      "grad_norm": 3.0958683490753174,
      "learning_rate": 9.90001459541736e-05,
      "loss": 2.4968,
      "step": 3150
    },
    {
      "epoch": 0.19189894941397947,
      "grad_norm": 2.2631664276123047,
      "learning_rate": 9.899380900967371e-05,
      "loss": 2.8257,
      "step": 3160
    },
    {
      "epoch": 0.19250622457035282,
      "grad_norm": 3.1288034915924072,
      "learning_rate": 9.898745225129715e-05,
      "loss": 2.6786,
      "step": 3170
    },
    {
      "epoch": 0.19311349972672617,
      "grad_norm": 1.9375295639038086,
      "learning_rate": 9.898107568161472e-05,
      "loss": 3.1305,
      "step": 3180
    },
    {
      "epoch": 0.19372077488309952,
      "grad_norm": 3.6918818950653076,
      "learning_rate": 9.897467930320519e-05,
      "loss": 3.0223,
      "step": 3190
    },
    {
      "epoch": 0.19432805003947287,
      "grad_norm": 2.576720952987671,
      "learning_rate": 9.896826311865534e-05,
      "loss": 2.9044,
      "step": 3200
    },
    {
      "epoch": 0.19493532519584625,
      "grad_norm": 2.4078609943389893,
      "learning_rate": 9.896182713056001e-05,
      "loss": 2.8203,
      "step": 3210
    },
    {
      "epoch": 0.1955426003522196,
      "grad_norm": 2.632538318634033,
      "learning_rate": 9.8955371341522e-05,
      "loss": 2.6345,
      "step": 3220
    },
    {
      "epoch": 0.19614987550859295,
      "grad_norm": 4.950481414794922,
      "learning_rate": 9.894889575415214e-05,
      "loss": 2.704,
      "step": 3230
    },
    {
      "epoch": 0.1967571506649663,
      "grad_norm": 8.575681686401367,
      "learning_rate": 9.894240037106927e-05,
      "loss": 3.0063,
      "step": 3240
    },
    {
      "epoch": 0.19736442582133965,
      "grad_norm": 6.421568870544434,
      "learning_rate": 9.89358851949002e-05,
      "loss": 3.0515,
      "step": 3250
    },
    {
      "epoch": 0.197971700977713,
      "grad_norm": 4.907344818115234,
      "learning_rate": 9.892935022827978e-05,
      "loss": 2.7291,
      "step": 3260
    },
    {
      "epoch": 0.19857897613408634,
      "grad_norm": 1.9398540258407593,
      "learning_rate": 9.892279547385087e-05,
      "loss": 2.5695,
      "step": 3270
    },
    {
      "epoch": 0.19918625129045972,
      "grad_norm": 1.6908588409423828,
      "learning_rate": 9.891622093426429e-05,
      "loss": 2.4129,
      "step": 3280
    },
    {
      "epoch": 0.19979352644683307,
      "grad_norm": 2.649254322052002,
      "learning_rate": 9.890962661217892e-05,
      "loss": 2.6715,
      "step": 3290
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 2.925894021987915,
      "learning_rate": 9.89030125102616e-05,
      "loss": 2.2607,
      "step": 3300
    },
    {
      "epoch": 0.20100807675957977,
      "grad_norm": 4.990777969360352,
      "learning_rate": 9.889637863118715e-05,
      "loss": 3.1231,
      "step": 3310
    },
    {
      "epoch": 0.20161535191595312,
      "grad_norm": 5.378751754760742,
      "learning_rate": 9.888972497763844e-05,
      "loss": 2.6133,
      "step": 3320
    },
    {
      "epoch": 0.20222262707232647,
      "grad_norm": 3.6403887271881104,
      "learning_rate": 9.888305155230632e-05,
      "loss": 2.6412,
      "step": 3330
    },
    {
      "epoch": 0.20282990222869982,
      "grad_norm": 3.6154322624206543,
      "learning_rate": 9.887635835788965e-05,
      "loss": 2.4769,
      "step": 3340
    },
    {
      "epoch": 0.20343717738507316,
      "grad_norm": 3.419074773788452,
      "learning_rate": 9.886964539709519e-05,
      "loss": 3.2693,
      "step": 3350
    },
    {
      "epoch": 0.20404445254144654,
      "grad_norm": 2.529130458831787,
      "learning_rate": 9.886291267263783e-05,
      "loss": 2.8201,
      "step": 3360
    },
    {
      "epoch": 0.2046517276978199,
      "grad_norm": 3.813990592956543,
      "learning_rate": 9.885616018724037e-05,
      "loss": 3.0623,
      "step": 3370
    },
    {
      "epoch": 0.20525900285419324,
      "grad_norm": 3.8767035007476807,
      "learning_rate": 9.884938794363365e-05,
      "loss": 2.729,
      "step": 3380
    },
    {
      "epoch": 0.2058662780105666,
      "grad_norm": 3.180403709411621,
      "learning_rate": 9.884259594455643e-05,
      "loss": 3.0775,
      "step": 3390
    },
    {
      "epoch": 0.20647355316693994,
      "grad_norm": 7.682459831237793,
      "learning_rate": 9.883578419275553e-05,
      "loss": 3.0267,
      "step": 3400
    },
    {
      "epoch": 0.2070808283233133,
      "grad_norm": 5.398792743682861,
      "learning_rate": 9.882895269098572e-05,
      "loss": 2.7549,
      "step": 3410
    },
    {
      "epoch": 0.20768810347968664,
      "grad_norm": 2.222971200942993,
      "learning_rate": 9.882210144200979e-05,
      "loss": 2.9555,
      "step": 3420
    },
    {
      "epoch": 0.20829537863605999,
      "grad_norm": 4.665426254272461,
      "learning_rate": 9.881523044859844e-05,
      "loss": 2.8633,
      "step": 3430
    },
    {
      "epoch": 0.20890265379243336,
      "grad_norm": 2.7695417404174805,
      "learning_rate": 9.880833971353048e-05,
      "loss": 2.7149,
      "step": 3440
    },
    {
      "epoch": 0.2095099289488067,
      "grad_norm": 2.1049559116363525,
      "learning_rate": 9.880142923959258e-05,
      "loss": 2.3567,
      "step": 3450
    },
    {
      "epoch": 0.21011720410518006,
      "grad_norm": 2.7914845943450928,
      "learning_rate": 9.879449902957946e-05,
      "loss": 2.4455,
      "step": 3460
    },
    {
      "epoch": 0.2107244792615534,
      "grad_norm": 3.024057149887085,
      "learning_rate": 9.878754908629384e-05,
      "loss": 3.0334,
      "step": 3470
    },
    {
      "epoch": 0.21133175441792676,
      "grad_norm": 4.8428144454956055,
      "learning_rate": 9.878057941254634e-05,
      "loss": 2.9151,
      "step": 3480
    },
    {
      "epoch": 0.2119390295743001,
      "grad_norm": 4.5899553298950195,
      "learning_rate": 9.877359001115563e-05,
      "loss": 2.9749,
      "step": 3490
    },
    {
      "epoch": 0.21254630473067346,
      "grad_norm": 2.6557130813598633,
      "learning_rate": 9.876658088494832e-05,
      "loss": 2.6519,
      "step": 3500
    },
    {
      "epoch": 0.21315357988704683,
      "grad_norm": 1.2040389776229858,
      "learning_rate": 9.875955203675905e-05,
      "loss": 2.6116,
      "step": 3510
    },
    {
      "epoch": 0.21376085504342018,
      "grad_norm": 1.7318850755691528,
      "learning_rate": 9.875250346943035e-05,
      "loss": 2.8264,
      "step": 3520
    },
    {
      "epoch": 0.21436813019979353,
      "grad_norm": 3.6481919288635254,
      "learning_rate": 9.874543518581279e-05,
      "loss": 2.6761,
      "step": 3530
    },
    {
      "epoch": 0.21497540535616688,
      "grad_norm": 2.101008892059326,
      "learning_rate": 9.873834718876491e-05,
      "loss": 2.8667,
      "step": 3540
    },
    {
      "epoch": 0.21558268051254023,
      "grad_norm": 2.6305294036865234,
      "learning_rate": 9.873123948115321e-05,
      "loss": 2.4738,
      "step": 3550
    },
    {
      "epoch": 0.21618995566891358,
      "grad_norm": 1.9954285621643066,
      "learning_rate": 9.872411206585215e-05,
      "loss": 2.6254,
      "step": 3560
    },
    {
      "epoch": 0.21679723082528693,
      "grad_norm": 2.778085231781006,
      "learning_rate": 9.871696494574417e-05,
      "loss": 2.7256,
      "step": 3570
    },
    {
      "epoch": 0.21740450598166028,
      "grad_norm": 3.180994987487793,
      "learning_rate": 9.870979812371967e-05,
      "loss": 2.8233,
      "step": 3580
    },
    {
      "epoch": 0.21801178113803366,
      "grad_norm": 3.7479608058929443,
      "learning_rate": 9.870261160267704e-05,
      "loss": 2.6833,
      "step": 3590
    },
    {
      "epoch": 0.218619056294407,
      "grad_norm": 2.2941341400146484,
      "learning_rate": 9.869540538552263e-05,
      "loss": 2.9172,
      "step": 3600
    },
    {
      "epoch": 0.21922633145078035,
      "grad_norm": 3.186859607696533,
      "learning_rate": 9.868817947517073e-05,
      "loss": 2.7491,
      "step": 3610
    },
    {
      "epoch": 0.2198336066071537,
      "grad_norm": 2.1318600177764893,
      "learning_rate": 9.868093387454362e-05,
      "loss": 3.046,
      "step": 3620
    },
    {
      "epoch": 0.22044088176352705,
      "grad_norm": 1.967147946357727,
      "learning_rate": 9.867366858657155e-05,
      "loss": 2.6917,
      "step": 3630
    },
    {
      "epoch": 0.2210481569199004,
      "grad_norm": 3.2916390895843506,
      "learning_rate": 9.866638361419269e-05,
      "loss": 2.6346,
      "step": 3640
    },
    {
      "epoch": 0.22165543207627375,
      "grad_norm": 2.7402875423431396,
      "learning_rate": 9.865907896035324e-05,
      "loss": 2.6547,
      "step": 3650
    },
    {
      "epoch": 0.2222627072326471,
      "grad_norm": 3.9069907665252686,
      "learning_rate": 9.865175462800727e-05,
      "loss": 3.0796,
      "step": 3660
    },
    {
      "epoch": 0.22286998238902048,
      "grad_norm": 2.532153844833374,
      "learning_rate": 9.86444106201169e-05,
      "loss": 2.9444,
      "step": 3670
    },
    {
      "epoch": 0.22347725754539383,
      "grad_norm": 3.4992754459381104,
      "learning_rate": 9.863704693965214e-05,
      "loss": 3.0165,
      "step": 3680
    },
    {
      "epoch": 0.22408453270176718,
      "grad_norm": 2.366154432296753,
      "learning_rate": 9.862966358959099e-05,
      "loss": 2.5481,
      "step": 3690
    },
    {
      "epoch": 0.22469180785814052,
      "grad_norm": 1.6107680797576904,
      "learning_rate": 9.862226057291937e-05,
      "loss": 2.4174,
      "step": 3700
    },
    {
      "epoch": 0.22529908301451387,
      "grad_norm": 1.3923448324203491,
      "learning_rate": 9.861483789263122e-05,
      "loss": 2.6058,
      "step": 3710
    },
    {
      "epoch": 0.22590635817088722,
      "grad_norm": 1.8598190546035767,
      "learning_rate": 9.860739555172835e-05,
      "loss": 2.6598,
      "step": 3720
    },
    {
      "epoch": 0.22651363332726057,
      "grad_norm": 2.556229591369629,
      "learning_rate": 9.859993355322058e-05,
      "loss": 2.9561,
      "step": 3730
    },
    {
      "epoch": 0.22712090848363392,
      "grad_norm": 1.6984142065048218,
      "learning_rate": 9.859245190012566e-05,
      "loss": 2.5227,
      "step": 3740
    },
    {
      "epoch": 0.2277281836400073,
      "grad_norm": 1.9401142597198486,
      "learning_rate": 9.85849505954693e-05,
      "loss": 2.8414,
      "step": 3750
    },
    {
      "epoch": 0.22833545879638065,
      "grad_norm": 4.698843955993652,
      "learning_rate": 9.857742964228512e-05,
      "loss": 2.5664,
      "step": 3760
    },
    {
      "epoch": 0.228942733952754,
      "grad_norm": 2.3820245265960693,
      "learning_rate": 9.856988904361474e-05,
      "loss": 2.631,
      "step": 3770
    },
    {
      "epoch": 0.22955000910912735,
      "grad_norm": 1.9540956020355225,
      "learning_rate": 9.856232880250769e-05,
      "loss": 3.1236,
      "step": 3780
    },
    {
      "epoch": 0.2301572842655007,
      "grad_norm": 4.295039176940918,
      "learning_rate": 9.855474892202144e-05,
      "loss": 2.9479,
      "step": 3790
    },
    {
      "epoch": 0.23076455942187404,
      "grad_norm": 3.698817014694214,
      "learning_rate": 9.854714940522142e-05,
      "loss": 2.8484,
      "step": 3800
    },
    {
      "epoch": 0.2313718345782474,
      "grad_norm": 3.4100019931793213,
      "learning_rate": 9.853953025518102e-05,
      "loss": 2.8153,
      "step": 3810
    },
    {
      "epoch": 0.23197910973462077,
      "grad_norm": 2.0789802074432373,
      "learning_rate": 9.853189147498149e-05,
      "loss": 2.4377,
      "step": 3820
    },
    {
      "epoch": 0.23258638489099412,
      "grad_norm": 2.612973213195801,
      "learning_rate": 9.852423306771214e-05,
      "loss": 2.5839,
      "step": 3830
    },
    {
      "epoch": 0.23319366004736747,
      "grad_norm": 1.8075920343399048,
      "learning_rate": 9.85165550364701e-05,
      "loss": 2.47,
      "step": 3840
    },
    {
      "epoch": 0.23380093520374082,
      "grad_norm": 2.2793915271759033,
      "learning_rate": 9.850885738436053e-05,
      "loss": 2.5342,
      "step": 3850
    },
    {
      "epoch": 0.23440821036011417,
      "grad_norm": 2.8722705841064453,
      "learning_rate": 9.850114011449645e-05,
      "loss": 2.7638,
      "step": 3860
    },
    {
      "epoch": 0.23501548551648752,
      "grad_norm": 4.083259105682373,
      "learning_rate": 9.849340322999886e-05,
      "loss": 2.5271,
      "step": 3870
    },
    {
      "epoch": 0.23562276067286086,
      "grad_norm": 3.0729217529296875,
      "learning_rate": 9.848564673399667e-05,
      "loss": 2.6378,
      "step": 3880
    },
    {
      "epoch": 0.23623003582923421,
      "grad_norm": 2.4880032539367676,
      "learning_rate": 9.847787062962675e-05,
      "loss": 2.6918,
      "step": 3890
    },
    {
      "epoch": 0.2368373109856076,
      "grad_norm": 3.247230052947998,
      "learning_rate": 9.847007492003388e-05,
      "loss": 2.823,
      "step": 3900
    },
    {
      "epoch": 0.23744458614198094,
      "grad_norm": 1.654036521911621,
      "learning_rate": 9.846225960837075e-05,
      "loss": 2.525,
      "step": 3910
    },
    {
      "epoch": 0.2380518612983543,
      "grad_norm": 3.1407470703125,
      "learning_rate": 9.8454424697798e-05,
      "loss": 2.4235,
      "step": 3920
    },
    {
      "epoch": 0.23865913645472764,
      "grad_norm": 3.7034871578216553,
      "learning_rate": 9.844657019148418e-05,
      "loss": 2.6436,
      "step": 3930
    },
    {
      "epoch": 0.239266411611101,
      "grad_norm": 1.5710562467575073,
      "learning_rate": 9.843869609260583e-05,
      "loss": 2.453,
      "step": 3940
    },
    {
      "epoch": 0.23987368676747434,
      "grad_norm": 2.282179594039917,
      "learning_rate": 9.84308024043473e-05,
      "loss": 2.5833,
      "step": 3950
    },
    {
      "epoch": 0.24048096192384769,
      "grad_norm": 3.436450242996216,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.6254,
      "step": 3960
    },
    {
      "epoch": 0.24108823708022104,
      "grad_norm": 1.9166789054870605,
      "learning_rate": 9.841495627246707e-05,
      "loss": 2.4173,
      "step": 3970
    },
    {
      "epoch": 0.2416955122365944,
      "grad_norm": 2.1434919834136963,
      "learning_rate": 9.840700383525376e-05,
      "loss": 2.6311,
      "step": 3980
    },
    {
      "epoch": 0.24230278739296776,
      "grad_norm": 2.5648553371429443,
      "learning_rate": 9.839903182147717e-05,
      "loss": 2.7569,
      "step": 3990
    },
    {
      "epoch": 0.2429100625493411,
      "grad_norm": 2.6100587844848633,
      "learning_rate": 9.839104023436128e-05,
      "loss": 2.7054,
      "step": 4000
    },
    {
      "epoch": 0.24351733770571446,
      "grad_norm": 1.9812371730804443,
      "learning_rate": 9.8383029077138e-05,
      "loss": 3.1172,
      "step": 4010
    },
    {
      "epoch": 0.2441246128620878,
      "grad_norm": 6.004777908325195,
      "learning_rate": 9.837499835304724e-05,
      "loss": 3.0409,
      "step": 4020
    },
    {
      "epoch": 0.24473188801846116,
      "grad_norm": 3.04767107963562,
      "learning_rate": 9.836694806533669e-05,
      "loss": 2.5618,
      "step": 4030
    },
    {
      "epoch": 0.2453391631748345,
      "grad_norm": 2.0543160438537598,
      "learning_rate": 9.835887821726202e-05,
      "loss": 2.2253,
      "step": 4040
    },
    {
      "epoch": 0.24594643833120788,
      "grad_norm": 2.045971155166626,
      "learning_rate": 9.835078881208681e-05,
      "loss": 2.5982,
      "step": 4050
    },
    {
      "epoch": 0.24655371348758123,
      "grad_norm": 2.458138942718506,
      "learning_rate": 9.834267985308256e-05,
      "loss": 2.8482,
      "step": 4060
    },
    {
      "epoch": 0.24716098864395458,
      "grad_norm": 2.7389681339263916,
      "learning_rate": 9.833455134352866e-05,
      "loss": 2.7584,
      "step": 4070
    },
    {
      "epoch": 0.24776826380032793,
      "grad_norm": 1.898208498954773,
      "learning_rate": 9.832640328671238e-05,
      "loss": 2.8349,
      "step": 4080
    },
    {
      "epoch": 0.24837553895670128,
      "grad_norm": 2.0789105892181396,
      "learning_rate": 9.831823568592897e-05,
      "loss": 2.638,
      "step": 4090
    },
    {
      "epoch": 0.24898281411307463,
      "grad_norm": 2.387619733810425,
      "learning_rate": 9.831004854448152e-05,
      "loss": 2.74,
      "step": 4100
    },
    {
      "epoch": 0.24959008926944798,
      "grad_norm": 3.229060411453247,
      "learning_rate": 9.830184186568101e-05,
      "loss": 3.019,
      "step": 4110
    },
    {
      "epoch": 0.25019736442582136,
      "grad_norm": 4.252000331878662,
      "learning_rate": 9.829361565284639e-05,
      "loss": 2.8731,
      "step": 4120
    },
    {
      "epoch": 0.2508046395821947,
      "grad_norm": 3.3002915382385254,
      "learning_rate": 9.828536990930444e-05,
      "loss": 2.8973,
      "step": 4130
    },
    {
      "epoch": 0.25141191473856805,
      "grad_norm": 2.830226421356201,
      "learning_rate": 9.82771046383899e-05,
      "loss": 2.7015,
      "step": 4140
    },
    {
      "epoch": 0.2520191898949414,
      "grad_norm": 3.1835336685180664,
      "learning_rate": 9.826881984344536e-05,
      "loss": 2.8561,
      "step": 4150
    },
    {
      "epoch": 0.25262646505131475,
      "grad_norm": 3.126784086227417,
      "learning_rate": 9.826051552782132e-05,
      "loss": 2.6197,
      "step": 4160
    },
    {
      "epoch": 0.25323374020768813,
      "grad_norm": 2.8286173343658447,
      "learning_rate": 9.825219169487618e-05,
      "loss": 2.7157,
      "step": 4170
    },
    {
      "epoch": 0.25384101536406145,
      "grad_norm": 4.459084987640381,
      "learning_rate": 9.824384834797625e-05,
      "loss": 2.9645,
      "step": 4180
    },
    {
      "epoch": 0.2544482905204348,
      "grad_norm": 3.0777666568756104,
      "learning_rate": 9.823548549049569e-05,
      "loss": 2.978,
      "step": 4190
    },
    {
      "epoch": 0.25505556567680815,
      "grad_norm": 4.031843662261963,
      "learning_rate": 9.82271031258166e-05,
      "loss": 2.9968,
      "step": 4200
    },
    {
      "epoch": 0.2556628408331815,
      "grad_norm": 3.6430177688598633,
      "learning_rate": 9.82187012573289e-05,
      "loss": 2.8404,
      "step": 4210
    },
    {
      "epoch": 0.25627011598955485,
      "grad_norm": 3.5341033935546875,
      "learning_rate": 9.821027988843046e-05,
      "loss": 3.172,
      "step": 4220
    },
    {
      "epoch": 0.2568773911459282,
      "grad_norm": 3.5384671688079834,
      "learning_rate": 9.820183902252702e-05,
      "loss": 2.5363,
      "step": 4230
    },
    {
      "epoch": 0.25748466630230155,
      "grad_norm": 2.224579095840454,
      "learning_rate": 9.81933786630322e-05,
      "loss": 2.5384,
      "step": 4240
    },
    {
      "epoch": 0.2580919414586749,
      "grad_norm": 3.1783366203308105,
      "learning_rate": 9.81848988133675e-05,
      "loss": 2.5745,
      "step": 4250
    },
    {
      "epoch": 0.2586992166150483,
      "grad_norm": 2.7298851013183594,
      "learning_rate": 9.817639947696231e-05,
      "loss": 2.5298,
      "step": 4260
    },
    {
      "epoch": 0.2593064917714216,
      "grad_norm": 3.2882938385009766,
      "learning_rate": 9.816788065725389e-05,
      "loss": 3.176,
      "step": 4270
    },
    {
      "epoch": 0.259913766927795,
      "grad_norm": 4.401654243469238,
      "learning_rate": 9.81593423576874e-05,
      "loss": 3.2236,
      "step": 4280
    },
    {
      "epoch": 0.2605210420841683,
      "grad_norm": 4.930937767028809,
      "learning_rate": 9.815078458171585e-05,
      "loss": 3.1625,
      "step": 4290
    },
    {
      "epoch": 0.2611283172405417,
      "grad_norm": 2.7699246406555176,
      "learning_rate": 9.814220733280015e-05,
      "loss": 2.6576,
      "step": 4300
    },
    {
      "epoch": 0.261735592396915,
      "grad_norm": 2.4644980430603027,
      "learning_rate": 9.813361061440907e-05,
      "loss": 2.6309,
      "step": 4310
    },
    {
      "epoch": 0.2623428675532884,
      "grad_norm": 1.7884347438812256,
      "learning_rate": 9.812499443001926e-05,
      "loss": 2.488,
      "step": 4320
    },
    {
      "epoch": 0.26295014270966177,
      "grad_norm": 1.940576434135437,
      "learning_rate": 9.811635878311524e-05,
      "loss": 2.6877,
      "step": 4330
    },
    {
      "epoch": 0.2635574178660351,
      "grad_norm": 2.7647581100463867,
      "learning_rate": 9.810770367718942e-05,
      "loss": 2.3834,
      "step": 4340
    },
    {
      "epoch": 0.26416469302240847,
      "grad_norm": 3.5230863094329834,
      "learning_rate": 9.809902911574204e-05,
      "loss": 2.4744,
      "step": 4350
    },
    {
      "epoch": 0.2647719681787818,
      "grad_norm": 2.936452627182007,
      "learning_rate": 9.809033510228126e-05,
      "loss": 2.9551,
      "step": 4360
    },
    {
      "epoch": 0.26537924333515517,
      "grad_norm": 4.348246097564697,
      "learning_rate": 9.808162164032304e-05,
      "loss": 2.706,
      "step": 4370
    },
    {
      "epoch": 0.2659865184915285,
      "grad_norm": 1.4412906169891357,
      "learning_rate": 9.807288873339126e-05,
      "loss": 2.5095,
      "step": 4380
    },
    {
      "epoch": 0.26659379364790187,
      "grad_norm": 5.069746971130371,
      "learning_rate": 9.806413638501766e-05,
      "loss": 2.6952,
      "step": 4390
    },
    {
      "epoch": 0.26720106880427524,
      "grad_norm": 3.189910411834717,
      "learning_rate": 9.805536459874182e-05,
      "loss": 2.6572,
      "step": 4400
    },
    {
      "epoch": 0.26780834396064856,
      "grad_norm": 3.3955888748168945,
      "learning_rate": 9.804657337811119e-05,
      "loss": 2.3982,
      "step": 4410
    },
    {
      "epoch": 0.26841561911702194,
      "grad_norm": 2.066145181655884,
      "learning_rate": 9.803776272668106e-05,
      "loss": 2.8661,
      "step": 4420
    },
    {
      "epoch": 0.26902289427339526,
      "grad_norm": 2.840942621231079,
      "learning_rate": 9.802893264801462e-05,
      "loss": 2.3854,
      "step": 4430
    },
    {
      "epoch": 0.26963016942976864,
      "grad_norm": 3.1056833267211914,
      "learning_rate": 9.80200831456829e-05,
      "loss": 2.7086,
      "step": 4440
    },
    {
      "epoch": 0.27023744458614196,
      "grad_norm": 3.766754388809204,
      "learning_rate": 9.801121422326475e-05,
      "loss": 2.8505,
      "step": 4450
    },
    {
      "epoch": 0.27084471974251534,
      "grad_norm": 3.0864100456237793,
      "learning_rate": 9.800232588434692e-05,
      "loss": 2.9679,
      "step": 4460
    },
    {
      "epoch": 0.27145199489888866,
      "grad_norm": 3.811190366744995,
      "learning_rate": 9.799341813252402e-05,
      "loss": 2.8361,
      "step": 4470
    },
    {
      "epoch": 0.27205927005526204,
      "grad_norm": 5.219405174255371,
      "learning_rate": 9.798449097139845e-05,
      "loss": 3.0922,
      "step": 4480
    },
    {
      "epoch": 0.2726665452116354,
      "grad_norm": 5.0281476974487305,
      "learning_rate": 9.797554440458052e-05,
      "loss": 2.7412,
      "step": 4490
    },
    {
      "epoch": 0.27327382036800874,
      "grad_norm": 3.0546631813049316,
      "learning_rate": 9.796657843568835e-05,
      "loss": 2.6744,
      "step": 4500
    },
    {
      "epoch": 0.2738810955243821,
      "grad_norm": 4.419579982757568,
      "learning_rate": 9.795759306834793e-05,
      "loss": 2.8684,
      "step": 4510
    },
    {
      "epoch": 0.27448837068075543,
      "grad_norm": 3.260345935821533,
      "learning_rate": 9.794858830619307e-05,
      "loss": 2.4682,
      "step": 4520
    },
    {
      "epoch": 0.2750956458371288,
      "grad_norm": 2.4572837352752686,
      "learning_rate": 9.793956415286545e-05,
      "loss": 2.8247,
      "step": 4530
    },
    {
      "epoch": 0.27570292099350213,
      "grad_norm": 3.4128365516662598,
      "learning_rate": 9.79305206120146e-05,
      "loss": 2.8046,
      "step": 4540
    },
    {
      "epoch": 0.2763101961498755,
      "grad_norm": 2.2186639308929443,
      "learning_rate": 9.792145768729785e-05,
      "loss": 2.57,
      "step": 4550
    },
    {
      "epoch": 0.2769174713062489,
      "grad_norm": 3.764800548553467,
      "learning_rate": 9.791237538238038e-05,
      "loss": 2.5216,
      "step": 4560
    },
    {
      "epoch": 0.2775247464626222,
      "grad_norm": 3.591568946838379,
      "learning_rate": 9.790327370093525e-05,
      "loss": 2.5234,
      "step": 4570
    },
    {
      "epoch": 0.2781320216189956,
      "grad_norm": 4.236179351806641,
      "learning_rate": 9.78941526466433e-05,
      "loss": 2.4984,
      "step": 4580
    },
    {
      "epoch": 0.2787392967753689,
      "grad_norm": 1.7386977672576904,
      "learning_rate": 9.788501222319324e-05,
      "loss": 2.5498,
      "step": 4590
    },
    {
      "epoch": 0.2793465719317423,
      "grad_norm": 2.146381378173828,
      "learning_rate": 9.78758524342816e-05,
      "loss": 2.7855,
      "step": 4600
    },
    {
      "epoch": 0.2799538470881156,
      "grad_norm": 3.2318713665008545,
      "learning_rate": 9.786667328361274e-05,
      "loss": 2.6095,
      "step": 4610
    },
    {
      "epoch": 0.280561122244489,
      "grad_norm": 3.175516366958618,
      "learning_rate": 9.785747477489887e-05,
      "loss": 2.5665,
      "step": 4620
    },
    {
      "epoch": 0.28116839740086236,
      "grad_norm": 2.4212725162506104,
      "learning_rate": 9.784825691186e-05,
      "loss": 2.6612,
      "step": 4630
    },
    {
      "epoch": 0.2817756725572357,
      "grad_norm": 2.450639486312866,
      "learning_rate": 9.783901969822399e-05,
      "loss": 2.7483,
      "step": 4640
    },
    {
      "epoch": 0.28238294771360906,
      "grad_norm": 3.0986649990081787,
      "learning_rate": 9.78297631377265e-05,
      "loss": 2.6039,
      "step": 4650
    },
    {
      "epoch": 0.2829902228699824,
      "grad_norm": 2.457470178604126,
      "learning_rate": 9.782048723411106e-05,
      "loss": 2.5853,
      "step": 4660
    },
    {
      "epoch": 0.28359749802635575,
      "grad_norm": 3.015305519104004,
      "learning_rate": 9.781119199112896e-05,
      "loss": 2.3475,
      "step": 4670
    },
    {
      "epoch": 0.2842047731827291,
      "grad_norm": 1.8087433576583862,
      "learning_rate": 9.780187741253935e-05,
      "loss": 2.5334,
      "step": 4680
    },
    {
      "epoch": 0.28481204833910245,
      "grad_norm": 3.7137460708618164,
      "learning_rate": 9.779254350210922e-05,
      "loss": 2.6033,
      "step": 4690
    },
    {
      "epoch": 0.2854193234954758,
      "grad_norm": 2.8049583435058594,
      "learning_rate": 9.778319026361332e-05,
      "loss": 2.672,
      "step": 4700
    },
    {
      "epoch": 0.28602659865184915,
      "grad_norm": 3.4322335720062256,
      "learning_rate": 9.777381770083426e-05,
      "loss": 2.8106,
      "step": 4710
    },
    {
      "epoch": 0.28663387380822253,
      "grad_norm": 3.062579393386841,
      "learning_rate": 9.776442581756248e-05,
      "loss": 2.876,
      "step": 4720
    },
    {
      "epoch": 0.28724114896459585,
      "grad_norm": 2.783580780029297,
      "learning_rate": 9.775501461759617e-05,
      "loss": 3.1445,
      "step": 4730
    },
    {
      "epoch": 0.2878484241209692,
      "grad_norm": 2.365514039993286,
      "learning_rate": 9.774558410474139e-05,
      "loss": 2.7749,
      "step": 4740
    },
    {
      "epoch": 0.28845569927734255,
      "grad_norm": 3.6384057998657227,
      "learning_rate": 9.773613428281196e-05,
      "loss": 3.0153,
      "step": 4750
    },
    {
      "epoch": 0.2890629744337159,
      "grad_norm": 2.773881673812866,
      "learning_rate": 9.772666515562958e-05,
      "loss": 2.8961,
      "step": 4760
    },
    {
      "epoch": 0.28967024959008925,
      "grad_norm": 2.42541766166687,
      "learning_rate": 9.77171767270237e-05,
      "loss": 2.855,
      "step": 4770
    },
    {
      "epoch": 0.2902775247464626,
      "grad_norm": 2.4531123638153076,
      "learning_rate": 9.77076690008316e-05,
      "loss": 2.8623,
      "step": 4780
    },
    {
      "epoch": 0.290884799902836,
      "grad_norm": 2.739696502685547,
      "learning_rate": 9.769814198089832e-05,
      "loss": 2.8321,
      "step": 4790
    },
    {
      "epoch": 0.2914920750592093,
      "grad_norm": 3.1892666816711426,
      "learning_rate": 9.768859567107677e-05,
      "loss": 2.826,
      "step": 4800
    },
    {
      "epoch": 0.2920993502155827,
      "grad_norm": 2.444465160369873,
      "learning_rate": 9.767903007522763e-05,
      "loss": 2.6544,
      "step": 4810
    },
    {
      "epoch": 0.292706625371956,
      "grad_norm": 1.8131179809570312,
      "learning_rate": 9.766944519721937e-05,
      "loss": 2.6181,
      "step": 4820
    },
    {
      "epoch": 0.2933139005283294,
      "grad_norm": 2.7964577674865723,
      "learning_rate": 9.765984104092826e-05,
      "loss": 2.6987,
      "step": 4830
    },
    {
      "epoch": 0.2939211756847027,
      "grad_norm": 1.9979743957519531,
      "learning_rate": 9.765021761023838e-05,
      "loss": 2.8659,
      "step": 4840
    },
    {
      "epoch": 0.2945284508410761,
      "grad_norm": 3.546776294708252,
      "learning_rate": 9.764057490904162e-05,
      "loss": 2.9624,
      "step": 4850
    },
    {
      "epoch": 0.29513572599744947,
      "grad_norm": 4.064489364624023,
      "learning_rate": 9.763091294123762e-05,
      "loss": 2.9224,
      "step": 4860
    },
    {
      "epoch": 0.2957430011538228,
      "grad_norm": 5.455779552459717,
      "learning_rate": 9.762123171073383e-05,
      "loss": 2.9502,
      "step": 4870
    },
    {
      "epoch": 0.29635027631019617,
      "grad_norm": 4.005153656005859,
      "learning_rate": 9.76115312214455e-05,
      "loss": 2.5794,
      "step": 4880
    },
    {
      "epoch": 0.2969575514665695,
      "grad_norm": 2.5948123931884766,
      "learning_rate": 9.760181147729568e-05,
      "loss": 2.7633,
      "step": 4890
    },
    {
      "epoch": 0.29756482662294287,
      "grad_norm": 2.356860876083374,
      "learning_rate": 9.759207248221516e-05,
      "loss": 2.9715,
      "step": 4900
    },
    {
      "epoch": 0.2981721017793162,
      "grad_norm": 2.8338937759399414,
      "learning_rate": 9.758231424014256e-05,
      "loss": 2.6093,
      "step": 4910
    },
    {
      "epoch": 0.29877937693568957,
      "grad_norm": 3.4076318740844727,
      "learning_rate": 9.757253675502426e-05,
      "loss": 2.8141,
      "step": 4920
    },
    {
      "epoch": 0.2993866520920629,
      "grad_norm": 2.944585084915161,
      "learning_rate": 9.756274003081444e-05,
      "loss": 2.4748,
      "step": 4930
    },
    {
      "epoch": 0.29999392724843627,
      "grad_norm": 2.2025809288024902,
      "learning_rate": 9.755292407147505e-05,
      "loss": 2.7593,
      "step": 4940
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 2.979404926300049,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.6501,
      "step": 4950
    },
    {
      "epoch": 0.30120847756118296,
      "grad_norm": 3.1154956817626953,
      "learning_rate": 9.753323446329427e-05,
      "loss": 2.6268,
      "step": 4960
    },
    {
      "epoch": 0.30181575271755634,
      "grad_norm": 2.550661325454712,
      "learning_rate": 9.752336082241564e-05,
      "loss": 2.9036,
      "step": 4970
    },
    {
      "epoch": 0.30242302787392966,
      "grad_norm": 4.2608160972595215,
      "learning_rate": 9.751346796233305e-05,
      "loss": 2.7858,
      "step": 4980
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 3.105896472930908,
      "learning_rate": 9.750355588704727e-05,
      "loss": 2.6708,
      "step": 4990
    },
    {
      "epoch": 0.30363757818667636,
      "grad_norm": 2.5458173751831055,
      "learning_rate": 9.749362460056694e-05,
      "loss": 2.6296,
      "step": 5000
    },
    {
      "epoch": 0.30363757818667636,
      "eval_loss": 4.745305061340332,
      "eval_runtime": 2133.9565,
      "eval_samples_per_second": 7.717,
      "eval_steps_per_second": 1.929,
      "step": 5000
    },
    {
      "epoch": 0.30424485334304974,
      "grad_norm": 2.6741912364959717,
      "learning_rate": 9.748367410690842e-05,
      "loss": 4.1674,
      "step": 5010
    },
    {
      "epoch": 0.3048521284994231,
      "grad_norm": 4.83868932723999,
      "learning_rate": 9.747370441009584e-05,
      "loss": 3.3061,
      "step": 5020
    },
    {
      "epoch": 0.30545940365579644,
      "grad_norm": 4.027044296264648,
      "learning_rate": 9.746371551416113e-05,
      "loss": 2.8619,
      "step": 5030
    },
    {
      "epoch": 0.3060666788121698,
      "grad_norm": 2.5786869525909424,
      "learning_rate": 9.745370742314393e-05,
      "loss": 2.5718,
      "step": 5040
    },
    {
      "epoch": 0.30667395396854313,
      "grad_norm": 2.781271457672119,
      "learning_rate": 9.744368014109166e-05,
      "loss": 2.5327,
      "step": 5050
    },
    {
      "epoch": 0.3072812291249165,
      "grad_norm": 2.3781819343566895,
      "learning_rate": 9.743363367205957e-05,
      "loss": 2.5165,
      "step": 5060
    },
    {
      "epoch": 0.30788850428128983,
      "grad_norm": 2.4959161281585693,
      "learning_rate": 9.742356802011054e-05,
      "loss": 2.7677,
      "step": 5070
    },
    {
      "epoch": 0.3084957794376632,
      "grad_norm": 1.902867078781128,
      "learning_rate": 9.741348318931535e-05,
      "loss": 2.9411,
      "step": 5080
    },
    {
      "epoch": 0.30910305459403653,
      "grad_norm": 4.863512992858887,
      "learning_rate": 9.740337918375242e-05,
      "loss": 2.6791,
      "step": 5090
    },
    {
      "epoch": 0.3097103297504099,
      "grad_norm": 2.3409323692321777,
      "learning_rate": 9.739325600750797e-05,
      "loss": 2.8515,
      "step": 5100
    },
    {
      "epoch": 0.3103176049067833,
      "grad_norm": 2.2715625762939453,
      "learning_rate": 9.7383113664676e-05,
      "loss": 2.6985,
      "step": 5110
    },
    {
      "epoch": 0.3109248800631566,
      "grad_norm": 1.7824196815490723,
      "learning_rate": 9.737295215935822e-05,
      "loss": 2.3571,
      "step": 5120
    },
    {
      "epoch": 0.31153215521953,
      "grad_norm": 2.7616944313049316,
      "learning_rate": 9.73627714956641e-05,
      "loss": 2.5082,
      "step": 5130
    },
    {
      "epoch": 0.3121394303759033,
      "grad_norm": 2.158716917037964,
      "learning_rate": 9.735257167771088e-05,
      "loss": 2.3475,
      "step": 5140
    },
    {
      "epoch": 0.3127467055322767,
      "grad_norm": 2.547314167022705,
      "learning_rate": 9.73423527096235e-05,
      "loss": 2.4819,
      "step": 5150
    },
    {
      "epoch": 0.31335398068865,
      "grad_norm": 2.22430682182312,
      "learning_rate": 9.73321145955347e-05,
      "loss": 2.6029,
      "step": 5160
    },
    {
      "epoch": 0.3139612558450234,
      "grad_norm": 2.1192755699157715,
      "learning_rate": 9.732185733958493e-05,
      "loss": 2.96,
      "step": 5170
    },
    {
      "epoch": 0.31456853100139676,
      "grad_norm": 3.70296049118042,
      "learning_rate": 9.731158094592238e-05,
      "loss": 2.9238,
      "step": 5180
    },
    {
      "epoch": 0.3151758061577701,
      "grad_norm": 2.5480382442474365,
      "learning_rate": 9.7301285418703e-05,
      "loss": 2.7029,
      "step": 5190
    },
    {
      "epoch": 0.31578308131414345,
      "grad_norm": 3.7655277252197266,
      "learning_rate": 9.729097076209046e-05,
      "loss": 2.7033,
      "step": 5200
    },
    {
      "epoch": 0.3163903564705168,
      "grad_norm": 1.9767515659332275,
      "learning_rate": 9.728063698025616e-05,
      "loss": 2.2749,
      "step": 5210
    },
    {
      "epoch": 0.31699763162689015,
      "grad_norm": 1.9304218292236328,
      "learning_rate": 9.727028407737926e-05,
      "loss": 2.1663,
      "step": 5220
    },
    {
      "epoch": 0.3176049067832635,
      "grad_norm": 2.0667853355407715,
      "learning_rate": 9.725991205764662e-05,
      "loss": 2.4721,
      "step": 5230
    },
    {
      "epoch": 0.31821218193963685,
      "grad_norm": 2.021442174911499,
      "learning_rate": 9.724952092525287e-05,
      "loss": 2.6251,
      "step": 5240
    },
    {
      "epoch": 0.31881945709601023,
      "grad_norm": 2.164907217025757,
      "learning_rate": 9.723911068440036e-05,
      "loss": 2.8916,
      "step": 5250
    },
    {
      "epoch": 0.31942673225238355,
      "grad_norm": 1.4785090684890747,
      "learning_rate": 9.722868133929912e-05,
      "loss": 2.4771,
      "step": 5260
    },
    {
      "epoch": 0.3200340074087569,
      "grad_norm": 1.9456863403320312,
      "learning_rate": 9.721823289416698e-05,
      "loss": 2.5875,
      "step": 5270
    },
    {
      "epoch": 0.32064128256513025,
      "grad_norm": 5.382901191711426,
      "learning_rate": 9.720776535322943e-05,
      "loss": 2.5853,
      "step": 5280
    },
    {
      "epoch": 0.3212485577215036,
      "grad_norm": 2.786808490753174,
      "learning_rate": 9.719727872071973e-05,
      "loss": 2.73,
      "step": 5290
    },
    {
      "epoch": 0.32185583287787695,
      "grad_norm": 2.620911121368408,
      "learning_rate": 9.718677300087882e-05,
      "loss": 2.3491,
      "step": 5300
    },
    {
      "epoch": 0.3224631080342503,
      "grad_norm": 3.645674228668213,
      "learning_rate": 9.71762481979554e-05,
      "loss": 2.6868,
      "step": 5310
    },
    {
      "epoch": 0.32307038319062364,
      "grad_norm": 5.367486476898193,
      "learning_rate": 9.716570431620586e-05,
      "loss": 2.6366,
      "step": 5320
    },
    {
      "epoch": 0.323677658346997,
      "grad_norm": 3.89725661277771,
      "learning_rate": 9.715514135989432e-05,
      "loss": 2.92,
      "step": 5330
    },
    {
      "epoch": 0.3242849335033704,
      "grad_norm": 3.4178075790405273,
      "learning_rate": 9.714455933329259e-05,
      "loss": 2.9118,
      "step": 5340
    },
    {
      "epoch": 0.3248922086597437,
      "grad_norm": 2.1153080463409424,
      "learning_rate": 9.713395824068024e-05,
      "loss": 2.5333,
      "step": 5350
    },
    {
      "epoch": 0.3254994838161171,
      "grad_norm": 2.962195873260498,
      "learning_rate": 9.712333808634448e-05,
      "loss": 2.2438,
      "step": 5360
    },
    {
      "epoch": 0.3261067589724904,
      "grad_norm": 2.856480598449707,
      "learning_rate": 9.711269887458032e-05,
      "loss": 2.445,
      "step": 5370
    },
    {
      "epoch": 0.3267140341288638,
      "grad_norm": 3.4594435691833496,
      "learning_rate": 9.710204060969038e-05,
      "loss": 2.6681,
      "step": 5380
    },
    {
      "epoch": 0.3273213092852371,
      "grad_norm": 3.5631556510925293,
      "learning_rate": 9.709136329598505e-05,
      "loss": 2.8097,
      "step": 5390
    },
    {
      "epoch": 0.3279285844416105,
      "grad_norm": 3.2399189472198486,
      "learning_rate": 9.708066693778241e-05,
      "loss": 2.5277,
      "step": 5400
    },
    {
      "epoch": 0.32853585959798387,
      "grad_norm": 5.5873823165893555,
      "learning_rate": 9.706995153940825e-05,
      "loss": 2.3689,
      "step": 5410
    },
    {
      "epoch": 0.3291431347543572,
      "grad_norm": 0.855164110660553,
      "learning_rate": 9.705921710519602e-05,
      "loss": 2.0454,
      "step": 5420
    },
    {
      "epoch": 0.32975040991073057,
      "grad_norm": 2.3624320030212402,
      "learning_rate": 9.704846363948692e-05,
      "loss": 2.7751,
      "step": 5430
    },
    {
      "epoch": 0.3303576850671039,
      "grad_norm": 4.844125270843506,
      "learning_rate": 9.70376911466298e-05,
      "loss": 2.6428,
      "step": 5440
    },
    {
      "epoch": 0.33096496022347727,
      "grad_norm": 2.5000205039978027,
      "learning_rate": 9.702689963098124e-05,
      "loss": 2.98,
      "step": 5450
    },
    {
      "epoch": 0.3315722353798506,
      "grad_norm": 2.8151426315307617,
      "learning_rate": 9.701608909690549e-05,
      "loss": 2.7286,
      "step": 5460
    },
    {
      "epoch": 0.33217951053622397,
      "grad_norm": 3.7303924560546875,
      "learning_rate": 9.700525954877453e-05,
      "loss": 2.5384,
      "step": 5470
    },
    {
      "epoch": 0.33278678569259734,
      "grad_norm": 2.46016001701355,
      "learning_rate": 9.699441099096797e-05,
      "loss": 2.6791,
      "step": 5480
    },
    {
      "epoch": 0.33339406084897066,
      "grad_norm": 4.865095615386963,
      "learning_rate": 9.698354342787317e-05,
      "loss": 2.4577,
      "step": 5490
    },
    {
      "epoch": 0.33400133600534404,
      "grad_norm": 2.2835659980773926,
      "learning_rate": 9.697265686388512e-05,
      "loss": 3.079,
      "step": 5500
    },
    {
      "epoch": 0.33460861116171736,
      "grad_norm": 2.5528340339660645,
      "learning_rate": 9.696175130340653e-05,
      "loss": 2.976,
      "step": 5510
    },
    {
      "epoch": 0.33521588631809074,
      "grad_norm": 2.856400966644287,
      "learning_rate": 9.695082675084778e-05,
      "loss": 2.8864,
      "step": 5520
    },
    {
      "epoch": 0.33582316147446406,
      "grad_norm": 3.2843663692474365,
      "learning_rate": 9.693988321062692e-05,
      "loss": 2.6729,
      "step": 5530
    },
    {
      "epoch": 0.33643043663083744,
      "grad_norm": 2.5637011528015137,
      "learning_rate": 9.692892068716974e-05,
      "loss": 2.7483,
      "step": 5540
    },
    {
      "epoch": 0.33703771178721076,
      "grad_norm": 4.352338790893555,
      "learning_rate": 9.69179391849096e-05,
      "loss": 2.5233,
      "step": 5550
    },
    {
      "epoch": 0.33764498694358414,
      "grad_norm": 2.356658458709717,
      "learning_rate": 9.690693870828763e-05,
      "loss": 2.7246,
      "step": 5560
    },
    {
      "epoch": 0.3382522620999575,
      "grad_norm": 3.144735097885132,
      "learning_rate": 9.689591926175257e-05,
      "loss": 3.192,
      "step": 5570
    },
    {
      "epoch": 0.33885953725633083,
      "grad_norm": 3.546539068222046,
      "learning_rate": 9.68848808497609e-05,
      "loss": 2.678,
      "step": 5580
    },
    {
      "epoch": 0.3394668124127042,
      "grad_norm": 2.3875279426574707,
      "learning_rate": 9.68738234767767e-05,
      "loss": 2.5789,
      "step": 5590
    },
    {
      "epoch": 0.34007408756907753,
      "grad_norm": 2.5592949390411377,
      "learning_rate": 9.686274714727175e-05,
      "loss": 2.3757,
      "step": 5600
    },
    {
      "epoch": 0.3406813627254509,
      "grad_norm": 2.857823371887207,
      "learning_rate": 9.68516518657255e-05,
      "loss": 2.6297,
      "step": 5610
    },
    {
      "epoch": 0.34128863788182423,
      "grad_norm": 2.2419705390930176,
      "learning_rate": 9.684053763662507e-05,
      "loss": 2.7652,
      "step": 5620
    },
    {
      "epoch": 0.3418959130381976,
      "grad_norm": 2.3232789039611816,
      "learning_rate": 9.68294044644652e-05,
      "loss": 3.0079,
      "step": 5630
    },
    {
      "epoch": 0.342503188194571,
      "grad_norm": 2.2901551723480225,
      "learning_rate": 9.681825235374835e-05,
      "loss": 2.9549,
      "step": 5640
    },
    {
      "epoch": 0.3431104633509443,
      "grad_norm": 9.80458927154541,
      "learning_rate": 9.68070813089846e-05,
      "loss": 3.0286,
      "step": 5650
    },
    {
      "epoch": 0.3437177385073177,
      "grad_norm": 2.2812693119049072,
      "learning_rate": 9.67958913346917e-05,
      "loss": 2.6461,
      "step": 5660
    },
    {
      "epoch": 0.344325013663691,
      "grad_norm": 2.9942290782928467,
      "learning_rate": 9.678468243539505e-05,
      "loss": 2.7652,
      "step": 5670
    },
    {
      "epoch": 0.3449322888200644,
      "grad_norm": 3.8595285415649414,
      "learning_rate": 9.677345461562773e-05,
      "loss": 2.9439,
      "step": 5680
    },
    {
      "epoch": 0.3455395639764377,
      "grad_norm": 5.829931259155273,
      "learning_rate": 9.67622078799304e-05,
      "loss": 2.8122,
      "step": 5690
    },
    {
      "epoch": 0.3461468391328111,
      "grad_norm": 3.653079032897949,
      "learning_rate": 9.675094223285147e-05,
      "loss": 2.8178,
      "step": 5700
    },
    {
      "epoch": 0.34675411428918446,
      "grad_norm": 2.572143316268921,
      "learning_rate": 9.673965767894693e-05,
      "loss": 3.0294,
      "step": 5710
    },
    {
      "epoch": 0.3473613894455578,
      "grad_norm": 2.2131054401397705,
      "learning_rate": 9.672835422278042e-05,
      "loss": 3.148,
      "step": 5720
    },
    {
      "epoch": 0.34796866460193115,
      "grad_norm": 2.2237515449523926,
      "learning_rate": 9.671703186892325e-05,
      "loss": 2.9621,
      "step": 5730
    },
    {
      "epoch": 0.3485759397583045,
      "grad_norm": 2.1067306995391846,
      "learning_rate": 9.670569062195436e-05,
      "loss": 2.7177,
      "step": 5740
    },
    {
      "epoch": 0.34918321491467785,
      "grad_norm": 4.085829257965088,
      "learning_rate": 9.669433048646032e-05,
      "loss": 2.6314,
      "step": 5750
    },
    {
      "epoch": 0.3497904900710512,
      "grad_norm": 2.664611577987671,
      "learning_rate": 9.668295146703537e-05,
      "loss": 3.0858,
      "step": 5760
    },
    {
      "epoch": 0.35039776522742455,
      "grad_norm": 3.259093761444092,
      "learning_rate": 9.667155356828135e-05,
      "loss": 2.6583,
      "step": 5770
    },
    {
      "epoch": 0.3510050403837979,
      "grad_norm": 2.971238136291504,
      "learning_rate": 9.666013679480777e-05,
      "loss": 3.0113,
      "step": 5780
    },
    {
      "epoch": 0.35161231554017125,
      "grad_norm": 4.043671607971191,
      "learning_rate": 9.664870115123172e-05,
      "loss": 2.8654,
      "step": 5790
    },
    {
      "epoch": 0.3522195906965446,
      "grad_norm": 6.598629951477051,
      "learning_rate": 9.6637246642178e-05,
      "loss": 2.63,
      "step": 5800
    },
    {
      "epoch": 0.35282686585291795,
      "grad_norm": 2.4109833240509033,
      "learning_rate": 9.662577327227896e-05,
      "loss": 2.7285,
      "step": 5810
    },
    {
      "epoch": 0.3534341410092913,
      "grad_norm": 2.4366977214813232,
      "learning_rate": 9.661428104617463e-05,
      "loss": 2.7914,
      "step": 5820
    },
    {
      "epoch": 0.35404141616566465,
      "grad_norm": 2.6925415992736816,
      "learning_rate": 9.660276996851265e-05,
      "loss": 2.5074,
      "step": 5830
    },
    {
      "epoch": 0.354648691322038,
      "grad_norm": 3.3890340328216553,
      "learning_rate": 9.659124004394828e-05,
      "loss": 2.7476,
      "step": 5840
    },
    {
      "epoch": 0.35525596647841134,
      "grad_norm": 1.851601481437683,
      "learning_rate": 9.657969127714441e-05,
      "loss": 2.5746,
      "step": 5850
    },
    {
      "epoch": 0.3558632416347847,
      "grad_norm": 2.098733901977539,
      "learning_rate": 9.656812367277154e-05,
      "loss": 2.7625,
      "step": 5860
    },
    {
      "epoch": 0.3564705167911581,
      "grad_norm": 2.95941424369812,
      "learning_rate": 9.655653723550779e-05,
      "loss": 2.5518,
      "step": 5870
    },
    {
      "epoch": 0.3570777919475314,
      "grad_norm": 2.5173494815826416,
      "learning_rate": 9.65449319700389e-05,
      "loss": 2.5035,
      "step": 5880
    },
    {
      "epoch": 0.3576850671039048,
      "grad_norm": 2.6833133697509766,
      "learning_rate": 9.653330788105823e-05,
      "loss": 2.7432,
      "step": 5890
    },
    {
      "epoch": 0.3582923422602781,
      "grad_norm": 6.134881973266602,
      "learning_rate": 9.652166497326675e-05,
      "loss": 2.6336,
      "step": 5900
    },
    {
      "epoch": 0.3588996174166515,
      "grad_norm": 2.0269761085510254,
      "learning_rate": 9.651000325137304e-05,
      "loss": 3.0107,
      "step": 5910
    },
    {
      "epoch": 0.3595068925730248,
      "grad_norm": 5.320703029632568,
      "learning_rate": 9.649832272009327e-05,
      "loss": 2.9408,
      "step": 5920
    },
    {
      "epoch": 0.3601141677293982,
      "grad_norm": 3.326845407485962,
      "learning_rate": 9.648662338415124e-05,
      "loss": 2.823,
      "step": 5930
    },
    {
      "epoch": 0.36072144288577157,
      "grad_norm": 5.812035083770752,
      "learning_rate": 9.647490524827834e-05,
      "loss": 2.6478,
      "step": 5940
    },
    {
      "epoch": 0.3613287180421449,
      "grad_norm": 2.835839033126831,
      "learning_rate": 9.646316831721359e-05,
      "loss": 2.8917,
      "step": 5950
    },
    {
      "epoch": 0.36193599319851827,
      "grad_norm": 6.6656413078308105,
      "learning_rate": 9.645141259570358e-05,
      "loss": 3.1907,
      "step": 5960
    },
    {
      "epoch": 0.3625432683548916,
      "grad_norm": 3.2498443126678467,
      "learning_rate": 9.643963808850252e-05,
      "loss": 2.5695,
      "step": 5970
    },
    {
      "epoch": 0.36315054351126497,
      "grad_norm": 2.7686610221862793,
      "learning_rate": 9.642784480037218e-05,
      "loss": 2.5823,
      "step": 5980
    },
    {
      "epoch": 0.3637578186676383,
      "grad_norm": 4.087450981140137,
      "learning_rate": 9.6416032736082e-05,
      "loss": 2.4717,
      "step": 5990
    },
    {
      "epoch": 0.36436509382401167,
      "grad_norm": 3.5569775104522705,
      "learning_rate": 9.640420190040893e-05,
      "loss": 2.7406,
      "step": 6000
    },
    {
      "epoch": 0.364972368980385,
      "grad_norm": 3.855123996734619,
      "learning_rate": 9.639235229813754e-05,
      "loss": 2.7359,
      "step": 6010
    },
    {
      "epoch": 0.36557964413675836,
      "grad_norm": 2.241460084915161,
      "learning_rate": 9.638048393406006e-05,
      "loss": 2.2879,
      "step": 6020
    },
    {
      "epoch": 0.36618691929313174,
      "grad_norm": 2.7894084453582764,
      "learning_rate": 9.636859681297616e-05,
      "loss": 2.7243,
      "step": 6030
    },
    {
      "epoch": 0.36679419444950506,
      "grad_norm": 2.346806764602661,
      "learning_rate": 9.635669093969323e-05,
      "loss": 2.3544,
      "step": 6040
    },
    {
      "epoch": 0.36740146960587844,
      "grad_norm": 2.815704822540283,
      "learning_rate": 9.634476631902623e-05,
      "loss": 2.4551,
      "step": 6050
    },
    {
      "epoch": 0.36800874476225176,
      "grad_norm": 2.600898265838623,
      "learning_rate": 9.633282295579758e-05,
      "loss": 2.9787,
      "step": 6060
    },
    {
      "epoch": 0.36861601991862514,
      "grad_norm": 3.494194269180298,
      "learning_rate": 9.632086085483742e-05,
      "loss": 2.7354,
      "step": 6070
    },
    {
      "epoch": 0.36922329507499846,
      "grad_norm": 3.285033941268921,
      "learning_rate": 9.630888002098343e-05,
      "loss": 2.3451,
      "step": 6080
    },
    {
      "epoch": 0.36983057023137184,
      "grad_norm": 2.113001585006714,
      "learning_rate": 9.629688045908081e-05,
      "loss": 2.4789,
      "step": 6090
    },
    {
      "epoch": 0.3704378453877452,
      "grad_norm": 3.1952943801879883,
      "learning_rate": 9.628486217398238e-05,
      "loss": 2.7839,
      "step": 6100
    },
    {
      "epoch": 0.37104512054411853,
      "grad_norm": 4.726895332336426,
      "learning_rate": 9.627282517054854e-05,
      "loss": 2.6738,
      "step": 6110
    },
    {
      "epoch": 0.3716523957004919,
      "grad_norm": 1.644710898399353,
      "learning_rate": 9.626076945364726e-05,
      "loss": 2.7804,
      "step": 6120
    },
    {
      "epoch": 0.37225967085686523,
      "grad_norm": 2.2642054557800293,
      "learning_rate": 9.624869502815404e-05,
      "loss": 2.8596,
      "step": 6130
    },
    {
      "epoch": 0.3728669460132386,
      "grad_norm": 2.588972330093384,
      "learning_rate": 9.623660189895196e-05,
      "loss": 3.1691,
      "step": 6140
    },
    {
      "epoch": 0.37347422116961193,
      "grad_norm": 2.1809396743774414,
      "learning_rate": 9.622449007093169e-05,
      "loss": 2.5126,
      "step": 6150
    },
    {
      "epoch": 0.3740814963259853,
      "grad_norm": 2.0968565940856934,
      "learning_rate": 9.621235954899146e-05,
      "loss": 2.8423,
      "step": 6160
    },
    {
      "epoch": 0.37468877148235863,
      "grad_norm": 1.7743654251098633,
      "learning_rate": 9.620021033803701e-05,
      "loss": 2.6257,
      "step": 6170
    },
    {
      "epoch": 0.375296046638732,
      "grad_norm": 2.880082845687866,
      "learning_rate": 9.618804244298171e-05,
      "loss": 2.73,
      "step": 6180
    },
    {
      "epoch": 0.3759033217951054,
      "grad_norm": 3.946518659591675,
      "learning_rate": 9.617585586874645e-05,
      "loss": 2.7765,
      "step": 6190
    },
    {
      "epoch": 0.3765105969514787,
      "grad_norm": 2.165799379348755,
      "learning_rate": 9.616365062025965e-05,
      "loss": 2.8204,
      "step": 6200
    },
    {
      "epoch": 0.3771178721078521,
      "grad_norm": 5.1953043937683105,
      "learning_rate": 9.615142670245731e-05,
      "loss": 2.8649,
      "step": 6210
    },
    {
      "epoch": 0.3777251472642254,
      "grad_norm": 2.68890380859375,
      "learning_rate": 9.6139184120283e-05,
      "loss": 2.8869,
      "step": 6220
    },
    {
      "epoch": 0.3783324224205988,
      "grad_norm": 1.847641110420227,
      "learning_rate": 9.612692287868778e-05,
      "loss": 2.8794,
      "step": 6230
    },
    {
      "epoch": 0.3789396975769721,
      "grad_norm": 3.9583866596221924,
      "learning_rate": 9.611464298263034e-05,
      "loss": 2.9029,
      "step": 6240
    },
    {
      "epoch": 0.3795469727333455,
      "grad_norm": 4.001104831695557,
      "learning_rate": 9.610234443707682e-05,
      "loss": 2.8142,
      "step": 6250
    },
    {
      "epoch": 0.38015424788971885,
      "grad_norm": 2.4900574684143066,
      "learning_rate": 9.609002724700097e-05,
      "loss": 2.3983,
      "step": 6260
    },
    {
      "epoch": 0.3807615230460922,
      "grad_norm": 3.2619574069976807,
      "learning_rate": 9.607769141738405e-05,
      "loss": 2.9884,
      "step": 6270
    },
    {
      "epoch": 0.38136879820246555,
      "grad_norm": 2.988677501678467,
      "learning_rate": 9.606533695321486e-05,
      "loss": 2.507,
      "step": 6280
    },
    {
      "epoch": 0.3819760733588389,
      "grad_norm": 4.381251335144043,
      "learning_rate": 9.605296385948976e-05,
      "loss": 2.5648,
      "step": 6290
    },
    {
      "epoch": 0.38258334851521225,
      "grad_norm": 5.964694499969482,
      "learning_rate": 9.604057214121263e-05,
      "loss": 2.5482,
      "step": 6300
    },
    {
      "epoch": 0.3831906236715856,
      "grad_norm": 2.1384005546569824,
      "learning_rate": 9.602816180339484e-05,
      "loss": 2.209,
      "step": 6310
    },
    {
      "epoch": 0.38379789882795895,
      "grad_norm": 2.4828503131866455,
      "learning_rate": 9.601573285105539e-05,
      "loss": 2.355,
      "step": 6320
    },
    {
      "epoch": 0.3844051739843323,
      "grad_norm": 2.697730302810669,
      "learning_rate": 9.600328528922068e-05,
      "loss": 2.5957,
      "step": 6330
    },
    {
      "epoch": 0.38501244914070565,
      "grad_norm": 3.3610942363739014,
      "learning_rate": 9.599081912292473e-05,
      "loss": 2.7611,
      "step": 6340
    },
    {
      "epoch": 0.385619724297079,
      "grad_norm": 3.4536027908325195,
      "learning_rate": 9.597833435720908e-05,
      "loss": 2.7009,
      "step": 6350
    },
    {
      "epoch": 0.38622699945345235,
      "grad_norm": 4.879227161407471,
      "learning_rate": 9.596583099712272e-05,
      "loss": 3.4286,
      "step": 6360
    },
    {
      "epoch": 0.3868342746098257,
      "grad_norm": 3.6915879249572754,
      "learning_rate": 9.595330904772226e-05,
      "loss": 3.1466,
      "step": 6370
    },
    {
      "epoch": 0.38744154976619904,
      "grad_norm": 4.521791458129883,
      "learning_rate": 9.594076851407175e-05,
      "loss": 2.8609,
      "step": 6380
    },
    {
      "epoch": 0.3880488249225724,
      "grad_norm": 2.948370933532715,
      "learning_rate": 9.592820940124276e-05,
      "loss": 2.7174,
      "step": 6390
    },
    {
      "epoch": 0.38865610007894574,
      "grad_norm": 1.7646528482437134,
      "learning_rate": 9.591563171431444e-05,
      "loss": 2.9236,
      "step": 6400
    },
    {
      "epoch": 0.3892633752353191,
      "grad_norm": 4.30991268157959,
      "learning_rate": 9.590303545837337e-05,
      "loss": 2.5504,
      "step": 6410
    },
    {
      "epoch": 0.3898706503916925,
      "grad_norm": 4.38593864440918,
      "learning_rate": 9.58904206385137e-05,
      "loss": 2.6177,
      "step": 6420
    },
    {
      "epoch": 0.3904779255480658,
      "grad_norm": 2.877537727355957,
      "learning_rate": 9.587778725983705e-05,
      "loss": 2.6163,
      "step": 6430
    },
    {
      "epoch": 0.3910852007044392,
      "grad_norm": 2.008472442626953,
      "learning_rate": 9.586513532745256e-05,
      "loss": 2.714,
      "step": 6440
    },
    {
      "epoch": 0.3916924758608125,
      "grad_norm": 2.9940168857574463,
      "learning_rate": 9.585246484647688e-05,
      "loss": 2.7893,
      "step": 6450
    },
    {
      "epoch": 0.3922997510171859,
      "grad_norm": 1.5191490650177002,
      "learning_rate": 9.583977582203415e-05,
      "loss": 2.7277,
      "step": 6460
    },
    {
      "epoch": 0.3929070261735592,
      "grad_norm": 2.197955846786499,
      "learning_rate": 9.582706825925601e-05,
      "loss": 2.8536,
      "step": 6470
    },
    {
      "epoch": 0.3935143013299326,
      "grad_norm": 2.9618451595306396,
      "learning_rate": 9.581434216328162e-05,
      "loss": 2.5112,
      "step": 6480
    },
    {
      "epoch": 0.39412157648630597,
      "grad_norm": 3.819021224975586,
      "learning_rate": 9.580159753925759e-05,
      "loss": 2.4261,
      "step": 6490
    },
    {
      "epoch": 0.3947288516426793,
      "grad_norm": 1.8810676336288452,
      "learning_rate": 9.578883439233807e-05,
      "loss": 2.8192,
      "step": 6500
    },
    {
      "epoch": 0.39533612679905267,
      "grad_norm": 2.5150134563446045,
      "learning_rate": 9.577605272768466e-05,
      "loss": 3.0349,
      "step": 6510
    },
    {
      "epoch": 0.395943401955426,
      "grad_norm": 2.6379692554473877,
      "learning_rate": 9.576325255046649e-05,
      "loss": 2.759,
      "step": 6520
    },
    {
      "epoch": 0.39655067711179937,
      "grad_norm": 1.5309852361679077,
      "learning_rate": 9.575043386586013e-05,
      "loss": 2.2774,
      "step": 6530
    },
    {
      "epoch": 0.3971579522681727,
      "grad_norm": 1.9458774328231812,
      "learning_rate": 9.573759667904968e-05,
      "loss": 2.3237,
      "step": 6540
    },
    {
      "epoch": 0.39776522742454606,
      "grad_norm": 2.6952147483825684,
      "learning_rate": 9.57247409952267e-05,
      "loss": 2.5261,
      "step": 6550
    },
    {
      "epoch": 0.39837250258091944,
      "grad_norm": 3.182999610900879,
      "learning_rate": 9.571186681959023e-05,
      "loss": 2.9124,
      "step": 6560
    },
    {
      "epoch": 0.39897977773729276,
      "grad_norm": 3.764650821685791,
      "learning_rate": 9.569897415734681e-05,
      "loss": 2.9634,
      "step": 6570
    },
    {
      "epoch": 0.39958705289366614,
      "grad_norm": 3.330183267593384,
      "learning_rate": 9.56860630137104e-05,
      "loss": 2.874,
      "step": 6580
    },
    {
      "epoch": 0.40019432805003946,
      "grad_norm": 1.3074390888214111,
      "learning_rate": 9.567313339390251e-05,
      "loss": 2.766,
      "step": 6590
    },
    {
      "epoch": 0.40080160320641284,
      "grad_norm": 2.626110553741455,
      "learning_rate": 9.566018530315204e-05,
      "loss": 2.7003,
      "step": 6600
    },
    {
      "epoch": 0.40140887836278616,
      "grad_norm": 4.799874782562256,
      "learning_rate": 9.564721874669545e-05,
      "loss": 3.174,
      "step": 6610
    },
    {
      "epoch": 0.40201615351915954,
      "grad_norm": 2.031907558441162,
      "learning_rate": 9.563423372977661e-05,
      "loss": 2.6529,
      "step": 6620
    },
    {
      "epoch": 0.40262342867553286,
      "grad_norm": 1.4973608255386353,
      "learning_rate": 9.562123025764684e-05,
      "loss": 2.3761,
      "step": 6630
    },
    {
      "epoch": 0.40323070383190623,
      "grad_norm": 2.6374387741088867,
      "learning_rate": 9.560820833556498e-05,
      "loss": 2.603,
      "step": 6640
    },
    {
      "epoch": 0.4038379789882796,
      "grad_norm": 4.395895957946777,
      "learning_rate": 9.55951679687973e-05,
      "loss": 2.7188,
      "step": 6650
    },
    {
      "epoch": 0.40444525414465293,
      "grad_norm": 3.860273838043213,
      "learning_rate": 9.558210916261751e-05,
      "loss": 2.8806,
      "step": 6660
    },
    {
      "epoch": 0.4050525293010263,
      "grad_norm": 5.64556360244751,
      "learning_rate": 9.556903192230684e-05,
      "loss": 2.858,
      "step": 6670
    },
    {
      "epoch": 0.40565980445739963,
      "grad_norm": 2.739227771759033,
      "learning_rate": 9.55559362531539e-05,
      "loss": 2.9502,
      "step": 6680
    },
    {
      "epoch": 0.406267079613773,
      "grad_norm": 2.178692102432251,
      "learning_rate": 9.55428221604548e-05,
      "loss": 2.3658,
      "step": 6690
    },
    {
      "epoch": 0.40687435477014633,
      "grad_norm": 1.8897972106933594,
      "learning_rate": 9.552968964951307e-05,
      "loss": 2.3181,
      "step": 6700
    },
    {
      "epoch": 0.4074816299265197,
      "grad_norm": 2.3382785320281982,
      "learning_rate": 9.551653872563975e-05,
      "loss": 3.0448,
      "step": 6710
    },
    {
      "epoch": 0.4080889050828931,
      "grad_norm": 3.991377115249634,
      "learning_rate": 9.550336939415323e-05,
      "loss": 2.8662,
      "step": 6720
    },
    {
      "epoch": 0.4086961802392664,
      "grad_norm": 2.7902309894561768,
      "learning_rate": 9.549018166037943e-05,
      "loss": 2.5172,
      "step": 6730
    },
    {
      "epoch": 0.4093034553956398,
      "grad_norm": 1.8063932657241821,
      "learning_rate": 9.547697552965167e-05,
      "loss": 2.6203,
      "step": 6740
    },
    {
      "epoch": 0.4099107305520131,
      "grad_norm": 2.726249933242798,
      "learning_rate": 9.546375100731073e-05,
      "loss": 2.6711,
      "step": 6750
    },
    {
      "epoch": 0.4105180057083865,
      "grad_norm": 2.440382242202759,
      "learning_rate": 9.54505080987048e-05,
      "loss": 3.0946,
      "step": 6760
    },
    {
      "epoch": 0.4111252808647598,
      "grad_norm": 2.070815324783325,
      "learning_rate": 9.543724680918953e-05,
      "loss": 2.8491,
      "step": 6770
    },
    {
      "epoch": 0.4117325560211332,
      "grad_norm": 3.2073957920074463,
      "learning_rate": 9.5423967144128e-05,
      "loss": 2.5598,
      "step": 6780
    },
    {
      "epoch": 0.41233983117750656,
      "grad_norm": 2.121354341506958,
      "learning_rate": 9.541066910889071e-05,
      "loss": 2.7388,
      "step": 6790
    },
    {
      "epoch": 0.4129471063338799,
      "grad_norm": 2.785533905029297,
      "learning_rate": 9.539735270885562e-05,
      "loss": 2.908,
      "step": 6800
    },
    {
      "epoch": 0.41355438149025325,
      "grad_norm": 1.7827931642532349,
      "learning_rate": 9.538401794940808e-05,
      "loss": 2.5497,
      "step": 6810
    },
    {
      "epoch": 0.4141616566466266,
      "grad_norm": 2.3772876262664795,
      "learning_rate": 9.537066483594086e-05,
      "loss": 2.5483,
      "step": 6820
    },
    {
      "epoch": 0.41476893180299995,
      "grad_norm": 1.4526681900024414,
      "learning_rate": 9.53572933738542e-05,
      "loss": 2.451,
      "step": 6830
    },
    {
      "epoch": 0.4153762069593733,
      "grad_norm": 3.186890125274658,
      "learning_rate": 9.534390356855571e-05,
      "loss": 2.4843,
      "step": 6840
    },
    {
      "epoch": 0.41598348211574665,
      "grad_norm": 1.6987366676330566,
      "learning_rate": 9.533049542546046e-05,
      "loss": 2.2903,
      "step": 6850
    },
    {
      "epoch": 0.41659075727211997,
      "grad_norm": 1.4047526121139526,
      "learning_rate": 9.531706894999091e-05,
      "loss": 2.6323,
      "step": 6860
    },
    {
      "epoch": 0.41719803242849335,
      "grad_norm": 2.185570478439331,
      "learning_rate": 9.530362414757694e-05,
      "loss": 2.3132,
      "step": 6870
    },
    {
      "epoch": 0.4178053075848667,
      "grad_norm": 2.0347447395324707,
      "learning_rate": 9.529016102365584e-05,
      "loss": 3.1552,
      "step": 6880
    },
    {
      "epoch": 0.41841258274124005,
      "grad_norm": 3.7183549404144287,
      "learning_rate": 9.52766795836723e-05,
      "loss": 2.5769,
      "step": 6890
    },
    {
      "epoch": 0.4190198578976134,
      "grad_norm": 2.3504745960235596,
      "learning_rate": 9.526317983307847e-05,
      "loss": 2.5845,
      "step": 6900
    },
    {
      "epoch": 0.41962713305398675,
      "grad_norm": 3.5823423862457275,
      "learning_rate": 9.524966177733382e-05,
      "loss": 2.5182,
      "step": 6910
    },
    {
      "epoch": 0.4202344082103601,
      "grad_norm": 1.897058129310608,
      "learning_rate": 9.523612542190528e-05,
      "loss": 3.0682,
      "step": 6920
    },
    {
      "epoch": 0.42084168336673344,
      "grad_norm": 3.456033945083618,
      "learning_rate": 9.522257077226717e-05,
      "loss": 2.6041,
      "step": 6930
    },
    {
      "epoch": 0.4214489585231068,
      "grad_norm": 3.0606629848480225,
      "learning_rate": 9.52089978339012e-05,
      "loss": 2.7816,
      "step": 6940
    },
    {
      "epoch": 0.4220562336794802,
      "grad_norm": 4.168856143951416,
      "learning_rate": 9.519540661229651e-05,
      "loss": 2.84,
      "step": 6950
    },
    {
      "epoch": 0.4226635088358535,
      "grad_norm": 3.647380828857422,
      "learning_rate": 9.518179711294956e-05,
      "loss": 2.6999,
      "step": 6960
    },
    {
      "epoch": 0.4232707839922269,
      "grad_norm": 2.4895718097686768,
      "learning_rate": 9.51681693413643e-05,
      "loss": 2.6372,
      "step": 6970
    },
    {
      "epoch": 0.4238780591486002,
      "grad_norm": 3.7437005043029785,
      "learning_rate": 9.515452330305198e-05,
      "loss": 2.8081,
      "step": 6980
    },
    {
      "epoch": 0.4244853343049736,
      "grad_norm": 3.923025131225586,
      "learning_rate": 9.514085900353128e-05,
      "loss": 3.0923,
      "step": 6990
    },
    {
      "epoch": 0.4250926094613469,
      "grad_norm": 2.4463391304016113,
      "learning_rate": 9.512717644832828e-05,
      "loss": 3.0319,
      "step": 7000
    },
    {
      "epoch": 0.4256998846177203,
      "grad_norm": 2.593069076538086,
      "learning_rate": 9.511347564297642e-05,
      "loss": 3.0317,
      "step": 7010
    },
    {
      "epoch": 0.42630715977409367,
      "grad_norm": 3.0984816551208496,
      "learning_rate": 9.509975659301649e-05,
      "loss": 3.1768,
      "step": 7020
    },
    {
      "epoch": 0.426914434930467,
      "grad_norm": 3.3235104084014893,
      "learning_rate": 9.508601930399673e-05,
      "loss": 2.6453,
      "step": 7030
    },
    {
      "epoch": 0.42752171008684037,
      "grad_norm": 3.1662566661834717,
      "learning_rate": 9.50722637814727e-05,
      "loss": 2.6087,
      "step": 7040
    },
    {
      "epoch": 0.4281289852432137,
      "grad_norm": 4.349977016448975,
      "learning_rate": 9.505849003100736e-05,
      "loss": 2.4466,
      "step": 7050
    },
    {
      "epoch": 0.42873626039958707,
      "grad_norm": 3.6620123386383057,
      "learning_rate": 9.504469805817105e-05,
      "loss": 3.0549,
      "step": 7060
    },
    {
      "epoch": 0.4293435355559604,
      "grad_norm": 2.4505999088287354,
      "learning_rate": 9.503088786854141e-05,
      "loss": 2.7093,
      "step": 7070
    },
    {
      "epoch": 0.42995081071233376,
      "grad_norm": 2.448587417602539,
      "learning_rate": 9.501705946770356e-05,
      "loss": 2.9238,
      "step": 7080
    },
    {
      "epoch": 0.4305580858687071,
      "grad_norm": 1.719582200050354,
      "learning_rate": 9.50032128612499e-05,
      "loss": 2.7233,
      "step": 7090
    },
    {
      "epoch": 0.43116536102508046,
      "grad_norm": 2.9183897972106934,
      "learning_rate": 9.498934805478021e-05,
      "loss": 2.8875,
      "step": 7100
    },
    {
      "epoch": 0.43177263618145384,
      "grad_norm": 4.628803253173828,
      "learning_rate": 9.497546505390167e-05,
      "loss": 2.8595,
      "step": 7110
    },
    {
      "epoch": 0.43237991133782716,
      "grad_norm": 2.5070011615753174,
      "learning_rate": 9.496156386422875e-05,
      "loss": 2.6951,
      "step": 7120
    },
    {
      "epoch": 0.43298718649420054,
      "grad_norm": 2.1720826625823975,
      "learning_rate": 9.494764449138331e-05,
      "loss": 2.5029,
      "step": 7130
    },
    {
      "epoch": 0.43359446165057386,
      "grad_norm": 1.9122594594955444,
      "learning_rate": 9.493370694099462e-05,
      "loss": 2.4437,
      "step": 7140
    },
    {
      "epoch": 0.43420173680694724,
      "grad_norm": 3.514885902404785,
      "learning_rate": 9.491975121869919e-05,
      "loss": 2.7069,
      "step": 7150
    },
    {
      "epoch": 0.43480901196332056,
      "grad_norm": 3.273972749710083,
      "learning_rate": 9.490577733014094e-05,
      "loss": 2.7308,
      "step": 7160
    },
    {
      "epoch": 0.43541628711969393,
      "grad_norm": 3.3102753162384033,
      "learning_rate": 9.489178528097117e-05,
      "loss": 2.8353,
      "step": 7170
    },
    {
      "epoch": 0.4360235622760673,
      "grad_norm": 3.7430219650268555,
      "learning_rate": 9.487777507684848e-05,
      "loss": 2.8223,
      "step": 7180
    },
    {
      "epoch": 0.43663083743244063,
      "grad_norm": 3.646674871444702,
      "learning_rate": 9.486374672343878e-05,
      "loss": 2.8986,
      "step": 7190
    },
    {
      "epoch": 0.437238112588814,
      "grad_norm": 4.895406246185303,
      "learning_rate": 9.484970022641541e-05,
      "loss": 2.5558,
      "step": 7200
    },
    {
      "epoch": 0.43784538774518733,
      "grad_norm": 3.374790906906128,
      "learning_rate": 9.483563559145898e-05,
      "loss": 2.5032,
      "step": 7210
    },
    {
      "epoch": 0.4384526629015607,
      "grad_norm": 1.6497316360473633,
      "learning_rate": 9.482155282425742e-05,
      "loss": 2.4587,
      "step": 7220
    },
    {
      "epoch": 0.43905993805793403,
      "grad_norm": 2.925004482269287,
      "learning_rate": 9.480745193050607e-05,
      "loss": 2.8504,
      "step": 7230
    },
    {
      "epoch": 0.4396672132143074,
      "grad_norm": 3.6087300777435303,
      "learning_rate": 9.479333291590753e-05,
      "loss": 3.1204,
      "step": 7240
    },
    {
      "epoch": 0.4402744883706807,
      "grad_norm": 5.488052845001221,
      "learning_rate": 9.477919578617176e-05,
      "loss": 2.7963,
      "step": 7250
    },
    {
      "epoch": 0.4408817635270541,
      "grad_norm": 4.955268859863281,
      "learning_rate": 9.476504054701605e-05,
      "loss": 2.5205,
      "step": 7260
    },
    {
      "epoch": 0.4414890386834275,
      "grad_norm": 1.8225622177124023,
      "learning_rate": 9.475086720416499e-05,
      "loss": 2.771,
      "step": 7270
    },
    {
      "epoch": 0.4420963138398008,
      "grad_norm": 2.3806283473968506,
      "learning_rate": 9.473667576335052e-05,
      "loss": 2.6292,
      "step": 7280
    },
    {
      "epoch": 0.4427035889961742,
      "grad_norm": 3.2257490158081055,
      "learning_rate": 9.472246623031186e-05,
      "loss": 2.4668,
      "step": 7290
    },
    {
      "epoch": 0.4433108641525475,
      "grad_norm": 1.6855899095535278,
      "learning_rate": 9.470823861079561e-05,
      "loss": 2.2081,
      "step": 7300
    },
    {
      "epoch": 0.4439181393089209,
      "grad_norm": 2.096592426300049,
      "learning_rate": 9.469399291055562e-05,
      "loss": 2.2414,
      "step": 7310
    },
    {
      "epoch": 0.4445254144652942,
      "grad_norm": 3.2058587074279785,
      "learning_rate": 9.467972913535308e-05,
      "loss": 2.5989,
      "step": 7320
    },
    {
      "epoch": 0.4451326896216676,
      "grad_norm": 1.3780303001403809,
      "learning_rate": 9.46654472909565e-05,
      "loss": 2.2135,
      "step": 7330
    },
    {
      "epoch": 0.44573996477804095,
      "grad_norm": 1.989880919456482,
      "learning_rate": 9.465114738314166e-05,
      "loss": 2.5152,
      "step": 7340
    },
    {
      "epoch": 0.4463472399344143,
      "grad_norm": 1.6085102558135986,
      "learning_rate": 9.46368294176917e-05,
      "loss": 2.4733,
      "step": 7350
    },
    {
      "epoch": 0.44695451509078765,
      "grad_norm": 2.012315273284912,
      "learning_rate": 9.4622493400397e-05,
      "loss": 2.5995,
      "step": 7360
    },
    {
      "epoch": 0.447561790247161,
      "grad_norm": 2.7757766246795654,
      "learning_rate": 9.460813933705531e-05,
      "loss": 2.5682,
      "step": 7370
    },
    {
      "epoch": 0.44816906540353435,
      "grad_norm": 2.7023942470550537,
      "learning_rate": 9.459376723347161e-05,
      "loss": 2.7265,
      "step": 7380
    },
    {
      "epoch": 0.44877634055990767,
      "grad_norm": 3.4476265907287598,
      "learning_rate": 9.457937709545823e-05,
      "loss": 2.3926,
      "step": 7390
    },
    {
      "epoch": 0.44938361571628105,
      "grad_norm": 2.5528769493103027,
      "learning_rate": 9.456496892883477e-05,
      "loss": 2.4089,
      "step": 7400
    },
    {
      "epoch": 0.4499908908726544,
      "grad_norm": 1.8749299049377441,
      "learning_rate": 9.455054273942811e-05,
      "loss": 3.0185,
      "step": 7410
    },
    {
      "epoch": 0.45059816602902775,
      "grad_norm": 4.879673957824707,
      "learning_rate": 9.453609853307244e-05,
      "loss": 2.6527,
      "step": 7420
    },
    {
      "epoch": 0.4512054411854011,
      "grad_norm": 3.1053240299224854,
      "learning_rate": 9.452163631560922e-05,
      "loss": 2.564,
      "step": 7430
    },
    {
      "epoch": 0.45181271634177445,
      "grad_norm": 1.679781198501587,
      "learning_rate": 9.45071560928872e-05,
      "loss": 2.5215,
      "step": 7440
    },
    {
      "epoch": 0.4524199914981478,
      "grad_norm": 2.149834156036377,
      "learning_rate": 9.449265787076243e-05,
      "loss": 2.5366,
      "step": 7450
    },
    {
      "epoch": 0.45302726665452114,
      "grad_norm": 2.4370946884155273,
      "learning_rate": 9.44781416550982e-05,
      "loss": 2.5216,
      "step": 7460
    },
    {
      "epoch": 0.4536345418108945,
      "grad_norm": 1.7248789072036743,
      "learning_rate": 9.446360745176511e-05,
      "loss": 2.8253,
      "step": 7470
    },
    {
      "epoch": 0.45424181696726784,
      "grad_norm": 2.6355230808258057,
      "learning_rate": 9.444905526664103e-05,
      "loss": 2.7683,
      "step": 7480
    },
    {
      "epoch": 0.4548490921236412,
      "grad_norm": 3.016929864883423,
      "learning_rate": 9.443448510561109e-05,
      "loss": 2.5903,
      "step": 7490
    },
    {
      "epoch": 0.4554563672800146,
      "grad_norm": 3.217670202255249,
      "learning_rate": 9.441989697456767e-05,
      "loss": 2.6134,
      "step": 7500
    },
    {
      "epoch": 0.4560636424363879,
      "grad_norm": 3.5699570178985596,
      "learning_rate": 9.440529087941047e-05,
      "loss": 2.6705,
      "step": 7510
    },
    {
      "epoch": 0.4566709175927613,
      "grad_norm": 4.67714786529541,
      "learning_rate": 9.439066682604643e-05,
      "loss": 2.6455,
      "step": 7520
    },
    {
      "epoch": 0.4572781927491346,
      "grad_norm": 3.102698802947998,
      "learning_rate": 9.437602482038974e-05,
      "loss": 2.7091,
      "step": 7530
    },
    {
      "epoch": 0.457885467905508,
      "grad_norm": 4.027877330780029,
      "learning_rate": 9.436136486836186e-05,
      "loss": 2.5843,
      "step": 7540
    },
    {
      "epoch": 0.4584927430618813,
      "grad_norm": 3.514023780822754,
      "learning_rate": 9.43466869758915e-05,
      "loss": 2.9912,
      "step": 7550
    },
    {
      "epoch": 0.4591000182182547,
      "grad_norm": 2.1756556034088135,
      "learning_rate": 9.433199114891467e-05,
      "loss": 3.0732,
      "step": 7560
    },
    {
      "epoch": 0.45970729337462807,
      "grad_norm": 3.14790678024292,
      "learning_rate": 9.431727739337454e-05,
      "loss": 3.0225,
      "step": 7570
    },
    {
      "epoch": 0.4603145685310014,
      "grad_norm": 2.4650678634643555,
      "learning_rate": 9.430254571522163e-05,
      "loss": 2.4016,
      "step": 7580
    },
    {
      "epoch": 0.46092184368737477,
      "grad_norm": 2.506086587905884,
      "learning_rate": 9.428779612041368e-05,
      "loss": 2.4367,
      "step": 7590
    },
    {
      "epoch": 0.4615291188437481,
      "grad_norm": 1.934072732925415,
      "learning_rate": 9.427302861491561e-05,
      "loss": 2.9873,
      "step": 7600
    },
    {
      "epoch": 0.46213639400012146,
      "grad_norm": 2.5189051628112793,
      "learning_rate": 9.425824320469964e-05,
      "loss": 2.6232,
      "step": 7610
    },
    {
      "epoch": 0.4627436691564948,
      "grad_norm": 2.6924445629119873,
      "learning_rate": 9.424343989574526e-05,
      "loss": 2.3938,
      "step": 7620
    },
    {
      "epoch": 0.46335094431286816,
      "grad_norm": 3.7002062797546387,
      "learning_rate": 9.422861869403916e-05,
      "loss": 2.77,
      "step": 7630
    },
    {
      "epoch": 0.46395821946924154,
      "grad_norm": 5.072664737701416,
      "learning_rate": 9.421377960557525e-05,
      "loss": 2.6419,
      "step": 7640
    },
    {
      "epoch": 0.46456549462561486,
      "grad_norm": 2.215675115585327,
      "learning_rate": 9.419892263635468e-05,
      "loss": 2.1278,
      "step": 7650
    },
    {
      "epoch": 0.46517276978198824,
      "grad_norm": 1.8106184005737305,
      "learning_rate": 9.41840477923859e-05,
      "loss": 2.3305,
      "step": 7660
    },
    {
      "epoch": 0.46578004493836156,
      "grad_norm": 2.77698016166687,
      "learning_rate": 9.416915507968449e-05,
      "loss": 3.1725,
      "step": 7670
    },
    {
      "epoch": 0.46638732009473494,
      "grad_norm": 2.998445749282837,
      "learning_rate": 9.41542445042733e-05,
      "loss": 2.8129,
      "step": 7680
    },
    {
      "epoch": 0.46699459525110826,
      "grad_norm": 3.0965330600738525,
      "learning_rate": 9.41393160721824e-05,
      "loss": 2.7269,
      "step": 7690
    },
    {
      "epoch": 0.46760187040748163,
      "grad_norm": 3.4374258518218994,
      "learning_rate": 9.412436978944912e-05,
      "loss": 2.4391,
      "step": 7700
    },
    {
      "epoch": 0.46820914556385496,
      "grad_norm": 2.6855132579803467,
      "learning_rate": 9.410940566211797e-05,
      "loss": 2.8351,
      "step": 7710
    },
    {
      "epoch": 0.46881642072022833,
      "grad_norm": 2.646430492401123,
      "learning_rate": 9.409442369624065e-05,
      "loss": 2.3042,
      "step": 7720
    },
    {
      "epoch": 0.4694236958766017,
      "grad_norm": 3.019862651824951,
      "learning_rate": 9.40794238978761e-05,
      "loss": 2.5122,
      "step": 7730
    },
    {
      "epoch": 0.47003097103297503,
      "grad_norm": 1.4807242155075073,
      "learning_rate": 9.406440627309053e-05,
      "loss": 2.6303,
      "step": 7740
    },
    {
      "epoch": 0.4706382461893484,
      "grad_norm": 2.430718183517456,
      "learning_rate": 9.404937082795726e-05,
      "loss": 2.4062,
      "step": 7750
    },
    {
      "epoch": 0.47124552134572173,
      "grad_norm": 1.5898776054382324,
      "learning_rate": 9.40343175685569e-05,
      "loss": 2.2317,
      "step": 7760
    },
    {
      "epoch": 0.4718527965020951,
      "grad_norm": 1.5612772703170776,
      "learning_rate": 9.401924650097718e-05,
      "loss": 2.1182,
      "step": 7770
    },
    {
      "epoch": 0.47246007165846843,
      "grad_norm": 1.915563941001892,
      "learning_rate": 9.400415763131312e-05,
      "loss": 2.6029,
      "step": 7780
    },
    {
      "epoch": 0.4730673468148418,
      "grad_norm": 2.616560935974121,
      "learning_rate": 9.398905096566688e-05,
      "loss": 2.5409,
      "step": 7790
    },
    {
      "epoch": 0.4736746219712152,
      "grad_norm": 2.1955933570861816,
      "learning_rate": 9.397392651014785e-05,
      "loss": 2.4947,
      "step": 7800
    },
    {
      "epoch": 0.4742818971275885,
      "grad_norm": 2.8965368270874023,
      "learning_rate": 9.39587842708726e-05,
      "loss": 2.5056,
      "step": 7810
    },
    {
      "epoch": 0.4748891722839619,
      "grad_norm": 3.6725261211395264,
      "learning_rate": 9.394362425396486e-05,
      "loss": 2.9713,
      "step": 7820
    },
    {
      "epoch": 0.4754964474403352,
      "grad_norm": 3.6710331439971924,
      "learning_rate": 9.392844646555563e-05,
      "loss": 2.7342,
      "step": 7830
    },
    {
      "epoch": 0.4761037225967086,
      "grad_norm": 4.00915002822876,
      "learning_rate": 9.391325091178303e-05,
      "loss": 3.0118,
      "step": 7840
    },
    {
      "epoch": 0.4767109977530819,
      "grad_norm": 3.5996651649475098,
      "learning_rate": 9.389803759879237e-05,
      "loss": 2.4888,
      "step": 7850
    },
    {
      "epoch": 0.4773182729094553,
      "grad_norm": 3.6046597957611084,
      "learning_rate": 9.388280653273617e-05,
      "loss": 2.6802,
      "step": 7860
    },
    {
      "epoch": 0.47792554806582865,
      "grad_norm": 2.3442304134368896,
      "learning_rate": 9.386755771977412e-05,
      "loss": 2.6404,
      "step": 7870
    },
    {
      "epoch": 0.478532823222202,
      "grad_norm": 3.0264041423797607,
      "learning_rate": 9.385229116607306e-05,
      "loss": 2.8266,
      "step": 7880
    },
    {
      "epoch": 0.47914009837857535,
      "grad_norm": 2.3976848125457764,
      "learning_rate": 9.383700687780706e-05,
      "loss": 2.6906,
      "step": 7890
    },
    {
      "epoch": 0.4797473735349487,
      "grad_norm": 2.113375425338745,
      "learning_rate": 9.382170486115728e-05,
      "loss": 2.4663,
      "step": 7900
    },
    {
      "epoch": 0.48035464869132205,
      "grad_norm": 2.3001787662506104,
      "learning_rate": 9.380638512231216e-05,
      "loss": 2.673,
      "step": 7910
    },
    {
      "epoch": 0.48096192384769537,
      "grad_norm": 1.926323652267456,
      "learning_rate": 9.379104766746722e-05,
      "loss": 2.6582,
      "step": 7920
    },
    {
      "epoch": 0.48156919900406875,
      "grad_norm": 2.9960079193115234,
      "learning_rate": 9.377569250282517e-05,
      "loss": 2.7805,
      "step": 7930
    },
    {
      "epoch": 0.48217647416044207,
      "grad_norm": 1.6593061685562134,
      "learning_rate": 9.376031963459589e-05,
      "loss": 2.842,
      "step": 7940
    },
    {
      "epoch": 0.48278374931681545,
      "grad_norm": 4.092904090881348,
      "learning_rate": 9.37449290689964e-05,
      "loss": 2.6456,
      "step": 7950
    },
    {
      "epoch": 0.4833910244731888,
      "grad_norm": 2.164579153060913,
      "learning_rate": 9.372952081225088e-05,
      "loss": 3.0422,
      "step": 7960
    },
    {
      "epoch": 0.48399829962956215,
      "grad_norm": 4.314453125,
      "learning_rate": 9.371409487059069e-05,
      "loss": 2.9576,
      "step": 7970
    },
    {
      "epoch": 0.4846055747859355,
      "grad_norm": 3.728839874267578,
      "learning_rate": 9.369865125025435e-05,
      "loss": 2.9114,
      "step": 7980
    },
    {
      "epoch": 0.48521284994230884,
      "grad_norm": 3.7674007415771484,
      "learning_rate": 9.368318995748746e-05,
      "loss": 2.6874,
      "step": 7990
    },
    {
      "epoch": 0.4858201250986822,
      "grad_norm": 3.278688669204712,
      "learning_rate": 9.366771099854283e-05,
      "loss": 2.7435,
      "step": 8000
    },
    {
      "epoch": 0.48642740025505554,
      "grad_norm": 3.8234033584594727,
      "learning_rate": 9.365221437968042e-05,
      "loss": 2.5061,
      "step": 8010
    },
    {
      "epoch": 0.4870346754114289,
      "grad_norm": 2.1587259769439697,
      "learning_rate": 9.363670010716729e-05,
      "loss": 2.738,
      "step": 8020
    },
    {
      "epoch": 0.4876419505678023,
      "grad_norm": 2.7021327018737793,
      "learning_rate": 9.362116818727767e-05,
      "loss": 2.5219,
      "step": 8030
    },
    {
      "epoch": 0.4882492257241756,
      "grad_norm": 3.9081032276153564,
      "learning_rate": 9.360561862629287e-05,
      "loss": 2.7049,
      "step": 8040
    },
    {
      "epoch": 0.488856500880549,
      "grad_norm": 2.50628662109375,
      "learning_rate": 9.359005143050146e-05,
      "loss": 2.6939,
      "step": 8050
    },
    {
      "epoch": 0.4894637760369223,
      "grad_norm": 4.318023681640625,
      "learning_rate": 9.3574466606199e-05,
      "loss": 2.5919,
      "step": 8060
    },
    {
      "epoch": 0.4900710511932957,
      "grad_norm": 2.3998186588287354,
      "learning_rate": 9.355886415968827e-05,
      "loss": 2.8246,
      "step": 8070
    },
    {
      "epoch": 0.490678326349669,
      "grad_norm": 2.9687182903289795,
      "learning_rate": 9.354324409727911e-05,
      "loss": 2.5997,
      "step": 8080
    },
    {
      "epoch": 0.4912856015060424,
      "grad_norm": 1.7515419721603394,
      "learning_rate": 9.352760642528857e-05,
      "loss": 2.4532,
      "step": 8090
    },
    {
      "epoch": 0.49189287666241577,
      "grad_norm": 2.1764914989471436,
      "learning_rate": 9.351195115004076e-05,
      "loss": 2.5743,
      "step": 8100
    },
    {
      "epoch": 0.4925001518187891,
      "grad_norm": 2.48704195022583,
      "learning_rate": 9.349627827786691e-05,
      "loss": 2.4206,
      "step": 8110
    },
    {
      "epoch": 0.49310742697516247,
      "grad_norm": 2.233799934387207,
      "learning_rate": 9.348058781510538e-05,
      "loss": 2.788,
      "step": 8120
    },
    {
      "epoch": 0.4937147021315358,
      "grad_norm": 2.7355732917785645,
      "learning_rate": 9.346487976810166e-05,
      "loss": 2.9354,
      "step": 8130
    },
    {
      "epoch": 0.49432197728790916,
      "grad_norm": 4.997883319854736,
      "learning_rate": 9.344915414320831e-05,
      "loss": 2.7163,
      "step": 8140
    },
    {
      "epoch": 0.4949292524442825,
      "grad_norm": 4.480586051940918,
      "learning_rate": 9.343341094678504e-05,
      "loss": 2.6584,
      "step": 8150
    },
    {
      "epoch": 0.49553652760065586,
      "grad_norm": 4.502597332000732,
      "learning_rate": 9.341765018519865e-05,
      "loss": 2.7053,
      "step": 8160
    },
    {
      "epoch": 0.4961438027570292,
      "grad_norm": 3.7352914810180664,
      "learning_rate": 9.340187186482304e-05,
      "loss": 2.8722,
      "step": 8170
    },
    {
      "epoch": 0.49675107791340256,
      "grad_norm": 3.0358588695526123,
      "learning_rate": 9.338607599203919e-05,
      "loss": 2.9471,
      "step": 8180
    },
    {
      "epoch": 0.49735835306977594,
      "grad_norm": 4.207695960998535,
      "learning_rate": 9.337026257323524e-05,
      "loss": 2.6321,
      "step": 8190
    },
    {
      "epoch": 0.49796562822614926,
      "grad_norm": 2.7590532302856445,
      "learning_rate": 9.33544316148064e-05,
      "loss": 2.9008,
      "step": 8200
    },
    {
      "epoch": 0.49857290338252264,
      "grad_norm": 2.5286595821380615,
      "learning_rate": 9.333858312315489e-05,
      "loss": 2.6424,
      "step": 8210
    },
    {
      "epoch": 0.49918017853889596,
      "grad_norm": 3.27569580078125,
      "learning_rate": 9.332271710469016e-05,
      "loss": 2.9763,
      "step": 8220
    },
    {
      "epoch": 0.49978745369526933,
      "grad_norm": 2.6068081855773926,
      "learning_rate": 9.330683356582866e-05,
      "loss": 2.5292,
      "step": 8230
    },
    {
      "epoch": 0.5003947288516427,
      "grad_norm": 3.4370248317718506,
      "learning_rate": 9.329093251299393e-05,
      "loss": 2.5873,
      "step": 8240
    },
    {
      "epoch": 0.501002004008016,
      "grad_norm": 3.428018093109131,
      "learning_rate": 9.327501395261664e-05,
      "loss": 2.7017,
      "step": 8250
    },
    {
      "epoch": 0.5016092791643894,
      "grad_norm": 2.049440622329712,
      "learning_rate": 9.325907789113448e-05,
      "loss": 2.3502,
      "step": 8260
    },
    {
      "epoch": 0.5022165543207627,
      "grad_norm": 2.0507030487060547,
      "learning_rate": 9.324312433499227e-05,
      "loss": 2.4364,
      "step": 8270
    },
    {
      "epoch": 0.5028238294771361,
      "grad_norm": 1.7440346479415894,
      "learning_rate": 9.322715329064187e-05,
      "loss": 2.1051,
      "step": 8280
    },
    {
      "epoch": 0.5034311046335095,
      "grad_norm": 2.3405163288116455,
      "learning_rate": 9.321116476454222e-05,
      "loss": 2.2712,
      "step": 8290
    },
    {
      "epoch": 0.5040383797898828,
      "grad_norm": 2.085508346557617,
      "learning_rate": 9.319515876315934e-05,
      "loss": 2.5339,
      "step": 8300
    },
    {
      "epoch": 0.5046456549462561,
      "grad_norm": 2.6546781063079834,
      "learning_rate": 9.317913529296631e-05,
      "loss": 2.8437,
      "step": 8310
    },
    {
      "epoch": 0.5052529301026295,
      "grad_norm": 2.4225594997406006,
      "learning_rate": 9.316309436044328e-05,
      "loss": 2.4838,
      "step": 8320
    },
    {
      "epoch": 0.5058602052590029,
      "grad_norm": 2.281322956085205,
      "learning_rate": 9.314703597207747e-05,
      "loss": 2.1517,
      "step": 8330
    },
    {
      "epoch": 0.5064674804153763,
      "grad_norm": 2.2999441623687744,
      "learning_rate": 9.313096013436313e-05,
      "loss": 2.8284,
      "step": 8340
    },
    {
      "epoch": 0.5070747555717495,
      "grad_norm": 3.9494712352752686,
      "learning_rate": 9.311486685380158e-05,
      "loss": 2.6715,
      "step": 8350
    },
    {
      "epoch": 0.5076820307281229,
      "grad_norm": 2.617413282394409,
      "learning_rate": 9.309875613690122e-05,
      "loss": 2.6376,
      "step": 8360
    },
    {
      "epoch": 0.5082893058844963,
      "grad_norm": 3.9300129413604736,
      "learning_rate": 9.308262799017748e-05,
      "loss": 2.828,
      "step": 8370
    },
    {
      "epoch": 0.5088965810408697,
      "grad_norm": 4.20673942565918,
      "learning_rate": 9.306648242015281e-05,
      "loss": 2.3246,
      "step": 8380
    },
    {
      "epoch": 0.5095038561972429,
      "grad_norm": 2.1740453243255615,
      "learning_rate": 9.305031943335679e-05,
      "loss": 2.8561,
      "step": 8390
    },
    {
      "epoch": 0.5101111313536163,
      "grad_norm": 2.886338710784912,
      "learning_rate": 9.303413903632591e-05,
      "loss": 2.8841,
      "step": 8400
    },
    {
      "epoch": 0.5107184065099897,
      "grad_norm": 2.4414420127868652,
      "learning_rate": 9.301794123560384e-05,
      "loss": 2.4731,
      "step": 8410
    },
    {
      "epoch": 0.511325681666363,
      "grad_norm": 2.5578315258026123,
      "learning_rate": 9.300172603774123e-05,
      "loss": 2.8023,
      "step": 8420
    },
    {
      "epoch": 0.5119329568227364,
      "grad_norm": 3.034677505493164,
      "learning_rate": 9.298549344929573e-05,
      "loss": 2.6267,
      "step": 8430
    },
    {
      "epoch": 0.5125402319791097,
      "grad_norm": 2.887077569961548,
      "learning_rate": 9.296924347683208e-05,
      "loss": 2.9105,
      "step": 8440
    },
    {
      "epoch": 0.5131475071354831,
      "grad_norm": 3.859330177307129,
      "learning_rate": 9.295297612692202e-05,
      "loss": 2.8344,
      "step": 8450
    },
    {
      "epoch": 0.5137547822918564,
      "grad_norm": 2.583037853240967,
      "learning_rate": 9.293669140614433e-05,
      "loss": 2.7605,
      "step": 8460
    },
    {
      "epoch": 0.5143620574482298,
      "grad_norm": 3.434964656829834,
      "learning_rate": 9.29203893210848e-05,
      "loss": 2.9616,
      "step": 8470
    },
    {
      "epoch": 0.5149693326046031,
      "grad_norm": 3.3745269775390625,
      "learning_rate": 9.290406987833629e-05,
      "loss": 2.5456,
      "step": 8480
    },
    {
      "epoch": 0.5155766077609765,
      "grad_norm": 2.89732027053833,
      "learning_rate": 9.288773308449859e-05,
      "loss": 2.6553,
      "step": 8490
    },
    {
      "epoch": 0.5161838829173498,
      "grad_norm": 2.3033018112182617,
      "learning_rate": 9.287137894617858e-05,
      "loss": 2.8854,
      "step": 8500
    },
    {
      "epoch": 0.5167911580737232,
      "grad_norm": 2.6318411827087402,
      "learning_rate": 9.285500746999014e-05,
      "loss": 3.0198,
      "step": 8510
    },
    {
      "epoch": 0.5173984332300966,
      "grad_norm": 2.9535653591156006,
      "learning_rate": 9.283861866255416e-05,
      "loss": 2.8865,
      "step": 8520
    },
    {
      "epoch": 0.5180057083864699,
      "grad_norm": 3.6491000652313232,
      "learning_rate": 9.282221253049853e-05,
      "loss": 2.5361,
      "step": 8530
    },
    {
      "epoch": 0.5186129835428432,
      "grad_norm": 2.561466932296753,
      "learning_rate": 9.280578908045814e-05,
      "loss": 2.3282,
      "step": 8540
    },
    {
      "epoch": 0.5192202586992166,
      "grad_norm": 2.05450177192688,
      "learning_rate": 9.278934831907491e-05,
      "loss": 2.6731,
      "step": 8550
    },
    {
      "epoch": 0.51982753385559,
      "grad_norm": 2.401393413543701,
      "learning_rate": 9.277289025299773e-05,
      "loss": 2.4737,
      "step": 8560
    },
    {
      "epoch": 0.5204348090119634,
      "grad_norm": 2.1560776233673096,
      "learning_rate": 9.275641488888253e-05,
      "loss": 2.6976,
      "step": 8570
    },
    {
      "epoch": 0.5210420841683366,
      "grad_norm": 3.038699150085449,
      "learning_rate": 9.273992223339219e-05,
      "loss": 2.6546,
      "step": 8580
    },
    {
      "epoch": 0.52164935932471,
      "grad_norm": 3.4714887142181396,
      "learning_rate": 9.27234122931966e-05,
      "loss": 2.4515,
      "step": 8590
    },
    {
      "epoch": 0.5222566344810834,
      "grad_norm": 3.4321978092193604,
      "learning_rate": 9.270688507497265e-05,
      "loss": 2.254,
      "step": 8600
    },
    {
      "epoch": 0.5228639096374568,
      "grad_norm": 2.754814624786377,
      "learning_rate": 9.26903405854042e-05,
      "loss": 2.7328,
      "step": 8610
    },
    {
      "epoch": 0.52347118479383,
      "grad_norm": 2.6998469829559326,
      "learning_rate": 9.267377883118214e-05,
      "loss": 2.5244,
      "step": 8620
    },
    {
      "epoch": 0.5240784599502034,
      "grad_norm": 3.4286863803863525,
      "learning_rate": 9.265719981900424e-05,
      "loss": 2.6906,
      "step": 8630
    },
    {
      "epoch": 0.5246857351065768,
      "grad_norm": 3.3450706005096436,
      "learning_rate": 9.264060355557539e-05,
      "loss": 2.7307,
      "step": 8640
    },
    {
      "epoch": 0.5252930102629502,
      "grad_norm": 2.756676435470581,
      "learning_rate": 9.262399004760734e-05,
      "loss": 2.4914,
      "step": 8650
    },
    {
      "epoch": 0.5259002854193235,
      "grad_norm": 2.3217933177948,
      "learning_rate": 9.260735930181888e-05,
      "loss": 2.7161,
      "step": 8660
    },
    {
      "epoch": 0.5265075605756968,
      "grad_norm": 2.3757433891296387,
      "learning_rate": 9.259071132493571e-05,
      "loss": 2.5233,
      "step": 8670
    },
    {
      "epoch": 0.5271148357320702,
      "grad_norm": 1.7993361949920654,
      "learning_rate": 9.257404612369059e-05,
      "loss": 2.3803,
      "step": 8680
    },
    {
      "epoch": 0.5277221108884436,
      "grad_norm": 1.3621126413345337,
      "learning_rate": 9.255736370482315e-05,
      "loss": 2.1982,
      "step": 8690
    },
    {
      "epoch": 0.5283293860448169,
      "grad_norm": 2.282522439956665,
      "learning_rate": 9.254066407508005e-05,
      "loss": 2.6286,
      "step": 8700
    },
    {
      "epoch": 0.5289366612011902,
      "grad_norm": 2.3235487937927246,
      "learning_rate": 9.252394724121486e-05,
      "loss": 2.4848,
      "step": 8710
    },
    {
      "epoch": 0.5295439363575636,
      "grad_norm": 2.042772054672241,
      "learning_rate": 9.250721320998819e-05,
      "loss": 2.6098,
      "step": 8720
    },
    {
      "epoch": 0.530151211513937,
      "grad_norm": 2.215090751647949,
      "learning_rate": 9.249046198816749e-05,
      "loss": 2.4768,
      "step": 8730
    },
    {
      "epoch": 0.5307584866703103,
      "grad_norm": 3.3738009929656982,
      "learning_rate": 9.247369358252723e-05,
      "loss": 2.7767,
      "step": 8740
    },
    {
      "epoch": 0.5313657618266837,
      "grad_norm": 3.1761646270751953,
      "learning_rate": 9.245690799984885e-05,
      "loss": 2.7782,
      "step": 8750
    },
    {
      "epoch": 0.531973036983057,
      "grad_norm": 3.5126588344573975,
      "learning_rate": 9.244010524692068e-05,
      "loss": 2.5357,
      "step": 8760
    },
    {
      "epoch": 0.5325803121394304,
      "grad_norm": 4.811452865600586,
      "learning_rate": 9.242328533053804e-05,
      "loss": 2.348,
      "step": 8770
    },
    {
      "epoch": 0.5331875872958037,
      "grad_norm": 2.6814370155334473,
      "learning_rate": 9.240644825750315e-05,
      "loss": 2.856,
      "step": 8780
    },
    {
      "epoch": 0.5337948624521771,
      "grad_norm": 4.358221054077148,
      "learning_rate": 9.23895940346252e-05,
      "loss": 2.63,
      "step": 8790
    },
    {
      "epoch": 0.5344021376085505,
      "grad_norm": 2.156928062438965,
      "learning_rate": 9.237272266872032e-05,
      "loss": 2.7542,
      "step": 8800
    },
    {
      "epoch": 0.5350094127649238,
      "grad_norm": 3.846644401550293,
      "learning_rate": 9.235583416661154e-05,
      "loss": 2.8066,
      "step": 8810
    },
    {
      "epoch": 0.5356166879212971,
      "grad_norm": 2.9687836170196533,
      "learning_rate": 9.233892853512887e-05,
      "loss": 2.78,
      "step": 8820
    },
    {
      "epoch": 0.5362239630776705,
      "grad_norm": 3.8720600605010986,
      "learning_rate": 9.232200578110917e-05,
      "loss": 3.0496,
      "step": 8830
    },
    {
      "epoch": 0.5368312382340439,
      "grad_norm": 2.7625999450683594,
      "learning_rate": 9.23050659113963e-05,
      "loss": 2.9895,
      "step": 8840
    },
    {
      "epoch": 0.5374385133904171,
      "grad_norm": 4.074100971221924,
      "learning_rate": 9.2288108932841e-05,
      "loss": 3.1235,
      "step": 8850
    },
    {
      "epoch": 0.5380457885467905,
      "grad_norm": 2.419468879699707,
      "learning_rate": 9.227113485230095e-05,
      "loss": 2.61,
      "step": 8860
    },
    {
      "epoch": 0.5386530637031639,
      "grad_norm": 2.3268392086029053,
      "learning_rate": 9.225414367664076e-05,
      "loss": 2.5407,
      "step": 8870
    },
    {
      "epoch": 0.5392603388595373,
      "grad_norm": 2.8448939323425293,
      "learning_rate": 9.22371354127319e-05,
      "loss": 2.8844,
      "step": 8880
    },
    {
      "epoch": 0.5398676140159107,
      "grad_norm": 3.642194986343384,
      "learning_rate": 9.222011006745279e-05,
      "loss": 2.6256,
      "step": 8890
    },
    {
      "epoch": 0.5404748891722839,
      "grad_norm": 2.444279432296753,
      "learning_rate": 9.220306764768876e-05,
      "loss": 2.2917,
      "step": 8900
    },
    {
      "epoch": 0.5410821643286573,
      "grad_norm": 2.050840377807617,
      "learning_rate": 9.2186008160332e-05,
      "loss": 2.2483,
      "step": 8910
    },
    {
      "epoch": 0.5416894394850307,
      "grad_norm": 3.3334858417510986,
      "learning_rate": 9.21689316122817e-05,
      "loss": 2.6326,
      "step": 8920
    },
    {
      "epoch": 0.542296714641404,
      "grad_norm": 3.7421674728393555,
      "learning_rate": 9.215183801044385e-05,
      "loss": 2.4356,
      "step": 8930
    },
    {
      "epoch": 0.5429039897977773,
      "grad_norm": 3.843282699584961,
      "learning_rate": 9.213472736173139e-05,
      "loss": 2.2288,
      "step": 8940
    },
    {
      "epoch": 0.5435112649541507,
      "grad_norm": 1.670165777206421,
      "learning_rate": 9.211759967306412e-05,
      "loss": 2.4954,
      "step": 8950
    },
    {
      "epoch": 0.5441185401105241,
      "grad_norm": 1.9334416389465332,
      "learning_rate": 9.210045495136876e-05,
      "loss": 2.7149,
      "step": 8960
    },
    {
      "epoch": 0.5447258152668975,
      "grad_norm": 2.482754707336426,
      "learning_rate": 9.208329320357891e-05,
      "loss": 2.7614,
      "step": 8970
    },
    {
      "epoch": 0.5453330904232708,
      "grad_norm": 2.254801034927368,
      "learning_rate": 9.206611443663506e-05,
      "loss": 2.7673,
      "step": 8980
    },
    {
      "epoch": 0.5459403655796441,
      "grad_norm": 2.2495224475860596,
      "learning_rate": 9.204891865748457e-05,
      "loss": 2.6479,
      "step": 8990
    },
    {
      "epoch": 0.5465476407360175,
      "grad_norm": 1.5805885791778564,
      "learning_rate": 9.203170587308169e-05,
      "loss": 2.5001,
      "step": 9000
    },
    {
      "epoch": 0.5471549158923908,
      "grad_norm": 2.119523525238037,
      "learning_rate": 9.201447609038754e-05,
      "loss": 2.7054,
      "step": 9010
    },
    {
      "epoch": 0.5477621910487642,
      "grad_norm": 2.1764426231384277,
      "learning_rate": 9.19972293163701e-05,
      "loss": 2.5219,
      "step": 9020
    },
    {
      "epoch": 0.5483694662051376,
      "grad_norm": 2.223766803741455,
      "learning_rate": 9.197996555800427e-05,
      "loss": 2.5759,
      "step": 9030
    },
    {
      "epoch": 0.5489767413615109,
      "grad_norm": 2.4237983226776123,
      "learning_rate": 9.196268482227179e-05,
      "loss": 2.8021,
      "step": 9040
    },
    {
      "epoch": 0.5495840165178842,
      "grad_norm": 3.00878643989563,
      "learning_rate": 9.194538711616126e-05,
      "loss": 2.5738,
      "step": 9050
    },
    {
      "epoch": 0.5501912916742576,
      "grad_norm": 3.0554587841033936,
      "learning_rate": 9.192807244666811e-05,
      "loss": 2.8598,
      "step": 9060
    },
    {
      "epoch": 0.550798566830631,
      "grad_norm": 4.312196254730225,
      "learning_rate": 9.191074082079472e-05,
      "loss": 2.8193,
      "step": 9070
    },
    {
      "epoch": 0.5514058419870043,
      "grad_norm": 4.312788963317871,
      "learning_rate": 9.189339224555025e-05,
      "loss": 2.7963,
      "step": 9080
    },
    {
      "epoch": 0.5520131171433776,
      "grad_norm": 4.152872085571289,
      "learning_rate": 9.187602672795074e-05,
      "loss": 2.6666,
      "step": 9090
    },
    {
      "epoch": 0.552620392299751,
      "grad_norm": 2.745159149169922,
      "learning_rate": 9.18586442750191e-05,
      "loss": 2.6546,
      "step": 9100
    },
    {
      "epoch": 0.5532276674561244,
      "grad_norm": 2.926445484161377,
      "learning_rate": 9.184124489378505e-05,
      "loss": 2.2973,
      "step": 9110
    },
    {
      "epoch": 0.5538349426124978,
      "grad_norm": 2.207275867462158,
      "learning_rate": 9.182382859128518e-05,
      "loss": 2.5548,
      "step": 9120
    },
    {
      "epoch": 0.554442217768871,
      "grad_norm": 2.433108329772949,
      "learning_rate": 9.180639537456293e-05,
      "loss": 2.2483,
      "step": 9130
    },
    {
      "epoch": 0.5550494929252444,
      "grad_norm": 1.2097314596176147,
      "learning_rate": 9.178894525066857e-05,
      "loss": 2.503,
      "step": 9140
    },
    {
      "epoch": 0.5556567680816178,
      "grad_norm": 1.049151062965393,
      "learning_rate": 9.177147822665919e-05,
      "loss": 2.3907,
      "step": 9150
    },
    {
      "epoch": 0.5562640432379912,
      "grad_norm": 2.179959297180176,
      "learning_rate": 9.175399430959877e-05,
      "loss": 2.3967,
      "step": 9160
    },
    {
      "epoch": 0.5568713183943644,
      "grad_norm": 2.0745961666107178,
      "learning_rate": 9.173649350655804e-05,
      "loss": 2.6812,
      "step": 9170
    },
    {
      "epoch": 0.5574785935507378,
      "grad_norm": 3.733910083770752,
      "learning_rate": 9.171897582461461e-05,
      "loss": 2.2475,
      "step": 9180
    },
    {
      "epoch": 0.5580858687071112,
      "grad_norm": 2.1554391384124756,
      "learning_rate": 9.170144127085296e-05,
      "loss": 2.4422,
      "step": 9190
    },
    {
      "epoch": 0.5586931438634846,
      "grad_norm": 3.088184356689453,
      "learning_rate": 9.168388985236428e-05,
      "loss": 2.2391,
      "step": 9200
    },
    {
      "epoch": 0.5593004190198579,
      "grad_norm": 2.9224274158477783,
      "learning_rate": 9.166632157624668e-05,
      "loss": 2.7062,
      "step": 9210
    },
    {
      "epoch": 0.5599076941762312,
      "grad_norm": 3.772465467453003,
      "learning_rate": 9.164873644960503e-05,
      "loss": 2.8173,
      "step": 9220
    },
    {
      "epoch": 0.5605149693326046,
      "grad_norm": 3.8275277614593506,
      "learning_rate": 9.163113447955106e-05,
      "loss": 2.4358,
      "step": 9230
    },
    {
      "epoch": 0.561122244488978,
      "grad_norm": 2.31160306930542,
      "learning_rate": 9.161351567320327e-05,
      "loss": 2.5579,
      "step": 9240
    },
    {
      "epoch": 0.5617295196453513,
      "grad_norm": 1.6431629657745361,
      "learning_rate": 9.159588003768698e-05,
      "loss": 2.3111,
      "step": 9250
    },
    {
      "epoch": 0.5623367948017247,
      "grad_norm": 2.2089381217956543,
      "learning_rate": 9.157822758013433e-05,
      "loss": 2.5561,
      "step": 9260
    },
    {
      "epoch": 0.562944069958098,
      "grad_norm": 2.585564613342285,
      "learning_rate": 9.156055830768426e-05,
      "loss": 2.7944,
      "step": 9270
    },
    {
      "epoch": 0.5635513451144714,
      "grad_norm": 3.0225629806518555,
      "learning_rate": 9.15428722274825e-05,
      "loss": 2.9054,
      "step": 9280
    },
    {
      "epoch": 0.5641586202708447,
      "grad_norm": 3.01806378364563,
      "learning_rate": 9.152516934668158e-05,
      "loss": 2.9856,
      "step": 9290
    },
    {
      "epoch": 0.5647658954272181,
      "grad_norm": 1.8974509239196777,
      "learning_rate": 9.150744967244082e-05,
      "loss": 2.7362,
      "step": 9300
    },
    {
      "epoch": 0.5653731705835914,
      "grad_norm": 2.1247942447662354,
      "learning_rate": 9.148971321192637e-05,
      "loss": 2.3613,
      "step": 9310
    },
    {
      "epoch": 0.5659804457399648,
      "grad_norm": 1.9230101108551025,
      "learning_rate": 9.147195997231111e-05,
      "loss": 2.0896,
      "step": 9320
    },
    {
      "epoch": 0.5665877208963381,
      "grad_norm": 2.805325984954834,
      "learning_rate": 9.145418996077473e-05,
      "loss": 2.342,
      "step": 9330
    },
    {
      "epoch": 0.5671949960527115,
      "grad_norm": 2.7146236896514893,
      "learning_rate": 9.143640318450371e-05,
      "loss": 2.6729,
      "step": 9340
    },
    {
      "epoch": 0.5678022712090849,
      "grad_norm": 2.5027623176574707,
      "learning_rate": 9.141859965069132e-05,
      "loss": 2.7811,
      "step": 9350
    },
    {
      "epoch": 0.5684095463654582,
      "grad_norm": 2.7780065536499023,
      "learning_rate": 9.140077936653759e-05,
      "loss": 2.5006,
      "step": 9360
    },
    {
      "epoch": 0.5690168215218315,
      "grad_norm": 2.5715548992156982,
      "learning_rate": 9.13829423392493e-05,
      "loss": 2.5283,
      "step": 9370
    },
    {
      "epoch": 0.5696240966782049,
      "grad_norm": 3.0956239700317383,
      "learning_rate": 9.136508857604005e-05,
      "loss": 2.3328,
      "step": 9380
    },
    {
      "epoch": 0.5702313718345783,
      "grad_norm": 2.059330940246582,
      "learning_rate": 9.134721808413019e-05,
      "loss": 2.1262,
      "step": 9390
    },
    {
      "epoch": 0.5708386469909515,
      "grad_norm": 2.5747153759002686,
      "learning_rate": 9.132933087074682e-05,
      "loss": 2.7765,
      "step": 9400
    },
    {
      "epoch": 0.5714459221473249,
      "grad_norm": 2.900970458984375,
      "learning_rate": 9.131142694312382e-05,
      "loss": 2.6665,
      "step": 9410
    },
    {
      "epoch": 0.5720531973036983,
      "grad_norm": 4.3412580490112305,
      "learning_rate": 9.129350630850182e-05,
      "loss": 2.6946,
      "step": 9420
    },
    {
      "epoch": 0.5726604724600717,
      "grad_norm": 2.9860892295837402,
      "learning_rate": 9.127556897412821e-05,
      "loss": 2.5846,
      "step": 9430
    },
    {
      "epoch": 0.5732677476164451,
      "grad_norm": 1.9620758295059204,
      "learning_rate": 9.125761494725715e-05,
      "loss": 3.0773,
      "step": 9440
    },
    {
      "epoch": 0.5738750227728183,
      "grad_norm": 3.7679266929626465,
      "learning_rate": 9.12396442351495e-05,
      "loss": 2.823,
      "step": 9450
    },
    {
      "epoch": 0.5744822979291917,
      "grad_norm": 5.046072483062744,
      "learning_rate": 9.122165684507293e-05,
      "loss": 2.5968,
      "step": 9460
    },
    {
      "epoch": 0.5750895730855651,
      "grad_norm": 1.9491885900497437,
      "learning_rate": 9.120365278430183e-05,
      "loss": 3.0062,
      "step": 9470
    },
    {
      "epoch": 0.5756968482419385,
      "grad_norm": 4.38067102432251,
      "learning_rate": 9.118563206011731e-05,
      "loss": 2.4087,
      "step": 9480
    },
    {
      "epoch": 0.5763041233983118,
      "grad_norm": 1.4641151428222656,
      "learning_rate": 9.116759467980725e-05,
      "loss": 2.2465,
      "step": 9490
    },
    {
      "epoch": 0.5769113985546851,
      "grad_norm": 2.1689882278442383,
      "learning_rate": 9.114954065066624e-05,
      "loss": 2.6353,
      "step": 9500
    },
    {
      "epoch": 0.5775186737110585,
      "grad_norm": 3.785841703414917,
      "learning_rate": 9.113146997999563e-05,
      "loss": 2.6589,
      "step": 9510
    },
    {
      "epoch": 0.5781259488674318,
      "grad_norm": 2.3648929595947266,
      "learning_rate": 9.111338267510349e-05,
      "loss": 2.7095,
      "step": 9520
    },
    {
      "epoch": 0.5787332240238052,
      "grad_norm": 1.4841372966766357,
      "learning_rate": 9.10952787433046e-05,
      "loss": 2.6039,
      "step": 9530
    },
    {
      "epoch": 0.5793404991801785,
      "grad_norm": 1.9671565294265747,
      "learning_rate": 9.107715819192048e-05,
      "loss": 2.7013,
      "step": 9540
    },
    {
      "epoch": 0.5799477743365519,
      "grad_norm": 3.0984926223754883,
      "learning_rate": 9.105902102827939e-05,
      "loss": 2.4391,
      "step": 9550
    },
    {
      "epoch": 0.5805550494929252,
      "grad_norm": 3.298555850982666,
      "learning_rate": 9.104086725971628e-05,
      "loss": 2.5517,
      "step": 9560
    },
    {
      "epoch": 0.5811623246492986,
      "grad_norm": 4.183902740478516,
      "learning_rate": 9.102269689357281e-05,
      "loss": 2.783,
      "step": 9570
    },
    {
      "epoch": 0.581769599805672,
      "grad_norm": 3.7079145908355713,
      "learning_rate": 9.100450993719735e-05,
      "loss": 3.5075,
      "step": 9580
    },
    {
      "epoch": 0.5823768749620453,
      "grad_norm": 3.7830655574798584,
      "learning_rate": 9.098630639794506e-05,
      "loss": 2.6811,
      "step": 9590
    },
    {
      "epoch": 0.5829841501184186,
      "grad_norm": 4.058245658874512,
      "learning_rate": 9.096808628317767e-05,
      "loss": 2.7707,
      "step": 9600
    },
    {
      "epoch": 0.583591425274792,
      "grad_norm": 3.4169678688049316,
      "learning_rate": 9.094984960026372e-05,
      "loss": 2.9494,
      "step": 9610
    },
    {
      "epoch": 0.5841987004311654,
      "grad_norm": 3.9686453342437744,
      "learning_rate": 9.093159635657839e-05,
      "loss": 2.9488,
      "step": 9620
    },
    {
      "epoch": 0.5848059755875387,
      "grad_norm": 3.646360158920288,
      "learning_rate": 9.091332655950362e-05,
      "loss": 3.0149,
      "step": 9630
    },
    {
      "epoch": 0.585413250743912,
      "grad_norm": 2.396937847137451,
      "learning_rate": 9.089504021642798e-05,
      "loss": 2.8229,
      "step": 9640
    },
    {
      "epoch": 0.5860205259002854,
      "grad_norm": 2.203836679458618,
      "learning_rate": 9.087673733474678e-05,
      "loss": 2.761,
      "step": 9650
    },
    {
      "epoch": 0.5866278010566588,
      "grad_norm": 2.8856050968170166,
      "learning_rate": 9.085841792186196e-05,
      "loss": 2.6408,
      "step": 9660
    },
    {
      "epoch": 0.5872350762130322,
      "grad_norm": 1.8261661529541016,
      "learning_rate": 9.084008198518222e-05,
      "loss": 2.5403,
      "step": 9670
    },
    {
      "epoch": 0.5878423513694054,
      "grad_norm": 2.132263660430908,
      "learning_rate": 9.08217295321229e-05,
      "loss": 2.4362,
      "step": 9680
    },
    {
      "epoch": 0.5884496265257788,
      "grad_norm": 2.0998337268829346,
      "learning_rate": 9.080336057010599e-05,
      "loss": 2.7841,
      "step": 9690
    },
    {
      "epoch": 0.5890569016821522,
      "grad_norm": 4.562199592590332,
      "learning_rate": 9.078497510656024e-05,
      "loss": 2.624,
      "step": 9700
    },
    {
      "epoch": 0.5896641768385256,
      "grad_norm": 2.3481674194335938,
      "learning_rate": 9.076657314892096e-05,
      "loss": 3.0731,
      "step": 9710
    },
    {
      "epoch": 0.5902714519948989,
      "grad_norm": 2.6896145343780518,
      "learning_rate": 9.074815470463027e-05,
      "loss": 2.5867,
      "step": 9720
    },
    {
      "epoch": 0.5908787271512722,
      "grad_norm": 2.883991241455078,
      "learning_rate": 9.072971978113684e-05,
      "loss": 2.7674,
      "step": 9730
    },
    {
      "epoch": 0.5914860023076456,
      "grad_norm": 2.470949649810791,
      "learning_rate": 9.071126838589603e-05,
      "loss": 2.7026,
      "step": 9740
    },
    {
      "epoch": 0.592093277464019,
      "grad_norm": 4.5680365562438965,
      "learning_rate": 9.069280052636993e-05,
      "loss": 2.8056,
      "step": 9750
    },
    {
      "epoch": 0.5927005526203923,
      "grad_norm": 2.3898797035217285,
      "learning_rate": 9.067431621002719e-05,
      "loss": 2.7765,
      "step": 9760
    },
    {
      "epoch": 0.5933078277767656,
      "grad_norm": 2.4615769386291504,
      "learning_rate": 9.065581544434319e-05,
      "loss": 2.5106,
      "step": 9770
    },
    {
      "epoch": 0.593915102933139,
      "grad_norm": 2.837794065475464,
      "learning_rate": 9.063729823679991e-05,
      "loss": 2.3273,
      "step": 9780
    },
    {
      "epoch": 0.5945223780895124,
      "grad_norm": 2.801990032196045,
      "learning_rate": 9.061876459488603e-05,
      "loss": 2.3706,
      "step": 9790
    },
    {
      "epoch": 0.5951296532458857,
      "grad_norm": 2.73704195022583,
      "learning_rate": 9.060021452609684e-05,
      "loss": 2.902,
      "step": 9800
    },
    {
      "epoch": 0.5957369284022591,
      "grad_norm": 2.635852813720703,
      "learning_rate": 9.058164803793426e-05,
      "loss": 2.3976,
      "step": 9810
    },
    {
      "epoch": 0.5963442035586324,
      "grad_norm": 2.1522393226623535,
      "learning_rate": 9.056306513790692e-05,
      "loss": 2.1641,
      "step": 9820
    },
    {
      "epoch": 0.5969514787150058,
      "grad_norm": 2.0981948375701904,
      "learning_rate": 9.054446583352999e-05,
      "loss": 2.4576,
      "step": 9830
    },
    {
      "epoch": 0.5975587538713791,
      "grad_norm": 1.9624325037002563,
      "learning_rate": 9.052585013232535e-05,
      "loss": 2.5108,
      "step": 9840
    },
    {
      "epoch": 0.5981660290277525,
      "grad_norm": 2.8008410930633545,
      "learning_rate": 9.05072180418215e-05,
      "loss": 2.6024,
      "step": 9850
    },
    {
      "epoch": 0.5987733041841258,
      "grad_norm": 3.3611714839935303,
      "learning_rate": 9.048856956955354e-05,
      "loss": 2.9277,
      "step": 9860
    },
    {
      "epoch": 0.5993805793404992,
      "grad_norm": 2.315218210220337,
      "learning_rate": 9.046990472306321e-05,
      "loss": 2.5729,
      "step": 9870
    },
    {
      "epoch": 0.5999878544968725,
      "grad_norm": 2.476536989212036,
      "learning_rate": 9.045122350989885e-05,
      "loss": 2.6438,
      "step": 9880
    },
    {
      "epoch": 0.6005951296532459,
      "grad_norm": 2.064065456390381,
      "learning_rate": 9.043252593761548e-05,
      "loss": 2.8244,
      "step": 9890
    },
    {
      "epoch": 0.6012024048096193,
      "grad_norm": 3.2125096321105957,
      "learning_rate": 9.041381201377468e-05,
      "loss": 2.7139,
      "step": 9900
    },
    {
      "epoch": 0.6018096799659925,
      "grad_norm": 3.5533645153045654,
      "learning_rate": 9.039508174594464e-05,
      "loss": 2.8335,
      "step": 9910
    },
    {
      "epoch": 0.6024169551223659,
      "grad_norm": 3.4883644580841064,
      "learning_rate": 9.03763351417002e-05,
      "loss": 2.7731,
      "step": 9920
    },
    {
      "epoch": 0.6030242302787393,
      "grad_norm": 2.076507091522217,
      "learning_rate": 9.035757220862277e-05,
      "loss": 2.4063,
      "step": 9930
    },
    {
      "epoch": 0.6036315054351127,
      "grad_norm": 1.9273531436920166,
      "learning_rate": 9.033879295430041e-05,
      "loss": 2.7969,
      "step": 9940
    },
    {
      "epoch": 0.6042387805914861,
      "grad_norm": 1.4880943298339844,
      "learning_rate": 9.031999738632772e-05,
      "loss": 2.6493,
      "step": 9950
    },
    {
      "epoch": 0.6048460557478593,
      "grad_norm": 2.436595916748047,
      "learning_rate": 9.030118551230593e-05,
      "loss": 2.5708,
      "step": 9960
    },
    {
      "epoch": 0.6054533309042327,
      "grad_norm": 1.9618167877197266,
      "learning_rate": 9.028235733984286e-05,
      "loss": 2.6785,
      "step": 9970
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 3.6942684650421143,
      "learning_rate": 9.026351287655294e-05,
      "loss": 2.5683,
      "step": 9980
    },
    {
      "epoch": 0.6066678812169795,
      "grad_norm": 2.56027889251709,
      "learning_rate": 9.024465213005715e-05,
      "loss": 2.4463,
      "step": 9990
    },
    {
      "epoch": 0.6072751563733527,
      "grad_norm": 2.117037057876587,
      "learning_rate": 9.022577510798308e-05,
      "loss": 2.6252,
      "step": 10000
    },
    {
      "epoch": 0.6072751563733527,
      "eval_loss": 4.912322044372559,
      "eval_runtime": 2112.1823,
      "eval_samples_per_second": 7.796,
      "eval_steps_per_second": 1.949,
      "step": 10000
    },
    {
      "epoch": 0.6078824315297261,
      "grad_norm": 2.2089006900787354,
      "learning_rate": 9.020688181796493e-05,
      "loss": 4.4058,
      "step": 10010
    },
    {
      "epoch": 0.6084897066860995,
      "grad_norm": 4.058055877685547,
      "learning_rate": 9.01879722676434e-05,
      "loss": 3.3541,
      "step": 10020
    },
    {
      "epoch": 0.6090969818424729,
      "grad_norm": 1.8588566780090332,
      "learning_rate": 9.016904646466584e-05,
      "loss": 2.3142,
      "step": 10030
    },
    {
      "epoch": 0.6097042569988462,
      "grad_norm": 2.169917106628418,
      "learning_rate": 9.015010441668615e-05,
      "loss": 2.3346,
      "step": 10040
    },
    {
      "epoch": 0.6103115321552195,
      "grad_norm": 2.1887362003326416,
      "learning_rate": 9.013114613136478e-05,
      "loss": 2.6831,
      "step": 10050
    },
    {
      "epoch": 0.6109188073115929,
      "grad_norm": 2.590998888015747,
      "learning_rate": 9.011217161636877e-05,
      "loss": 2.8704,
      "step": 10060
    },
    {
      "epoch": 0.6115260824679662,
      "grad_norm": 1.7748466730117798,
      "learning_rate": 9.009318087937171e-05,
      "loss": 2.6795,
      "step": 10070
    },
    {
      "epoch": 0.6121333576243396,
      "grad_norm": 1.7711999416351318,
      "learning_rate": 9.007417392805377e-05,
      "loss": 2.3384,
      "step": 10080
    },
    {
      "epoch": 0.6127406327807129,
      "grad_norm": 2.4893064498901367,
      "learning_rate": 9.005515077010166e-05,
      "loss": 2.3035,
      "step": 10090
    },
    {
      "epoch": 0.6133479079370863,
      "grad_norm": 1.6896036863327026,
      "learning_rate": 9.003611141320863e-05,
      "loss": 2.7661,
      "step": 10100
    },
    {
      "epoch": 0.6139551830934596,
      "grad_norm": 2.0185558795928955,
      "learning_rate": 9.001705586507453e-05,
      "loss": 2.454,
      "step": 10110
    },
    {
      "epoch": 0.614562458249833,
      "grad_norm": 2.332995653152466,
      "learning_rate": 8.99979841334057e-05,
      "loss": 2.2403,
      "step": 10120
    },
    {
      "epoch": 0.6151697334062064,
      "grad_norm": 2.084296226501465,
      "learning_rate": 8.997889622591507e-05,
      "loss": 2.2552,
      "step": 10130
    },
    {
      "epoch": 0.6157770085625797,
      "grad_norm": 2.278883218765259,
      "learning_rate": 8.995979215032207e-05,
      "loss": 2.4915,
      "step": 10140
    },
    {
      "epoch": 0.616384283718953,
      "grad_norm": 2.3067238330841064,
      "learning_rate": 8.994067191435274e-05,
      "loss": 2.8312,
      "step": 10150
    },
    {
      "epoch": 0.6169915588753264,
      "grad_norm": 2.038983106613159,
      "learning_rate": 8.992153552573954e-05,
      "loss": 2.7042,
      "step": 10160
    },
    {
      "epoch": 0.6175988340316998,
      "grad_norm": 3.5154874324798584,
      "learning_rate": 8.990238299222159e-05,
      "loss": 3.0351,
      "step": 10170
    },
    {
      "epoch": 0.6182061091880731,
      "grad_norm": 5.487258434295654,
      "learning_rate": 8.988321432154445e-05,
      "loss": 2.82,
      "step": 10180
    },
    {
      "epoch": 0.6188133843444464,
      "grad_norm": 3.8198728561401367,
      "learning_rate": 8.986402952146025e-05,
      "loss": 2.7594,
      "step": 10190
    },
    {
      "epoch": 0.6194206595008198,
      "grad_norm": 1.9797871112823486,
      "learning_rate": 8.98448285997276e-05,
      "loss": 2.3301,
      "step": 10200
    },
    {
      "epoch": 0.6200279346571932,
      "grad_norm": 2.834728717803955,
      "learning_rate": 8.982561156411172e-05,
      "loss": 2.2869,
      "step": 10210
    },
    {
      "epoch": 0.6206352098135666,
      "grad_norm": 3.892502784729004,
      "learning_rate": 8.980637842238421e-05,
      "loss": 2.5827,
      "step": 10220
    },
    {
      "epoch": 0.6212424849699398,
      "grad_norm": 3.398155450820923,
      "learning_rate": 8.97871291823233e-05,
      "loss": 2.7535,
      "step": 10230
    },
    {
      "epoch": 0.6218497601263132,
      "grad_norm": 4.210861682891846,
      "learning_rate": 8.976786385171368e-05,
      "loss": 2.7788,
      "step": 10240
    },
    {
      "epoch": 0.6224570352826866,
      "grad_norm": 3.2156825065612793,
      "learning_rate": 8.974858243834656e-05,
      "loss": 2.9751,
      "step": 10250
    },
    {
      "epoch": 0.62306431043906,
      "grad_norm": 2.745006799697876,
      "learning_rate": 8.972928495001967e-05,
      "loss": 2.9075,
      "step": 10260
    },
    {
      "epoch": 0.6236715855954333,
      "grad_norm": 2.0557539463043213,
      "learning_rate": 8.970997139453719e-05,
      "loss": 2.3257,
      "step": 10270
    },
    {
      "epoch": 0.6242788607518066,
      "grad_norm": 2.6282713413238525,
      "learning_rate": 8.969064177970982e-05,
      "loss": 2.7527,
      "step": 10280
    },
    {
      "epoch": 0.62488613590818,
      "grad_norm": 2.0287322998046875,
      "learning_rate": 8.967129611335481e-05,
      "loss": 2.6762,
      "step": 10290
    },
    {
      "epoch": 0.6254934110645534,
      "grad_norm": 2.2328684329986572,
      "learning_rate": 8.965193440329583e-05,
      "loss": 2.6319,
      "step": 10300
    },
    {
      "epoch": 0.6261006862209267,
      "grad_norm": 2.5318331718444824,
      "learning_rate": 8.963255665736305e-05,
      "loss": 2.8311,
      "step": 10310
    },
    {
      "epoch": 0.6267079613773,
      "grad_norm": 7.200995922088623,
      "learning_rate": 8.961316288339316e-05,
      "loss": 2.9531,
      "step": 10320
    },
    {
      "epoch": 0.6273152365336734,
      "grad_norm": 2.681396484375,
      "learning_rate": 8.959375308922932e-05,
      "loss": 2.5783,
      "step": 10330
    },
    {
      "epoch": 0.6279225116900468,
      "grad_norm": 3.849821090698242,
      "learning_rate": 8.957432728272113e-05,
      "loss": 2.7449,
      "step": 10340
    },
    {
      "epoch": 0.6285297868464201,
      "grad_norm": 6.071225643157959,
      "learning_rate": 8.95548854717247e-05,
      "loss": 2.89,
      "step": 10350
    },
    {
      "epoch": 0.6291370620027935,
      "grad_norm": 2.530510425567627,
      "learning_rate": 8.953542766410263e-05,
      "loss": 2.8881,
      "step": 10360
    },
    {
      "epoch": 0.6297443371591668,
      "grad_norm": 3.32671856880188,
      "learning_rate": 8.951595386772397e-05,
      "loss": 2.6087,
      "step": 10370
    },
    {
      "epoch": 0.6303516123155402,
      "grad_norm": 2.258554458618164,
      "learning_rate": 8.94964640904642e-05,
      "loss": 2.3352,
      "step": 10380
    },
    {
      "epoch": 0.6309588874719135,
      "grad_norm": 2.110459089279175,
      "learning_rate": 8.947695834020532e-05,
      "loss": 2.2227,
      "step": 10390
    },
    {
      "epoch": 0.6315661626282869,
      "grad_norm": 1.5866705179214478,
      "learning_rate": 8.945743662483577e-05,
      "loss": 2.0304,
      "step": 10400
    },
    {
      "epoch": 0.6321734377846602,
      "grad_norm": 2.2812256813049316,
      "learning_rate": 8.943789895225043e-05,
      "loss": 2.3083,
      "step": 10410
    },
    {
      "epoch": 0.6327807129410336,
      "grad_norm": 2.852524757385254,
      "learning_rate": 8.941834533035064e-05,
      "loss": 2.354,
      "step": 10420
    },
    {
      "epoch": 0.6333879880974069,
      "grad_norm": 1.7107365131378174,
      "learning_rate": 8.93987757670442e-05,
      "loss": 2.5489,
      "step": 10430
    },
    {
      "epoch": 0.6339952632537803,
      "grad_norm": 2.428335666656494,
      "learning_rate": 8.937919027024539e-05,
      "loss": 2.337,
      "step": 10440
    },
    {
      "epoch": 0.6346025384101537,
      "grad_norm": 3.005936861038208,
      "learning_rate": 8.935958884787485e-05,
      "loss": 2.8562,
      "step": 10450
    },
    {
      "epoch": 0.635209813566527,
      "grad_norm": 2.3938517570495605,
      "learning_rate": 8.933997150785971e-05,
      "loss": 2.7627,
      "step": 10460
    },
    {
      "epoch": 0.6358170887229003,
      "grad_norm": 2.9320836067199707,
      "learning_rate": 8.932033825813356e-05,
      "loss": 2.7771,
      "step": 10470
    },
    {
      "epoch": 0.6364243638792737,
      "grad_norm": 6.365469455718994,
      "learning_rate": 8.930068910663638e-05,
      "loss": 2.9138,
      "step": 10480
    },
    {
      "epoch": 0.6370316390356471,
      "grad_norm": 4.689997673034668,
      "learning_rate": 8.928102406131463e-05,
      "loss": 2.5295,
      "step": 10490
    },
    {
      "epoch": 0.6376389141920205,
      "grad_norm": 3.5468082427978516,
      "learning_rate": 8.926134313012112e-05,
      "loss": 2.5422,
      "step": 10500
    },
    {
      "epoch": 0.6382461893483937,
      "grad_norm": 2.767849922180176,
      "learning_rate": 8.924164632101518e-05,
      "loss": 3.1936,
      "step": 10510
    },
    {
      "epoch": 0.6388534645047671,
      "grad_norm": 4.595058917999268,
      "learning_rate": 8.922193364196245e-05,
      "loss": 2.9073,
      "step": 10520
    },
    {
      "epoch": 0.6394607396611405,
      "grad_norm": 3.6268558502197266,
      "learning_rate": 8.920220510093511e-05,
      "loss": 2.4697,
      "step": 10530
    },
    {
      "epoch": 0.6400680148175139,
      "grad_norm": 2.212949752807617,
      "learning_rate": 8.91824607059117e-05,
      "loss": 2.3658,
      "step": 10540
    },
    {
      "epoch": 0.6406752899738871,
      "grad_norm": 2.22955060005188,
      "learning_rate": 8.916270046487711e-05,
      "loss": 2.3404,
      "step": 10550
    },
    {
      "epoch": 0.6412825651302605,
      "grad_norm": 2.5555500984191895,
      "learning_rate": 8.914292438582275e-05,
      "loss": 2.9888,
      "step": 10560
    },
    {
      "epoch": 0.6418898402866339,
      "grad_norm": 2.1594462394714355,
      "learning_rate": 8.912313247674636e-05,
      "loss": 2.4328,
      "step": 10570
    },
    {
      "epoch": 0.6424971154430072,
      "grad_norm": 2.984328269958496,
      "learning_rate": 8.91033247456521e-05,
      "loss": 2.6422,
      "step": 10580
    },
    {
      "epoch": 0.6431043905993806,
      "grad_norm": 2.15474796295166,
      "learning_rate": 8.908350120055056e-05,
      "loss": 2.5272,
      "step": 10590
    },
    {
      "epoch": 0.6437116657557539,
      "grad_norm": 4.261043548583984,
      "learning_rate": 8.906366184945865e-05,
      "loss": 2.8698,
      "step": 10600
    },
    {
      "epoch": 0.6443189409121273,
      "grad_norm": 9.493303298950195,
      "learning_rate": 8.904380670039975e-05,
      "loss": 2.7115,
      "step": 10610
    },
    {
      "epoch": 0.6449262160685006,
      "grad_norm": 4.349341869354248,
      "learning_rate": 8.90239357614036e-05,
      "loss": 2.7939,
      "step": 10620
    },
    {
      "epoch": 0.645533491224874,
      "grad_norm": 5.980238914489746,
      "learning_rate": 8.900404904050632e-05,
      "loss": 2.7271,
      "step": 10630
    },
    {
      "epoch": 0.6461407663812473,
      "grad_norm": 3.214116096496582,
      "learning_rate": 8.89841465457504e-05,
      "loss": 2.829,
      "step": 10640
    },
    {
      "epoch": 0.6467480415376207,
      "grad_norm": 4.741450786590576,
      "learning_rate": 8.896422828518475e-05,
      "loss": 2.9343,
      "step": 10650
    },
    {
      "epoch": 0.647355316693994,
      "grad_norm": 7.71877908706665,
      "learning_rate": 8.894429426686461e-05,
      "loss": 2.6869,
      "step": 10660
    },
    {
      "epoch": 0.6479625918503674,
      "grad_norm": 3.979635238647461,
      "learning_rate": 8.892434449885164e-05,
      "loss": 3.1936,
      "step": 10670
    },
    {
      "epoch": 0.6485698670067408,
      "grad_norm": 4.815994739532471,
      "learning_rate": 8.890437898921382e-05,
      "loss": 3.0488,
      "step": 10680
    },
    {
      "epoch": 0.6491771421631141,
      "grad_norm": 2.4551713466644287,
      "learning_rate": 8.888439774602554e-05,
      "loss": 2.727,
      "step": 10690
    },
    {
      "epoch": 0.6497844173194874,
      "grad_norm": 2.9058430194854736,
      "learning_rate": 8.886440077736752e-05,
      "loss": 2.8861,
      "step": 10700
    },
    {
      "epoch": 0.6503916924758608,
      "grad_norm": 2.2738003730773926,
      "learning_rate": 8.884438809132685e-05,
      "loss": 2.7299,
      "step": 10710
    },
    {
      "epoch": 0.6509989676322342,
      "grad_norm": 3.873317241668701,
      "learning_rate": 8.882435969599699e-05,
      "loss": 2.548,
      "step": 10720
    },
    {
      "epoch": 0.6516062427886076,
      "grad_norm": 4.0629706382751465,
      "learning_rate": 8.880431559947774e-05,
      "loss": 2.7922,
      "step": 10730
    },
    {
      "epoch": 0.6522135179449808,
      "grad_norm": 2.272305727005005,
      "learning_rate": 8.878425580987524e-05,
      "loss": 2.3024,
      "step": 10740
    },
    {
      "epoch": 0.6528207931013542,
      "grad_norm": 2.4305732250213623,
      "learning_rate": 8.876418033530201e-05,
      "loss": 2.3947,
      "step": 10750
    },
    {
      "epoch": 0.6534280682577276,
      "grad_norm": 1.3792331218719482,
      "learning_rate": 8.874408918387685e-05,
      "loss": 2.1574,
      "step": 10760
    },
    {
      "epoch": 0.654035343414101,
      "grad_norm": 1.8467713594436646,
      "learning_rate": 8.872398236372499e-05,
      "loss": 2.1583,
      "step": 10770
    },
    {
      "epoch": 0.6546426185704742,
      "grad_norm": 1.4462515115737915,
      "learning_rate": 8.870385988297794e-05,
      "loss": 2.9172,
      "step": 10780
    },
    {
      "epoch": 0.6552498937268476,
      "grad_norm": 2.642557382583618,
      "learning_rate": 8.868372174977352e-05,
      "loss": 3.1461,
      "step": 10790
    },
    {
      "epoch": 0.655857168883221,
      "grad_norm": 2.5145263671875,
      "learning_rate": 8.866356797225594e-05,
      "loss": 2.4733,
      "step": 10800
    },
    {
      "epoch": 0.6564644440395944,
      "grad_norm": 3.5415046215057373,
      "learning_rate": 8.86433985585757e-05,
      "loss": 2.3729,
      "step": 10810
    },
    {
      "epoch": 0.6570717191959677,
      "grad_norm": 2.475087881088257,
      "learning_rate": 8.862321351688963e-05,
      "loss": 2.8702,
      "step": 10820
    },
    {
      "epoch": 0.657678994352341,
      "grad_norm": 5.557733058929443,
      "learning_rate": 8.86030128553609e-05,
      "loss": 2.6325,
      "step": 10830
    },
    {
      "epoch": 0.6582862695087144,
      "grad_norm": 2.7273004055023193,
      "learning_rate": 8.858279658215895e-05,
      "loss": 2.4851,
      "step": 10840
    },
    {
      "epoch": 0.6588935446650878,
      "grad_norm": 4.250517845153809,
      "learning_rate": 8.856256470545957e-05,
      "loss": 2.5072,
      "step": 10850
    },
    {
      "epoch": 0.6595008198214611,
      "grad_norm": 2.956315040588379,
      "learning_rate": 8.854231723344487e-05,
      "loss": 2.804,
      "step": 10860
    },
    {
      "epoch": 0.6601080949778344,
      "grad_norm": 3.007634401321411,
      "learning_rate": 8.852205417430323e-05,
      "loss": 2.2522,
      "step": 10870
    },
    {
      "epoch": 0.6607153701342078,
      "grad_norm": 4.037303924560547,
      "learning_rate": 8.850177553622938e-05,
      "loss": 2.6744,
      "step": 10880
    },
    {
      "epoch": 0.6613226452905812,
      "grad_norm": 2.677964687347412,
      "learning_rate": 8.848148132742431e-05,
      "loss": 2.7492,
      "step": 10890
    },
    {
      "epoch": 0.6619299204469545,
      "grad_norm": 1.9471018314361572,
      "learning_rate": 8.846117155609532e-05,
      "loss": 2.7286,
      "step": 10900
    },
    {
      "epoch": 0.6625371956033279,
      "grad_norm": 2.0866291522979736,
      "learning_rate": 8.844084623045599e-05,
      "loss": 2.6185,
      "step": 10910
    },
    {
      "epoch": 0.6631444707597012,
      "grad_norm": 2.773078203201294,
      "learning_rate": 8.842050535872623e-05,
      "loss": 2.7191,
      "step": 10920
    },
    {
      "epoch": 0.6637517459160746,
      "grad_norm": 3.9292306900024414,
      "learning_rate": 8.84001489491322e-05,
      "loss": 2.4712,
      "step": 10930
    },
    {
      "epoch": 0.6643590210724479,
      "grad_norm": 2.839804172515869,
      "learning_rate": 8.837977700990636e-05,
      "loss": 2.626,
      "step": 10940
    },
    {
      "epoch": 0.6649662962288213,
      "grad_norm": 2.9352707862854004,
      "learning_rate": 8.835938954928744e-05,
      "loss": 2.7944,
      "step": 10950
    },
    {
      "epoch": 0.6655735713851947,
      "grad_norm": 2.5160224437713623,
      "learning_rate": 8.833898657552046e-05,
      "loss": 2.7949,
      "step": 10960
    },
    {
      "epoch": 0.666180846541568,
      "grad_norm": 4.403510570526123,
      "learning_rate": 8.831856809685672e-05,
      "loss": 2.9798,
      "step": 10970
    },
    {
      "epoch": 0.6667881216979413,
      "grad_norm": 3.61562180519104,
      "learning_rate": 8.829813412155375e-05,
      "loss": 2.707,
      "step": 10980
    },
    {
      "epoch": 0.6673953968543147,
      "grad_norm": 2.4952192306518555,
      "learning_rate": 8.82776846578754e-05,
      "loss": 2.5957,
      "step": 10990
    },
    {
      "epoch": 0.6680026720106881,
      "grad_norm": 1.7647647857666016,
      "learning_rate": 8.825721971409173e-05,
      "loss": 2.5981,
      "step": 11000
    },
    {
      "epoch": 0.6686099471670613,
      "grad_norm": 4.116425514221191,
      "learning_rate": 8.823673929847914e-05,
      "loss": 2.5215,
      "step": 11010
    },
    {
      "epoch": 0.6692172223234347,
      "grad_norm": 4.874483108520508,
      "learning_rate": 8.821624341932018e-05,
      "loss": 2.4546,
      "step": 11020
    },
    {
      "epoch": 0.6698244974798081,
      "grad_norm": 3.7582414150238037,
      "learning_rate": 8.819573208490373e-05,
      "loss": 2.6514,
      "step": 11030
    },
    {
      "epoch": 0.6704317726361815,
      "grad_norm": 2.530439615249634,
      "learning_rate": 8.817520530352491e-05,
      "loss": 2.5559,
      "step": 11040
    },
    {
      "epoch": 0.6710390477925549,
      "grad_norm": 3.6360392570495605,
      "learning_rate": 8.815466308348508e-05,
      "loss": 2.4417,
      "step": 11050
    },
    {
      "epoch": 0.6716463229489281,
      "grad_norm": 3.265213966369629,
      "learning_rate": 8.813410543309184e-05,
      "loss": 2.5242,
      "step": 11060
    },
    {
      "epoch": 0.6722535981053015,
      "grad_norm": 2.3759727478027344,
      "learning_rate": 8.8113532360659e-05,
      "loss": 2.3538,
      "step": 11070
    },
    {
      "epoch": 0.6728608732616749,
      "grad_norm": 4.0193867683410645,
      "learning_rate": 8.809294387450668e-05,
      "loss": 2.4257,
      "step": 11080
    },
    {
      "epoch": 0.6734681484180483,
      "grad_norm": 2.947474718093872,
      "learning_rate": 8.807233998296117e-05,
      "loss": 2.6569,
      "step": 11090
    },
    {
      "epoch": 0.6740754235744215,
      "grad_norm": 4.456961154937744,
      "learning_rate": 8.805172069435501e-05,
      "loss": 2.8226,
      "step": 11100
    },
    {
      "epoch": 0.6746826987307949,
      "grad_norm": 3.5374889373779297,
      "learning_rate": 8.803108601702698e-05,
      "loss": 2.3563,
      "step": 11110
    },
    {
      "epoch": 0.6752899738871683,
      "grad_norm": 2.3209660053253174,
      "learning_rate": 8.801043595932206e-05,
      "loss": 2.487,
      "step": 11120
    },
    {
      "epoch": 0.6758972490435416,
      "grad_norm": 3.065013885498047,
      "learning_rate": 8.798977052959148e-05,
      "loss": 2.7292,
      "step": 11130
    },
    {
      "epoch": 0.676504524199915,
      "grad_norm": 2.2940053939819336,
      "learning_rate": 8.796908973619265e-05,
      "loss": 2.5019,
      "step": 11140
    },
    {
      "epoch": 0.6771117993562883,
      "grad_norm": 2.1252551078796387,
      "learning_rate": 8.794839358748923e-05,
      "loss": 2.4758,
      "step": 11150
    },
    {
      "epoch": 0.6777190745126617,
      "grad_norm": 3.1050667762756348,
      "learning_rate": 8.792768209185105e-05,
      "loss": 2.5286,
      "step": 11160
    },
    {
      "epoch": 0.678326349669035,
      "grad_norm": 2.2792067527770996,
      "learning_rate": 8.790695525765418e-05,
      "loss": 2.2327,
      "step": 11170
    },
    {
      "epoch": 0.6789336248254084,
      "grad_norm": 1.800503134727478,
      "learning_rate": 8.788621309328087e-05,
      "loss": 2.3787,
      "step": 11180
    },
    {
      "epoch": 0.6795408999817818,
      "grad_norm": 3.5722227096557617,
      "learning_rate": 8.786545560711962e-05,
      "loss": 2.6486,
      "step": 11190
    },
    {
      "epoch": 0.6801481751381551,
      "grad_norm": 2.6020336151123047,
      "learning_rate": 8.784468280756505e-05,
      "loss": 2.9084,
      "step": 11200
    },
    {
      "epoch": 0.6807554502945284,
      "grad_norm": 3.054267168045044,
      "learning_rate": 8.782389470301803e-05,
      "loss": 2.9536,
      "step": 11210
    },
    {
      "epoch": 0.6813627254509018,
      "grad_norm": 2.9091684818267822,
      "learning_rate": 8.780309130188558e-05,
      "loss": 2.5672,
      "step": 11220
    },
    {
      "epoch": 0.6819700006072752,
      "grad_norm": 3.2918758392333984,
      "learning_rate": 8.778227261258095e-05,
      "loss": 2.5299,
      "step": 11230
    },
    {
      "epoch": 0.6825772757636485,
      "grad_norm": 3.3552262783050537,
      "learning_rate": 8.776143864352352e-05,
      "loss": 2.436,
      "step": 11240
    },
    {
      "epoch": 0.6831845509200218,
      "grad_norm": 2.4322407245635986,
      "learning_rate": 8.774058940313894e-05,
      "loss": 2.6237,
      "step": 11250
    },
    {
      "epoch": 0.6837918260763952,
      "grad_norm": 4.940098762512207,
      "learning_rate": 8.771972489985891e-05,
      "loss": 2.6856,
      "step": 11260
    },
    {
      "epoch": 0.6843991012327686,
      "grad_norm": 1.7388874292373657,
      "learning_rate": 8.769884514212139e-05,
      "loss": 2.4366,
      "step": 11270
    },
    {
      "epoch": 0.685006376389142,
      "grad_norm": 1.8291981220245361,
      "learning_rate": 8.767795013837048e-05,
      "loss": 2.3607,
      "step": 11280
    },
    {
      "epoch": 0.6856136515455152,
      "grad_norm": 1.2473247051239014,
      "learning_rate": 8.765703989705647e-05,
      "loss": 2.1691,
      "step": 11290
    },
    {
      "epoch": 0.6862209267018886,
      "grad_norm": 2.85966157913208,
      "learning_rate": 8.76361144266358e-05,
      "loss": 2.6303,
      "step": 11300
    },
    {
      "epoch": 0.686828201858262,
      "grad_norm": 2.0811448097229004,
      "learning_rate": 8.761517373557102e-05,
      "loss": 2.8045,
      "step": 11310
    },
    {
      "epoch": 0.6874354770146354,
      "grad_norm": 1.9388803243637085,
      "learning_rate": 8.759421783233092e-05,
      "loss": 2.7006,
      "step": 11320
    },
    {
      "epoch": 0.6880427521710086,
      "grad_norm": 2.6547622680664062,
      "learning_rate": 8.757324672539039e-05,
      "loss": 2.5633,
      "step": 11330
    },
    {
      "epoch": 0.688650027327382,
      "grad_norm": 2.2205355167388916,
      "learning_rate": 8.755226042323048e-05,
      "loss": 2.7194,
      "step": 11340
    },
    {
      "epoch": 0.6892573024837554,
      "grad_norm": 3.7737982273101807,
      "learning_rate": 8.753125893433838e-05,
      "loss": 2.5384,
      "step": 11350
    },
    {
      "epoch": 0.6898645776401288,
      "grad_norm": 1.988598346710205,
      "learning_rate": 8.751024226720742e-05,
      "loss": 2.4252,
      "step": 11360
    },
    {
      "epoch": 0.6904718527965021,
      "grad_norm": 2.4520511627197266,
      "learning_rate": 8.748921043033708e-05,
      "loss": 2.5214,
      "step": 11370
    },
    {
      "epoch": 0.6910791279528754,
      "grad_norm": 1.5719748735427856,
      "learning_rate": 8.746816343223298e-05,
      "loss": 2.2938,
      "step": 11380
    },
    {
      "epoch": 0.6916864031092488,
      "grad_norm": 2.2496488094329834,
      "learning_rate": 8.744710128140688e-05,
      "loss": 2.7491,
      "step": 11390
    },
    {
      "epoch": 0.6922936782656222,
      "grad_norm": 2.555913209915161,
      "learning_rate": 8.74260239863766e-05,
      "loss": 2.8803,
      "step": 11400
    },
    {
      "epoch": 0.6929009534219955,
      "grad_norm": 3.3615286350250244,
      "learning_rate": 8.740493155566616e-05,
      "loss": 2.5856,
      "step": 11410
    },
    {
      "epoch": 0.6935082285783689,
      "grad_norm": 1.917283296585083,
      "learning_rate": 8.738382399780567e-05,
      "loss": 2.7377,
      "step": 11420
    },
    {
      "epoch": 0.6941155037347422,
      "grad_norm": 3.162139415740967,
      "learning_rate": 8.736270132133138e-05,
      "loss": 3.1367,
      "step": 11430
    },
    {
      "epoch": 0.6947227788911156,
      "grad_norm": 1.6371134519577026,
      "learning_rate": 8.734156353478561e-05,
      "loss": 2.2094,
      "step": 11440
    },
    {
      "epoch": 0.6953300540474889,
      "grad_norm": 2.7362639904022217,
      "learning_rate": 8.732041064671684e-05,
      "loss": 2.5233,
      "step": 11450
    },
    {
      "epoch": 0.6959373292038623,
      "grad_norm": 2.136077880859375,
      "learning_rate": 8.72992426656796e-05,
      "loss": 2.7374,
      "step": 11460
    },
    {
      "epoch": 0.6965446043602356,
      "grad_norm": 1.9806041717529297,
      "learning_rate": 8.72780596002346e-05,
      "loss": 2.5506,
      "step": 11470
    },
    {
      "epoch": 0.697151879516609,
      "grad_norm": 2.2678589820861816,
      "learning_rate": 8.72568614589486e-05,
      "loss": 2.6835,
      "step": 11480
    },
    {
      "epoch": 0.6977591546729823,
      "grad_norm": 3.502558946609497,
      "learning_rate": 8.723564825039446e-05,
      "loss": 2.9273,
      "step": 11490
    },
    {
      "epoch": 0.6983664298293557,
      "grad_norm": 6.088322639465332,
      "learning_rate": 8.721441998315112e-05,
      "loss": 2.8049,
      "step": 11500
    },
    {
      "epoch": 0.6989737049857291,
      "grad_norm": 2.8904550075531006,
      "learning_rate": 8.719317666580365e-05,
      "loss": 2.4399,
      "step": 11510
    },
    {
      "epoch": 0.6995809801421023,
      "grad_norm": 2.9362099170684814,
      "learning_rate": 8.71719183069432e-05,
      "loss": 2.2859,
      "step": 11520
    },
    {
      "epoch": 0.7001882552984757,
      "grad_norm": 2.4345951080322266,
      "learning_rate": 8.715064491516696e-05,
      "loss": 2.7715,
      "step": 11530
    },
    {
      "epoch": 0.7007955304548491,
      "grad_norm": 2.755814790725708,
      "learning_rate": 8.712935649907824e-05,
      "loss": 2.5602,
      "step": 11540
    },
    {
      "epoch": 0.7014028056112225,
      "grad_norm": 2.603759527206421,
      "learning_rate": 8.710805306728641e-05,
      "loss": 2.8788,
      "step": 11550
    },
    {
      "epoch": 0.7020100807675957,
      "grad_norm": 2.198115110397339,
      "learning_rate": 8.708673462840693e-05,
      "loss": 2.2456,
      "step": 11560
    },
    {
      "epoch": 0.7026173559239691,
      "grad_norm": 2.4499433040618896,
      "learning_rate": 8.70654011910613e-05,
      "loss": 2.4557,
      "step": 11570
    },
    {
      "epoch": 0.7032246310803425,
      "grad_norm": 5.295528411865234,
      "learning_rate": 8.704405276387713e-05,
      "loss": 2.8329,
      "step": 11580
    },
    {
      "epoch": 0.7038319062367159,
      "grad_norm": 4.217625141143799,
      "learning_rate": 8.702268935548802e-05,
      "loss": 3.0539,
      "step": 11590
    },
    {
      "epoch": 0.7044391813930893,
      "grad_norm": 3.700033664703369,
      "learning_rate": 8.70013109745337e-05,
      "loss": 2.9996,
      "step": 11600
    },
    {
      "epoch": 0.7050464565494625,
      "grad_norm": 3.209825038909912,
      "learning_rate": 8.697991762965993e-05,
      "loss": 2.5647,
      "step": 11610
    },
    {
      "epoch": 0.7056537317058359,
      "grad_norm": 2.8921821117401123,
      "learning_rate": 8.695850932951852e-05,
      "loss": 2.8254,
      "step": 11620
    },
    {
      "epoch": 0.7062610068622093,
      "grad_norm": 4.585761547088623,
      "learning_rate": 8.693708608276732e-05,
      "loss": 2.9103,
      "step": 11630
    },
    {
      "epoch": 0.7068682820185826,
      "grad_norm": 5.734632968902588,
      "learning_rate": 8.691564789807023e-05,
      "loss": 2.195,
      "step": 11640
    },
    {
      "epoch": 0.707475557174956,
      "grad_norm": 3.0060434341430664,
      "learning_rate": 8.689419478409719e-05,
      "loss": 2.5286,
      "step": 11650
    },
    {
      "epoch": 0.7080828323313293,
      "grad_norm": 2.747028350830078,
      "learning_rate": 8.68727267495242e-05,
      "loss": 2.936,
      "step": 11660
    },
    {
      "epoch": 0.7086901074877027,
      "grad_norm": 3.3051393032073975,
      "learning_rate": 8.685124380303325e-05,
      "loss": 2.8923,
      "step": 11670
    },
    {
      "epoch": 0.709297382644076,
      "grad_norm": 3.7912790775299072,
      "learning_rate": 8.682974595331242e-05,
      "loss": 2.8268,
      "step": 11680
    },
    {
      "epoch": 0.7099046578004494,
      "grad_norm": 3.7449381351470947,
      "learning_rate": 8.680823320905573e-05,
      "loss": 2.5486,
      "step": 11690
    },
    {
      "epoch": 0.7105119329568227,
      "grad_norm": 2.2083044052124023,
      "learning_rate": 8.678670557896332e-05,
      "loss": 2.6734,
      "step": 11700
    },
    {
      "epoch": 0.7111192081131961,
      "grad_norm": 2.0788979530334473,
      "learning_rate": 8.676516307174129e-05,
      "loss": 2.7564,
      "step": 11710
    },
    {
      "epoch": 0.7117264832695694,
      "grad_norm": 2.7857635021209717,
      "learning_rate": 8.674360569610179e-05,
      "loss": 2.8324,
      "step": 11720
    },
    {
      "epoch": 0.7123337584259428,
      "grad_norm": 2.9760494232177734,
      "learning_rate": 8.672203346076296e-05,
      "loss": 2.7563,
      "step": 11730
    },
    {
      "epoch": 0.7129410335823162,
      "grad_norm": 3.7328386306762695,
      "learning_rate": 8.670044637444891e-05,
      "loss": 2.5437,
      "step": 11740
    },
    {
      "epoch": 0.7135483087386895,
      "grad_norm": 2.398681640625,
      "learning_rate": 8.667884444588988e-05,
      "loss": 2.5923,
      "step": 11750
    },
    {
      "epoch": 0.7141555838950628,
      "grad_norm": 2.139324426651001,
      "learning_rate": 8.665722768382199e-05,
      "loss": 2.4943,
      "step": 11760
    },
    {
      "epoch": 0.7147628590514362,
      "grad_norm": 2.6767804622650146,
      "learning_rate": 8.663559609698739e-05,
      "loss": 2.708,
      "step": 11770
    },
    {
      "epoch": 0.7153701342078096,
      "grad_norm": 3.5837228298187256,
      "learning_rate": 8.661394969413427e-05,
      "loss": 2.8189,
      "step": 11780
    },
    {
      "epoch": 0.7159774093641829,
      "grad_norm": 2.4622445106506348,
      "learning_rate": 8.659228848401675e-05,
      "loss": 2.7948,
      "step": 11790
    },
    {
      "epoch": 0.7165846845205562,
      "grad_norm": 2.9472427368164062,
      "learning_rate": 8.657061247539499e-05,
      "loss": 2.7871,
      "step": 11800
    },
    {
      "epoch": 0.7171919596769296,
      "grad_norm": 4.214468002319336,
      "learning_rate": 8.65489216770351e-05,
      "loss": 2.5591,
      "step": 11810
    },
    {
      "epoch": 0.717799234833303,
      "grad_norm": 3.0277366638183594,
      "learning_rate": 8.65272160977092e-05,
      "loss": 2.3493,
      "step": 11820
    },
    {
      "epoch": 0.7184065099896764,
      "grad_norm": 2.917217493057251,
      "learning_rate": 8.650549574619537e-05,
      "loss": 2.9131,
      "step": 11830
    },
    {
      "epoch": 0.7190137851460496,
      "grad_norm": 2.0434906482696533,
      "learning_rate": 8.648376063127765e-05,
      "loss": 2.6957,
      "step": 11840
    },
    {
      "epoch": 0.719621060302423,
      "grad_norm": 1.3094667196273804,
      "learning_rate": 8.646201076174608e-05,
      "loss": 2.2594,
      "step": 11850
    },
    {
      "epoch": 0.7202283354587964,
      "grad_norm": 1.6405599117279053,
      "learning_rate": 8.644024614639665e-05,
      "loss": 2.5667,
      "step": 11860
    },
    {
      "epoch": 0.7208356106151698,
      "grad_norm": 2.586542844772339,
      "learning_rate": 8.641846679403131e-05,
      "loss": 2.8943,
      "step": 11870
    },
    {
      "epoch": 0.7214428857715431,
      "grad_norm": 2.5706448554992676,
      "learning_rate": 8.639667271345798e-05,
      "loss": 2.5985,
      "step": 11880
    },
    {
      "epoch": 0.7220501609279164,
      "grad_norm": 2.664156675338745,
      "learning_rate": 8.637486391349055e-05,
      "loss": 2.6547,
      "step": 11890
    },
    {
      "epoch": 0.7226574360842898,
      "grad_norm": 3.352874517440796,
      "learning_rate": 8.635304040294883e-05,
      "loss": 2.3388,
      "step": 11900
    },
    {
      "epoch": 0.7232647112406632,
      "grad_norm": 2.9698915481567383,
      "learning_rate": 8.63312021906586e-05,
      "loss": 2.6595,
      "step": 11910
    },
    {
      "epoch": 0.7238719863970365,
      "grad_norm": 3.0032286643981934,
      "learning_rate": 8.630934928545156e-05,
      "loss": 3.2125,
      "step": 11920
    },
    {
      "epoch": 0.7244792615534098,
      "grad_norm": 3.8083279132843018,
      "learning_rate": 8.62874816961654e-05,
      "loss": 2.8127,
      "step": 11930
    },
    {
      "epoch": 0.7250865367097832,
      "grad_norm": 3.460561990737915,
      "learning_rate": 8.626559943164371e-05,
      "loss": 2.9802,
      "step": 11940
    },
    {
      "epoch": 0.7256938118661566,
      "grad_norm": 2.7927427291870117,
      "learning_rate": 8.624370250073606e-05,
      "loss": 2.775,
      "step": 11950
    },
    {
      "epoch": 0.7263010870225299,
      "grad_norm": 2.146242141723633,
      "learning_rate": 8.622179091229785e-05,
      "loss": 2.7294,
      "step": 11960
    },
    {
      "epoch": 0.7269083621789033,
      "grad_norm": 1.7230453491210938,
      "learning_rate": 8.619986467519052e-05,
      "loss": 2.4231,
      "step": 11970
    },
    {
      "epoch": 0.7275156373352766,
      "grad_norm": 1.879225730895996,
      "learning_rate": 8.61779237982814e-05,
      "loss": 2.1984,
      "step": 11980
    },
    {
      "epoch": 0.72812291249165,
      "grad_norm": 2.745657205581665,
      "learning_rate": 8.615596829044371e-05,
      "loss": 2.0588,
      "step": 11990
    },
    {
      "epoch": 0.7287301876480233,
      "grad_norm": 1.8498908281326294,
      "learning_rate": 8.613399816055658e-05,
      "loss": 2.6608,
      "step": 12000
    },
    {
      "epoch": 0.7293374628043967,
      "grad_norm": 2.108635902404785,
      "learning_rate": 8.611201341750514e-05,
      "loss": 2.6645,
      "step": 12010
    },
    {
      "epoch": 0.72994473796077,
      "grad_norm": 5.095727443695068,
      "learning_rate": 8.609001407018032e-05,
      "loss": 2.9831,
      "step": 12020
    },
    {
      "epoch": 0.7305520131171434,
      "grad_norm": 2.1052653789520264,
      "learning_rate": 8.606800012747904e-05,
      "loss": 2.7633,
      "step": 12030
    },
    {
      "epoch": 0.7311592882735167,
      "grad_norm": 2.5036182403564453,
      "learning_rate": 8.604597159830407e-05,
      "loss": 2.7189,
      "step": 12040
    },
    {
      "epoch": 0.7317665634298901,
      "grad_norm": 3.092167377471924,
      "learning_rate": 8.60239284915641e-05,
      "loss": 2.6797,
      "step": 12050
    },
    {
      "epoch": 0.7323738385862635,
      "grad_norm": 2.647402763366699,
      "learning_rate": 8.600187081617372e-05,
      "loss": 2.8066,
      "step": 12060
    },
    {
      "epoch": 0.7329811137426367,
      "grad_norm": 1.951668381690979,
      "learning_rate": 8.59797985810534e-05,
      "loss": 2.6388,
      "step": 12070
    },
    {
      "epoch": 0.7335883888990101,
      "grad_norm": 1.1969759464263916,
      "learning_rate": 8.59577117951295e-05,
      "loss": 2.394,
      "step": 12080
    },
    {
      "epoch": 0.7341956640553835,
      "grad_norm": 2.772616386413574,
      "learning_rate": 8.593561046733429e-05,
      "loss": 3.0806,
      "step": 12090
    },
    {
      "epoch": 0.7348029392117569,
      "grad_norm": 3.291264772415161,
      "learning_rate": 8.591349460660586e-05,
      "loss": 2.7417,
      "step": 12100
    },
    {
      "epoch": 0.7354102143681303,
      "grad_norm": 3.351583957672119,
      "learning_rate": 8.589136422188825e-05,
      "loss": 2.925,
      "step": 12110
    },
    {
      "epoch": 0.7360174895245035,
      "grad_norm": 3.158633232116699,
      "learning_rate": 8.586921932213133e-05,
      "loss": 2.9981,
      "step": 12120
    },
    {
      "epoch": 0.7366247646808769,
      "grad_norm": 3.6460838317871094,
      "learning_rate": 8.584705991629085e-05,
      "loss": 2.5145,
      "step": 12130
    },
    {
      "epoch": 0.7372320398372503,
      "grad_norm": 4.022024154663086,
      "learning_rate": 8.582488601332842e-05,
      "loss": 3.0135,
      "step": 12140
    },
    {
      "epoch": 0.7378393149936237,
      "grad_norm": 5.045088768005371,
      "learning_rate": 8.580269762221152e-05,
      "loss": 2.6971,
      "step": 12150
    },
    {
      "epoch": 0.7384465901499969,
      "grad_norm": 2.6589720249176025,
      "learning_rate": 8.578049475191349e-05,
      "loss": 2.9139,
      "step": 12160
    },
    {
      "epoch": 0.7390538653063703,
      "grad_norm": 2.1947734355926514,
      "learning_rate": 8.575827741141356e-05,
      "loss": 2.6848,
      "step": 12170
    },
    {
      "epoch": 0.7396611404627437,
      "grad_norm": 3.3396546840667725,
      "learning_rate": 8.573604560969672e-05,
      "loss": 3.0729,
      "step": 12180
    },
    {
      "epoch": 0.740268415619117,
      "grad_norm": 3.7067103385925293,
      "learning_rate": 8.571379935575388e-05,
      "loss": 2.524,
      "step": 12190
    },
    {
      "epoch": 0.7408756907754904,
      "grad_norm": 1.3656337261199951,
      "learning_rate": 8.56915386585818e-05,
      "loss": 2.1886,
      "step": 12200
    },
    {
      "epoch": 0.7414829659318637,
      "grad_norm": 1.5431345701217651,
      "learning_rate": 8.566926352718305e-05,
      "loss": 2.4723,
      "step": 12210
    },
    {
      "epoch": 0.7420902410882371,
      "grad_norm": 1.5240191221237183,
      "learning_rate": 8.564697397056605e-05,
      "loss": 2.7711,
      "step": 12220
    },
    {
      "epoch": 0.7426975162446104,
      "grad_norm": 3.461606502532959,
      "learning_rate": 8.562466999774503e-05,
      "loss": 2.657,
      "step": 12230
    },
    {
      "epoch": 0.7433047914009838,
      "grad_norm": 1.8117313385009766,
      "learning_rate": 8.56023516177401e-05,
      "loss": 2.8409,
      "step": 12240
    },
    {
      "epoch": 0.7439120665573571,
      "grad_norm": 2.578498125076294,
      "learning_rate": 8.558001883957717e-05,
      "loss": 2.8768,
      "step": 12250
    },
    {
      "epoch": 0.7445193417137305,
      "grad_norm": 2.5922679901123047,
      "learning_rate": 8.555767167228796e-05,
      "loss": 2.355,
      "step": 12260
    },
    {
      "epoch": 0.7451266168701038,
      "grad_norm": 2.369999408721924,
      "learning_rate": 8.553531012491e-05,
      "loss": 2.4201,
      "step": 12270
    },
    {
      "epoch": 0.7457338920264772,
      "grad_norm": 1.4619932174682617,
      "learning_rate": 8.55129342064867e-05,
      "loss": 2.1753,
      "step": 12280
    },
    {
      "epoch": 0.7463411671828506,
      "grad_norm": 2.6856765747070312,
      "learning_rate": 8.549054392606719e-05,
      "loss": 2.5162,
      "step": 12290
    },
    {
      "epoch": 0.7469484423392239,
      "grad_norm": 2.703508138656616,
      "learning_rate": 8.54681392927065e-05,
      "loss": 2.8527,
      "step": 12300
    },
    {
      "epoch": 0.7475557174955972,
      "grad_norm": 4.25213098526001,
      "learning_rate": 8.544572031546539e-05,
      "loss": 2.4144,
      "step": 12310
    },
    {
      "epoch": 0.7481629926519706,
      "grad_norm": 3.3756210803985596,
      "learning_rate": 8.542328700341046e-05,
      "loss": 2.3933,
      "step": 12320
    },
    {
      "epoch": 0.748770267808344,
      "grad_norm": 3.1736977100372314,
      "learning_rate": 8.54008393656141e-05,
      "loss": 2.4568,
      "step": 12330
    },
    {
      "epoch": 0.7493775429647173,
      "grad_norm": 2.402637004852295,
      "learning_rate": 8.537837741115449e-05,
      "loss": 2.5222,
      "step": 12340
    },
    {
      "epoch": 0.7499848181210906,
      "grad_norm": 2.606038808822632,
      "learning_rate": 8.535590114911561e-05,
      "loss": 2.8157,
      "step": 12350
    },
    {
      "epoch": 0.750592093277464,
      "grad_norm": 1.6174124479293823,
      "learning_rate": 8.533341058858721e-05,
      "loss": 2.3689,
      "step": 12360
    },
    {
      "epoch": 0.7511993684338374,
      "grad_norm": 1.3491549491882324,
      "learning_rate": 8.531090573866485e-05,
      "loss": 2.1125,
      "step": 12370
    },
    {
      "epoch": 0.7518066435902108,
      "grad_norm": 3.0584466457366943,
      "learning_rate": 8.528838660844982e-05,
      "loss": 3.3236,
      "step": 12380
    },
    {
      "epoch": 0.752413918746584,
      "grad_norm": 3.101771354675293,
      "learning_rate": 8.526585320704923e-05,
      "loss": 2.7283,
      "step": 12390
    },
    {
      "epoch": 0.7530211939029574,
      "grad_norm": 5.311209201812744,
      "learning_rate": 8.524330554357594e-05,
      "loss": 2.8997,
      "step": 12400
    },
    {
      "epoch": 0.7536284690593308,
      "grad_norm": 3.589155673980713,
      "learning_rate": 8.522074362714859e-05,
      "loss": 2.706,
      "step": 12410
    },
    {
      "epoch": 0.7542357442157042,
      "grad_norm": 3.25150465965271,
      "learning_rate": 8.519816746689157e-05,
      "loss": 2.7296,
      "step": 12420
    },
    {
      "epoch": 0.7548430193720775,
      "grad_norm": 2.769139528274536,
      "learning_rate": 8.517557707193506e-05,
      "loss": 2.7433,
      "step": 12430
    },
    {
      "epoch": 0.7554502945284508,
      "grad_norm": 3.1503448486328125,
      "learning_rate": 8.515297245141497e-05,
      "loss": 2.8324,
      "step": 12440
    },
    {
      "epoch": 0.7560575696848242,
      "grad_norm": 2.887573719024658,
      "learning_rate": 8.513035361447294e-05,
      "loss": 2.9386,
      "step": 12450
    },
    {
      "epoch": 0.7566648448411976,
      "grad_norm": 2.476949453353882,
      "learning_rate": 8.510772057025643e-05,
      "loss": 2.3289,
      "step": 12460
    },
    {
      "epoch": 0.7572721199975709,
      "grad_norm": 2.30305814743042,
      "learning_rate": 8.508507332791857e-05,
      "loss": 2.4107,
      "step": 12470
    },
    {
      "epoch": 0.7578793951539442,
      "grad_norm": 1.2754542827606201,
      "learning_rate": 8.506241189661827e-05,
      "loss": 2.0882,
      "step": 12480
    },
    {
      "epoch": 0.7584866703103176,
      "grad_norm": 1.9855531454086304,
      "learning_rate": 8.50397362855202e-05,
      "loss": 2.2088,
      "step": 12490
    },
    {
      "epoch": 0.759093945466691,
      "grad_norm": 2.5470457077026367,
      "learning_rate": 8.50170465037947e-05,
      "loss": 2.5548,
      "step": 12500
    },
    {
      "epoch": 0.7597012206230643,
      "grad_norm": 2.3090388774871826,
      "learning_rate": 8.49943425606179e-05,
      "loss": 2.8005,
      "step": 12510
    },
    {
      "epoch": 0.7603084957794377,
      "grad_norm": 2.8157882690429688,
      "learning_rate": 8.497162446517164e-05,
      "loss": 2.4855,
      "step": 12520
    },
    {
      "epoch": 0.760915770935811,
      "grad_norm": 3.259483575820923,
      "learning_rate": 8.494889222664348e-05,
      "loss": 2.4971,
      "step": 12530
    },
    {
      "epoch": 0.7615230460921844,
      "grad_norm": 3.2357449531555176,
      "learning_rate": 8.492614585422669e-05,
      "loss": 2.8964,
      "step": 12540
    },
    {
      "epoch": 0.7621303212485577,
      "grad_norm": 2.967263698577881,
      "learning_rate": 8.490338535712026e-05,
      "loss": 2.7193,
      "step": 12550
    },
    {
      "epoch": 0.7627375964049311,
      "grad_norm": 2.671717405319214,
      "learning_rate": 8.488061074452891e-05,
      "loss": 3.0018,
      "step": 12560
    },
    {
      "epoch": 0.7633448715613044,
      "grad_norm": 3.279575824737549,
      "learning_rate": 8.485782202566306e-05,
      "loss": 3.0169,
      "step": 12570
    },
    {
      "epoch": 0.7639521467176777,
      "grad_norm": 3.300611972808838,
      "learning_rate": 8.483501920973882e-05,
      "loss": 2.6485,
      "step": 12580
    },
    {
      "epoch": 0.7645594218740511,
      "grad_norm": 2.1488521099090576,
      "learning_rate": 8.481220230597801e-05,
      "loss": 2.7012,
      "step": 12590
    },
    {
      "epoch": 0.7651666970304245,
      "grad_norm": 2.4776837825775146,
      "learning_rate": 8.478937132360816e-05,
      "loss": 2.8426,
      "step": 12600
    },
    {
      "epoch": 0.7657739721867979,
      "grad_norm": 2.7846860885620117,
      "learning_rate": 8.476652627186248e-05,
      "loss": 2.6427,
      "step": 12610
    },
    {
      "epoch": 0.7663812473431711,
      "grad_norm": 4.276706218719482,
      "learning_rate": 8.474366715997986e-05,
      "loss": 2.6833,
      "step": 12620
    },
    {
      "epoch": 0.7669885224995445,
      "grad_norm": 2.8197853565216064,
      "learning_rate": 8.47207939972049e-05,
      "loss": 3.0665,
      "step": 12630
    },
    {
      "epoch": 0.7675957976559179,
      "grad_norm": 3.9417333602905273,
      "learning_rate": 8.469790679278789e-05,
      "loss": 2.5822,
      "step": 12640
    },
    {
      "epoch": 0.7682030728122913,
      "grad_norm": 2.5085790157318115,
      "learning_rate": 8.467500555598473e-05,
      "loss": 2.6133,
      "step": 12650
    },
    {
      "epoch": 0.7688103479686647,
      "grad_norm": 2.901658296585083,
      "learning_rate": 8.465209029605709e-05,
      "loss": 2.6548,
      "step": 12660
    },
    {
      "epoch": 0.7694176231250379,
      "grad_norm": 2.4988815784454346,
      "learning_rate": 8.462916102227226e-05,
      "loss": 2.4482,
      "step": 12670
    },
    {
      "epoch": 0.7700248982814113,
      "grad_norm": 6.427372455596924,
      "learning_rate": 8.460621774390319e-05,
      "loss": 2.565,
      "step": 12680
    },
    {
      "epoch": 0.7706321734377847,
      "grad_norm": 2.1104378700256348,
      "learning_rate": 8.458326047022852e-05,
      "loss": 2.4357,
      "step": 12690
    },
    {
      "epoch": 0.771239448594158,
      "grad_norm": 2.0396695137023926,
      "learning_rate": 8.45602892105325e-05,
      "loss": 2.7173,
      "step": 12700
    },
    {
      "epoch": 0.7718467237505313,
      "grad_norm": 2.3522706031799316,
      "learning_rate": 8.453730397410512e-05,
      "loss": 2.4901,
      "step": 12710
    },
    {
      "epoch": 0.7724539989069047,
      "grad_norm": 2.369676351547241,
      "learning_rate": 8.451430477024196e-05,
      "loss": 2.2697,
      "step": 12720
    },
    {
      "epoch": 0.7730612740632781,
      "grad_norm": 2.0157129764556885,
      "learning_rate": 8.449129160824427e-05,
      "loss": 2.3216,
      "step": 12730
    },
    {
      "epoch": 0.7736685492196514,
      "grad_norm": 2.0080716609954834,
      "learning_rate": 8.446826449741891e-05,
      "loss": 2.0508,
      "step": 12740
    },
    {
      "epoch": 0.7742758243760248,
      "grad_norm": 2.124408483505249,
      "learning_rate": 8.444522344707844e-05,
      "loss": 2.4639,
      "step": 12750
    },
    {
      "epoch": 0.7748830995323981,
      "grad_norm": 2.5271897315979004,
      "learning_rate": 8.442216846654102e-05,
      "loss": 2.4073,
      "step": 12760
    },
    {
      "epoch": 0.7754903746887715,
      "grad_norm": 1.6745035648345947,
      "learning_rate": 8.439909956513046e-05,
      "loss": 2.5853,
      "step": 12770
    },
    {
      "epoch": 0.7760976498451448,
      "grad_norm": 2.7319743633270264,
      "learning_rate": 8.437601675217616e-05,
      "loss": 2.8176,
      "step": 12780
    },
    {
      "epoch": 0.7767049250015182,
      "grad_norm": 3.9992873668670654,
      "learning_rate": 8.435292003701323e-05,
      "loss": 2.5552,
      "step": 12790
    },
    {
      "epoch": 0.7773122001578915,
      "grad_norm": 3.1645894050598145,
      "learning_rate": 8.43298094289823e-05,
      "loss": 2.6233,
      "step": 12800
    },
    {
      "epoch": 0.7779194753142649,
      "grad_norm": 2.704256057739258,
      "learning_rate": 8.43066849374297e-05,
      "loss": 2.6764,
      "step": 12810
    },
    {
      "epoch": 0.7785267504706382,
      "grad_norm": 2.559857130050659,
      "learning_rate": 8.42835465717073e-05,
      "loss": 2.2085,
      "step": 12820
    },
    {
      "epoch": 0.7791340256270116,
      "grad_norm": 2.833829641342163,
      "learning_rate": 8.426039434117268e-05,
      "loss": 2.9147,
      "step": 12830
    },
    {
      "epoch": 0.779741300783385,
      "grad_norm": 1.9597432613372803,
      "learning_rate": 8.423722825518893e-05,
      "loss": 2.5081,
      "step": 12840
    },
    {
      "epoch": 0.7803485759397583,
      "grad_norm": 2.1296281814575195,
      "learning_rate": 8.421404832312482e-05,
      "loss": 2.75,
      "step": 12850
    },
    {
      "epoch": 0.7809558510961316,
      "grad_norm": 2.0866172313690186,
      "learning_rate": 8.419085455435465e-05,
      "loss": 2.4545,
      "step": 12860
    },
    {
      "epoch": 0.781563126252505,
      "grad_norm": 3.5649213790893555,
      "learning_rate": 8.416764695825835e-05,
      "loss": 2.9691,
      "step": 12870
    },
    {
      "epoch": 0.7821704014088784,
      "grad_norm": 3.2858567237854004,
      "learning_rate": 8.414442554422146e-05,
      "loss": 3.1158,
      "step": 12880
    },
    {
      "epoch": 0.7827776765652518,
      "grad_norm": 2.397604465484619,
      "learning_rate": 8.412119032163508e-05,
      "loss": 2.7672,
      "step": 12890
    },
    {
      "epoch": 0.783384951721625,
      "grad_norm": 3.281830310821533,
      "learning_rate": 8.40979412998959e-05,
      "loss": 2.8445,
      "step": 12900
    },
    {
      "epoch": 0.7839922268779984,
      "grad_norm": 2.851888656616211,
      "learning_rate": 8.407467848840621e-05,
      "loss": 2.5461,
      "step": 12910
    },
    {
      "epoch": 0.7845995020343718,
      "grad_norm": 2.6113715171813965,
      "learning_rate": 8.405140189657385e-05,
      "loss": 2.6418,
      "step": 12920
    },
    {
      "epoch": 0.7852067771907452,
      "grad_norm": 2.2430620193481445,
      "learning_rate": 8.402811153381224e-05,
      "loss": 2.7154,
      "step": 12930
    },
    {
      "epoch": 0.7858140523471184,
      "grad_norm": 2.186270236968994,
      "learning_rate": 8.40048074095404e-05,
      "loss": 2.65,
      "step": 12940
    },
    {
      "epoch": 0.7864213275034918,
      "grad_norm": 2.664656639099121,
      "learning_rate": 8.398148953318285e-05,
      "loss": 2.5786,
      "step": 12950
    },
    {
      "epoch": 0.7870286026598652,
      "grad_norm": 2.108841896057129,
      "learning_rate": 8.395815791416975e-05,
      "loss": 2.625,
      "step": 12960
    },
    {
      "epoch": 0.7876358778162386,
      "grad_norm": 2.7099287509918213,
      "learning_rate": 8.393481256193674e-05,
      "loss": 2.9007,
      "step": 12970
    },
    {
      "epoch": 0.7882431529726119,
      "grad_norm": 2.640650749206543,
      "learning_rate": 8.391145348592506e-05,
      "loss": 2.3823,
      "step": 12980
    },
    {
      "epoch": 0.7888504281289852,
      "grad_norm": 4.88588809967041,
      "learning_rate": 8.388808069558153e-05,
      "loss": 2.3546,
      "step": 12990
    },
    {
      "epoch": 0.7894577032853586,
      "grad_norm": 2.7603070735931396,
      "learning_rate": 8.386469420035845e-05,
      "loss": 2.9959,
      "step": 13000
    },
    {
      "epoch": 0.790064978441732,
      "grad_norm": 2.2702150344848633,
      "learning_rate": 8.384129400971368e-05,
      "loss": 2.5861,
      "step": 13010
    },
    {
      "epoch": 0.7906722535981053,
      "grad_norm": 2.468120813369751,
      "learning_rate": 8.381788013311065e-05,
      "loss": 2.6327,
      "step": 13020
    },
    {
      "epoch": 0.7912795287544786,
      "grad_norm": 4.602829456329346,
      "learning_rate": 8.379445258001828e-05,
      "loss": 2.5708,
      "step": 13030
    },
    {
      "epoch": 0.791886803910852,
      "grad_norm": 3.4682581424713135,
      "learning_rate": 8.377101135991108e-05,
      "loss": 2.9803,
      "step": 13040
    },
    {
      "epoch": 0.7924940790672254,
      "grad_norm": 3.573861837387085,
      "learning_rate": 8.374755648226903e-05,
      "loss": 2.8627,
      "step": 13050
    },
    {
      "epoch": 0.7931013542235987,
      "grad_norm": 2.318204879760742,
      "learning_rate": 8.372408795657766e-05,
      "loss": 2.4493,
      "step": 13060
    },
    {
      "epoch": 0.7937086293799721,
      "grad_norm": 2.1180083751678467,
      "learning_rate": 8.370060579232802e-05,
      "loss": 2.4818,
      "step": 13070
    },
    {
      "epoch": 0.7943159045363454,
      "grad_norm": 2.53090238571167,
      "learning_rate": 8.367710999901667e-05,
      "loss": 2.6464,
      "step": 13080
    },
    {
      "epoch": 0.7949231796927188,
      "grad_norm": 2.1220881938934326,
      "learning_rate": 8.365360058614567e-05,
      "loss": 2.6028,
      "step": 13090
    },
    {
      "epoch": 0.7955304548490921,
      "grad_norm": 3.1992530822753906,
      "learning_rate": 8.363007756322263e-05,
      "loss": 2.6526,
      "step": 13100
    },
    {
      "epoch": 0.7961377300054655,
      "grad_norm": 2.8467330932617188,
      "learning_rate": 8.360654093976061e-05,
      "loss": 2.467,
      "step": 13110
    },
    {
      "epoch": 0.7967450051618389,
      "grad_norm": 3.714524269104004,
      "learning_rate": 8.35829907252782e-05,
      "loss": 2.8228,
      "step": 13120
    },
    {
      "epoch": 0.7973522803182121,
      "grad_norm": 4.246834754943848,
      "learning_rate": 8.355942692929948e-05,
      "loss": 2.51,
      "step": 13130
    },
    {
      "epoch": 0.7979595554745855,
      "grad_norm": 4.946491241455078,
      "learning_rate": 8.353584956135405e-05,
      "loss": 2.4447,
      "step": 13140
    },
    {
      "epoch": 0.7985668306309589,
      "grad_norm": 3.2885987758636475,
      "learning_rate": 8.351225863097693e-05,
      "loss": 3.2305,
      "step": 13150
    },
    {
      "epoch": 0.7991741057873323,
      "grad_norm": 3.7308285236358643,
      "learning_rate": 8.34886541477087e-05,
      "loss": 2.9978,
      "step": 13160
    },
    {
      "epoch": 0.7997813809437055,
      "grad_norm": 2.9337024688720703,
      "learning_rate": 8.346503612109537e-05,
      "loss": 2.3974,
      "step": 13170
    },
    {
      "epoch": 0.8003886561000789,
      "grad_norm": 3.062741279602051,
      "learning_rate": 8.344140456068846e-05,
      "loss": 2.7451,
      "step": 13180
    },
    {
      "epoch": 0.8009959312564523,
      "grad_norm": 2.4935550689697266,
      "learning_rate": 8.341775947604495e-05,
      "loss": 3.1726,
      "step": 13190
    },
    {
      "epoch": 0.8016032064128257,
      "grad_norm": 3.0826053619384766,
      "learning_rate": 8.339410087672727e-05,
      "loss": 2.9983,
      "step": 13200
    },
    {
      "epoch": 0.802210481569199,
      "grad_norm": 2.1114518642425537,
      "learning_rate": 8.337042877230337e-05,
      "loss": 2.9409,
      "step": 13210
    },
    {
      "epoch": 0.8028177567255723,
      "grad_norm": 2.4459645748138428,
      "learning_rate": 8.334674317234659e-05,
      "loss": 2.4006,
      "step": 13220
    },
    {
      "epoch": 0.8034250318819457,
      "grad_norm": 2.3676462173461914,
      "learning_rate": 8.332304408643577e-05,
      "loss": 2.5018,
      "step": 13230
    },
    {
      "epoch": 0.8040323070383191,
      "grad_norm": 2.706798791885376,
      "learning_rate": 8.32993315241552e-05,
      "loss": 2.4862,
      "step": 13240
    },
    {
      "epoch": 0.8046395821946924,
      "grad_norm": 3.5075178146362305,
      "learning_rate": 8.327560549509465e-05,
      "loss": 2.8529,
      "step": 13250
    },
    {
      "epoch": 0.8052468573510657,
      "grad_norm": 3.9236693382263184,
      "learning_rate": 8.325186600884926e-05,
      "loss": 2.6546,
      "step": 13260
    },
    {
      "epoch": 0.8058541325074391,
      "grad_norm": 4.5302815437316895,
      "learning_rate": 8.322811307501968e-05,
      "loss": 2.5015,
      "step": 13270
    },
    {
      "epoch": 0.8064614076638125,
      "grad_norm": 3.452198028564453,
      "learning_rate": 8.320434670321196e-05,
      "loss": 2.7999,
      "step": 13280
    },
    {
      "epoch": 0.8070686828201858,
      "grad_norm": 2.905460834503174,
      "learning_rate": 8.318056690303762e-05,
      "loss": 2.6646,
      "step": 13290
    },
    {
      "epoch": 0.8076759579765592,
      "grad_norm": 2.957852840423584,
      "learning_rate": 8.315677368411356e-05,
      "loss": 2.585,
      "step": 13300
    },
    {
      "epoch": 0.8082832331329325,
      "grad_norm": 3.2213077545166016,
      "learning_rate": 8.313296705606217e-05,
      "loss": 2.6983,
      "step": 13310
    },
    {
      "epoch": 0.8088905082893059,
      "grad_norm": 3.5750601291656494,
      "learning_rate": 8.31091470285112e-05,
      "loss": 2.9155,
      "step": 13320
    },
    {
      "epoch": 0.8094977834456792,
      "grad_norm": 6.338039398193359,
      "learning_rate": 8.308531361109389e-05,
      "loss": 3.0318,
      "step": 13330
    },
    {
      "epoch": 0.8101050586020526,
      "grad_norm": 3.3787012100219727,
      "learning_rate": 8.30614668134488e-05,
      "loss": 2.4836,
      "step": 13340
    },
    {
      "epoch": 0.810712333758426,
      "grad_norm": 2.3711884021759033,
      "learning_rate": 8.303760664521998e-05,
      "loss": 2.474,
      "step": 13350
    },
    {
      "epoch": 0.8113196089147993,
      "grad_norm": 2.3435661792755127,
      "learning_rate": 8.301373311605687e-05,
      "loss": 2.5917,
      "step": 13360
    },
    {
      "epoch": 0.8119268840711726,
      "grad_norm": 2.0812199115753174,
      "learning_rate": 8.29898462356143e-05,
      "loss": 2.466,
      "step": 13370
    },
    {
      "epoch": 0.812534159227546,
      "grad_norm": 3.004516124725342,
      "learning_rate": 8.29659460135525e-05,
      "loss": 2.1401,
      "step": 13380
    },
    {
      "epoch": 0.8131414343839194,
      "grad_norm": 2.035146951675415,
      "learning_rate": 8.294203245953709e-05,
      "loss": 2.5056,
      "step": 13390
    },
    {
      "epoch": 0.8137487095402927,
      "grad_norm": 4.892518043518066,
      "learning_rate": 8.291810558323911e-05,
      "loss": 2.527,
      "step": 13400
    },
    {
      "epoch": 0.814355984696666,
      "grad_norm": 2.5417938232421875,
      "learning_rate": 8.289416539433498e-05,
      "loss": 2.2183,
      "step": 13410
    },
    {
      "epoch": 0.8149632598530394,
      "grad_norm": 3.328275680541992,
      "learning_rate": 8.287021190250647e-05,
      "loss": 2.287,
      "step": 13420
    },
    {
      "epoch": 0.8155705350094128,
      "grad_norm": 2.5895652770996094,
      "learning_rate": 8.284624511744076e-05,
      "loss": 2.747,
      "step": 13430
    },
    {
      "epoch": 0.8161778101657862,
      "grad_norm": 2.7834959030151367,
      "learning_rate": 8.282226504883042e-05,
      "loss": 2.8209,
      "step": 13440
    },
    {
      "epoch": 0.8167850853221594,
      "grad_norm": 2.2598257064819336,
      "learning_rate": 8.279827170637333e-05,
      "loss": 2.3267,
      "step": 13450
    },
    {
      "epoch": 0.8173923604785328,
      "grad_norm": 2.617892265319824,
      "learning_rate": 8.277426509977281e-05,
      "loss": 2.4029,
      "step": 13460
    },
    {
      "epoch": 0.8179996356349062,
      "grad_norm": 2.703598976135254,
      "learning_rate": 8.275024523873753e-05,
      "loss": 2.572,
      "step": 13470
    },
    {
      "epoch": 0.8186069107912796,
      "grad_norm": 2.8887383937835693,
      "learning_rate": 8.272621213298146e-05,
      "loss": 2.6649,
      "step": 13480
    },
    {
      "epoch": 0.8192141859476528,
      "grad_norm": 5.651155471801758,
      "learning_rate": 8.270216579222398e-05,
      "loss": 2.5843,
      "step": 13490
    },
    {
      "epoch": 0.8198214611040262,
      "grad_norm": 5.088900089263916,
      "learning_rate": 8.267810622618986e-05,
      "loss": 2.7693,
      "step": 13500
    },
    {
      "epoch": 0.8204287362603996,
      "grad_norm": 2.5485780239105225,
      "learning_rate": 8.265403344460911e-05,
      "loss": 2.9295,
      "step": 13510
    },
    {
      "epoch": 0.821036011416773,
      "grad_norm": 4.277029991149902,
      "learning_rate": 8.26299474572172e-05,
      "loss": 2.5491,
      "step": 13520
    },
    {
      "epoch": 0.8216432865731463,
      "grad_norm": 2.7404866218566895,
      "learning_rate": 8.260584827375484e-05,
      "loss": 2.3941,
      "step": 13530
    },
    {
      "epoch": 0.8222505617295196,
      "grad_norm": 2.024494171142578,
      "learning_rate": 8.258173590396814e-05,
      "loss": 2.4685,
      "step": 13540
    },
    {
      "epoch": 0.822857836885893,
      "grad_norm": 3.049452543258667,
      "learning_rate": 8.255761035760853e-05,
      "loss": 2.6008,
      "step": 13550
    },
    {
      "epoch": 0.8234651120422664,
      "grad_norm": 3.5197200775146484,
      "learning_rate": 8.25334716444328e-05,
      "loss": 3.0087,
      "step": 13560
    },
    {
      "epoch": 0.8240723871986397,
      "grad_norm": 3.2070493698120117,
      "learning_rate": 8.250931977420296e-05,
      "loss": 2.6272,
      "step": 13570
    },
    {
      "epoch": 0.8246796623550131,
      "grad_norm": 1.143296718597412,
      "learning_rate": 8.248515475668645e-05,
      "loss": 2.6303,
      "step": 13580
    },
    {
      "epoch": 0.8252869375113864,
      "grad_norm": 1.9507150650024414,
      "learning_rate": 8.2460976601656e-05,
      "loss": 2.7216,
      "step": 13590
    },
    {
      "epoch": 0.8258942126677598,
      "grad_norm": 2.2383086681365967,
      "learning_rate": 8.243678531888962e-05,
      "loss": 2.3851,
      "step": 13600
    },
    {
      "epoch": 0.8265014878241331,
      "grad_norm": 1.920028805732727,
      "learning_rate": 8.241258091817066e-05,
      "loss": 2.5097,
      "step": 13610
    },
    {
      "epoch": 0.8271087629805065,
      "grad_norm": 2.589015007019043,
      "learning_rate": 8.238836340928775e-05,
      "loss": 2.3668,
      "step": 13620
    },
    {
      "epoch": 0.8277160381368798,
      "grad_norm": 2.6698989868164062,
      "learning_rate": 8.236413280203486e-05,
      "loss": 2.4565,
      "step": 13630
    },
    {
      "epoch": 0.8283233132932531,
      "grad_norm": 1.6911736726760864,
      "learning_rate": 8.233988910621121e-05,
      "loss": 2.6937,
      "step": 13640
    },
    {
      "epoch": 0.8289305884496265,
      "grad_norm": 1.6290740966796875,
      "learning_rate": 8.231563233162134e-05,
      "loss": 2.4604,
      "step": 13650
    },
    {
      "epoch": 0.8295378636059999,
      "grad_norm": 2.1418144702911377,
      "learning_rate": 8.22913624880751e-05,
      "loss": 2.6297,
      "step": 13660
    },
    {
      "epoch": 0.8301451387623733,
      "grad_norm": 2.9271202087402344,
      "learning_rate": 8.226707958538757e-05,
      "loss": 2.603,
      "step": 13670
    },
    {
      "epoch": 0.8307524139187465,
      "grad_norm": 1.913049340248108,
      "learning_rate": 8.224278363337916e-05,
      "loss": 2.7885,
      "step": 13680
    },
    {
      "epoch": 0.8313596890751199,
      "grad_norm": 2.999790668487549,
      "learning_rate": 8.221847464187556e-05,
      "loss": 2.6547,
      "step": 13690
    },
    {
      "epoch": 0.8319669642314933,
      "grad_norm": 4.71673583984375,
      "learning_rate": 8.219415262070766e-05,
      "loss": 2.8231,
      "step": 13700
    },
    {
      "epoch": 0.8325742393878667,
      "grad_norm": 2.8988780975341797,
      "learning_rate": 8.216981757971173e-05,
      "loss": 2.7964,
      "step": 13710
    },
    {
      "epoch": 0.8331815145442399,
      "grad_norm": 2.0947608947753906,
      "learning_rate": 8.21454695287292e-05,
      "loss": 2.5131,
      "step": 13720
    },
    {
      "epoch": 0.8337887897006133,
      "grad_norm": 3.0665132999420166,
      "learning_rate": 8.212110847760684e-05,
      "loss": 2.6657,
      "step": 13730
    },
    {
      "epoch": 0.8343960648569867,
      "grad_norm": 1.6195399761199951,
      "learning_rate": 8.209673443619664e-05,
      "loss": 2.1886,
      "step": 13740
    },
    {
      "epoch": 0.8350033400133601,
      "grad_norm": 3.0355377197265625,
      "learning_rate": 8.207234741435585e-05,
      "loss": 2.2757,
      "step": 13750
    },
    {
      "epoch": 0.8356106151697335,
      "grad_norm": 1.7412289381027222,
      "learning_rate": 8.204794742194698e-05,
      "loss": 2.7116,
      "step": 13760
    },
    {
      "epoch": 0.8362178903261067,
      "grad_norm": 2.312296152114868,
      "learning_rate": 8.202353446883773e-05,
      "loss": 2.8632,
      "step": 13770
    },
    {
      "epoch": 0.8368251654824801,
      "grad_norm": 1.8419843912124634,
      "learning_rate": 8.199910856490114e-05,
      "loss": 2.5774,
      "step": 13780
    },
    {
      "epoch": 0.8374324406388535,
      "grad_norm": 2.6907005310058594,
      "learning_rate": 8.197466972001542e-05,
      "loss": 2.6354,
      "step": 13790
    },
    {
      "epoch": 0.8380397157952268,
      "grad_norm": 2.0746636390686035,
      "learning_rate": 8.195021794406401e-05,
      "loss": 2.6214,
      "step": 13800
    },
    {
      "epoch": 0.8386469909516002,
      "grad_norm": 2.723839044570923,
      "learning_rate": 8.192575324693564e-05,
      "loss": 2.7635,
      "step": 13810
    },
    {
      "epoch": 0.8392542661079735,
      "grad_norm": 3.3390326499938965,
      "learning_rate": 8.190127563852417e-05,
      "loss": 2.8694,
      "step": 13820
    },
    {
      "epoch": 0.8398615412643469,
      "grad_norm": 2.8011059761047363,
      "learning_rate": 8.187678512872875e-05,
      "loss": 2.4158,
      "step": 13830
    },
    {
      "epoch": 0.8404688164207202,
      "grad_norm": 2.3120718002319336,
      "learning_rate": 8.185228172745376e-05,
      "loss": 2.532,
      "step": 13840
    },
    {
      "epoch": 0.8410760915770936,
      "grad_norm": 3.379791498184204,
      "learning_rate": 8.182776544460875e-05,
      "loss": 2.6579,
      "step": 13850
    },
    {
      "epoch": 0.8416833667334669,
      "grad_norm": 2.3938870429992676,
      "learning_rate": 8.180323629010848e-05,
      "loss": 2.8796,
      "step": 13860
    },
    {
      "epoch": 0.8422906418898403,
      "grad_norm": 2.939929962158203,
      "learning_rate": 8.177869427387296e-05,
      "loss": 2.6073,
      "step": 13870
    },
    {
      "epoch": 0.8428979170462136,
      "grad_norm": 3.598761558532715,
      "learning_rate": 8.175413940582734e-05,
      "loss": 2.4251,
      "step": 13880
    },
    {
      "epoch": 0.843505192202587,
      "grad_norm": 3.4409122467041016,
      "learning_rate": 8.172957169590202e-05,
      "loss": 2.8538,
      "step": 13890
    },
    {
      "epoch": 0.8441124673589604,
      "grad_norm": 2.1559860706329346,
      "learning_rate": 8.170499115403256e-05,
      "loss": 2.6863,
      "step": 13900
    },
    {
      "epoch": 0.8447197425153337,
      "grad_norm": 3.6016998291015625,
      "learning_rate": 8.168039779015974e-05,
      "loss": 2.4733,
      "step": 13910
    },
    {
      "epoch": 0.845327017671707,
      "grad_norm": 2.1058647632598877,
      "learning_rate": 8.16557916142295e-05,
      "loss": 2.8266,
      "step": 13920
    },
    {
      "epoch": 0.8459342928280804,
      "grad_norm": 1.9242230653762817,
      "learning_rate": 8.163117263619297e-05,
      "loss": 2.486,
      "step": 13930
    },
    {
      "epoch": 0.8465415679844538,
      "grad_norm": 2.277340888977051,
      "learning_rate": 8.160654086600646e-05,
      "loss": 2.6214,
      "step": 13940
    },
    {
      "epoch": 0.8471488431408271,
      "grad_norm": 2.1289610862731934,
      "learning_rate": 8.158189631363142e-05,
      "loss": 2.7078,
      "step": 13950
    },
    {
      "epoch": 0.8477561182972004,
      "grad_norm": 2.1670894622802734,
      "learning_rate": 8.155723898903455e-05,
      "loss": 2.3534,
      "step": 13960
    },
    {
      "epoch": 0.8483633934535738,
      "grad_norm": 2.658057451248169,
      "learning_rate": 8.153256890218764e-05,
      "loss": 2.4735,
      "step": 13970
    },
    {
      "epoch": 0.8489706686099472,
      "grad_norm": 2.1516916751861572,
      "learning_rate": 8.150788606306765e-05,
      "loss": 2.3593,
      "step": 13980
    },
    {
      "epoch": 0.8495779437663206,
      "grad_norm": 2.4329752922058105,
      "learning_rate": 8.148319048165674e-05,
      "loss": 2.4361,
      "step": 13990
    },
    {
      "epoch": 0.8501852189226938,
      "grad_norm": 1.775348424911499,
      "learning_rate": 8.145848216794219e-05,
      "loss": 2.9853,
      "step": 14000
    },
    {
      "epoch": 0.8507924940790672,
      "grad_norm": 3.109250068664551,
      "learning_rate": 8.143376113191641e-05,
      "loss": 2.9198,
      "step": 14010
    },
    {
      "epoch": 0.8513997692354406,
      "grad_norm": 2.0915563106536865,
      "learning_rate": 8.140902738357702e-05,
      "loss": 2.3778,
      "step": 14020
    },
    {
      "epoch": 0.852007044391814,
      "grad_norm": 2.6379427909851074,
      "learning_rate": 8.138428093292672e-05,
      "loss": 2.4514,
      "step": 14030
    },
    {
      "epoch": 0.8526143195481873,
      "grad_norm": 1.9398858547210693,
      "learning_rate": 8.135952178997338e-05,
      "loss": 2.6252,
      "step": 14040
    },
    {
      "epoch": 0.8532215947045606,
      "grad_norm": 2.134084939956665,
      "learning_rate": 8.133474996472996e-05,
      "loss": 2.6094,
      "step": 14050
    },
    {
      "epoch": 0.853828869860934,
      "grad_norm": 2.1794958114624023,
      "learning_rate": 8.130996546721462e-05,
      "loss": 2.8344,
      "step": 14060
    },
    {
      "epoch": 0.8544361450173074,
      "grad_norm": 2.569192886352539,
      "learning_rate": 8.128516830745058e-05,
      "loss": 2.6641,
      "step": 14070
    },
    {
      "epoch": 0.8550434201736807,
      "grad_norm": 2.5195233821868896,
      "learning_rate": 8.126035849546623e-05,
      "loss": 2.595,
      "step": 14080
    },
    {
      "epoch": 0.855650695330054,
      "grad_norm": 6.2418131828308105,
      "learning_rate": 8.123553604129504e-05,
      "loss": 2.391,
      "step": 14090
    },
    {
      "epoch": 0.8562579704864274,
      "grad_norm": 3.816922187805176,
      "learning_rate": 8.12107009549756e-05,
      "loss": 2.8766,
      "step": 14100
    },
    {
      "epoch": 0.8568652456428008,
      "grad_norm": 4.034281253814697,
      "learning_rate": 8.118585324655161e-05,
      "loss": 2.4108,
      "step": 14110
    },
    {
      "epoch": 0.8574725207991741,
      "grad_norm": 2.6012611389160156,
      "learning_rate": 8.116099292607189e-05,
      "loss": 2.6098,
      "step": 14120
    },
    {
      "epoch": 0.8580797959555475,
      "grad_norm": 2.745487689971924,
      "learning_rate": 8.113612000359036e-05,
      "loss": 2.6292,
      "step": 14130
    },
    {
      "epoch": 0.8586870711119208,
      "grad_norm": 2.59731125831604,
      "learning_rate": 8.111123448916602e-05,
      "loss": 2.6749,
      "step": 14140
    },
    {
      "epoch": 0.8592943462682942,
      "grad_norm": 3.3899283409118652,
      "learning_rate": 8.108633639286294e-05,
      "loss": 3.0304,
      "step": 14150
    },
    {
      "epoch": 0.8599016214246675,
      "grad_norm": 2.629342794418335,
      "learning_rate": 8.106142572475035e-05,
      "loss": 2.6691,
      "step": 14160
    },
    {
      "epoch": 0.8605088965810409,
      "grad_norm": 3.308920383453369,
      "learning_rate": 8.103650249490249e-05,
      "loss": 2.5422,
      "step": 14170
    },
    {
      "epoch": 0.8611161717374142,
      "grad_norm": 2.389808416366577,
      "learning_rate": 8.101156671339873e-05,
      "loss": 2.4573,
      "step": 14180
    },
    {
      "epoch": 0.8617234468937875,
      "grad_norm": 3.3948938846588135,
      "learning_rate": 8.098661839032349e-05,
      "loss": 2.2096,
      "step": 14190
    },
    {
      "epoch": 0.8623307220501609,
      "grad_norm": 2.498847007751465,
      "learning_rate": 8.096165753576625e-05,
      "loss": 2.2042,
      "step": 14200
    },
    {
      "epoch": 0.8629379972065343,
      "grad_norm": 1.3209956884384155,
      "learning_rate": 8.093668415982161e-05,
      "loss": 2.3115,
      "step": 14210
    },
    {
      "epoch": 0.8635452723629077,
      "grad_norm": 2.235952854156494,
      "learning_rate": 8.091169827258918e-05,
      "loss": 2.6845,
      "step": 14220
    },
    {
      "epoch": 0.864152547519281,
      "grad_norm": 2.3516621589660645,
      "learning_rate": 8.088669988417366e-05,
      "loss": 2.6627,
      "step": 14230
    },
    {
      "epoch": 0.8647598226756543,
      "grad_norm": 2.418454885482788,
      "learning_rate": 8.086168900468478e-05,
      "loss": 2.8301,
      "step": 14240
    },
    {
      "epoch": 0.8653670978320277,
      "grad_norm": 3.9909777641296387,
      "learning_rate": 8.083666564423736e-05,
      "loss": 2.4873,
      "step": 14250
    },
    {
      "epoch": 0.8659743729884011,
      "grad_norm": 1.8619825839996338,
      "learning_rate": 8.081162981295123e-05,
      "loss": 2.3778,
      "step": 14260
    },
    {
      "epoch": 0.8665816481447745,
      "grad_norm": 2.5034406185150146,
      "learning_rate": 8.07865815209513e-05,
      "loss": 2.6619,
      "step": 14270
    },
    {
      "epoch": 0.8671889233011477,
      "grad_norm": 2.7615697383880615,
      "learning_rate": 8.076152077836747e-05,
      "loss": 2.8079,
      "step": 14280
    },
    {
      "epoch": 0.8677961984575211,
      "grad_norm": 2.6887967586517334,
      "learning_rate": 8.073644759533473e-05,
      "loss": 2.6655,
      "step": 14290
    },
    {
      "epoch": 0.8684034736138945,
      "grad_norm": 2.1175453662872314,
      "learning_rate": 8.071136198199305e-05,
      "loss": 2.4962,
      "step": 14300
    },
    {
      "epoch": 0.8690107487702678,
      "grad_norm": 2.6229300498962402,
      "learning_rate": 8.068626394848748e-05,
      "loss": 2.8142,
      "step": 14310
    },
    {
      "epoch": 0.8696180239266411,
      "grad_norm": 2.8107903003692627,
      "learning_rate": 8.066115350496802e-05,
      "loss": 2.7406,
      "step": 14320
    },
    {
      "epoch": 0.8702252990830145,
      "grad_norm": 2.3131282329559326,
      "learning_rate": 8.063603066158978e-05,
      "loss": 2.3288,
      "step": 14330
    },
    {
      "epoch": 0.8708325742393879,
      "grad_norm": 2.349074363708496,
      "learning_rate": 8.06108954285128e-05,
      "loss": 2.5306,
      "step": 14340
    },
    {
      "epoch": 0.8714398493957612,
      "grad_norm": 1.3907008171081543,
      "learning_rate": 8.058574781590221e-05,
      "loss": 2.3337,
      "step": 14350
    },
    {
      "epoch": 0.8720471245521346,
      "grad_norm": 2.056511878967285,
      "learning_rate": 8.056058783392807e-05,
      "loss": 2.5289,
      "step": 14360
    },
    {
      "epoch": 0.8726543997085079,
      "grad_norm": 2.934088706970215,
      "learning_rate": 8.053541549276549e-05,
      "loss": 2.6579,
      "step": 14370
    },
    {
      "epoch": 0.8732616748648813,
      "grad_norm": 3.036989450454712,
      "learning_rate": 8.051023080259459e-05,
      "loss": 2.5616,
      "step": 14380
    },
    {
      "epoch": 0.8738689500212546,
      "grad_norm": 2.241931676864624,
      "learning_rate": 8.04850337736004e-05,
      "loss": 2.8967,
      "step": 14390
    },
    {
      "epoch": 0.874476225177628,
      "grad_norm": 2.8939216136932373,
      "learning_rate": 8.045982441597307e-05,
      "loss": 2.7606,
      "step": 14400
    },
    {
      "epoch": 0.8750835003340013,
      "grad_norm": 2.3644866943359375,
      "learning_rate": 8.04346027399076e-05,
      "loss": 2.4792,
      "step": 14410
    },
    {
      "epoch": 0.8756907754903747,
      "grad_norm": 1.5839014053344727,
      "learning_rate": 8.040936875560408e-05,
      "loss": 2.3881,
      "step": 14420
    },
    {
      "epoch": 0.876298050646748,
      "grad_norm": 1.9949836730957031,
      "learning_rate": 8.038412247326752e-05,
      "loss": 2.1448,
      "step": 14430
    },
    {
      "epoch": 0.8769053258031214,
      "grad_norm": 2.379394769668579,
      "learning_rate": 8.035886390310792e-05,
      "loss": 2.6641,
      "step": 14440
    },
    {
      "epoch": 0.8775126009594948,
      "grad_norm": 2.1171581745147705,
      "learning_rate": 8.033359305534025e-05,
      "loss": 2.6009,
      "step": 14450
    },
    {
      "epoch": 0.8781198761158681,
      "grad_norm": 2.6411945819854736,
      "learning_rate": 8.030830994018446e-05,
      "loss": 2.3802,
      "step": 14460
    },
    {
      "epoch": 0.8787271512722414,
      "grad_norm": 2.3550329208374023,
      "learning_rate": 8.02830145678654e-05,
      "loss": 2.5315,
      "step": 14470
    },
    {
      "epoch": 0.8793344264286148,
      "grad_norm": 2.068042755126953,
      "learning_rate": 8.025770694861299e-05,
      "loss": 2.4475,
      "step": 14480
    },
    {
      "epoch": 0.8799417015849882,
      "grad_norm": 2.587454319000244,
      "learning_rate": 8.023238709266196e-05,
      "loss": 2.3492,
      "step": 14490
    },
    {
      "epoch": 0.8805489767413615,
      "grad_norm": 1.9671556949615479,
      "learning_rate": 8.02070550102521e-05,
      "loss": 2.0328,
      "step": 14500
    },
    {
      "epoch": 0.8811562518977348,
      "grad_norm": 2.1339352130889893,
      "learning_rate": 8.018171071162812e-05,
      "loss": 2.3576,
      "step": 14510
    },
    {
      "epoch": 0.8817635270541082,
      "grad_norm": 2.5772972106933594,
      "learning_rate": 8.015635420703963e-05,
      "loss": 2.9391,
      "step": 14520
    },
    {
      "epoch": 0.8823708022104816,
      "grad_norm": 3.4716503620147705,
      "learning_rate": 8.013098550674122e-05,
      "loss": 2.486,
      "step": 14530
    },
    {
      "epoch": 0.882978077366855,
      "grad_norm": 3.7748708724975586,
      "learning_rate": 8.010560462099238e-05,
      "loss": 2.7982,
      "step": 14540
    },
    {
      "epoch": 0.8835853525232282,
      "grad_norm": 2.512662172317505,
      "learning_rate": 8.008021156005758e-05,
      "loss": 2.351,
      "step": 14550
    },
    {
      "epoch": 0.8841926276796016,
      "grad_norm": 2.4649572372436523,
      "learning_rate": 8.005480633420614e-05,
      "loss": 2.8414,
      "step": 14560
    },
    {
      "epoch": 0.884799902835975,
      "grad_norm": 2.062098741531372,
      "learning_rate": 8.002938895371237e-05,
      "loss": 2.6511,
      "step": 14570
    },
    {
      "epoch": 0.8854071779923484,
      "grad_norm": 2.082336187362671,
      "learning_rate": 8.000395942885543e-05,
      "loss": 2.0328,
      "step": 14580
    },
    {
      "epoch": 0.8860144531487217,
      "grad_norm": 1.8998624086380005,
      "learning_rate": 7.997851776991945e-05,
      "loss": 2.7328,
      "step": 14590
    },
    {
      "epoch": 0.886621728305095,
      "grad_norm": 1.8148220777511597,
      "learning_rate": 7.995306398719342e-05,
      "loss": 2.7259,
      "step": 14600
    },
    {
      "epoch": 0.8872290034614684,
      "grad_norm": 4.7422261238098145,
      "learning_rate": 7.992759809097128e-05,
      "loss": 2.885,
      "step": 14610
    },
    {
      "epoch": 0.8878362786178418,
      "grad_norm": 2.7476389408111572,
      "learning_rate": 7.990212009155184e-05,
      "loss": 2.8535,
      "step": 14620
    },
    {
      "epoch": 0.8884435537742151,
      "grad_norm": 2.571005344390869,
      "learning_rate": 7.987662999923879e-05,
      "loss": 2.9545,
      "step": 14630
    },
    {
      "epoch": 0.8890508289305884,
      "grad_norm": 2.6924760341644287,
      "learning_rate": 7.985112782434074e-05,
      "loss": 2.374,
      "step": 14640
    },
    {
      "epoch": 0.8896581040869618,
      "grad_norm": 2.4890646934509277,
      "learning_rate": 7.98256135771712e-05,
      "loss": 2.5333,
      "step": 14650
    },
    {
      "epoch": 0.8902653792433352,
      "grad_norm": 2.379798650741577,
      "learning_rate": 7.980008726804849e-05,
      "loss": 2.6687,
      "step": 14660
    },
    {
      "epoch": 0.8908726543997085,
      "grad_norm": 2.417634963989258,
      "learning_rate": 7.97745489072959e-05,
      "loss": 2.3298,
      "step": 14670
    },
    {
      "epoch": 0.8914799295560819,
      "grad_norm": 2.517517328262329,
      "learning_rate": 7.974899850524151e-05,
      "loss": 2.5721,
      "step": 14680
    },
    {
      "epoch": 0.8920872047124552,
      "grad_norm": 1.8741064071655273,
      "learning_rate": 7.972343607221836e-05,
      "loss": 2.3886,
      "step": 14690
    },
    {
      "epoch": 0.8926944798688285,
      "grad_norm": 2.290876626968384,
      "learning_rate": 7.969786161856424e-05,
      "loss": 2.7699,
      "step": 14700
    },
    {
      "epoch": 0.8933017550252019,
      "grad_norm": 3.9234490394592285,
      "learning_rate": 7.967227515462191e-05,
      "loss": 2.6229,
      "step": 14710
    },
    {
      "epoch": 0.8939090301815753,
      "grad_norm": 2.376491069793701,
      "learning_rate": 7.964667669073894e-05,
      "loss": 2.4501,
      "step": 14720
    },
    {
      "epoch": 0.8945163053379486,
      "grad_norm": 3.05165696144104,
      "learning_rate": 7.962106623726775e-05,
      "loss": 2.426,
      "step": 14730
    },
    {
      "epoch": 0.895123580494322,
      "grad_norm": 1.9079333543777466,
      "learning_rate": 7.959544380456563e-05,
      "loss": 2.2505,
      "step": 14740
    },
    {
      "epoch": 0.8957308556506953,
      "grad_norm": 2.0356626510620117,
      "learning_rate": 7.956980940299467e-05,
      "loss": 2.5389,
      "step": 14750
    },
    {
      "epoch": 0.8963381308070687,
      "grad_norm": 2.4736993312835693,
      "learning_rate": 7.954416304292185e-05,
      "loss": 2.7243,
      "step": 14760
    },
    {
      "epoch": 0.8969454059634421,
      "grad_norm": 2.5699164867401123,
      "learning_rate": 7.951850473471897e-05,
      "loss": 2.521,
      "step": 14770
    },
    {
      "epoch": 0.8975526811198153,
      "grad_norm": 3.0978384017944336,
      "learning_rate": 7.949283448876264e-05,
      "loss": 3.0472,
      "step": 14780
    },
    {
      "epoch": 0.8981599562761887,
      "grad_norm": 2.895653486251831,
      "learning_rate": 7.946715231543434e-05,
      "loss": 2.9597,
      "step": 14790
    },
    {
      "epoch": 0.8987672314325621,
      "grad_norm": 3.332357406616211,
      "learning_rate": 7.944145822512034e-05,
      "loss": 3.0028,
      "step": 14800
    },
    {
      "epoch": 0.8993745065889355,
      "grad_norm": 2.5046966075897217,
      "learning_rate": 7.941575222821171e-05,
      "loss": 2.565,
      "step": 14810
    },
    {
      "epoch": 0.8999817817453089,
      "grad_norm": 1.5438228845596313,
      "learning_rate": 7.939003433510442e-05,
      "loss": 2.3253,
      "step": 14820
    },
    {
      "epoch": 0.9005890569016821,
      "grad_norm": 1.7415012121200562,
      "learning_rate": 7.936430455619917e-05,
      "loss": 2.8228,
      "step": 14830
    },
    {
      "epoch": 0.9011963320580555,
      "grad_norm": 2.1517465114593506,
      "learning_rate": 7.933856290190149e-05,
      "loss": 2.3897,
      "step": 14840
    },
    {
      "epoch": 0.9018036072144289,
      "grad_norm": 2.473191738128662,
      "learning_rate": 7.931280938262169e-05,
      "loss": 2.1578,
      "step": 14850
    },
    {
      "epoch": 0.9024108823708022,
      "grad_norm": 1.8050251007080078,
      "learning_rate": 7.928704400877495e-05,
      "loss": 2.4338,
      "step": 14860
    },
    {
      "epoch": 0.9030181575271755,
      "grad_norm": 2.5782124996185303,
      "learning_rate": 7.926126679078116e-05,
      "loss": 2.571,
      "step": 14870
    },
    {
      "epoch": 0.9036254326835489,
      "grad_norm": 1.972538709640503,
      "learning_rate": 7.923547773906507e-05,
      "loss": 2.748,
      "step": 14880
    },
    {
      "epoch": 0.9042327078399223,
      "grad_norm": 3.678603172302246,
      "learning_rate": 7.920967686405616e-05,
      "loss": 2.9295,
      "step": 14890
    },
    {
      "epoch": 0.9048399829962956,
      "grad_norm": 4.780652046203613,
      "learning_rate": 7.918386417618872e-05,
      "loss": 2.778,
      "step": 14900
    },
    {
      "epoch": 0.905447258152669,
      "grad_norm": 2.3860175609588623,
      "learning_rate": 7.915803968590181e-05,
      "loss": 2.4125,
      "step": 14910
    },
    {
      "epoch": 0.9060545333090423,
      "grad_norm": 8.559432029724121,
      "learning_rate": 7.913220340363927e-05,
      "loss": 2.392,
      "step": 14920
    },
    {
      "epoch": 0.9066618084654157,
      "grad_norm": 3.12302827835083,
      "learning_rate": 7.91063553398497e-05,
      "loss": 2.4423,
      "step": 14930
    },
    {
      "epoch": 0.907269083621789,
      "grad_norm": 2.6987040042877197,
      "learning_rate": 7.908049550498648e-05,
      "loss": 2.4704,
      "step": 14940
    },
    {
      "epoch": 0.9078763587781624,
      "grad_norm": 2.7523820400238037,
      "learning_rate": 7.905462390950772e-05,
      "loss": 2.2398,
      "step": 14950
    },
    {
      "epoch": 0.9084836339345357,
      "grad_norm": 4.242203235626221,
      "learning_rate": 7.902874056387633e-05,
      "loss": 2.6613,
      "step": 14960
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.059253215789795,
      "learning_rate": 7.900284547855991e-05,
      "loss": 2.3792,
      "step": 14970
    },
    {
      "epoch": 0.9096981842472824,
      "grad_norm": 3.505707263946533,
      "learning_rate": 7.897693866403089e-05,
      "loss": 2.9219,
      "step": 14980
    },
    {
      "epoch": 0.9103054594036558,
      "grad_norm": 3.96840500831604,
      "learning_rate": 7.895102013076637e-05,
      "loss": 2.9001,
      "step": 14990
    },
    {
      "epoch": 0.9109127345600292,
      "grad_norm": 2.3061699867248535,
      "learning_rate": 7.892508988924822e-05,
      "loss": 2.9996,
      "step": 15000
    },
    {
      "epoch": 0.9109127345600292,
      "eval_loss": 4.545451641082764,
      "eval_runtime": 2143.481,
      "eval_samples_per_second": 7.682,
      "eval_steps_per_second": 1.921,
      "step": 15000
    },
    {
      "epoch": 0.9115200097164025,
      "grad_norm": 2.818756341934204,
      "learning_rate": 7.889914794996304e-05,
      "loss": 4.0734,
      "step": 15010
    },
    {
      "epoch": 0.9121272848727758,
      "grad_norm": 3.187847375869751,
      "learning_rate": 7.887319432340218e-05,
      "loss": 3.4388,
      "step": 15020
    },
    {
      "epoch": 0.9127345600291492,
      "grad_norm": 2.3311924934387207,
      "learning_rate": 7.884722902006168e-05,
      "loss": 2.7703,
      "step": 15030
    },
    {
      "epoch": 0.9133418351855226,
      "grad_norm": 2.9243855476379395,
      "learning_rate": 7.882125205044234e-05,
      "loss": 2.6534,
      "step": 15040
    },
    {
      "epoch": 0.913949110341896,
      "grad_norm": 2.5576345920562744,
      "learning_rate": 7.879526342504964e-05,
      "loss": 2.5157,
      "step": 15050
    },
    {
      "epoch": 0.9145563854982692,
      "grad_norm": 3.529256820678711,
      "learning_rate": 7.876926315439382e-05,
      "loss": 2.2938,
      "step": 15060
    },
    {
      "epoch": 0.9151636606546426,
      "grad_norm": 3.987426519393921,
      "learning_rate": 7.874325124898978e-05,
      "loss": 2.6304,
      "step": 15070
    },
    {
      "epoch": 0.915770935811016,
      "grad_norm": 3.6875784397125244,
      "learning_rate": 7.871722771935716e-05,
      "loss": 2.6538,
      "step": 15080
    },
    {
      "epoch": 0.9163782109673894,
      "grad_norm": 4.38572883605957,
      "learning_rate": 7.86911925760203e-05,
      "loss": 2.5464,
      "step": 15090
    },
    {
      "epoch": 0.9169854861237626,
      "grad_norm": 2.5597400665283203,
      "learning_rate": 7.86651458295082e-05,
      "loss": 2.7728,
      "step": 15100
    },
    {
      "epoch": 0.917592761280136,
      "grad_norm": 3.8266263008117676,
      "learning_rate": 7.86390874903546e-05,
      "loss": 2.9848,
      "step": 15110
    },
    {
      "epoch": 0.9182000364365094,
      "grad_norm": 3.2809934616088867,
      "learning_rate": 7.861301756909791e-05,
      "loss": 3.0384,
      "step": 15120
    },
    {
      "epoch": 0.9188073115928828,
      "grad_norm": 2.6841819286346436,
      "learning_rate": 7.858693607628121e-05,
      "loss": 2.7912,
      "step": 15130
    },
    {
      "epoch": 0.9194145867492561,
      "grad_norm": 3.2330892086029053,
      "learning_rate": 7.856084302245225e-05,
      "loss": 2.5797,
      "step": 15140
    },
    {
      "epoch": 0.9200218619056294,
      "grad_norm": 2.388387441635132,
      "learning_rate": 7.853473841816353e-05,
      "loss": 2.6376,
      "step": 15150
    },
    {
      "epoch": 0.9206291370620028,
      "grad_norm": 3.3510892391204834,
      "learning_rate": 7.850862227397213e-05,
      "loss": 2.3561,
      "step": 15160
    },
    {
      "epoch": 0.9212364122183762,
      "grad_norm": 2.2813687324523926,
      "learning_rate": 7.848249460043984e-05,
      "loss": 2.6046,
      "step": 15170
    },
    {
      "epoch": 0.9218436873747495,
      "grad_norm": 1.9930171966552734,
      "learning_rate": 7.84563554081331e-05,
      "loss": 2.4686,
      "step": 15180
    },
    {
      "epoch": 0.9224509625311228,
      "grad_norm": 1.2043567895889282,
      "learning_rate": 7.843020470762305e-05,
      "loss": 2.07,
      "step": 15190
    },
    {
      "epoch": 0.9230582376874962,
      "grad_norm": 1.862377643585205,
      "learning_rate": 7.84040425094854e-05,
      "loss": 2.1918,
      "step": 15200
    },
    {
      "epoch": 0.9236655128438696,
      "grad_norm": 1.951730489730835,
      "learning_rate": 7.83778688243006e-05,
      "loss": 2.308,
      "step": 15210
    },
    {
      "epoch": 0.9242727880002429,
      "grad_norm": 2.489386796951294,
      "learning_rate": 7.835168366265368e-05,
      "loss": 2.6435,
      "step": 15220
    },
    {
      "epoch": 0.9248800631566163,
      "grad_norm": 2.448267936706543,
      "learning_rate": 7.832548703513436e-05,
      "loss": 2.5487,
      "step": 15230
    },
    {
      "epoch": 0.9254873383129896,
      "grad_norm": 3.3651440143585205,
      "learning_rate": 7.829927895233695e-05,
      "loss": 2.6041,
      "step": 15240
    },
    {
      "epoch": 0.926094613469363,
      "grad_norm": 2.9471521377563477,
      "learning_rate": 7.827305942486043e-05,
      "loss": 2.5782,
      "step": 15250
    },
    {
      "epoch": 0.9267018886257363,
      "grad_norm": 6.139317989349365,
      "learning_rate": 7.824682846330837e-05,
      "loss": 2.5439,
      "step": 15260
    },
    {
      "epoch": 0.9273091637821097,
      "grad_norm": 4.973601341247559,
      "learning_rate": 7.822058607828904e-05,
      "loss": 3.0704,
      "step": 15270
    },
    {
      "epoch": 0.9279164389384831,
      "grad_norm": 2.7332611083984375,
      "learning_rate": 7.819433228041524e-05,
      "loss": 2.7555,
      "step": 15280
    },
    {
      "epoch": 0.9285237140948563,
      "grad_norm": 2.5334632396698,
      "learning_rate": 7.816806708030442e-05,
      "loss": 2.2437,
      "step": 15290
    },
    {
      "epoch": 0.9291309892512297,
      "grad_norm": 2.223763942718506,
      "learning_rate": 7.814179048857866e-05,
      "loss": 2.9005,
      "step": 15300
    },
    {
      "epoch": 0.9297382644076031,
      "grad_norm": 4.0428972244262695,
      "learning_rate": 7.811550251586463e-05,
      "loss": 2.599,
      "step": 15310
    },
    {
      "epoch": 0.9303455395639765,
      "grad_norm": 4.429723262786865,
      "learning_rate": 7.808920317279362e-05,
      "loss": 2.4823,
      "step": 15320
    },
    {
      "epoch": 0.9309528147203497,
      "grad_norm": 3.454373836517334,
      "learning_rate": 7.806289247000146e-05,
      "loss": 2.4955,
      "step": 15330
    },
    {
      "epoch": 0.9315600898767231,
      "grad_norm": 2.6288821697235107,
      "learning_rate": 7.803657041812866e-05,
      "loss": 3.0184,
      "step": 15340
    },
    {
      "epoch": 0.9321673650330965,
      "grad_norm": 4.6979756355285645,
      "learning_rate": 7.801023702782027e-05,
      "loss": 2.9868,
      "step": 15350
    },
    {
      "epoch": 0.9327746401894699,
      "grad_norm": 3.9723870754241943,
      "learning_rate": 7.798389230972592e-05,
      "loss": 2.7455,
      "step": 15360
    },
    {
      "epoch": 0.9333819153458432,
      "grad_norm": 2.615980386734009,
      "learning_rate": 7.795753627449982e-05,
      "loss": 2.6869,
      "step": 15370
    },
    {
      "epoch": 0.9339891905022165,
      "grad_norm": 2.721989393234253,
      "learning_rate": 7.79311689328008e-05,
      "loss": 2.3231,
      "step": 15380
    },
    {
      "epoch": 0.9345964656585899,
      "grad_norm": 2.605137825012207,
      "learning_rate": 7.790479029529221e-05,
      "loss": 2.7783,
      "step": 15390
    },
    {
      "epoch": 0.9352037408149633,
      "grad_norm": 3.501462936401367,
      "learning_rate": 7.787840037264202e-05,
      "loss": 3.0557,
      "step": 15400
    },
    {
      "epoch": 0.9358110159713366,
      "grad_norm": 2.8267807960510254,
      "learning_rate": 7.78519991755227e-05,
      "loss": 2.425,
      "step": 15410
    },
    {
      "epoch": 0.9364182911277099,
      "grad_norm": 2.622826099395752,
      "learning_rate": 7.782558671461134e-05,
      "loss": 2.41,
      "step": 15420
    },
    {
      "epoch": 0.9370255662840833,
      "grad_norm": 3.012139320373535,
      "learning_rate": 7.779916300058955e-05,
      "loss": 2.5881,
      "step": 15430
    },
    {
      "epoch": 0.9376328414404567,
      "grad_norm": 5.130153179168701,
      "learning_rate": 7.77727280441435e-05,
      "loss": 2.5863,
      "step": 15440
    },
    {
      "epoch": 0.93824011659683,
      "grad_norm": 2.1754872798919678,
      "learning_rate": 7.774628185596391e-05,
      "loss": 2.7808,
      "step": 15450
    },
    {
      "epoch": 0.9388473917532034,
      "grad_norm": 3.039182424545288,
      "learning_rate": 7.771982444674604e-05,
      "loss": 3.0081,
      "step": 15460
    },
    {
      "epoch": 0.9394546669095767,
      "grad_norm": 4.128143310546875,
      "learning_rate": 7.769335582718968e-05,
      "loss": 2.5113,
      "step": 15470
    },
    {
      "epoch": 0.9400619420659501,
      "grad_norm": 1.9872045516967773,
      "learning_rate": 7.766687600799918e-05,
      "loss": 2.4338,
      "step": 15480
    },
    {
      "epoch": 0.9406692172223234,
      "grad_norm": 2.0867507457733154,
      "learning_rate": 7.764038499988338e-05,
      "loss": 2.3794,
      "step": 15490
    },
    {
      "epoch": 0.9412764923786968,
      "grad_norm": 2.5744786262512207,
      "learning_rate": 7.761388281355569e-05,
      "loss": 2.4786,
      "step": 15500
    },
    {
      "epoch": 0.9418837675350702,
      "grad_norm": 3.383315324783325,
      "learning_rate": 7.7587369459734e-05,
      "loss": 2.3553,
      "step": 15510
    },
    {
      "epoch": 0.9424910426914435,
      "grad_norm": 2.7532711029052734,
      "learning_rate": 7.75608449491407e-05,
      "loss": 2.9316,
      "step": 15520
    },
    {
      "epoch": 0.9430983178478168,
      "grad_norm": 2.5905752182006836,
      "learning_rate": 7.753430929250278e-05,
      "loss": 2.7622,
      "step": 15530
    },
    {
      "epoch": 0.9437055930041902,
      "grad_norm": 1.9110822677612305,
      "learning_rate": 7.750776250055165e-05,
      "loss": 2.4103,
      "step": 15540
    },
    {
      "epoch": 0.9443128681605636,
      "grad_norm": 1.4409369230270386,
      "learning_rate": 7.748120458402328e-05,
      "loss": 2.4121,
      "step": 15550
    },
    {
      "epoch": 0.9449201433169369,
      "grad_norm": 2.2000420093536377,
      "learning_rate": 7.745463555365809e-05,
      "loss": 2.4437,
      "step": 15560
    },
    {
      "epoch": 0.9455274184733102,
      "grad_norm": 1.9475761651992798,
      "learning_rate": 7.742805542020104e-05,
      "loss": 2.9017,
      "step": 15570
    },
    {
      "epoch": 0.9461346936296836,
      "grad_norm": 1.8525832891464233,
      "learning_rate": 7.740146419440153e-05,
      "loss": 2.5027,
      "step": 15580
    },
    {
      "epoch": 0.946741968786057,
      "grad_norm": 3.7798030376434326,
      "learning_rate": 7.737486188701351e-05,
      "loss": 2.6016,
      "step": 15590
    },
    {
      "epoch": 0.9473492439424304,
      "grad_norm": 4.240975856781006,
      "learning_rate": 7.734824850879535e-05,
      "loss": 2.5943,
      "step": 15600
    },
    {
      "epoch": 0.9479565190988036,
      "grad_norm": 5.773946285247803,
      "learning_rate": 7.732162407050994e-05,
      "loss": 2.6034,
      "step": 15610
    },
    {
      "epoch": 0.948563794255177,
      "grad_norm": 4.732071399688721,
      "learning_rate": 7.729498858292461e-05,
      "loss": 2.7066,
      "step": 15620
    },
    {
      "epoch": 0.9491710694115504,
      "grad_norm": 3.123365640640259,
      "learning_rate": 7.72683420568112e-05,
      "loss": 2.5288,
      "step": 15630
    },
    {
      "epoch": 0.9497783445679238,
      "grad_norm": 3.0149495601654053,
      "learning_rate": 7.724168450294597e-05,
      "loss": 2.626,
      "step": 15640
    },
    {
      "epoch": 0.950385619724297,
      "grad_norm": 4.829661846160889,
      "learning_rate": 7.721501593210967e-05,
      "loss": 2.5694,
      "step": 15650
    },
    {
      "epoch": 0.9509928948806704,
      "grad_norm": 4.79658317565918,
      "learning_rate": 7.718833635508748e-05,
      "loss": 2.1978,
      "step": 15660
    },
    {
      "epoch": 0.9516001700370438,
      "grad_norm": 6.594405174255371,
      "learning_rate": 7.716164578266907e-05,
      "loss": 2.4431,
      "step": 15670
    },
    {
      "epoch": 0.9522074451934172,
      "grad_norm": 2.6498844623565674,
      "learning_rate": 7.71349442256485e-05,
      "loss": 2.6841,
      "step": 15680
    },
    {
      "epoch": 0.9528147203497905,
      "grad_norm": 3.149005889892578,
      "learning_rate": 7.710823169482433e-05,
      "loss": 2.7773,
      "step": 15690
    },
    {
      "epoch": 0.9534219955061638,
      "grad_norm": 4.206252098083496,
      "learning_rate": 7.708150820099953e-05,
      "loss": 2.7162,
      "step": 15700
    },
    {
      "epoch": 0.9540292706625372,
      "grad_norm": 2.7239675521850586,
      "learning_rate": 7.705477375498148e-05,
      "loss": 3.1155,
      "step": 15710
    },
    {
      "epoch": 0.9546365458189106,
      "grad_norm": 3.421138286590576,
      "learning_rate": 7.702802836758207e-05,
      "loss": 2.798,
      "step": 15720
    },
    {
      "epoch": 0.9552438209752839,
      "grad_norm": 5.001954555511475,
      "learning_rate": 7.700127204961752e-05,
      "loss": 2.39,
      "step": 15730
    },
    {
      "epoch": 0.9558510961316573,
      "grad_norm": 1.719947099685669,
      "learning_rate": 7.697450481190851e-05,
      "loss": 2.8295,
      "step": 15740
    },
    {
      "epoch": 0.9564583712880306,
      "grad_norm": 1.9332243204116821,
      "learning_rate": 7.694772666528016e-05,
      "loss": 2.6504,
      "step": 15750
    },
    {
      "epoch": 0.957065646444404,
      "grad_norm": 3.0003116130828857,
      "learning_rate": 7.692093762056196e-05,
      "loss": 2.6804,
      "step": 15760
    },
    {
      "epoch": 0.9576729216007773,
      "grad_norm": 3.8981382846832275,
      "learning_rate": 7.689413768858783e-05,
      "loss": 3.0236,
      "step": 15770
    },
    {
      "epoch": 0.9582801967571507,
      "grad_norm": 2.998016595840454,
      "learning_rate": 7.686732688019611e-05,
      "loss": 2.6289,
      "step": 15780
    },
    {
      "epoch": 0.958887471913524,
      "grad_norm": 3.464873790740967,
      "learning_rate": 7.684050520622947e-05,
      "loss": 2.6314,
      "step": 15790
    },
    {
      "epoch": 0.9594947470698973,
      "grad_norm": 1.3364366292953491,
      "learning_rate": 7.681367267753508e-05,
      "loss": 2.3234,
      "step": 15800
    },
    {
      "epoch": 0.9601020222262707,
      "grad_norm": 2.254599094390869,
      "learning_rate": 7.678682930496439e-05,
      "loss": 2.5988,
      "step": 15810
    },
    {
      "epoch": 0.9607092973826441,
      "grad_norm": 2.222191095352173,
      "learning_rate": 7.675997509937333e-05,
      "loss": 2.7497,
      "step": 15820
    },
    {
      "epoch": 0.9613165725390175,
      "grad_norm": 4.258634567260742,
      "learning_rate": 7.673311007162214e-05,
      "loss": 2.6471,
      "step": 15830
    },
    {
      "epoch": 0.9619238476953907,
      "grad_norm": 3.144561290740967,
      "learning_rate": 7.670623423257548e-05,
      "loss": 2.5082,
      "step": 15840
    },
    {
      "epoch": 0.9625311228517641,
      "grad_norm": 2.7921102046966553,
      "learning_rate": 7.667934759310236e-05,
      "loss": 2.3927,
      "step": 15850
    },
    {
      "epoch": 0.9631383980081375,
      "grad_norm": 2.4249026775360107,
      "learning_rate": 7.665245016407617e-05,
      "loss": 2.7323,
      "step": 15860
    },
    {
      "epoch": 0.9637456731645109,
      "grad_norm": 2.797694683074951,
      "learning_rate": 7.662554195637464e-05,
      "loss": 2.557,
      "step": 15870
    },
    {
      "epoch": 0.9643529483208841,
      "grad_norm": 3.417673349380493,
      "learning_rate": 7.659862298087991e-05,
      "loss": 2.5272,
      "step": 15880
    },
    {
      "epoch": 0.9649602234772575,
      "grad_norm": 2.326739549636841,
      "learning_rate": 7.657169324847842e-05,
      "loss": 2.5518,
      "step": 15890
    },
    {
      "epoch": 0.9655674986336309,
      "grad_norm": 3.1135358810424805,
      "learning_rate": 7.654475277006098e-05,
      "loss": 3.137,
      "step": 15900
    },
    {
      "epoch": 0.9661747737900043,
      "grad_norm": 9.474428176879883,
      "learning_rate": 7.651780155652277e-05,
      "loss": 2.6408,
      "step": 15910
    },
    {
      "epoch": 0.9667820489463776,
      "grad_norm": 2.4326260089874268,
      "learning_rate": 7.649083961876329e-05,
      "loss": 2.7547,
      "step": 15920
    },
    {
      "epoch": 0.9673893241027509,
      "grad_norm": 5.971506118774414,
      "learning_rate": 7.646386696768635e-05,
      "loss": 2.6662,
      "step": 15930
    },
    {
      "epoch": 0.9679965992591243,
      "grad_norm": 2.768423318862915,
      "learning_rate": 7.643688361420014e-05,
      "loss": 2.6572,
      "step": 15940
    },
    {
      "epoch": 0.9686038744154977,
      "grad_norm": 2.522735118865967,
      "learning_rate": 7.640988956921715e-05,
      "loss": 2.9743,
      "step": 15950
    },
    {
      "epoch": 0.969211149571871,
      "grad_norm": 3.3215818405151367,
      "learning_rate": 7.63828848436542e-05,
      "loss": 2.3609,
      "step": 15960
    },
    {
      "epoch": 0.9698184247282444,
      "grad_norm": 3.981889247894287,
      "learning_rate": 7.635586944843242e-05,
      "loss": 2.5037,
      "step": 15970
    },
    {
      "epoch": 0.9704256998846177,
      "grad_norm": 1.9520206451416016,
      "learning_rate": 7.63288433944773e-05,
      "loss": 2.821,
      "step": 15980
    },
    {
      "epoch": 0.9710329750409911,
      "grad_norm": 3.2809367179870605,
      "learning_rate": 7.630180669271859e-05,
      "loss": 2.5177,
      "step": 15990
    },
    {
      "epoch": 0.9716402501973644,
      "grad_norm": 4.121602535247803,
      "learning_rate": 7.627475935409034e-05,
      "loss": 2.8042,
      "step": 16000
    },
    {
      "epoch": 0.9722475253537378,
      "grad_norm": 2.966729164123535,
      "learning_rate": 7.624770138953097e-05,
      "loss": 2.1284,
      "step": 16010
    },
    {
      "epoch": 0.9728548005101111,
      "grad_norm": 2.6385018825531006,
      "learning_rate": 7.622063280998311e-05,
      "loss": 2.6058,
      "step": 16020
    },
    {
      "epoch": 0.9734620756664845,
      "grad_norm": 2.2218170166015625,
      "learning_rate": 7.619355362639376e-05,
      "loss": 2.5581,
      "step": 16030
    },
    {
      "epoch": 0.9740693508228578,
      "grad_norm": 3.0016016960144043,
      "learning_rate": 7.616646384971413e-05,
      "loss": 3.241,
      "step": 16040
    },
    {
      "epoch": 0.9746766259792312,
      "grad_norm": 3.100301504135132,
      "learning_rate": 7.613936349089981e-05,
      "loss": 2.9608,
      "step": 16050
    },
    {
      "epoch": 0.9752839011356046,
      "grad_norm": 1.9538837671279907,
      "learning_rate": 7.611225256091057e-05,
      "loss": 2.3863,
      "step": 16060
    },
    {
      "epoch": 0.9758911762919779,
      "grad_norm": 2.065725326538086,
      "learning_rate": 7.608513107071052e-05,
      "loss": 2.3182,
      "step": 16070
    },
    {
      "epoch": 0.9764984514483512,
      "grad_norm": 3.2288033962249756,
      "learning_rate": 7.605799903126802e-05,
      "loss": 2.4686,
      "step": 16080
    },
    {
      "epoch": 0.9771057266047246,
      "grad_norm": 2.7893192768096924,
      "learning_rate": 7.603085645355571e-05,
      "loss": 2.5237,
      "step": 16090
    },
    {
      "epoch": 0.977713001761098,
      "grad_norm": 2.137354850769043,
      "learning_rate": 7.600370334855047e-05,
      "loss": 2.3509,
      "step": 16100
    },
    {
      "epoch": 0.9783202769174713,
      "grad_norm": 2.003647565841675,
      "learning_rate": 7.597653972723344e-05,
      "loss": 2.5339,
      "step": 16110
    },
    {
      "epoch": 0.9789275520738446,
      "grad_norm": 2.794715166091919,
      "learning_rate": 7.594936560059005e-05,
      "loss": 2.8953,
      "step": 16120
    },
    {
      "epoch": 0.979534827230218,
      "grad_norm": 4.004735946655273,
      "learning_rate": 7.592218097960993e-05,
      "loss": 2.7938,
      "step": 16130
    },
    {
      "epoch": 0.9801421023865914,
      "grad_norm": 6.684684753417969,
      "learning_rate": 7.589498587528697e-05,
      "loss": 2.5439,
      "step": 16140
    },
    {
      "epoch": 0.9807493775429648,
      "grad_norm": 3.1737406253814697,
      "learning_rate": 7.58677802986193e-05,
      "loss": 2.1974,
      "step": 16150
    },
    {
      "epoch": 0.981356652699338,
      "grad_norm": 2.0997440814971924,
      "learning_rate": 7.58405642606093e-05,
      "loss": 2.2839,
      "step": 16160
    },
    {
      "epoch": 0.9819639278557114,
      "grad_norm": 2.1111743450164795,
      "learning_rate": 7.581333777226356e-05,
      "loss": 2.2393,
      "step": 16170
    },
    {
      "epoch": 0.9825712030120848,
      "grad_norm": 1.7580664157867432,
      "learning_rate": 7.578610084459293e-05,
      "loss": 2.6101,
      "step": 16180
    },
    {
      "epoch": 0.9831784781684582,
      "grad_norm": 2.408210277557373,
      "learning_rate": 7.575885348861241e-05,
      "loss": 2.6321,
      "step": 16190
    },
    {
      "epoch": 0.9837857533248315,
      "grad_norm": 1.875331163406372,
      "learning_rate": 7.573159571534132e-05,
      "loss": 2.3097,
      "step": 16200
    },
    {
      "epoch": 0.9843930284812048,
      "grad_norm": 2.193192958831787,
      "learning_rate": 7.57043275358031e-05,
      "loss": 2.1831,
      "step": 16210
    },
    {
      "epoch": 0.9850003036375782,
      "grad_norm": 1.724212408065796,
      "learning_rate": 7.567704896102545e-05,
      "loss": 2.3991,
      "step": 16220
    },
    {
      "epoch": 0.9856075787939516,
      "grad_norm": 1.6211929321289062,
      "learning_rate": 7.564976000204024e-05,
      "loss": 2.9204,
      "step": 16230
    },
    {
      "epoch": 0.9862148539503249,
      "grad_norm": 2.180615186691284,
      "learning_rate": 7.562246066988362e-05,
      "loss": 2.589,
      "step": 16240
    },
    {
      "epoch": 0.9868221291066982,
      "grad_norm": 3.4159350395202637,
      "learning_rate": 7.559515097559579e-05,
      "loss": 2.3806,
      "step": 16250
    },
    {
      "epoch": 0.9874294042630716,
      "grad_norm": 1.737739086151123,
      "learning_rate": 7.556783093022128e-05,
      "loss": 2.6224,
      "step": 16260
    },
    {
      "epoch": 0.988036679419445,
      "grad_norm": 3.9608232975006104,
      "learning_rate": 7.554050054480875e-05,
      "loss": 2.6111,
      "step": 16270
    },
    {
      "epoch": 0.9886439545758183,
      "grad_norm": 3.0258584022521973,
      "learning_rate": 7.551315983041101e-05,
      "loss": 2.5952,
      "step": 16280
    },
    {
      "epoch": 0.9892512297321917,
      "grad_norm": 2.057565450668335,
      "learning_rate": 7.548580879808512e-05,
      "loss": 2.345,
      "step": 16290
    },
    {
      "epoch": 0.989858504888565,
      "grad_norm": 2.593660354614258,
      "learning_rate": 7.545844745889225e-05,
      "loss": 2.8697,
      "step": 16300
    },
    {
      "epoch": 0.9904657800449383,
      "grad_norm": 2.5403287410736084,
      "learning_rate": 7.543107582389776e-05,
      "loss": 2.7345,
      "step": 16310
    },
    {
      "epoch": 0.9910730552013117,
      "grad_norm": 5.358405113220215,
      "learning_rate": 7.540369390417119e-05,
      "loss": 2.871,
      "step": 16320
    },
    {
      "epoch": 0.9916803303576851,
      "grad_norm": 3.1764941215515137,
      "learning_rate": 7.53763017107862e-05,
      "loss": 2.6726,
      "step": 16330
    },
    {
      "epoch": 0.9922876055140584,
      "grad_norm": 4.832056999206543,
      "learning_rate": 7.534889925482067e-05,
      "loss": 2.5382,
      "step": 16340
    },
    {
      "epoch": 0.9928948806704317,
      "grad_norm": 3.00484037399292,
      "learning_rate": 7.532148654735653e-05,
      "loss": 2.503,
      "step": 16350
    },
    {
      "epoch": 0.9935021558268051,
      "grad_norm": 3.2258360385894775,
      "learning_rate": 7.529406359947999e-05,
      "loss": 2.5019,
      "step": 16360
    },
    {
      "epoch": 0.9941094309831785,
      "grad_norm": 2.908172607421875,
      "learning_rate": 7.526663042228125e-05,
      "loss": 2.6168,
      "step": 16370
    },
    {
      "epoch": 0.9947167061395519,
      "grad_norm": 2.869920253753662,
      "learning_rate": 7.52391870268548e-05,
      "loss": 3.135,
      "step": 16380
    },
    {
      "epoch": 0.9953239812959251,
      "grad_norm": 4.161388397216797,
      "learning_rate": 7.521173342429911e-05,
      "loss": 2.9917,
      "step": 16390
    },
    {
      "epoch": 0.9959312564522985,
      "grad_norm": 2.865880012512207,
      "learning_rate": 7.518426962571691e-05,
      "loss": 2.5355,
      "step": 16400
    },
    {
      "epoch": 0.9965385316086719,
      "grad_norm": 3.1607859134674072,
      "learning_rate": 7.515679564221498e-05,
      "loss": 2.5045,
      "step": 16410
    },
    {
      "epoch": 0.9971458067650453,
      "grad_norm": 1.8900246620178223,
      "learning_rate": 7.512931148490423e-05,
      "loss": 2.2001,
      "step": 16420
    },
    {
      "epoch": 0.9977530819214186,
      "grad_norm": 1.4433910846710205,
      "learning_rate": 7.51018171648997e-05,
      "loss": 2.2278,
      "step": 16430
    },
    {
      "epoch": 0.9983603570777919,
      "grad_norm": 2.0141758918762207,
      "learning_rate": 7.507431269332053e-05,
      "loss": 2.7033,
      "step": 16440
    },
    {
      "epoch": 0.9989676322341653,
      "grad_norm": 2.5526089668273926,
      "learning_rate": 7.504679808128995e-05,
      "loss": 2.5758,
      "step": 16450
    },
    {
      "epoch": 0.9995749073905387,
      "grad_norm": 2.0059871673583984,
      "learning_rate": 7.501927333993533e-05,
      "loss": 2.5736,
      "step": 16460
    },
    {
      "epoch": 1.000182182546912,
      "grad_norm": 3.5696563720703125,
      "learning_rate": 7.49917384803881e-05,
      "loss": 2.6949,
      "step": 16470
    },
    {
      "epoch": 1.0007894577032854,
      "grad_norm": 2.9639809131622314,
      "learning_rate": 7.49641935137838e-05,
      "loss": 2.9646,
      "step": 16480
    },
    {
      "epoch": 1.0013967328596587,
      "grad_norm": 2.305236577987671,
      "learning_rate": 7.493663845126205e-05,
      "loss": 2.6971,
      "step": 16490
    },
    {
      "epoch": 1.002004008016032,
      "grad_norm": 2.908209800720215,
      "learning_rate": 7.490907330396657e-05,
      "loss": 2.6019,
      "step": 16500
    },
    {
      "epoch": 1.0026112831724054,
      "grad_norm": 3.565988063812256,
      "learning_rate": 7.488149808304514e-05,
      "loss": 2.6707,
      "step": 16510
    },
    {
      "epoch": 1.0032185583287787,
      "grad_norm": 2.4926140308380127,
      "learning_rate": 7.485391279964958e-05,
      "loss": 2.64,
      "step": 16520
    },
    {
      "epoch": 1.0038258334851522,
      "grad_norm": 3.706219434738159,
      "learning_rate": 7.482631746493588e-05,
      "loss": 2.4459,
      "step": 16530
    },
    {
      "epoch": 1.0044331086415255,
      "grad_norm": 3.829655885696411,
      "learning_rate": 7.4798712090064e-05,
      "loss": 2.6642,
      "step": 16540
    },
    {
      "epoch": 1.0050403837978987,
      "grad_norm": 2.62497878074646,
      "learning_rate": 7.4771096686198e-05,
      "loss": 2.5972,
      "step": 16550
    },
    {
      "epoch": 1.0056476589542722,
      "grad_norm": 2.8009872436523438,
      "learning_rate": 7.474347126450596e-05,
      "loss": 2.7855,
      "step": 16560
    },
    {
      "epoch": 1.0062549341106455,
      "grad_norm": 3.0704774856567383,
      "learning_rate": 7.47158358361601e-05,
      "loss": 2.6279,
      "step": 16570
    },
    {
      "epoch": 1.006862209267019,
      "grad_norm": 2.726929187774658,
      "learning_rate": 7.468819041233656e-05,
      "loss": 2.6213,
      "step": 16580
    },
    {
      "epoch": 1.0074694844233922,
      "grad_norm": 2.19706654548645,
      "learning_rate": 7.466053500421565e-05,
      "loss": 2.2703,
      "step": 16590
    },
    {
      "epoch": 1.0080767595797655,
      "grad_norm": 4.118852615356445,
      "learning_rate": 7.463286962298162e-05,
      "loss": 2.7644,
      "step": 16600
    },
    {
      "epoch": 1.008684034736139,
      "grad_norm": 2.6421074867248535,
      "learning_rate": 7.460519427982281e-05,
      "loss": 2.4948,
      "step": 16610
    },
    {
      "epoch": 1.0092913098925123,
      "grad_norm": 2.736999034881592,
      "learning_rate": 7.457750898593154e-05,
      "loss": 2.3792,
      "step": 16620
    },
    {
      "epoch": 1.0098985850488857,
      "grad_norm": 1.7685282230377197,
      "learning_rate": 7.454981375250423e-05,
      "loss": 2.1273,
      "step": 16630
    },
    {
      "epoch": 1.010505860205259,
      "grad_norm": 2.3102903366088867,
      "learning_rate": 7.452210859074123e-05,
      "loss": 2.4928,
      "step": 16640
    },
    {
      "epoch": 1.0111131353616323,
      "grad_norm": 3.0107226371765137,
      "learning_rate": 7.449439351184698e-05,
      "loss": 2.7381,
      "step": 16650
    },
    {
      "epoch": 1.0117204105180058,
      "grad_norm": 5.144041538238525,
      "learning_rate": 7.446666852702987e-05,
      "loss": 2.8199,
      "step": 16660
    },
    {
      "epoch": 1.012327685674379,
      "grad_norm": 2.6925318241119385,
      "learning_rate": 7.443893364750236e-05,
      "loss": 2.6311,
      "step": 16670
    },
    {
      "epoch": 1.0129349608307525,
      "grad_norm": 2.7302708625793457,
      "learning_rate": 7.441118888448085e-05,
      "loss": 2.7089,
      "step": 16680
    },
    {
      "epoch": 1.0135422359871258,
      "grad_norm": 2.453130006790161,
      "learning_rate": 7.438343424918577e-05,
      "loss": 2.8494,
      "step": 16690
    },
    {
      "epoch": 1.014149511143499,
      "grad_norm": 2.685716390609741,
      "learning_rate": 7.435566975284154e-05,
      "loss": 2.2614,
      "step": 16700
    },
    {
      "epoch": 1.0147567862998725,
      "grad_norm": 1.703159213066101,
      "learning_rate": 7.432789540667657e-05,
      "loss": 2.3364,
      "step": 16710
    },
    {
      "epoch": 1.0153640614562458,
      "grad_norm": 2.3269429206848145,
      "learning_rate": 7.430011122192324e-05,
      "loss": 2.788,
      "step": 16720
    },
    {
      "epoch": 1.015971336612619,
      "grad_norm": 3.0985536575317383,
      "learning_rate": 7.427231720981791e-05,
      "loss": 2.555,
      "step": 16730
    },
    {
      "epoch": 1.0165786117689926,
      "grad_norm": 2.2030017375946045,
      "learning_rate": 7.424451338160095e-05,
      "loss": 2.5475,
      "step": 16740
    },
    {
      "epoch": 1.0171858869253658,
      "grad_norm": 2.9290428161621094,
      "learning_rate": 7.421669974851663e-05,
      "loss": 2.5339,
      "step": 16750
    },
    {
      "epoch": 1.0177931620817393,
      "grad_norm": 3.364712715148926,
      "learning_rate": 7.418887632181327e-05,
      "loss": 2.4045,
      "step": 16760
    },
    {
      "epoch": 1.0184004372381126,
      "grad_norm": 2.8892509937286377,
      "learning_rate": 7.416104311274304e-05,
      "loss": 2.6196,
      "step": 16770
    },
    {
      "epoch": 1.0190077123944858,
      "grad_norm": 1.9001325368881226,
      "learning_rate": 7.413320013256222e-05,
      "loss": 2.5814,
      "step": 16780
    },
    {
      "epoch": 1.0196149875508593,
      "grad_norm": 2.455770492553711,
      "learning_rate": 7.410534739253089e-05,
      "loss": 2.8901,
      "step": 16790
    },
    {
      "epoch": 1.0202222627072326,
      "grad_norm": 3.370716094970703,
      "learning_rate": 7.407748490391318e-05,
      "loss": 2.9043,
      "step": 16800
    },
    {
      "epoch": 1.020829537863606,
      "grad_norm": 1.6404520273208618,
      "learning_rate": 7.404961267797709e-05,
      "loss": 2.1608,
      "step": 16810
    },
    {
      "epoch": 1.0214368130199794,
      "grad_norm": 3.596844434738159,
      "learning_rate": 7.402173072599462e-05,
      "loss": 3.0464,
      "step": 16820
    },
    {
      "epoch": 1.0220440881763526,
      "grad_norm": 2.797666072845459,
      "learning_rate": 7.399383905924165e-05,
      "loss": 3.0117,
      "step": 16830
    },
    {
      "epoch": 1.022651363332726,
      "grad_norm": 3.6874091625213623,
      "learning_rate": 7.396593768899805e-05,
      "loss": 2.9209,
      "step": 16840
    },
    {
      "epoch": 1.0232586384890994,
      "grad_norm": 3.7592649459838867,
      "learning_rate": 7.393802662654755e-05,
      "loss": 2.6834,
      "step": 16850
    },
    {
      "epoch": 1.0238659136454729,
      "grad_norm": 2.8513996601104736,
      "learning_rate": 7.391010588317782e-05,
      "loss": 2.5558,
      "step": 16860
    },
    {
      "epoch": 1.0244731888018461,
      "grad_norm": 2.6886749267578125,
      "learning_rate": 7.388217547018048e-05,
      "loss": 2.5961,
      "step": 16870
    },
    {
      "epoch": 1.0250804639582194,
      "grad_norm": 2.092071533203125,
      "learning_rate": 7.385423539885103e-05,
      "loss": 2.8925,
      "step": 16880
    },
    {
      "epoch": 1.0256877391145929,
      "grad_norm": 2.1077380180358887,
      "learning_rate": 7.382628568048886e-05,
      "loss": 2.6448,
      "step": 16890
    },
    {
      "epoch": 1.0262950142709661,
      "grad_norm": 2.274260997772217,
      "learning_rate": 7.379832632639729e-05,
      "loss": 2.2537,
      "step": 16900
    },
    {
      "epoch": 1.0269022894273396,
      "grad_norm": 2.902233600616455,
      "learning_rate": 7.377035734788355e-05,
      "loss": 2.4085,
      "step": 16910
    },
    {
      "epoch": 1.027509564583713,
      "grad_norm": 5.215513229370117,
      "learning_rate": 7.374237875625872e-05,
      "loss": 2.562,
      "step": 16920
    },
    {
      "epoch": 1.0281168397400862,
      "grad_norm": 3.3538620471954346,
      "learning_rate": 7.371439056283779e-05,
      "loss": 2.3419,
      "step": 16930
    },
    {
      "epoch": 1.0287241148964597,
      "grad_norm": 2.0054843425750732,
      "learning_rate": 7.368639277893962e-05,
      "loss": 2.5335,
      "step": 16940
    },
    {
      "epoch": 1.029331390052833,
      "grad_norm": 2.2319717407226562,
      "learning_rate": 7.365838541588698e-05,
      "loss": 2.2831,
      "step": 16950
    },
    {
      "epoch": 1.0299386652092062,
      "grad_norm": 2.6192564964294434,
      "learning_rate": 7.363036848500649e-05,
      "loss": 2.2658,
      "step": 16960
    },
    {
      "epoch": 1.0305459403655797,
      "grad_norm": 2.6974270343780518,
      "learning_rate": 7.360234199762863e-05,
      "loss": 2.3064,
      "step": 16970
    },
    {
      "epoch": 1.031153215521953,
      "grad_norm": 2.8340959548950195,
      "learning_rate": 7.357430596508776e-05,
      "loss": 2.7565,
      "step": 16980
    },
    {
      "epoch": 1.0317604906783264,
      "grad_norm": 3.0585904121398926,
      "learning_rate": 7.354626039872212e-05,
      "loss": 2.7265,
      "step": 16990
    },
    {
      "epoch": 1.0323677658346997,
      "grad_norm": 2.1343190670013428,
      "learning_rate": 7.351820530987374e-05,
      "loss": 3.0808,
      "step": 17000
    },
    {
      "epoch": 1.032975040991073,
      "grad_norm": 4.023305892944336,
      "learning_rate": 7.34901407098886e-05,
      "loss": 2.9462,
      "step": 17010
    },
    {
      "epoch": 1.0335823161474464,
      "grad_norm": 5.337362766265869,
      "learning_rate": 7.346206661011643e-05,
      "loss": 2.7203,
      "step": 17020
    },
    {
      "epoch": 1.0341895913038197,
      "grad_norm": 3.4282121658325195,
      "learning_rate": 7.343398302191087e-05,
      "loss": 2.8453,
      "step": 17030
    },
    {
      "epoch": 1.0347968664601932,
      "grad_norm": 4.520294189453125,
      "learning_rate": 7.340588995662934e-05,
      "loss": 2.9285,
      "step": 17040
    },
    {
      "epoch": 1.0354041416165665,
      "grad_norm": 3.5796921253204346,
      "learning_rate": 7.337778742563315e-05,
      "loss": 2.6366,
      "step": 17050
    },
    {
      "epoch": 1.0360114167729397,
      "grad_norm": 3.1498332023620605,
      "learning_rate": 7.33496754402874e-05,
      "loss": 2.5264,
      "step": 17060
    },
    {
      "epoch": 1.0366186919293132,
      "grad_norm": 4.667843341827393,
      "learning_rate": 7.332155401196102e-05,
      "loss": 2.8352,
      "step": 17070
    },
    {
      "epoch": 1.0372259670856865,
      "grad_norm": 4.183180809020996,
      "learning_rate": 7.329342315202677e-05,
      "loss": 2.4784,
      "step": 17080
    },
    {
      "epoch": 1.03783324224206,
      "grad_norm": 3.170271873474121,
      "learning_rate": 7.326528287186123e-05,
      "loss": 2.4144,
      "step": 17090
    },
    {
      "epoch": 1.0384405173984332,
      "grad_norm": 2.8681728839874268,
      "learning_rate": 7.323713318284475e-05,
      "loss": 2.7161,
      "step": 17100
    },
    {
      "epoch": 1.0390477925548065,
      "grad_norm": 5.442270278930664,
      "learning_rate": 7.320897409636154e-05,
      "loss": 2.342,
      "step": 17110
    },
    {
      "epoch": 1.03965506771118,
      "grad_norm": 3.9321584701538086,
      "learning_rate": 7.318080562379956e-05,
      "loss": 2.3361,
      "step": 17120
    },
    {
      "epoch": 1.0402623428675533,
      "grad_norm": 4.17304801940918,
      "learning_rate": 7.31526277765506e-05,
      "loss": 2.5882,
      "step": 17130
    },
    {
      "epoch": 1.0408696180239267,
      "grad_norm": 3.363476037979126,
      "learning_rate": 7.312444056601023e-05,
      "loss": 2.6172,
      "step": 17140
    },
    {
      "epoch": 1.0414768931803,
      "grad_norm": 3.5349857807159424,
      "learning_rate": 7.30962440035778e-05,
      "loss": 2.8478,
      "step": 17150
    },
    {
      "epoch": 1.0420841683366733,
      "grad_norm": 2.695338726043701,
      "learning_rate": 7.306803810065647e-05,
      "loss": 2.5462,
      "step": 17160
    },
    {
      "epoch": 1.0426914434930468,
      "grad_norm": 3.8255035877227783,
      "learning_rate": 7.303982286865312e-05,
      "loss": 2.5827,
      "step": 17170
    },
    {
      "epoch": 1.04329871864942,
      "grad_norm": 2.5237369537353516,
      "learning_rate": 7.301159831897848e-05,
      "loss": 2.4471,
      "step": 17180
    },
    {
      "epoch": 1.0439059938057933,
      "grad_norm": 3.3906197547912598,
      "learning_rate": 7.298336446304698e-05,
      "loss": 2.3169,
      "step": 17190
    },
    {
      "epoch": 1.0445132689621668,
      "grad_norm": 1.9506847858428955,
      "learning_rate": 7.295512131227686e-05,
      "loss": 2.7115,
      "step": 17200
    },
    {
      "epoch": 1.04512054411854,
      "grad_norm": 1.8367582559585571,
      "learning_rate": 7.292686887809005e-05,
      "loss": 2.582,
      "step": 17210
    },
    {
      "epoch": 1.0457278192749135,
      "grad_norm": 4.070536136627197,
      "learning_rate": 7.289860717191236e-05,
      "loss": 2.5275,
      "step": 17220
    },
    {
      "epoch": 1.0463350944312868,
      "grad_norm": 3.2649691104888916,
      "learning_rate": 7.287033620517324e-05,
      "loss": 2.6555,
      "step": 17230
    },
    {
      "epoch": 1.04694236958766,
      "grad_norm": 3.3568577766418457,
      "learning_rate": 7.284205598930592e-05,
      "loss": 2.5902,
      "step": 17240
    },
    {
      "epoch": 1.0475496447440336,
      "grad_norm": 3.44254207611084,
      "learning_rate": 7.281376653574733e-05,
      "loss": 2.5384,
      "step": 17250
    },
    {
      "epoch": 1.0481569199004068,
      "grad_norm": 2.9898288249969482,
      "learning_rate": 7.278546785593825e-05,
      "loss": 2.5814,
      "step": 17260
    },
    {
      "epoch": 1.0487641950567803,
      "grad_norm": 2.094463348388672,
      "learning_rate": 7.275715996132304e-05,
      "loss": 2.5154,
      "step": 17270
    },
    {
      "epoch": 1.0493714702131536,
      "grad_norm": 3.809095859527588,
      "learning_rate": 7.272884286334993e-05,
      "loss": 2.5171,
      "step": 17280
    },
    {
      "epoch": 1.0499787453695268,
      "grad_norm": 2.717747926712036,
      "learning_rate": 7.270051657347077e-05,
      "loss": 2.3393,
      "step": 17290
    },
    {
      "epoch": 1.0505860205259003,
      "grad_norm": 3.159123420715332,
      "learning_rate": 7.267218110314118e-05,
      "loss": 2.936,
      "step": 17300
    },
    {
      "epoch": 1.0511932956822736,
      "grad_norm": 3.119210720062256,
      "learning_rate": 7.264383646382045e-05,
      "loss": 2.7754,
      "step": 17310
    },
    {
      "epoch": 1.051800570838647,
      "grad_norm": 3.391801595687866,
      "learning_rate": 7.261548266697164e-05,
      "loss": 3.1063,
      "step": 17320
    },
    {
      "epoch": 1.0524078459950204,
      "grad_norm": 3.69594407081604,
      "learning_rate": 7.258711972406144e-05,
      "loss": 2.5956,
      "step": 17330
    },
    {
      "epoch": 1.0530151211513936,
      "grad_norm": 4.750375747680664,
      "learning_rate": 7.25587476465603e-05,
      "loss": 2.6608,
      "step": 17340
    },
    {
      "epoch": 1.053622396307767,
      "grad_norm": 3.197343587875366,
      "learning_rate": 7.253036644594231e-05,
      "loss": 2.3102,
      "step": 17350
    },
    {
      "epoch": 1.0542296714641404,
      "grad_norm": 2.6927905082702637,
      "learning_rate": 7.250197613368532e-05,
      "loss": 2.5602,
      "step": 17360
    },
    {
      "epoch": 1.0548369466205139,
      "grad_norm": 3.110706090927124,
      "learning_rate": 7.247357672127082e-05,
      "loss": 2.3333,
      "step": 17370
    },
    {
      "epoch": 1.0554442217768871,
      "grad_norm": 3.8056623935699463,
      "learning_rate": 7.244516822018395e-05,
      "loss": 2.6527,
      "step": 17380
    },
    {
      "epoch": 1.0560514969332604,
      "grad_norm": 2.5421383380889893,
      "learning_rate": 7.241675064191358e-05,
      "loss": 2.4274,
      "step": 17390
    },
    {
      "epoch": 1.0566587720896339,
      "grad_norm": 2.5299365520477295,
      "learning_rate": 7.238832399795223e-05,
      "loss": 2.5679,
      "step": 17400
    },
    {
      "epoch": 1.0572660472460071,
      "grad_norm": 3.119004249572754,
      "learning_rate": 7.235988829979611e-05,
      "loss": 3.0113,
      "step": 17410
    },
    {
      "epoch": 1.0578733224023804,
      "grad_norm": 4.342990875244141,
      "learning_rate": 7.233144355894504e-05,
      "loss": 2.5686,
      "step": 17420
    },
    {
      "epoch": 1.058480597558754,
      "grad_norm": 2.0902223587036133,
      "learning_rate": 7.230298978690253e-05,
      "loss": 2.8389,
      "step": 17430
    },
    {
      "epoch": 1.0590878727151272,
      "grad_norm": 2.8134303092956543,
      "learning_rate": 7.227452699517574e-05,
      "loss": 3.0768,
      "step": 17440
    },
    {
      "epoch": 1.0596951478715007,
      "grad_norm": 3.444227695465088,
      "learning_rate": 7.22460551952755e-05,
      "loss": 2.486,
      "step": 17450
    },
    {
      "epoch": 1.060302423027874,
      "grad_norm": 3.6042416095733643,
      "learning_rate": 7.221757439871623e-05,
      "loss": 2.6682,
      "step": 17460
    },
    {
      "epoch": 1.0609096981842472,
      "grad_norm": 3.7724883556365967,
      "learning_rate": 7.218908461701603e-05,
      "loss": 2.928,
      "step": 17470
    },
    {
      "epoch": 1.0615169733406207,
      "grad_norm": 2.889301300048828,
      "learning_rate": 7.21605858616966e-05,
      "loss": 2.3478,
      "step": 17480
    },
    {
      "epoch": 1.062124248496994,
      "grad_norm": 3.5593066215515137,
      "learning_rate": 7.213207814428331e-05,
      "loss": 2.5596,
      "step": 17490
    },
    {
      "epoch": 1.0627315236533674,
      "grad_norm": 3.6017465591430664,
      "learning_rate": 7.210356147630514e-05,
      "loss": 2.7811,
      "step": 17500
    },
    {
      "epoch": 1.0633387988097407,
      "grad_norm": 3.860481023788452,
      "learning_rate": 7.207503586929471e-05,
      "loss": 2.6486,
      "step": 17510
    },
    {
      "epoch": 1.063946073966114,
      "grad_norm": 3.025555372238159,
      "learning_rate": 7.204650133478816e-05,
      "loss": 2.5769,
      "step": 17520
    },
    {
      "epoch": 1.0645533491224874,
      "grad_norm": 2.909461498260498,
      "learning_rate": 7.201795788432536e-05,
      "loss": 2.529,
      "step": 17530
    },
    {
      "epoch": 1.0651606242788607,
      "grad_norm": 2.8039705753326416,
      "learning_rate": 7.198940552944973e-05,
      "loss": 2.5683,
      "step": 17540
    },
    {
      "epoch": 1.0657678994352342,
      "grad_norm": 2.1408965587615967,
      "learning_rate": 7.196084428170829e-05,
      "loss": 2.6315,
      "step": 17550
    },
    {
      "epoch": 1.0663751745916075,
      "grad_norm": 3.2391865253448486,
      "learning_rate": 7.193227415265166e-05,
      "loss": 2.5536,
      "step": 17560
    },
    {
      "epoch": 1.0669824497479807,
      "grad_norm": 3.813988208770752,
      "learning_rate": 7.190369515383407e-05,
      "loss": 2.4158,
      "step": 17570
    },
    {
      "epoch": 1.0675897249043542,
      "grad_norm": 2.779975414276123,
      "learning_rate": 7.18751072968133e-05,
      "loss": 3.013,
      "step": 17580
    },
    {
      "epoch": 1.0681970000607275,
      "grad_norm": 2.8391544818878174,
      "learning_rate": 7.184651059315076e-05,
      "loss": 2.5031,
      "step": 17590
    },
    {
      "epoch": 1.0688042752171008,
      "grad_norm": 4.4597649574279785,
      "learning_rate": 7.18179050544114e-05,
      "loss": 2.6035,
      "step": 17600
    },
    {
      "epoch": 1.0694115503734742,
      "grad_norm": 2.83122181892395,
      "learning_rate": 7.178929069216374e-05,
      "loss": 2.4672,
      "step": 17610
    },
    {
      "epoch": 1.0700188255298475,
      "grad_norm": 3.131908893585205,
      "learning_rate": 7.17606675179799e-05,
      "loss": 2.2271,
      "step": 17620
    },
    {
      "epoch": 1.070626100686221,
      "grad_norm": 2.0027015209198,
      "learning_rate": 7.173203554343556e-05,
      "loss": 2.407,
      "step": 17630
    },
    {
      "epoch": 1.0712333758425943,
      "grad_norm": 4.037001132965088,
      "learning_rate": 7.170339478010993e-05,
      "loss": 2.7633,
      "step": 17640
    },
    {
      "epoch": 1.0718406509989675,
      "grad_norm": 4.97284460067749,
      "learning_rate": 7.16747452395858e-05,
      "loss": 2.6919,
      "step": 17650
    },
    {
      "epoch": 1.072447926155341,
      "grad_norm": 2.9496850967407227,
      "learning_rate": 7.164608693344948e-05,
      "loss": 3.0091,
      "step": 17660
    },
    {
      "epoch": 1.0730552013117143,
      "grad_norm": 3.5390267372131348,
      "learning_rate": 7.161741987329087e-05,
      "loss": 2.7798,
      "step": 17670
    },
    {
      "epoch": 1.0736624764680878,
      "grad_norm": 2.5756869316101074,
      "learning_rate": 7.158874407070337e-05,
      "loss": 2.409,
      "step": 17680
    },
    {
      "epoch": 1.074269751624461,
      "grad_norm": 2.560357093811035,
      "learning_rate": 7.156005953728393e-05,
      "loss": 2.3871,
      "step": 17690
    },
    {
      "epoch": 1.0748770267808343,
      "grad_norm": 1.9331963062286377,
      "learning_rate": 7.153136628463307e-05,
      "loss": 2.5583,
      "step": 17700
    },
    {
      "epoch": 1.0754843019372078,
      "grad_norm": 1.6567964553833008,
      "learning_rate": 7.150266432435473e-05,
      "loss": 2.2763,
      "step": 17710
    },
    {
      "epoch": 1.076091577093581,
      "grad_norm": 1.5138652324676514,
      "learning_rate": 7.14739536680565e-05,
      "loss": 2.2568,
      "step": 17720
    },
    {
      "epoch": 1.0766988522499545,
      "grad_norm": 1.9885444641113281,
      "learning_rate": 7.144523432734939e-05,
      "loss": 2.6961,
      "step": 17730
    },
    {
      "epoch": 1.0773061274063278,
      "grad_norm": 3.025830030441284,
      "learning_rate": 7.141650631384798e-05,
      "loss": 2.8772,
      "step": 17740
    },
    {
      "epoch": 1.077913402562701,
      "grad_norm": 2.4588372707366943,
      "learning_rate": 7.138776963917033e-05,
      "loss": 2.5311,
      "step": 17750
    },
    {
      "epoch": 1.0785206777190746,
      "grad_norm": 4.7123517990112305,
      "learning_rate": 7.135902431493801e-05,
      "loss": 2.737,
      "step": 17760
    },
    {
      "epoch": 1.0791279528754478,
      "grad_norm": 3.3671653270721436,
      "learning_rate": 7.133027035277606e-05,
      "loss": 2.322,
      "step": 17770
    },
    {
      "epoch": 1.0797352280318213,
      "grad_norm": 3.0587332248687744,
      "learning_rate": 7.13015077643131e-05,
      "loss": 2.5985,
      "step": 17780
    },
    {
      "epoch": 1.0803425031881946,
      "grad_norm": 2.482253313064575,
      "learning_rate": 7.127273656118112e-05,
      "loss": 3.1584,
      "step": 17790
    },
    {
      "epoch": 1.0809497783445678,
      "grad_norm": 4.657707214355469,
      "learning_rate": 7.124395675501569e-05,
      "loss": 2.6284,
      "step": 17800
    },
    {
      "epoch": 1.0815570535009413,
      "grad_norm": 3.9397201538085938,
      "learning_rate": 7.121516835745581e-05,
      "loss": 2.6605,
      "step": 17810
    },
    {
      "epoch": 1.0821643286573146,
      "grad_norm": 4.011096477508545,
      "learning_rate": 7.118637138014396e-05,
      "loss": 2.7902,
      "step": 17820
    },
    {
      "epoch": 1.082771603813688,
      "grad_norm": 3.352184534072876,
      "learning_rate": 7.11575658347261e-05,
      "loss": 2.6706,
      "step": 17830
    },
    {
      "epoch": 1.0833788789700614,
      "grad_norm": 3.185959815979004,
      "learning_rate": 7.112875173285165e-05,
      "loss": 2.6247,
      "step": 17840
    },
    {
      "epoch": 1.0839861541264346,
      "grad_norm": 5.02034854888916,
      "learning_rate": 7.10999290861735e-05,
      "loss": 2.7764,
      "step": 17850
    },
    {
      "epoch": 1.084593429282808,
      "grad_norm": 4.250537395477295,
      "learning_rate": 7.107109790634797e-05,
      "loss": 2.7098,
      "step": 17860
    },
    {
      "epoch": 1.0852007044391814,
      "grad_norm": 2.9850857257843018,
      "learning_rate": 7.104225820503487e-05,
      "loss": 2.5584,
      "step": 17870
    },
    {
      "epoch": 1.0858079795955549,
      "grad_norm": 2.2625925540924072,
      "learning_rate": 7.101340999389743e-05,
      "loss": 2.8166,
      "step": 17880
    },
    {
      "epoch": 1.0864152547519281,
      "grad_norm": 2.359874725341797,
      "learning_rate": 7.098455328460232e-05,
      "loss": 2.4083,
      "step": 17890
    },
    {
      "epoch": 1.0870225299083014,
      "grad_norm": 2.2194080352783203,
      "learning_rate": 7.095568808881963e-05,
      "loss": 2.4447,
      "step": 17900
    },
    {
      "epoch": 1.0876298050646749,
      "grad_norm": 2.484419822692871,
      "learning_rate": 7.092681441822296e-05,
      "loss": 2.7651,
      "step": 17910
    },
    {
      "epoch": 1.0882370802210481,
      "grad_norm": 3.002315044403076,
      "learning_rate": 7.089793228448923e-05,
      "loss": 2.6247,
      "step": 17920
    },
    {
      "epoch": 1.0888443553774214,
      "grad_norm": 4.930439472198486,
      "learning_rate": 7.086904169929887e-05,
      "loss": 2.3892,
      "step": 17930
    },
    {
      "epoch": 1.089451630533795,
      "grad_norm": 2.4465086460113525,
      "learning_rate": 7.084014267433565e-05,
      "loss": 2.8646,
      "step": 17940
    },
    {
      "epoch": 1.0900589056901682,
      "grad_norm": 3.334939956665039,
      "learning_rate": 7.081123522128686e-05,
      "loss": 2.7291,
      "step": 17950
    },
    {
      "epoch": 1.0906661808465417,
      "grad_norm": 3.145217180252075,
      "learning_rate": 7.078231935184308e-05,
      "loss": 2.7523,
      "step": 17960
    },
    {
      "epoch": 1.091273456002915,
      "grad_norm": 3.15639591217041,
      "learning_rate": 7.075339507769837e-05,
      "loss": 2.8079,
      "step": 17970
    },
    {
      "epoch": 1.0918807311592882,
      "grad_norm": 2.554419994354248,
      "learning_rate": 7.072446241055017e-05,
      "loss": 2.8106,
      "step": 17980
    },
    {
      "epoch": 1.0924880063156617,
      "grad_norm": 2.8047170639038086,
      "learning_rate": 7.06955213620993e-05,
      "loss": 2.6052,
      "step": 17990
    },
    {
      "epoch": 1.093095281472035,
      "grad_norm": 3.908787965774536,
      "learning_rate": 7.066657194404996e-05,
      "loss": 2.8958,
      "step": 18000
    },
    {
      "epoch": 1.0937025566284084,
      "grad_norm": 2.111591339111328,
      "learning_rate": 7.063761416810981e-05,
      "loss": 2.4287,
      "step": 18010
    },
    {
      "epoch": 1.0943098317847817,
      "grad_norm": 3.2786405086517334,
      "learning_rate": 7.060864804598981e-05,
      "loss": 2.1086,
      "step": 18020
    },
    {
      "epoch": 1.094917106941155,
      "grad_norm": 2.068561315536499,
      "learning_rate": 7.057967358940431e-05,
      "loss": 2.3911,
      "step": 18030
    },
    {
      "epoch": 1.0955243820975284,
      "grad_norm": 1.4683327674865723,
      "learning_rate": 7.055069081007105e-05,
      "loss": 2.9973,
      "step": 18040
    },
    {
      "epoch": 1.0961316572539017,
      "grad_norm": 3.0237560272216797,
      "learning_rate": 7.052169971971113e-05,
      "loss": 3.2618,
      "step": 18050
    },
    {
      "epoch": 1.096738932410275,
      "grad_norm": 4.281639099121094,
      "learning_rate": 7.049270033004901e-05,
      "loss": 2.7846,
      "step": 18060
    },
    {
      "epoch": 1.0973462075666485,
      "grad_norm": 4.6092329025268555,
      "learning_rate": 7.046369265281249e-05,
      "loss": 2.6001,
      "step": 18070
    },
    {
      "epoch": 1.0979534827230217,
      "grad_norm": 3.1454153060913086,
      "learning_rate": 7.043467669973276e-05,
      "loss": 2.5281,
      "step": 18080
    },
    {
      "epoch": 1.0985607578793952,
      "grad_norm": 3.8030142784118652,
      "learning_rate": 7.040565248254431e-05,
      "loss": 2.4082,
      "step": 18090
    },
    {
      "epoch": 1.0991680330357685,
      "grad_norm": 2.1379196643829346,
      "learning_rate": 7.037662001298502e-05,
      "loss": 2.6701,
      "step": 18100
    },
    {
      "epoch": 1.0997753081921418,
      "grad_norm": 4.831038951873779,
      "learning_rate": 7.034757930279606e-05,
      "loss": 2.4651,
      "step": 18110
    },
    {
      "epoch": 1.1003825833485152,
      "grad_norm": 3.0512170791625977,
      "learning_rate": 7.031853036372197e-05,
      "loss": 2.5033,
      "step": 18120
    },
    {
      "epoch": 1.1009898585048885,
      "grad_norm": 3.281125545501709,
      "learning_rate": 7.02894732075106e-05,
      "loss": 2.7447,
      "step": 18130
    },
    {
      "epoch": 1.101597133661262,
      "grad_norm": 3.377993106842041,
      "learning_rate": 7.026040784591315e-05,
      "loss": 2.453,
      "step": 18140
    },
    {
      "epoch": 1.1022044088176353,
      "grad_norm": 2.870779037475586,
      "learning_rate": 7.023133429068405e-05,
      "loss": 2.5957,
      "step": 18150
    },
    {
      "epoch": 1.1028116839740085,
      "grad_norm": 3.0055549144744873,
      "learning_rate": 7.020225255358119e-05,
      "loss": 2.3426,
      "step": 18160
    },
    {
      "epoch": 1.103418959130382,
      "grad_norm": 2.3266804218292236,
      "learning_rate": 7.017316264636562e-05,
      "loss": 2.6966,
      "step": 18170
    },
    {
      "epoch": 1.1040262342867553,
      "grad_norm": 6.003339767456055,
      "learning_rate": 7.014406458080179e-05,
      "loss": 2.4635,
      "step": 18180
    },
    {
      "epoch": 1.1046335094431288,
      "grad_norm": 7.1976237297058105,
      "learning_rate": 7.011495836865744e-05,
      "loss": 2.4266,
      "step": 18190
    },
    {
      "epoch": 1.105240784599502,
      "grad_norm": 2.3721747398376465,
      "learning_rate": 7.008584402170357e-05,
      "loss": 2.0265,
      "step": 18200
    },
    {
      "epoch": 1.1058480597558753,
      "grad_norm": 3.236729383468628,
      "learning_rate": 7.005672155171445e-05,
      "loss": 2.2662,
      "step": 18210
    },
    {
      "epoch": 1.1064553349122488,
      "grad_norm": 4.027315139770508,
      "learning_rate": 7.002759097046772e-05,
      "loss": 2.4427,
      "step": 18220
    },
    {
      "epoch": 1.107062610068622,
      "grad_norm": 3.6635828018188477,
      "learning_rate": 6.999845228974425e-05,
      "loss": 2.385,
      "step": 18230
    },
    {
      "epoch": 1.1076698852249955,
      "grad_norm": 1.9050378799438477,
      "learning_rate": 6.996930552132816e-05,
      "loss": 2.3757,
      "step": 18240
    },
    {
      "epoch": 1.1082771603813688,
      "grad_norm": 2.2295055389404297,
      "learning_rate": 6.994015067700685e-05,
      "loss": 2.6774,
      "step": 18250
    },
    {
      "epoch": 1.108884435537742,
      "grad_norm": 2.437516450881958,
      "learning_rate": 6.991098776857108e-05,
      "loss": 2.8239,
      "step": 18260
    },
    {
      "epoch": 1.1094917106941156,
      "grad_norm": 4.6995134353637695,
      "learning_rate": 6.988181680781472e-05,
      "loss": 2.6566,
      "step": 18270
    },
    {
      "epoch": 1.1100989858504888,
      "grad_norm": 3.8658571243286133,
      "learning_rate": 6.985263780653498e-05,
      "loss": 2.8094,
      "step": 18280
    },
    {
      "epoch": 1.1107062610068623,
      "grad_norm": 4.020454406738281,
      "learning_rate": 6.982345077653237e-05,
      "loss": 3.0417,
      "step": 18290
    },
    {
      "epoch": 1.1113135361632356,
      "grad_norm": 3.8298380374908447,
      "learning_rate": 6.979425572961052e-05,
      "loss": 2.869,
      "step": 18300
    },
    {
      "epoch": 1.1119208113196088,
      "grad_norm": 3.816253662109375,
      "learning_rate": 6.976505267757642e-05,
      "loss": 2.7499,
      "step": 18310
    },
    {
      "epoch": 1.1125280864759823,
      "grad_norm": 1.9081485271453857,
      "learning_rate": 6.973584163224021e-05,
      "loss": 2.9905,
      "step": 18320
    },
    {
      "epoch": 1.1131353616323556,
      "grad_norm": 2.969937324523926,
      "learning_rate": 6.970662260541534e-05,
      "loss": 2.8443,
      "step": 18330
    },
    {
      "epoch": 1.113742636788729,
      "grad_norm": 3.726513147354126,
      "learning_rate": 6.967739560891843e-05,
      "loss": 2.8845,
      "step": 18340
    },
    {
      "epoch": 1.1143499119451024,
      "grad_norm": 3.7343363761901855,
      "learning_rate": 6.964816065456934e-05,
      "loss": 2.7346,
      "step": 18350
    },
    {
      "epoch": 1.1149571871014756,
      "grad_norm": 3.8043763637542725,
      "learning_rate": 6.961891775419116e-05,
      "loss": 2.7638,
      "step": 18360
    },
    {
      "epoch": 1.115564462257849,
      "grad_norm": 3.70357346534729,
      "learning_rate": 6.958966691961018e-05,
      "loss": 2.7459,
      "step": 18370
    },
    {
      "epoch": 1.1161717374142224,
      "grad_norm": 5.073623180389404,
      "learning_rate": 6.956040816265591e-05,
      "loss": 2.6264,
      "step": 18380
    },
    {
      "epoch": 1.1167790125705956,
      "grad_norm": 2.8859894275665283,
      "learning_rate": 6.953114149516104e-05,
      "loss": 2.5512,
      "step": 18390
    },
    {
      "epoch": 1.1173862877269691,
      "grad_norm": 3.585909366607666,
      "learning_rate": 6.950186692896152e-05,
      "loss": 2.9269,
      "step": 18400
    },
    {
      "epoch": 1.1179935628833424,
      "grad_norm": 4.732812404632568,
      "learning_rate": 6.947258447589638e-05,
      "loss": 2.865,
      "step": 18410
    },
    {
      "epoch": 1.1186008380397159,
      "grad_norm": 5.3286662101745605,
      "learning_rate": 6.944329414780796e-05,
      "loss": 2.4347,
      "step": 18420
    },
    {
      "epoch": 1.1192081131960891,
      "grad_norm": 4.634152412414551,
      "learning_rate": 6.941399595654176e-05,
      "loss": 2.6351,
      "step": 18430
    },
    {
      "epoch": 1.1198153883524624,
      "grad_norm": 3.5818326473236084,
      "learning_rate": 6.938468991394635e-05,
      "loss": 2.6235,
      "step": 18440
    },
    {
      "epoch": 1.120422663508836,
      "grad_norm": 2.7360734939575195,
      "learning_rate": 6.935537603187364e-05,
      "loss": 2.5989,
      "step": 18450
    },
    {
      "epoch": 1.1210299386652092,
      "grad_norm": 3.345114231109619,
      "learning_rate": 6.93260543221786e-05,
      "loss": 2.3695,
      "step": 18460
    },
    {
      "epoch": 1.1216372138215827,
      "grad_norm": 2.0613012313842773,
      "learning_rate": 6.92967247967194e-05,
      "loss": 2.4467,
      "step": 18470
    },
    {
      "epoch": 1.122244488977956,
      "grad_norm": 1.42849600315094,
      "learning_rate": 6.926738746735734e-05,
      "loss": 2.6503,
      "step": 18480
    },
    {
      "epoch": 1.1228517641343292,
      "grad_norm": 3.319246530532837,
      "learning_rate": 6.923804234595694e-05,
      "loss": 2.4178,
      "step": 18490
    },
    {
      "epoch": 1.1234590392907027,
      "grad_norm": 3.3159492015838623,
      "learning_rate": 6.920868944438584e-05,
      "loss": 2.6057,
      "step": 18500
    },
    {
      "epoch": 1.124066314447076,
      "grad_norm": 3.1787984371185303,
      "learning_rate": 6.917932877451478e-05,
      "loss": 2.3609,
      "step": 18510
    },
    {
      "epoch": 1.1246735896034492,
      "grad_norm": 7.378026008605957,
      "learning_rate": 6.914996034821771e-05,
      "loss": 2.3478,
      "step": 18520
    },
    {
      "epoch": 1.1252808647598227,
      "grad_norm": 2.6395769119262695,
      "learning_rate": 6.91205841773717e-05,
      "loss": 2.7692,
      "step": 18530
    },
    {
      "epoch": 1.125888139916196,
      "grad_norm": 3.748591899871826,
      "learning_rate": 6.909120027385691e-05,
      "loss": 2.8525,
      "step": 18540
    },
    {
      "epoch": 1.1264954150725695,
      "grad_norm": 4.807505130767822,
      "learning_rate": 6.906180864955668e-05,
      "loss": 2.2621,
      "step": 18550
    },
    {
      "epoch": 1.1271026902289427,
      "grad_norm": 2.3509368896484375,
      "learning_rate": 6.903240931635746e-05,
      "loss": 2.3464,
      "step": 18560
    },
    {
      "epoch": 1.127709965385316,
      "grad_norm": 2.0579123497009277,
      "learning_rate": 6.900300228614877e-05,
      "loss": 2.5716,
      "step": 18570
    },
    {
      "epoch": 1.1283172405416895,
      "grad_norm": 3.2704668045043945,
      "learning_rate": 6.897358757082333e-05,
      "loss": 2.5794,
      "step": 18580
    },
    {
      "epoch": 1.1289245156980627,
      "grad_norm": 3.829240322113037,
      "learning_rate": 6.894416518227688e-05,
      "loss": 2.7678,
      "step": 18590
    },
    {
      "epoch": 1.1295317908544362,
      "grad_norm": 3.326138496398926,
      "learning_rate": 6.891473513240834e-05,
      "loss": 2.4804,
      "step": 18600
    },
    {
      "epoch": 1.1301390660108095,
      "grad_norm": 2.1968395709991455,
      "learning_rate": 6.888529743311964e-05,
      "loss": 2.2106,
      "step": 18610
    },
    {
      "epoch": 1.1307463411671828,
      "grad_norm": 1.4931073188781738,
      "learning_rate": 6.88558520963159e-05,
      "loss": 2.4279,
      "step": 18620
    },
    {
      "epoch": 1.1313536163235562,
      "grad_norm": 2.174823045730591,
      "learning_rate": 6.882639913390526e-05,
      "loss": 2.3375,
      "step": 18630
    },
    {
      "epoch": 1.1319608914799295,
      "grad_norm": 2.6171014308929443,
      "learning_rate": 6.879693855779898e-05,
      "loss": 2.6919,
      "step": 18640
    },
    {
      "epoch": 1.132568166636303,
      "grad_norm": 2.253943681716919,
      "learning_rate": 6.876747037991137e-05,
      "loss": 2.5207,
      "step": 18650
    },
    {
      "epoch": 1.1331754417926763,
      "grad_norm": 2.632441759109497,
      "learning_rate": 6.873799461215983e-05,
      "loss": 2.5064,
      "step": 18660
    },
    {
      "epoch": 1.1337827169490495,
      "grad_norm": 2.696861982345581,
      "learning_rate": 6.870851126646487e-05,
      "loss": 2.6172,
      "step": 18670
    },
    {
      "epoch": 1.134389992105423,
      "grad_norm": 2.2447316646575928,
      "learning_rate": 6.867902035474997e-05,
      "loss": 2.4727,
      "step": 18680
    },
    {
      "epoch": 1.1349972672617963,
      "grad_norm": 4.079891681671143,
      "learning_rate": 6.864952188894176e-05,
      "loss": 2.7498,
      "step": 18690
    },
    {
      "epoch": 1.1356045424181698,
      "grad_norm": 2.826221466064453,
      "learning_rate": 6.86200158809699e-05,
      "loss": 2.5941,
      "step": 18700
    },
    {
      "epoch": 1.136211817574543,
      "grad_norm": 2.7132601737976074,
      "learning_rate": 6.859050234276703e-05,
      "loss": 2.1543,
      "step": 18710
    },
    {
      "epoch": 1.1368190927309163,
      "grad_norm": 2.2912204265594482,
      "learning_rate": 6.856098128626895e-05,
      "loss": 2.7191,
      "step": 18720
    },
    {
      "epoch": 1.1374263678872898,
      "grad_norm": 2.6681315898895264,
      "learning_rate": 6.853145272341442e-05,
      "loss": 2.801,
      "step": 18730
    },
    {
      "epoch": 1.138033643043663,
      "grad_norm": 2.373469114303589,
      "learning_rate": 6.850191666614529e-05,
      "loss": 2.6503,
      "step": 18740
    },
    {
      "epoch": 1.1386409182000365,
      "grad_norm": 2.610427141189575,
      "learning_rate": 6.847237312640636e-05,
      "loss": 2.9711,
      "step": 18750
    },
    {
      "epoch": 1.1392481933564098,
      "grad_norm": 3.5685172080993652,
      "learning_rate": 6.844282211614555e-05,
      "loss": 3.1339,
      "step": 18760
    },
    {
      "epoch": 1.139855468512783,
      "grad_norm": 4.443227291107178,
      "learning_rate": 6.841326364731378e-05,
      "loss": 2.8832,
      "step": 18770
    },
    {
      "epoch": 1.1404627436691566,
      "grad_norm": 4.449583530426025,
      "learning_rate": 6.838369773186489e-05,
      "loss": 2.6673,
      "step": 18780
    },
    {
      "epoch": 1.1410700188255298,
      "grad_norm": 1.6796214580535889,
      "learning_rate": 6.835412438175587e-05,
      "loss": 2.3514,
      "step": 18790
    },
    {
      "epoch": 1.1416772939819033,
      "grad_norm": 2.4434661865234375,
      "learning_rate": 6.832454360894664e-05,
      "loss": 2.7228,
      "step": 18800
    },
    {
      "epoch": 1.1422845691382766,
      "grad_norm": 4.639281272888184,
      "learning_rate": 6.829495542540013e-05,
      "loss": 2.5177,
      "step": 18810
    },
    {
      "epoch": 1.1428918442946499,
      "grad_norm": 2.321110248565674,
      "learning_rate": 6.826535984308226e-05,
      "loss": 2.5804,
      "step": 18820
    },
    {
      "epoch": 1.1434991194510233,
      "grad_norm": 2.468493938446045,
      "learning_rate": 6.823575687396197e-05,
      "loss": 2.5753,
      "step": 18830
    },
    {
      "epoch": 1.1441063946073966,
      "grad_norm": 3.132894515991211,
      "learning_rate": 6.820614653001118e-05,
      "loss": 2.4328,
      "step": 18840
    },
    {
      "epoch": 1.1447136697637699,
      "grad_norm": 3.638265609741211,
      "learning_rate": 6.817652882320478e-05,
      "loss": 2.8444,
      "step": 18850
    },
    {
      "epoch": 1.1453209449201434,
      "grad_norm": 2.322166681289673,
      "learning_rate": 6.814690376552061e-05,
      "loss": 2.934,
      "step": 18860
    },
    {
      "epoch": 1.1459282200765166,
      "grad_norm": 3.0211212635040283,
      "learning_rate": 6.811727136893958e-05,
      "loss": 2.771,
      "step": 18870
    },
    {
      "epoch": 1.1465354952328901,
      "grad_norm": 4.1532487869262695,
      "learning_rate": 6.808763164544545e-05,
      "loss": 2.7294,
      "step": 18880
    },
    {
      "epoch": 1.1471427703892634,
      "grad_norm": 3.232945203781128,
      "learning_rate": 6.805798460702501e-05,
      "loss": 3.0454,
      "step": 18890
    },
    {
      "epoch": 1.1477500455456366,
      "grad_norm": 5.279325008392334,
      "learning_rate": 6.802833026566798e-05,
      "loss": 2.3277,
      "step": 18900
    },
    {
      "epoch": 1.1483573207020101,
      "grad_norm": 1.9837315082550049,
      "learning_rate": 6.799866863336709e-05,
      "loss": 2.2346,
      "step": 18910
    },
    {
      "epoch": 1.1489645958583834,
      "grad_norm": 2.1307289600372314,
      "learning_rate": 6.796899972211793e-05,
      "loss": 2.5201,
      "step": 18920
    },
    {
      "epoch": 1.1495718710147567,
      "grad_norm": 2.9869518280029297,
      "learning_rate": 6.793932354391912e-05,
      "loss": 2.612,
      "step": 18930
    },
    {
      "epoch": 1.1501791461711302,
      "grad_norm": 3.9560749530792236,
      "learning_rate": 6.790964011077214e-05,
      "loss": 2.3342,
      "step": 18940
    },
    {
      "epoch": 1.1507864213275034,
      "grad_norm": 2.165665626525879,
      "learning_rate": 6.787994943468147e-05,
      "loss": 2.4677,
      "step": 18950
    },
    {
      "epoch": 1.151393696483877,
      "grad_norm": 1.880224585533142,
      "learning_rate": 6.78502515276545e-05,
      "loss": 2.584,
      "step": 18960
    },
    {
      "epoch": 1.1520009716402502,
      "grad_norm": 2.3593711853027344,
      "learning_rate": 6.78205464017015e-05,
      "loss": 2.7922,
      "step": 18970
    },
    {
      "epoch": 1.1526082467966234,
      "grad_norm": 3.686448574066162,
      "learning_rate": 6.779083406883574e-05,
      "loss": 2.925,
      "step": 18980
    },
    {
      "epoch": 1.153215521952997,
      "grad_norm": 3.2737913131713867,
      "learning_rate": 6.776111454107333e-05,
      "loss": 2.5389,
      "step": 18990
    },
    {
      "epoch": 1.1538227971093702,
      "grad_norm": 2.3274335861206055,
      "learning_rate": 6.773138783043332e-05,
      "loss": 2.2351,
      "step": 19000
    },
    {
      "epoch": 1.1544300722657437,
      "grad_norm": 2.3992109298706055,
      "learning_rate": 6.770165394893768e-05,
      "loss": 2.5591,
      "step": 19010
    },
    {
      "epoch": 1.155037347422117,
      "grad_norm": 1.8318568468093872,
      "learning_rate": 6.767191290861125e-05,
      "loss": 2.501,
      "step": 19020
    },
    {
      "epoch": 1.1556446225784902,
      "grad_norm": 2.349989891052246,
      "learning_rate": 6.76421647214818e-05,
      "loss": 2.495,
      "step": 19030
    },
    {
      "epoch": 1.1562518977348637,
      "grad_norm": 3.177196741104126,
      "learning_rate": 6.761240939957997e-05,
      "loss": 2.6767,
      "step": 19040
    },
    {
      "epoch": 1.156859172891237,
      "grad_norm": 2.8659939765930176,
      "learning_rate": 6.758264695493926e-05,
      "loss": 2.7225,
      "step": 19050
    },
    {
      "epoch": 1.1574664480476105,
      "grad_norm": 3.858532428741455,
      "learning_rate": 6.75528773995961e-05,
      "loss": 2.4633,
      "step": 19060
    },
    {
      "epoch": 1.1580737232039837,
      "grad_norm": 3.482025146484375,
      "learning_rate": 6.752310074558976e-05,
      "loss": 2.3787,
      "step": 19070
    },
    {
      "epoch": 1.158680998360357,
      "grad_norm": 3.8820321559906006,
      "learning_rate": 6.749331700496241e-05,
      "loss": 2.291,
      "step": 19080
    },
    {
      "epoch": 1.1592882735167305,
      "grad_norm": 1.5308717489242554,
      "learning_rate": 6.746352618975906e-05,
      "loss": 2.1055,
      "step": 19090
    },
    {
      "epoch": 1.1598955486731037,
      "grad_norm": 3.314790964126587,
      "learning_rate": 6.743372831202758e-05,
      "loss": 2.6144,
      "step": 19100
    },
    {
      "epoch": 1.1605028238294772,
      "grad_norm": 3.550525426864624,
      "learning_rate": 6.74039233838187e-05,
      "loss": 3.0324,
      "step": 19110
    },
    {
      "epoch": 1.1611100989858505,
      "grad_norm": 3.497748613357544,
      "learning_rate": 6.737411141718605e-05,
      "loss": 2.595,
      "step": 19120
    },
    {
      "epoch": 1.1617173741422238,
      "grad_norm": 3.83198881149292,
      "learning_rate": 6.7344292424186e-05,
      "loss": 2.7898,
      "step": 19130
    },
    {
      "epoch": 1.1623246492985972,
      "grad_norm": 2.334838390350342,
      "learning_rate": 6.73144664168779e-05,
      "loss": 2.7773,
      "step": 19140
    },
    {
      "epoch": 1.1629319244549705,
      "grad_norm": 2.8769452571868896,
      "learning_rate": 6.72846334073238e-05,
      "loss": 2.4315,
      "step": 19150
    },
    {
      "epoch": 1.163539199611344,
      "grad_norm": 2.4439210891723633,
      "learning_rate": 6.72547934075887e-05,
      "loss": 2.5814,
      "step": 19160
    },
    {
      "epoch": 1.1641464747677173,
      "grad_norm": 2.988670825958252,
      "learning_rate": 6.722494642974029e-05,
      "loss": 2.7846,
      "step": 19170
    },
    {
      "epoch": 1.1647537499240905,
      "grad_norm": 3.377523422241211,
      "learning_rate": 6.719509248584923e-05,
      "loss": 2.4309,
      "step": 19180
    },
    {
      "epoch": 1.165361025080464,
      "grad_norm": 3.076775312423706,
      "learning_rate": 6.716523158798894e-05,
      "loss": 2.3314,
      "step": 19190
    },
    {
      "epoch": 1.1659683002368373,
      "grad_norm": 2.3106672763824463,
      "learning_rate": 6.71353637482356e-05,
      "loss": 2.5311,
      "step": 19200
    },
    {
      "epoch": 1.1665755753932108,
      "grad_norm": 3.6481943130493164,
      "learning_rate": 6.710548897866823e-05,
      "loss": 3.0469,
      "step": 19210
    },
    {
      "epoch": 1.167182850549584,
      "grad_norm": 3.3571038246154785,
      "learning_rate": 6.70756072913687e-05,
      "loss": 2.6979,
      "step": 19220
    },
    {
      "epoch": 1.1677901257059573,
      "grad_norm": 4.9065423011779785,
      "learning_rate": 6.704571869842165e-05,
      "loss": 2.9417,
      "step": 19230
    },
    {
      "epoch": 1.1683974008623308,
      "grad_norm": 3.7374236583709717,
      "learning_rate": 6.701582321191447e-05,
      "loss": 2.5323,
      "step": 19240
    },
    {
      "epoch": 1.169004676018704,
      "grad_norm": 2.9483425617218018,
      "learning_rate": 6.698592084393737e-05,
      "loss": 2.4509,
      "step": 19250
    },
    {
      "epoch": 1.1696119511750775,
      "grad_norm": 1.431779384613037,
      "learning_rate": 6.695601160658337e-05,
      "loss": 2.393,
      "step": 19260
    },
    {
      "epoch": 1.1702192263314508,
      "grad_norm": 2.6285629272460938,
      "learning_rate": 6.692609551194823e-05,
      "loss": 2.4952,
      "step": 19270
    },
    {
      "epoch": 1.170826501487824,
      "grad_norm": 3.037973165512085,
      "learning_rate": 6.689617257213047e-05,
      "loss": 2.5693,
      "step": 19280
    },
    {
      "epoch": 1.1714337766441976,
      "grad_norm": 4.201833724975586,
      "learning_rate": 6.686624279923146e-05,
      "loss": 2.3125,
      "step": 19290
    },
    {
      "epoch": 1.1720410518005708,
      "grad_norm": 3.032525062561035,
      "learning_rate": 6.683630620535523e-05,
      "loss": 2.2741,
      "step": 19300
    },
    {
      "epoch": 1.172648326956944,
      "grad_norm": 2.4311795234680176,
      "learning_rate": 6.680636280260862e-05,
      "loss": 2.2582,
      "step": 19310
    },
    {
      "epoch": 1.1732556021133176,
      "grad_norm": 3.137951612472534,
      "learning_rate": 6.677641260310123e-05,
      "loss": 2.7797,
      "step": 19320
    },
    {
      "epoch": 1.1738628772696909,
      "grad_norm": 3.5777177810668945,
      "learning_rate": 6.674645561894542e-05,
      "loss": 2.7015,
      "step": 19330
    },
    {
      "epoch": 1.1744701524260643,
      "grad_norm": 3.0934813022613525,
      "learning_rate": 6.671649186225621e-05,
      "loss": 2.995,
      "step": 19340
    },
    {
      "epoch": 1.1750774275824376,
      "grad_norm": 3.1707093715667725,
      "learning_rate": 6.668652134515148e-05,
      "loss": 2.8876,
      "step": 19350
    },
    {
      "epoch": 1.1756847027388109,
      "grad_norm": 2.1778063774108887,
      "learning_rate": 6.665654407975175e-05,
      "loss": 2.6914,
      "step": 19360
    },
    {
      "epoch": 1.1762919778951844,
      "grad_norm": 3.9636690616607666,
      "learning_rate": 6.662656007818034e-05,
      "loss": 2.6877,
      "step": 19370
    },
    {
      "epoch": 1.1768992530515576,
      "grad_norm": 2.074467897415161,
      "learning_rate": 6.65965693525632e-05,
      "loss": 2.3225,
      "step": 19380
    },
    {
      "epoch": 1.177506528207931,
      "grad_norm": 2.52164626121521,
      "learning_rate": 6.65665719150291e-05,
      "loss": 2.0731,
      "step": 19390
    },
    {
      "epoch": 1.1781138033643044,
      "grad_norm": 2.157137155532837,
      "learning_rate": 6.653656777770948e-05,
      "loss": 2.369,
      "step": 19400
    },
    {
      "epoch": 1.1787210785206776,
      "grad_norm": 4.99690055847168,
      "learning_rate": 6.650655695273849e-05,
      "loss": 2.5966,
      "step": 19410
    },
    {
      "epoch": 1.1793283536770511,
      "grad_norm": 3.314547300338745,
      "learning_rate": 6.647653945225295e-05,
      "loss": 2.7663,
      "step": 19420
    },
    {
      "epoch": 1.1799356288334244,
      "grad_norm": 2.6381945610046387,
      "learning_rate": 6.644651528839247e-05,
      "loss": 2.3725,
      "step": 19430
    },
    {
      "epoch": 1.1805429039897977,
      "grad_norm": 4.326511383056641,
      "learning_rate": 6.641648447329927e-05,
      "loss": 2.4339,
      "step": 19440
    },
    {
      "epoch": 1.1811501791461712,
      "grad_norm": 2.83031964302063,
      "learning_rate": 6.638644701911827e-05,
      "loss": 2.7578,
      "step": 19450
    },
    {
      "epoch": 1.1817574543025444,
      "grad_norm": 3.0619211196899414,
      "learning_rate": 6.635640293799715e-05,
      "loss": 2.3061,
      "step": 19460
    },
    {
      "epoch": 1.182364729458918,
      "grad_norm": 2.3007211685180664,
      "learning_rate": 6.632635224208617e-05,
      "loss": 2.1853,
      "step": 19470
    },
    {
      "epoch": 1.1829720046152912,
      "grad_norm": 1.911182165145874,
      "learning_rate": 6.629629494353832e-05,
      "loss": 2.5253,
      "step": 19480
    },
    {
      "epoch": 1.1835792797716644,
      "grad_norm": 2.8553895950317383,
      "learning_rate": 6.626623105450926e-05,
      "loss": 2.5523,
      "step": 19490
    },
    {
      "epoch": 1.184186554928038,
      "grad_norm": 2.9371843338012695,
      "learning_rate": 6.62361605871573e-05,
      "loss": 2.6298,
      "step": 19500
    },
    {
      "epoch": 1.1847938300844112,
      "grad_norm": 2.189279079437256,
      "learning_rate": 6.62060835536434e-05,
      "loss": 2.8272,
      "step": 19510
    },
    {
      "epoch": 1.1854011052407847,
      "grad_norm": 3.0224616527557373,
      "learning_rate": 6.617599996613122e-05,
      "loss": 2.5262,
      "step": 19520
    },
    {
      "epoch": 1.186008380397158,
      "grad_norm": 4.615950107574463,
      "learning_rate": 6.614590983678702e-05,
      "loss": 2.4734,
      "step": 19530
    },
    {
      "epoch": 1.1866156555535312,
      "grad_norm": 3.03226637840271,
      "learning_rate": 6.611581317777975e-05,
      "loss": 2.7798,
      "step": 19540
    },
    {
      "epoch": 1.1872229307099047,
      "grad_norm": 3.1414928436279297,
      "learning_rate": 6.608571000128094e-05,
      "loss": 2.6025,
      "step": 19550
    },
    {
      "epoch": 1.187830205866278,
      "grad_norm": 3.17708420753479,
      "learning_rate": 6.605560031946484e-05,
      "loss": 2.5456,
      "step": 19560
    },
    {
      "epoch": 1.1884374810226515,
      "grad_norm": 2.9208476543426514,
      "learning_rate": 6.602548414450825e-05,
      "loss": 2.6694,
      "step": 19570
    },
    {
      "epoch": 1.1890447561790247,
      "grad_norm": 4.650207042694092,
      "learning_rate": 6.599536148859066e-05,
      "loss": 2.5452,
      "step": 19580
    },
    {
      "epoch": 1.189652031335398,
      "grad_norm": 1.6223384141921997,
      "learning_rate": 6.596523236389411e-05,
      "loss": 2.1304,
      "step": 19590
    },
    {
      "epoch": 1.1902593064917715,
      "grad_norm": 1.9125583171844482,
      "learning_rate": 6.593509678260336e-05,
      "loss": 2.302,
      "step": 19600
    },
    {
      "epoch": 1.1908665816481447,
      "grad_norm": 2.3811147212982178,
      "learning_rate": 6.590495475690564e-05,
      "loss": 2.621,
      "step": 19610
    },
    {
      "epoch": 1.1914738568045182,
      "grad_norm": 2.5505738258361816,
      "learning_rate": 6.587480629899094e-05,
      "loss": 2.5656,
      "step": 19620
    },
    {
      "epoch": 1.1920811319608915,
      "grad_norm": 2.776683807373047,
      "learning_rate": 6.584465142105175e-05,
      "loss": 2.7172,
      "step": 19630
    },
    {
      "epoch": 1.1926884071172648,
      "grad_norm": 1.8379416465759277,
      "learning_rate": 6.581449013528315e-05,
      "loss": 2.7121,
      "step": 19640
    },
    {
      "epoch": 1.1932956822736382,
      "grad_norm": 3.8580167293548584,
      "learning_rate": 6.578432245388287e-05,
      "loss": 3.1018,
      "step": 19650
    },
    {
      "epoch": 1.1939029574300115,
      "grad_norm": 3.6514689922332764,
      "learning_rate": 6.575414838905122e-05,
      "loss": 2.4873,
      "step": 19660
    },
    {
      "epoch": 1.194510232586385,
      "grad_norm": 2.1390268802642822,
      "learning_rate": 6.572396795299106e-05,
      "loss": 2.4466,
      "step": 19670
    },
    {
      "epoch": 1.1951175077427583,
      "grad_norm": 3.1555287837982178,
      "learning_rate": 6.569378115790779e-05,
      "loss": 2.6334,
      "step": 19680
    },
    {
      "epoch": 1.1957247828991315,
      "grad_norm": 2.860470771789551,
      "learning_rate": 6.566358801600951e-05,
      "loss": 2.7804,
      "step": 19690
    },
    {
      "epoch": 1.196332058055505,
      "grad_norm": 5.374425888061523,
      "learning_rate": 6.563338853950677e-05,
      "loss": 2.4454,
      "step": 19700
    },
    {
      "epoch": 1.1969393332118783,
      "grad_norm": 2.4752254486083984,
      "learning_rate": 6.560318274061272e-05,
      "loss": 2.0948,
      "step": 19710
    },
    {
      "epoch": 1.1975466083682518,
      "grad_norm": 2.066869020462036,
      "learning_rate": 6.557297063154306e-05,
      "loss": 2.031,
      "step": 19720
    },
    {
      "epoch": 1.198153883524625,
      "grad_norm": 1.978930950164795,
      "learning_rate": 6.554275222451606e-05,
      "loss": 2.1649,
      "step": 19730
    },
    {
      "epoch": 1.1987611586809983,
      "grad_norm": 3.4390852451324463,
      "learning_rate": 6.551252753175253e-05,
      "loss": 2.4745,
      "step": 19740
    },
    {
      "epoch": 1.1993684338373718,
      "grad_norm": 1.4366161823272705,
      "learning_rate": 6.548229656547581e-05,
      "loss": 2.325,
      "step": 19750
    },
    {
      "epoch": 1.199975708993745,
      "grad_norm": 2.9953651428222656,
      "learning_rate": 6.545205933791176e-05,
      "loss": 2.8764,
      "step": 19760
    },
    {
      "epoch": 1.2005829841501183,
      "grad_norm": 3.6707868576049805,
      "learning_rate": 6.542181586128884e-05,
      "loss": 2.7512,
      "step": 19770
    },
    {
      "epoch": 1.2011902593064918,
      "grad_norm": 2.9500763416290283,
      "learning_rate": 6.539156614783795e-05,
      "loss": 2.706,
      "step": 19780
    },
    {
      "epoch": 1.201797534462865,
      "grad_norm": 2.4825327396392822,
      "learning_rate": 6.536131020979259e-05,
      "loss": 2.6133,
      "step": 19790
    },
    {
      "epoch": 1.2024048096192386,
      "grad_norm": 2.2091760635375977,
      "learning_rate": 6.533104805938873e-05,
      "loss": 2.3617,
      "step": 19800
    },
    {
      "epoch": 1.2030120847756118,
      "grad_norm": 4.045386791229248,
      "learning_rate": 6.530077970886487e-05,
      "loss": 2.6707,
      "step": 19810
    },
    {
      "epoch": 1.203619359931985,
      "grad_norm": 2.7953388690948486,
      "learning_rate": 6.5270505170462e-05,
      "loss": 2.6265,
      "step": 19820
    },
    {
      "epoch": 1.2042266350883586,
      "grad_norm": 3.3442606925964355,
      "learning_rate": 6.524022445642364e-05,
      "loss": 2.5761,
      "step": 19830
    },
    {
      "epoch": 1.2048339102447319,
      "grad_norm": 3.958348274230957,
      "learning_rate": 6.520993757899578e-05,
      "loss": 2.7599,
      "step": 19840
    },
    {
      "epoch": 1.2054411854011051,
      "grad_norm": 3.774454355239868,
      "learning_rate": 6.517964455042694e-05,
      "loss": 2.5717,
      "step": 19850
    },
    {
      "epoch": 1.2060484605574786,
      "grad_norm": 3.190234899520874,
      "learning_rate": 6.514934538296806e-05,
      "loss": 2.9018,
      "step": 19860
    },
    {
      "epoch": 1.2066557357138519,
      "grad_norm": 2.958052158355713,
      "learning_rate": 6.511904008887266e-05,
      "loss": 2.714,
      "step": 19870
    },
    {
      "epoch": 1.2072630108702254,
      "grad_norm": 2.2357232570648193,
      "learning_rate": 6.508872868039665e-05,
      "loss": 2.685,
      "step": 19880
    },
    {
      "epoch": 1.2078702860265986,
      "grad_norm": 3.2062454223632812,
      "learning_rate": 6.505841116979844e-05,
      "loss": 2.2947,
      "step": 19890
    },
    {
      "epoch": 1.208477561182972,
      "grad_norm": 4.588972568511963,
      "learning_rate": 6.502808756933893e-05,
      "loss": 2.9961,
      "step": 19900
    },
    {
      "epoch": 1.2090848363393454,
      "grad_norm": 4.260053634643555,
      "learning_rate": 6.499775789128149e-05,
      "loss": 2.6051,
      "step": 19910
    },
    {
      "epoch": 1.2096921114957186,
      "grad_norm": 2.297118663787842,
      "learning_rate": 6.496742214789187e-05,
      "loss": 2.3968,
      "step": 19920
    },
    {
      "epoch": 1.2102993866520921,
      "grad_norm": 3.185351610183716,
      "learning_rate": 6.493708035143834e-05,
      "loss": 2.2503,
      "step": 19930
    },
    {
      "epoch": 1.2109066618084654,
      "grad_norm": 2.1761982440948486,
      "learning_rate": 6.490673251419167e-05,
      "loss": 2.1333,
      "step": 19940
    },
    {
      "epoch": 1.2115139369648387,
      "grad_norm": 2.4281084537506104,
      "learning_rate": 6.487637864842493e-05,
      "loss": 2.3706,
      "step": 19950
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 2.9184699058532715,
      "learning_rate": 6.484601876641375e-05,
      "loss": 2.5778,
      "step": 19960
    },
    {
      "epoch": 1.2127284872775854,
      "grad_norm": 2.3664495944976807,
      "learning_rate": 6.481565288043612e-05,
      "loss": 2.6005,
      "step": 19970
    },
    {
      "epoch": 1.213335762433959,
      "grad_norm": 3.119828701019287,
      "learning_rate": 6.478528100277252e-05,
      "loss": 2.3055,
      "step": 19980
    },
    {
      "epoch": 1.2139430375903322,
      "grad_norm": 2.3889520168304443,
      "learning_rate": 6.47549031457058e-05,
      "loss": 2.0424,
      "step": 19990
    },
    {
      "epoch": 1.2145503127467054,
      "grad_norm": 4.285065174102783,
      "learning_rate": 6.472451932152126e-05,
      "loss": 2.2603,
      "step": 20000
    },
    {
      "epoch": 1.2145503127467054,
      "eval_loss": 4.765407085418701,
      "eval_runtime": 2135.4904,
      "eval_samples_per_second": 7.711,
      "eval_steps_per_second": 1.928,
      "step": 20000
    },
    {
      "epoch": 1.215157587903079,
      "grad_norm": 3.4901137351989746,
      "learning_rate": 6.469412954250659e-05,
      "loss": 4.1374,
      "step": 20010
    },
    {
      "epoch": 1.2157648630594522,
      "grad_norm": 3.4074771404266357,
      "learning_rate": 6.466373382095193e-05,
      "loss": 3.2283,
      "step": 20020
    },
    {
      "epoch": 1.2163721382158257,
      "grad_norm": 3.2748515605926514,
      "learning_rate": 6.463333216914975e-05,
      "loss": 2.614,
      "step": 20030
    },
    {
      "epoch": 1.216979413372199,
      "grad_norm": 2.7810747623443604,
      "learning_rate": 6.460292459939499e-05,
      "loss": 2.8301,
      "step": 20040
    },
    {
      "epoch": 1.2175866885285722,
      "grad_norm": 2.8880527019500732,
      "learning_rate": 6.457251112398495e-05,
      "loss": 2.2043,
      "step": 20050
    },
    {
      "epoch": 1.2181939636849457,
      "grad_norm": 2.3751771450042725,
      "learning_rate": 6.454209175521933e-05,
      "loss": 2.341,
      "step": 20060
    },
    {
      "epoch": 1.218801238841319,
      "grad_norm": 2.4781720638275146,
      "learning_rate": 6.45116665054002e-05,
      "loss": 2.6841,
      "step": 20070
    },
    {
      "epoch": 1.2194085139976925,
      "grad_norm": 3.8175785541534424,
      "learning_rate": 6.448123538683202e-05,
      "loss": 2.6489,
      "step": 20080
    },
    {
      "epoch": 1.2200157891540657,
      "grad_norm": 4.409505367279053,
      "learning_rate": 6.44507984118216e-05,
      "loss": 2.9078,
      "step": 20090
    },
    {
      "epoch": 1.220623064310439,
      "grad_norm": 2.739489793777466,
      "learning_rate": 6.442035559267816e-05,
      "loss": 2.6939,
      "step": 20100
    },
    {
      "epoch": 1.2212303394668125,
      "grad_norm": 2.6375012397766113,
      "learning_rate": 6.438990694171324e-05,
      "loss": 2.5822,
      "step": 20110
    },
    {
      "epoch": 1.2218376146231857,
      "grad_norm": 2.273073673248291,
      "learning_rate": 6.43594524712408e-05,
      "loss": 2.7206,
      "step": 20120
    },
    {
      "epoch": 1.2224448897795592,
      "grad_norm": 2.0854604244232178,
      "learning_rate": 6.432899219357707e-05,
      "loss": 2.8262,
      "step": 20130
    },
    {
      "epoch": 1.2230521649359325,
      "grad_norm": 3.2935259342193604,
      "learning_rate": 6.429852612104068e-05,
      "loss": 2.6782,
      "step": 20140
    },
    {
      "epoch": 1.2236594400923058,
      "grad_norm": 3.040837049484253,
      "learning_rate": 6.426805426595264e-05,
      "loss": 2.4173,
      "step": 20150
    },
    {
      "epoch": 1.2242667152486792,
      "grad_norm": 2.465299606323242,
      "learning_rate": 6.423757664063618e-05,
      "loss": 2.5255,
      "step": 20160
    },
    {
      "epoch": 1.2248739904050525,
      "grad_norm": 3.119044542312622,
      "learning_rate": 6.420709325741698e-05,
      "loss": 2.9472,
      "step": 20170
    },
    {
      "epoch": 1.225481265561426,
      "grad_norm": 4.143502712249756,
      "learning_rate": 6.417660412862301e-05,
      "loss": 2.778,
      "step": 20180
    },
    {
      "epoch": 1.2260885407177993,
      "grad_norm": 4.70554780960083,
      "learning_rate": 6.414610926658454e-05,
      "loss": 2.6971,
      "step": 20190
    },
    {
      "epoch": 1.2266958158741725,
      "grad_norm": 5.717899322509766,
      "learning_rate": 6.411560868363418e-05,
      "loss": 2.7435,
      "step": 20200
    },
    {
      "epoch": 1.227303091030546,
      "grad_norm": 3.976651668548584,
      "learning_rate": 6.408510239210687e-05,
      "loss": 2.7478,
      "step": 20210
    },
    {
      "epoch": 1.2279103661869193,
      "grad_norm": 3.6981194019317627,
      "learning_rate": 6.405459040433982e-05,
      "loss": 2.6037,
      "step": 20220
    },
    {
      "epoch": 1.2285176413432926,
      "grad_norm": 4.247230529785156,
      "learning_rate": 6.402407273267258e-05,
      "loss": 2.7133,
      "step": 20230
    },
    {
      "epoch": 1.229124916499666,
      "grad_norm": 2.585404872894287,
      "learning_rate": 6.399354938944695e-05,
      "loss": 2.4433,
      "step": 20240
    },
    {
      "epoch": 1.2297321916560393,
      "grad_norm": 3.8890633583068848,
      "learning_rate": 6.39630203870071e-05,
      "loss": 2.2589,
      "step": 20250
    },
    {
      "epoch": 1.2303394668124128,
      "grad_norm": 3.353623390197754,
      "learning_rate": 6.393248573769944e-05,
      "loss": 2.4495,
      "step": 20260
    },
    {
      "epoch": 1.230946741968786,
      "grad_norm": 4.677420616149902,
      "learning_rate": 6.390194545387262e-05,
      "loss": 2.5765,
      "step": 20270
    },
    {
      "epoch": 1.2315540171251593,
      "grad_norm": 2.8406872749328613,
      "learning_rate": 6.387139954787766e-05,
      "loss": 2.8161,
      "step": 20280
    },
    {
      "epoch": 1.2321612922815328,
      "grad_norm": 4.3912739753723145,
      "learning_rate": 6.384084803206783e-05,
      "loss": 3.0866,
      "step": 20290
    },
    {
      "epoch": 1.232768567437906,
      "grad_norm": 4.975179672241211,
      "learning_rate": 6.381029091879859e-05,
      "loss": 2.7611,
      "step": 20300
    },
    {
      "epoch": 1.2333758425942793,
      "grad_norm": 4.9431633949279785,
      "learning_rate": 6.377972822042777e-05,
      "loss": 2.877,
      "step": 20310
    },
    {
      "epoch": 1.2339831177506528,
      "grad_norm": 3.741105794906616,
      "learning_rate": 6.374915994931538e-05,
      "loss": 2.7421,
      "step": 20320
    },
    {
      "epoch": 1.234590392907026,
      "grad_norm": 2.405423164367676,
      "learning_rate": 6.371858611782375e-05,
      "loss": 2.7408,
      "step": 20330
    },
    {
      "epoch": 1.2351976680633996,
      "grad_norm": 3.0889055728912354,
      "learning_rate": 6.368800673831739e-05,
      "loss": 2.3263,
      "step": 20340
    },
    {
      "epoch": 1.2358049432197729,
      "grad_norm": 2.361995220184326,
      "learning_rate": 6.365742182316311e-05,
      "loss": 2.5766,
      "step": 20350
    },
    {
      "epoch": 1.2364122183761461,
      "grad_norm": 2.4335343837738037,
      "learning_rate": 6.362683138472993e-05,
      "loss": 2.6098,
      "step": 20360
    },
    {
      "epoch": 1.2370194935325196,
      "grad_norm": 2.5569517612457275,
      "learning_rate": 6.359623543538911e-05,
      "loss": 2.6505,
      "step": 20370
    },
    {
      "epoch": 1.2376267686888929,
      "grad_norm": 3.406017780303955,
      "learning_rate": 6.356563398751414e-05,
      "loss": 2.6334,
      "step": 20380
    },
    {
      "epoch": 1.2382340438452664,
      "grad_norm": 2.017145872116089,
      "learning_rate": 6.353502705348076e-05,
      "loss": 2.4408,
      "step": 20390
    },
    {
      "epoch": 1.2388413190016396,
      "grad_norm": 2.447360038757324,
      "learning_rate": 6.350441464566685e-05,
      "loss": 2.3155,
      "step": 20400
    },
    {
      "epoch": 1.239448594158013,
      "grad_norm": 1.992370843887329,
      "learning_rate": 6.347379677645258e-05,
      "loss": 2.7824,
      "step": 20410
    },
    {
      "epoch": 1.2400558693143864,
      "grad_norm": 4.336535930633545,
      "learning_rate": 6.34431734582203e-05,
      "loss": 3.0446,
      "step": 20420
    },
    {
      "epoch": 1.2406631444707596,
      "grad_norm": 3.573961019515991,
      "learning_rate": 6.341254470335459e-05,
      "loss": 2.6619,
      "step": 20430
    },
    {
      "epoch": 1.2412704196271331,
      "grad_norm": 5.28442907333374,
      "learning_rate": 6.338191052424218e-05,
      "loss": 2.4311,
      "step": 20440
    },
    {
      "epoch": 1.2418776947835064,
      "grad_norm": 3.3319523334503174,
      "learning_rate": 6.335127093327202e-05,
      "loss": 2.3926,
      "step": 20450
    },
    {
      "epoch": 1.2424849699398797,
      "grad_norm": 3.057497978210449,
      "learning_rate": 6.332062594283528e-05,
      "loss": 2.8658,
      "step": 20460
    },
    {
      "epoch": 1.2430922450962532,
      "grad_norm": 2.911808967590332,
      "learning_rate": 6.328997556532523e-05,
      "loss": 2.7201,
      "step": 20470
    },
    {
      "epoch": 1.2436995202526264,
      "grad_norm": 2.543738603591919,
      "learning_rate": 6.325931981313742e-05,
      "loss": 2.5183,
      "step": 20480
    },
    {
      "epoch": 1.244306795409,
      "grad_norm": 2.2952888011932373,
      "learning_rate": 6.32286586986695e-05,
      "loss": 2.49,
      "step": 20490
    },
    {
      "epoch": 1.2449140705653732,
      "grad_norm": 1.9004569053649902,
      "learning_rate": 6.319799223432133e-05,
      "loss": 2.3376,
      "step": 20500
    },
    {
      "epoch": 1.2455213457217464,
      "grad_norm": 2.7379403114318848,
      "learning_rate": 6.316732043249487e-05,
      "loss": 2.401,
      "step": 20510
    },
    {
      "epoch": 1.24612862087812,
      "grad_norm": 4.507526397705078,
      "learning_rate": 6.313664330559434e-05,
      "loss": 2.3865,
      "step": 20520
    },
    {
      "epoch": 1.2467358960344932,
      "grad_norm": 3.9046521186828613,
      "learning_rate": 6.310596086602605e-05,
      "loss": 2.7231,
      "step": 20530
    },
    {
      "epoch": 1.2473431711908667,
      "grad_norm": 3.1867032051086426,
      "learning_rate": 6.307527312619844e-05,
      "loss": 2.4854,
      "step": 20540
    },
    {
      "epoch": 1.24795044634724,
      "grad_norm": 2.826472759246826,
      "learning_rate": 6.304458009852213e-05,
      "loss": 2.4239,
      "step": 20550
    },
    {
      "epoch": 1.2485577215036132,
      "grad_norm": 3.444580078125,
      "learning_rate": 6.301388179540989e-05,
      "loss": 2.4153,
      "step": 20560
    },
    {
      "epoch": 1.2491649966599867,
      "grad_norm": 3.936414957046509,
      "learning_rate": 6.298317822927657e-05,
      "loss": 2.7891,
      "step": 20570
    },
    {
      "epoch": 1.24977227181636,
      "grad_norm": 2.3163673877716064,
      "learning_rate": 6.295246941253921e-05,
      "loss": 2.7982,
      "step": 20580
    },
    {
      "epoch": 1.2503795469727335,
      "grad_norm": 3.969325304031372,
      "learning_rate": 6.292175535761693e-05,
      "loss": 3.0247,
      "step": 20590
    },
    {
      "epoch": 1.2509868221291067,
      "grad_norm": 2.7818760871887207,
      "learning_rate": 6.289103607693098e-05,
      "loss": 2.3491,
      "step": 20600
    },
    {
      "epoch": 1.25159409728548,
      "grad_norm": 3.09981632232666,
      "learning_rate": 6.286031158290475e-05,
      "loss": 2.8606,
      "step": 20610
    },
    {
      "epoch": 1.2522013724418535,
      "grad_norm": 2.6808972358703613,
      "learning_rate": 6.282958188796368e-05,
      "loss": 2.9483,
      "step": 20620
    },
    {
      "epoch": 1.2528086475982267,
      "grad_norm": 3.4457409381866455,
      "learning_rate": 6.27988470045354e-05,
      "loss": 2.631,
      "step": 20630
    },
    {
      "epoch": 1.2534159227546002,
      "grad_norm": 3.349346876144409,
      "learning_rate": 6.276810694504952e-05,
      "loss": 2.6214,
      "step": 20640
    },
    {
      "epoch": 1.2540231979109735,
      "grad_norm": 3.4724783897399902,
      "learning_rate": 6.273736172193784e-05,
      "loss": 2.5258,
      "step": 20650
    },
    {
      "epoch": 1.2546304730673468,
      "grad_norm": 2.964789628982544,
      "learning_rate": 6.270661134763422e-05,
      "loss": 2.7654,
      "step": 20660
    },
    {
      "epoch": 1.25523774822372,
      "grad_norm": 2.577388286590576,
      "learning_rate": 6.267585583457463e-05,
      "loss": 2.76,
      "step": 20670
    },
    {
      "epoch": 1.2558450233800935,
      "grad_norm": 4.024555206298828,
      "learning_rate": 6.264509519519702e-05,
      "loss": 2.6322,
      "step": 20680
    },
    {
      "epoch": 1.256452298536467,
      "grad_norm": 3.1269311904907227,
      "learning_rate": 6.261432944194155e-05,
      "loss": 2.6465,
      "step": 20690
    },
    {
      "epoch": 1.2570595736928403,
      "grad_norm": 4.282843112945557,
      "learning_rate": 6.258355858725034e-05,
      "loss": 2.4415,
      "step": 20700
    },
    {
      "epoch": 1.2576668488492135,
      "grad_norm": 2.955406665802002,
      "learning_rate": 6.255278264356764e-05,
      "loss": 2.1593,
      "step": 20710
    },
    {
      "epoch": 1.2582741240055868,
      "grad_norm": 1.9249638319015503,
      "learning_rate": 6.252200162333968e-05,
      "loss": 2.419,
      "step": 20720
    },
    {
      "epoch": 1.2588813991619603,
      "grad_norm": 3.936511754989624,
      "learning_rate": 6.249121553901487e-05,
      "loss": 3.0074,
      "step": 20730
    },
    {
      "epoch": 1.2594886743183336,
      "grad_norm": 3.4903860092163086,
      "learning_rate": 6.246042440304353e-05,
      "loss": 3.0311,
      "step": 20740
    },
    {
      "epoch": 1.260095949474707,
      "grad_norm": 3.6491518020629883,
      "learning_rate": 6.242962822787815e-05,
      "loss": 2.6126,
      "step": 20750
    },
    {
      "epoch": 1.2607032246310803,
      "grad_norm": 2.792163133621216,
      "learning_rate": 6.239882702597312e-05,
      "loss": 2.7509,
      "step": 20760
    },
    {
      "epoch": 1.2613104997874536,
      "grad_norm": 3.5425806045532227,
      "learning_rate": 6.2368020809785e-05,
      "loss": 2.6333,
      "step": 20770
    },
    {
      "epoch": 1.261917774943827,
      "grad_norm": 2.8736572265625,
      "learning_rate": 6.233720959177227e-05,
      "loss": 2.5302,
      "step": 20780
    },
    {
      "epoch": 1.2625250501002003,
      "grad_norm": 3.4156265258789062,
      "learning_rate": 6.230639338439549e-05,
      "loss": 2.4697,
      "step": 20790
    },
    {
      "epoch": 1.2631323252565738,
      "grad_norm": 5.400347709655762,
      "learning_rate": 6.227557220011724e-05,
      "loss": 2.0453,
      "step": 20800
    },
    {
      "epoch": 1.263739600412947,
      "grad_norm": 2.754124164581299,
      "learning_rate": 6.224474605140208e-05,
      "loss": 2.6736,
      "step": 20810
    },
    {
      "epoch": 1.2643468755693203,
      "grad_norm": 2.96878981590271,
      "learning_rate": 6.221391495071659e-05,
      "loss": 2.6931,
      "step": 20820
    },
    {
      "epoch": 1.2649541507256938,
      "grad_norm": 4.199796199798584,
      "learning_rate": 6.21830789105294e-05,
      "loss": 2.8396,
      "step": 20830
    },
    {
      "epoch": 1.265561425882067,
      "grad_norm": 2.667391777038574,
      "learning_rate": 6.215223794331105e-05,
      "loss": 2.6693,
      "step": 20840
    },
    {
      "epoch": 1.2661687010384406,
      "grad_norm": 3.71038556098938,
      "learning_rate": 6.212139206153411e-05,
      "loss": 2.5971,
      "step": 20850
    },
    {
      "epoch": 1.2667759761948139,
      "grad_norm": 3.978278160095215,
      "learning_rate": 6.209054127767319e-05,
      "loss": 2.2115,
      "step": 20860
    },
    {
      "epoch": 1.2673832513511871,
      "grad_norm": 3.128221035003662,
      "learning_rate": 6.205968560420483e-05,
      "loss": 2.9095,
      "step": 20870
    },
    {
      "epoch": 1.2679905265075606,
      "grad_norm": 2.8017990589141846,
      "learning_rate": 6.202882505360754e-05,
      "loss": 2.6776,
      "step": 20880
    },
    {
      "epoch": 1.2685978016639339,
      "grad_norm": 3.013211250305176,
      "learning_rate": 6.199795963836181e-05,
      "loss": 2.8064,
      "step": 20890
    },
    {
      "epoch": 1.2692050768203074,
      "grad_norm": 2.938857078552246,
      "learning_rate": 6.196708937095013e-05,
      "loss": 3.1583,
      "step": 20900
    },
    {
      "epoch": 1.2698123519766806,
      "grad_norm": 2.303321599960327,
      "learning_rate": 6.193621426385691e-05,
      "loss": 3.1094,
      "step": 20910
    },
    {
      "epoch": 1.270419627133054,
      "grad_norm": 3.929781675338745,
      "learning_rate": 6.190533432956856e-05,
      "loss": 2.6843,
      "step": 20920
    },
    {
      "epoch": 1.2710269022894274,
      "grad_norm": 2.545546531677246,
      "learning_rate": 6.187444958057337e-05,
      "loss": 2.8631,
      "step": 20930
    },
    {
      "epoch": 1.2716341774458007,
      "grad_norm": 3.201791286468506,
      "learning_rate": 6.184356002936167e-05,
      "loss": 2.8408,
      "step": 20940
    },
    {
      "epoch": 1.2722414526021741,
      "grad_norm": 2.328400135040283,
      "learning_rate": 6.181266568842566e-05,
      "loss": 2.3986,
      "step": 20950
    },
    {
      "epoch": 1.2728487277585474,
      "grad_norm": 2.021923065185547,
      "learning_rate": 6.178176657025951e-05,
      "loss": 2.4333,
      "step": 20960
    },
    {
      "epoch": 1.2734560029149207,
      "grad_norm": 1.6367363929748535,
      "learning_rate": 6.175086268735932e-05,
      "loss": 2.4897,
      "step": 20970
    },
    {
      "epoch": 1.2740632780712942,
      "grad_norm": 2.039733409881592,
      "learning_rate": 6.171995405222312e-05,
      "loss": 2.0474,
      "step": 20980
    },
    {
      "epoch": 1.2746705532276674,
      "grad_norm": 1.8712878227233887,
      "learning_rate": 6.168904067735082e-05,
      "loss": 2.6634,
      "step": 20990
    },
    {
      "epoch": 1.275277828384041,
      "grad_norm": 3.358741521835327,
      "learning_rate": 6.165812257524433e-05,
      "loss": 2.662,
      "step": 21000
    },
    {
      "epoch": 1.2758851035404142,
      "grad_norm": 1.8810309171676636,
      "learning_rate": 6.16271997584074e-05,
      "loss": 2.3464,
      "step": 21010
    },
    {
      "epoch": 1.2764923786967874,
      "grad_norm": 1.8315495252609253,
      "learning_rate": 6.159627223934572e-05,
      "loss": 2.6016,
      "step": 21020
    },
    {
      "epoch": 1.277099653853161,
      "grad_norm": 3.630162000656128,
      "learning_rate": 6.156534003056682e-05,
      "loss": 2.9792,
      "step": 21030
    },
    {
      "epoch": 1.2777069290095342,
      "grad_norm": 4.535848140716553,
      "learning_rate": 6.153440314458023e-05,
      "loss": 2.877,
      "step": 21040
    },
    {
      "epoch": 1.2783142041659077,
      "grad_norm": 2.838282823562622,
      "learning_rate": 6.150346159389733e-05,
      "loss": 2.5757,
      "step": 21050
    },
    {
      "epoch": 1.278921479322281,
      "grad_norm": 3.6043384075164795,
      "learning_rate": 6.147251539103131e-05,
      "loss": 2.6959,
      "step": 21060
    },
    {
      "epoch": 1.2795287544786542,
      "grad_norm": 2.9576737880706787,
      "learning_rate": 6.144156454849738e-05,
      "loss": 2.4675,
      "step": 21070
    },
    {
      "epoch": 1.2801360296350277,
      "grad_norm": 2.62068772315979,
      "learning_rate": 6.141060907881253e-05,
      "loss": 2.7074,
      "step": 21080
    },
    {
      "epoch": 1.280743304791401,
      "grad_norm": 3.4352567195892334,
      "learning_rate": 6.137964899449563e-05,
      "loss": 2.9334,
      "step": 21090
    },
    {
      "epoch": 1.2813505799477745,
      "grad_norm": 4.163893699645996,
      "learning_rate": 6.134868430806744e-05,
      "loss": 2.935,
      "step": 21100
    },
    {
      "epoch": 1.2819578551041477,
      "grad_norm": 4.15990686416626,
      "learning_rate": 6.131771503205059e-05,
      "loss": 2.872,
      "step": 21110
    },
    {
      "epoch": 1.282565130260521,
      "grad_norm": 2.8338327407836914,
      "learning_rate": 6.128674117896949e-05,
      "loss": 2.8753,
      "step": 21120
    },
    {
      "epoch": 1.2831724054168943,
      "grad_norm": 3.5615296363830566,
      "learning_rate": 6.125576276135052e-05,
      "loss": 2.6709,
      "step": 21130
    },
    {
      "epoch": 1.2837796805732677,
      "grad_norm": 3.7590079307556152,
      "learning_rate": 6.122477979172182e-05,
      "loss": 3.1619,
      "step": 21140
    },
    {
      "epoch": 1.2843869557296412,
      "grad_norm": 4.064615726470947,
      "learning_rate": 6.119379228261343e-05,
      "loss": 2.7933,
      "step": 21150
    },
    {
      "epoch": 1.2849942308860145,
      "grad_norm": 4.8393473625183105,
      "learning_rate": 6.116280024655713e-05,
      "loss": 2.5293,
      "step": 21160
    },
    {
      "epoch": 1.2856015060423878,
      "grad_norm": 2.614415168762207,
      "learning_rate": 6.113180369608665e-05,
      "loss": 2.1371,
      "step": 21170
    },
    {
      "epoch": 1.286208781198761,
      "grad_norm": 2.486380100250244,
      "learning_rate": 6.110080264373745e-05,
      "loss": 2.6536,
      "step": 21180
    },
    {
      "epoch": 1.2868160563551345,
      "grad_norm": 4.013667106628418,
      "learning_rate": 6.106979710204688e-05,
      "loss": 2.6351,
      "step": 21190
    },
    {
      "epoch": 1.2874233315115078,
      "grad_norm": 4.170774459838867,
      "learning_rate": 6.103878708355403e-05,
      "loss": 2.7686,
      "step": 21200
    },
    {
      "epoch": 1.2880306066678813,
      "grad_norm": 3.6690614223480225,
      "learning_rate": 6.1007772600799906e-05,
      "loss": 3.03,
      "step": 21210
    },
    {
      "epoch": 1.2886378818242545,
      "grad_norm": 3.255805730819702,
      "learning_rate": 6.097675366632722e-05,
      "loss": 2.8766,
      "step": 21220
    },
    {
      "epoch": 1.2892451569806278,
      "grad_norm": 4.471016883850098,
      "learning_rate": 6.0945730292680536e-05,
      "loss": 2.6339,
      "step": 21230
    },
    {
      "epoch": 1.2898524321370013,
      "grad_norm": 4.196210861206055,
      "learning_rate": 6.091470249240617e-05,
      "loss": 2.3186,
      "step": 21240
    },
    {
      "epoch": 1.2904597072933746,
      "grad_norm": 2.1683483123779297,
      "learning_rate": 6.088367027805232e-05,
      "loss": 2.412,
      "step": 21250
    },
    {
      "epoch": 1.291066982449748,
      "grad_norm": 3.4631378650665283,
      "learning_rate": 6.0852633662168844e-05,
      "loss": 2.7048,
      "step": 21260
    },
    {
      "epoch": 1.2916742576061213,
      "grad_norm": 3.541494131088257,
      "learning_rate": 6.082159265730749e-05,
      "loss": 2.9259,
      "step": 21270
    },
    {
      "epoch": 1.2922815327624946,
      "grad_norm": 4.253425121307373,
      "learning_rate": 6.07905472760217e-05,
      "loss": 2.7385,
      "step": 21280
    },
    {
      "epoch": 1.292888807918868,
      "grad_norm": 3.765054941177368,
      "learning_rate": 6.075949753086675e-05,
      "loss": 2.8073,
      "step": 21290
    },
    {
      "epoch": 1.2934960830752413,
      "grad_norm": 3.4373791217803955,
      "learning_rate": 6.072844343439964e-05,
      "loss": 2.5931,
      "step": 21300
    },
    {
      "epoch": 1.2941033582316148,
      "grad_norm": 2.0204315185546875,
      "learning_rate": 6.069738499917912e-05,
      "loss": 2.5877,
      "step": 21310
    },
    {
      "epoch": 1.294710633387988,
      "grad_norm": 1.9526100158691406,
      "learning_rate": 6.066632223776576e-05,
      "loss": 2.5474,
      "step": 21320
    },
    {
      "epoch": 1.2953179085443614,
      "grad_norm": 1.4473509788513184,
      "learning_rate": 6.0635255162721794e-05,
      "loss": 2.4153,
      "step": 21330
    },
    {
      "epoch": 1.2959251837007348,
      "grad_norm": 2.340562343597412,
      "learning_rate": 6.0604183786611267e-05,
      "loss": 2.7624,
      "step": 21340
    },
    {
      "epoch": 1.296532458857108,
      "grad_norm": 4.3537163734436035,
      "learning_rate": 6.057310812199993e-05,
      "loss": 3.0857,
      "step": 21350
    },
    {
      "epoch": 1.2971397340134816,
      "grad_norm": 5.142550468444824,
      "learning_rate": 6.054202818145528e-05,
      "loss": 3.159,
      "step": 21360
    },
    {
      "epoch": 1.2977470091698549,
      "grad_norm": 4.992645740509033,
      "learning_rate": 6.051094397754653e-05,
      "loss": 2.7896,
      "step": 21370
    },
    {
      "epoch": 1.2983542843262281,
      "grad_norm": 3.926384925842285,
      "learning_rate": 6.047985552284463e-05,
      "loss": 2.4322,
      "step": 21380
    },
    {
      "epoch": 1.2989615594826016,
      "grad_norm": 2.866549253463745,
      "learning_rate": 6.0448762829922264e-05,
      "loss": 2.4043,
      "step": 21390
    },
    {
      "epoch": 1.2995688346389749,
      "grad_norm": 4.98522424697876,
      "learning_rate": 6.041766591135379e-05,
      "loss": 2.4174,
      "step": 21400
    },
    {
      "epoch": 1.3001761097953484,
      "grad_norm": 2.309819459915161,
      "learning_rate": 6.0386564779715306e-05,
      "loss": 2.5319,
      "step": 21410
    },
    {
      "epoch": 1.3007833849517216,
      "grad_norm": 2.5871975421905518,
      "learning_rate": 6.0355459447584606e-05,
      "loss": 2.6414,
      "step": 21420
    },
    {
      "epoch": 1.301390660108095,
      "grad_norm": 3.943535566329956,
      "learning_rate": 6.032434992754118e-05,
      "loss": 2.8946,
      "step": 21430
    },
    {
      "epoch": 1.3019979352644684,
      "grad_norm": 3.230353355407715,
      "learning_rate": 6.0293236232166214e-05,
      "loss": 2.7543,
      "step": 21440
    },
    {
      "epoch": 1.3026052104208417,
      "grad_norm": 4.1385297775268555,
      "learning_rate": 6.026211837404256e-05,
      "loss": 2.621,
      "step": 21450
    },
    {
      "epoch": 1.3032124855772151,
      "grad_norm": 2.7003324031829834,
      "learning_rate": 6.0230996365754846e-05,
      "loss": 2.9408,
      "step": 21460
    },
    {
      "epoch": 1.3038197607335884,
      "grad_norm": 3.6790828704833984,
      "learning_rate": 6.019987021988922e-05,
      "loss": 2.9621,
      "step": 21470
    },
    {
      "epoch": 1.3044270358899617,
      "grad_norm": 2.835644006729126,
      "learning_rate": 6.0168739949033636e-05,
      "loss": 2.7848,
      "step": 21480
    },
    {
      "epoch": 1.3050343110463352,
      "grad_norm": 2.091580867767334,
      "learning_rate": 6.0137605565777675e-05,
      "loss": 2.5245,
      "step": 21490
    },
    {
      "epoch": 1.3056415862027084,
      "grad_norm": 1.9624614715576172,
      "learning_rate": 6.010646708271256e-05,
      "loss": 2.6904,
      "step": 21500
    },
    {
      "epoch": 1.306248861359082,
      "grad_norm": 3.4775118827819824,
      "learning_rate": 6.00753245124312e-05,
      "loss": 2.5138,
      "step": 21510
    },
    {
      "epoch": 1.3068561365154552,
      "grad_norm": 3.8629584312438965,
      "learning_rate": 6.0044177867528137e-05,
      "loss": 2.8084,
      "step": 21520
    },
    {
      "epoch": 1.3074634116718284,
      "grad_norm": 3.9971203804016113,
      "learning_rate": 6.001302716059959e-05,
      "loss": 2.6949,
      "step": 21530
    },
    {
      "epoch": 1.308070686828202,
      "grad_norm": 4.1520233154296875,
      "learning_rate": 5.998187240424337e-05,
      "loss": 3.0651,
      "step": 21540
    },
    {
      "epoch": 1.3086779619845752,
      "grad_norm": 4.637875080108643,
      "learning_rate": 5.9950713611058984e-05,
      "loss": 2.5664,
      "step": 21550
    },
    {
      "epoch": 1.3092852371409487,
      "grad_norm": 5.607118129730225,
      "learning_rate": 5.991955079364754e-05,
      "loss": 2.8098,
      "step": 21560
    },
    {
      "epoch": 1.309892512297322,
      "grad_norm": 2.292689561843872,
      "learning_rate": 5.988838396461176e-05,
      "loss": 2.7408,
      "step": 21570
    },
    {
      "epoch": 1.3104997874536952,
      "grad_norm": 3.7114505767822266,
      "learning_rate": 5.9857213136556025e-05,
      "loss": 2.7223,
      "step": 21580
    },
    {
      "epoch": 1.3111070626100685,
      "grad_norm": 7.141550540924072,
      "learning_rate": 5.982603832208631e-05,
      "loss": 2.8527,
      "step": 21590
    },
    {
      "epoch": 1.311714337766442,
      "grad_norm": 2.821082830429077,
      "learning_rate": 5.979485953381021e-05,
      "loss": 3.0179,
      "step": 21600
    },
    {
      "epoch": 1.3123216129228155,
      "grad_norm": 2.5703558921813965,
      "learning_rate": 5.976367678433691e-05,
      "loss": 3.0289,
      "step": 21610
    },
    {
      "epoch": 1.3129288880791887,
      "grad_norm": 3.4816243648529053,
      "learning_rate": 5.97324900862772e-05,
      "loss": 2.7871,
      "step": 21620
    },
    {
      "epoch": 1.313536163235562,
      "grad_norm": 3.2945876121520996,
      "learning_rate": 5.970129945224353e-05,
      "loss": 2.613,
      "step": 21630
    },
    {
      "epoch": 1.3141434383919353,
      "grad_norm": 3.9125823974609375,
      "learning_rate": 5.9670104894849835e-05,
      "loss": 2.7688,
      "step": 21640
    },
    {
      "epoch": 1.3147507135483087,
      "grad_norm": 3.0704874992370605,
      "learning_rate": 5.963890642671172e-05,
      "loss": 2.7326,
      "step": 21650
    },
    {
      "epoch": 1.315357988704682,
      "grad_norm": 3.2898261547088623,
      "learning_rate": 5.9607704060446324e-05,
      "loss": 2.6567,
      "step": 21660
    },
    {
      "epoch": 1.3159652638610555,
      "grad_norm": 3.456526756286621,
      "learning_rate": 5.95764978086724e-05,
      "loss": 2.5844,
      "step": 21670
    },
    {
      "epoch": 1.3165725390174288,
      "grad_norm": 3.346571207046509,
      "learning_rate": 5.954528768401022e-05,
      "loss": 2.9701,
      "step": 21680
    },
    {
      "epoch": 1.317179814173802,
      "grad_norm": 3.486804246902466,
      "learning_rate": 5.9514073699081695e-05,
      "loss": 2.874,
      "step": 21690
    },
    {
      "epoch": 1.3177870893301755,
      "grad_norm": 3.0383589267730713,
      "learning_rate": 5.948285586651024e-05,
      "loss": 2.7746,
      "step": 21700
    },
    {
      "epoch": 1.3183943644865488,
      "grad_norm": 2.869778633117676,
      "learning_rate": 5.945163419892086e-05,
      "loss": 2.4596,
      "step": 21710
    },
    {
      "epoch": 1.3190016396429223,
      "grad_norm": 2.5410208702087402,
      "learning_rate": 5.9420408708940054e-05,
      "loss": 2.4793,
      "step": 21720
    },
    {
      "epoch": 1.3196089147992955,
      "grad_norm": 3.3095149993896484,
      "learning_rate": 5.938917940919595e-05,
      "loss": 2.3251,
      "step": 21730
    },
    {
      "epoch": 1.3202161899556688,
      "grad_norm": 3.149768352508545,
      "learning_rate": 5.9357946312318146e-05,
      "loss": 2.6834,
      "step": 21740
    },
    {
      "epoch": 1.3208234651120423,
      "grad_norm": 2.3719444274902344,
      "learning_rate": 5.9326709430937824e-05,
      "loss": 2.6701,
      "step": 21750
    },
    {
      "epoch": 1.3214307402684156,
      "grad_norm": 4.254992485046387,
      "learning_rate": 5.9295468777687655e-05,
      "loss": 2.6941,
      "step": 21760
    },
    {
      "epoch": 1.322038015424789,
      "grad_norm": 3.289699077606201,
      "learning_rate": 5.926422436520188e-05,
      "loss": 2.7134,
      "step": 21770
    },
    {
      "epoch": 1.3226452905811623,
      "grad_norm": 4.876587867736816,
      "learning_rate": 5.923297620611623e-05,
      "loss": 2.6509,
      "step": 21780
    },
    {
      "epoch": 1.3232525657375356,
      "grad_norm": 2.805048942565918,
      "learning_rate": 5.920172431306793e-05,
      "loss": 2.7724,
      "step": 21790
    },
    {
      "epoch": 1.323859840893909,
      "grad_norm": 4.087673664093018,
      "learning_rate": 5.9170468698695806e-05,
      "loss": 2.5182,
      "step": 21800
    },
    {
      "epoch": 1.3244671160502823,
      "grad_norm": 2.6417086124420166,
      "learning_rate": 5.913920937564006e-05,
      "loss": 2.633,
      "step": 21810
    },
    {
      "epoch": 1.3250743912066558,
      "grad_norm": 2.8950648307800293,
      "learning_rate": 5.910794635654249e-05,
      "loss": 2.9212,
      "step": 21820
    },
    {
      "epoch": 1.325681666363029,
      "grad_norm": 2.3034918308258057,
      "learning_rate": 5.907667965404636e-05,
      "loss": 2.5715,
      "step": 21830
    },
    {
      "epoch": 1.3262889415194024,
      "grad_norm": 3.14546799659729,
      "learning_rate": 5.904540928079643e-05,
      "loss": 2.6411,
      "step": 21840
    },
    {
      "epoch": 1.3268962166757758,
      "grad_norm": 3.279974937438965,
      "learning_rate": 5.901413524943891e-05,
      "loss": 2.5793,
      "step": 21850
    },
    {
      "epoch": 1.327503491832149,
      "grad_norm": 3.7888078689575195,
      "learning_rate": 5.898285757262153e-05,
      "loss": 2.6487,
      "step": 21860
    },
    {
      "epoch": 1.3281107669885226,
      "grad_norm": 4.502017974853516,
      "learning_rate": 5.89515762629935e-05,
      "loss": 2.4727,
      "step": 21870
    },
    {
      "epoch": 1.3287180421448959,
      "grad_norm": 2.009223699569702,
      "learning_rate": 5.8920291333205456e-05,
      "loss": 2.7704,
      "step": 21880
    },
    {
      "epoch": 1.3293253173012691,
      "grad_norm": 5.767812728881836,
      "learning_rate": 5.888900279590952e-05,
      "loss": 3.1273,
      "step": 21890
    },
    {
      "epoch": 1.3299325924576426,
      "grad_norm": 3.68043851852417,
      "learning_rate": 5.885771066375929e-05,
      "loss": 2.4875,
      "step": 21900
    },
    {
      "epoch": 1.3305398676140159,
      "grad_norm": 3.085026502609253,
      "learning_rate": 5.88264149494098e-05,
      "loss": 2.6459,
      "step": 21910
    },
    {
      "epoch": 1.3311471427703894,
      "grad_norm": 4.43045711517334,
      "learning_rate": 5.879511566551753e-05,
      "loss": 2.8969,
      "step": 21920
    },
    {
      "epoch": 1.3317544179267626,
      "grad_norm": 6.409984111785889,
      "learning_rate": 5.8763812824740396e-05,
      "loss": 2.69,
      "step": 21930
    },
    {
      "epoch": 1.332361693083136,
      "grad_norm": 4.073386192321777,
      "learning_rate": 5.873250643973781e-05,
      "loss": 2.2704,
      "step": 21940
    },
    {
      "epoch": 1.3329689682395094,
      "grad_norm": 3.3479273319244385,
      "learning_rate": 5.870119652317051e-05,
      "loss": 2.6998,
      "step": 21950
    },
    {
      "epoch": 1.3335762433958827,
      "grad_norm": 2.5724830627441406,
      "learning_rate": 5.866988308770078e-05,
      "loss": 2.8148,
      "step": 21960
    },
    {
      "epoch": 1.3341835185522561,
      "grad_norm": 2.6555888652801514,
      "learning_rate": 5.863856614599223e-05,
      "loss": 2.9863,
      "step": 21970
    },
    {
      "epoch": 1.3347907937086294,
      "grad_norm": 3.493964195251465,
      "learning_rate": 5.860724571070997e-05,
      "loss": 2.7344,
      "step": 21980
    },
    {
      "epoch": 1.3353980688650027,
      "grad_norm": 4.195796489715576,
      "learning_rate": 5.857592179452045e-05,
      "loss": 2.0962,
      "step": 21990
    },
    {
      "epoch": 1.3360053440213762,
      "grad_norm": 2.120410680770874,
      "learning_rate": 5.854459441009157e-05,
      "loss": 2.5834,
      "step": 22000
    },
    {
      "epoch": 1.3366126191777494,
      "grad_norm": 2.760554075241089,
      "learning_rate": 5.851326357009264e-05,
      "loss": 2.5706,
      "step": 22010
    },
    {
      "epoch": 1.337219894334123,
      "grad_norm": 3.075066089630127,
      "learning_rate": 5.848192928719431e-05,
      "loss": 2.0138,
      "step": 22020
    },
    {
      "epoch": 1.3378271694904962,
      "grad_norm": 2.670544147491455,
      "learning_rate": 5.8450591574068714e-05,
      "loss": 2.3805,
      "step": 22030
    },
    {
      "epoch": 1.3384344446468694,
      "grad_norm": 3.305924654006958,
      "learning_rate": 5.84192504433893e-05,
      "loss": 2.4513,
      "step": 22040
    },
    {
      "epoch": 1.3390417198032427,
      "grad_norm": 2.3399271965026855,
      "learning_rate": 5.838790590783091e-05,
      "loss": 2.516,
      "step": 22050
    },
    {
      "epoch": 1.3396489949596162,
      "grad_norm": 3.9069762229919434,
      "learning_rate": 5.835655798006977e-05,
      "loss": 2.7584,
      "step": 22060
    },
    {
      "epoch": 1.3402562701159897,
      "grad_norm": 4.0764665603637695,
      "learning_rate": 5.83252066727835e-05,
      "loss": 3.1977,
      "step": 22070
    },
    {
      "epoch": 1.340863545272363,
      "grad_norm": 3.6798324584960938,
      "learning_rate": 5.829385199865107e-05,
      "loss": 2.8765,
      "step": 22080
    },
    {
      "epoch": 1.3414708204287362,
      "grad_norm": 4.898472785949707,
      "learning_rate": 5.826249397035281e-05,
      "loss": 2.8072,
      "step": 22090
    },
    {
      "epoch": 1.3420780955851095,
      "grad_norm": 3.4977941513061523,
      "learning_rate": 5.823113260057037e-05,
      "loss": 2.6501,
      "step": 22100
    },
    {
      "epoch": 1.342685370741483,
      "grad_norm": 3.924835205078125,
      "learning_rate": 5.819976790198683e-05,
      "loss": 2.8051,
      "step": 22110
    },
    {
      "epoch": 1.3432926458978562,
      "grad_norm": 6.537227153778076,
      "learning_rate": 5.816839988728655e-05,
      "loss": 2.9127,
      "step": 22120
    },
    {
      "epoch": 1.3438999210542297,
      "grad_norm": 3.4998831748962402,
      "learning_rate": 5.813702856915527e-05,
      "loss": 2.4606,
      "step": 22130
    },
    {
      "epoch": 1.344507196210603,
      "grad_norm": 2.6379802227020264,
      "learning_rate": 5.8105653960280046e-05,
      "loss": 2.6324,
      "step": 22140
    },
    {
      "epoch": 1.3451144713669763,
      "grad_norm": 2.821018934249878,
      "learning_rate": 5.807427607334927e-05,
      "loss": 2.7737,
      "step": 22150
    },
    {
      "epoch": 1.3457217465233497,
      "grad_norm": 2.850248098373413,
      "learning_rate": 5.8042894921052625e-05,
      "loss": 2.7474,
      "step": 22160
    },
    {
      "epoch": 1.346329021679723,
      "grad_norm": 3.9652552604675293,
      "learning_rate": 5.80115105160812e-05,
      "loss": 2.5097,
      "step": 22170
    },
    {
      "epoch": 1.3469362968360965,
      "grad_norm": 2.603175163269043,
      "learning_rate": 5.798012287112734e-05,
      "loss": 2.7573,
      "step": 22180
    },
    {
      "epoch": 1.3475435719924698,
      "grad_norm": 2.3527450561523438,
      "learning_rate": 5.7948731998884685e-05,
      "loss": 2.4207,
      "step": 22190
    },
    {
      "epoch": 1.348150847148843,
      "grad_norm": 3.2256553173065186,
      "learning_rate": 5.791733791204821e-05,
      "loss": 2.9746,
      "step": 22200
    },
    {
      "epoch": 1.3487581223052165,
      "grad_norm": 3.906283140182495,
      "learning_rate": 5.7885940623314196e-05,
      "loss": 2.8681,
      "step": 22210
    },
    {
      "epoch": 1.3493653974615898,
      "grad_norm": 2.486176013946533,
      "learning_rate": 5.78545401453802e-05,
      "loss": 2.7569,
      "step": 22220
    },
    {
      "epoch": 1.3499726726179633,
      "grad_norm": 2.351571798324585,
      "learning_rate": 5.7823136490945064e-05,
      "loss": 2.4502,
      "step": 22230
    },
    {
      "epoch": 1.3505799477743365,
      "grad_norm": 3.049487590789795,
      "learning_rate": 5.7791729672708935e-05,
      "loss": 2.654,
      "step": 22240
    },
    {
      "epoch": 1.3511872229307098,
      "grad_norm": 4.219225883483887,
      "learning_rate": 5.776031970337325e-05,
      "loss": 2.8522,
      "step": 22250
    },
    {
      "epoch": 1.3517944980870833,
      "grad_norm": 4.075656414031982,
      "learning_rate": 5.7728906595640675e-05,
      "loss": 2.8005,
      "step": 22260
    },
    {
      "epoch": 1.3524017732434566,
      "grad_norm": 2.510786771774292,
      "learning_rate": 5.769749036221517e-05,
      "loss": 2.776,
      "step": 22270
    },
    {
      "epoch": 1.35300904839983,
      "grad_norm": 2.4179751873016357,
      "learning_rate": 5.766607101580199e-05,
      "loss": 2.8535,
      "step": 22280
    },
    {
      "epoch": 1.3536163235562033,
      "grad_norm": 3.44973087310791,
      "learning_rate": 5.7634648569107576e-05,
      "loss": 2.6799,
      "step": 22290
    },
    {
      "epoch": 1.3542235987125766,
      "grad_norm": 2.944216728210449,
      "learning_rate": 5.7603223034839706e-05,
      "loss": 2.8292,
      "step": 22300
    },
    {
      "epoch": 1.35483087386895,
      "grad_norm": 3.320988655090332,
      "learning_rate": 5.757179442570733e-05,
      "loss": 2.7824,
      "step": 22310
    },
    {
      "epoch": 1.3554381490253233,
      "grad_norm": 3.166428327560425,
      "learning_rate": 5.754036275442072e-05,
      "loss": 2.5251,
      "step": 22320
    },
    {
      "epoch": 1.3560454241816968,
      "grad_norm": 2.2508811950683594,
      "learning_rate": 5.75089280336913e-05,
      "loss": 2.5814,
      "step": 22330
    },
    {
      "epoch": 1.35665269933807,
      "grad_norm": 2.174131155014038,
      "learning_rate": 5.74774902762318e-05,
      "loss": 2.4802,
      "step": 22340
    },
    {
      "epoch": 1.3572599744944434,
      "grad_norm": 3.5341365337371826,
      "learning_rate": 5.744604949475615e-05,
      "loss": 2.6805,
      "step": 22350
    },
    {
      "epoch": 1.3578672496508168,
      "grad_norm": 3.389753580093384,
      "learning_rate": 5.7414605701979495e-05,
      "loss": 2.8115,
      "step": 22360
    },
    {
      "epoch": 1.35847452480719,
      "grad_norm": 2.4739692211151123,
      "learning_rate": 5.738315891061819e-05,
      "loss": 2.5391,
      "step": 22370
    },
    {
      "epoch": 1.3590817999635636,
      "grad_norm": 3.220909833908081,
      "learning_rate": 5.7351709133389854e-05,
      "loss": 2.6945,
      "step": 22380
    },
    {
      "epoch": 1.3596890751199369,
      "grad_norm": 2.8787894248962402,
      "learning_rate": 5.732025638301325e-05,
      "loss": 2.7369,
      "step": 22390
    },
    {
      "epoch": 1.3602963502763101,
      "grad_norm": 2.8097362518310547,
      "learning_rate": 5.7288800672208375e-05,
      "loss": 2.4229,
      "step": 22400
    },
    {
      "epoch": 1.3609036254326836,
      "grad_norm": 2.210697650909424,
      "learning_rate": 5.725734201369643e-05,
      "loss": 2.6182,
      "step": 22410
    },
    {
      "epoch": 1.3615109005890569,
      "grad_norm": 3.180356502532959,
      "learning_rate": 5.722588042019979e-05,
      "loss": 2.4975,
      "step": 22420
    },
    {
      "epoch": 1.3621181757454304,
      "grad_norm": 2.918942928314209,
      "learning_rate": 5.7194415904442025e-05,
      "loss": 2.4865,
      "step": 22430
    },
    {
      "epoch": 1.3627254509018036,
      "grad_norm": 2.257718324661255,
      "learning_rate": 5.716294847914788e-05,
      "loss": 2.4308,
      "step": 22440
    },
    {
      "epoch": 1.363332726058177,
      "grad_norm": 2.5397374629974365,
      "learning_rate": 5.7131478157043296e-05,
      "loss": 2.5307,
      "step": 22450
    },
    {
      "epoch": 1.3639400012145504,
      "grad_norm": 2.7799534797668457,
      "learning_rate": 5.710000495085537e-05,
      "loss": 2.4707,
      "step": 22460
    },
    {
      "epoch": 1.3645472763709237,
      "grad_norm": 2.7174432277679443,
      "learning_rate": 5.706852887331237e-05,
      "loss": 2.7378,
      "step": 22470
    },
    {
      "epoch": 1.3651545515272971,
      "grad_norm": 4.260305404663086,
      "learning_rate": 5.70370499371437e-05,
      "loss": 2.6866,
      "step": 22480
    },
    {
      "epoch": 1.3657618266836704,
      "grad_norm": 3.2714710235595703,
      "learning_rate": 5.7005568155079984e-05,
      "loss": 2.7691,
      "step": 22490
    },
    {
      "epoch": 1.3663691018400437,
      "grad_norm": 2.305741310119629,
      "learning_rate": 5.69740835398529e-05,
      "loss": 2.6716,
      "step": 22500
    },
    {
      "epoch": 1.366976376996417,
      "grad_norm": 2.7289156913757324,
      "learning_rate": 5.694259610419539e-05,
      "loss": 2.9191,
      "step": 22510
    },
    {
      "epoch": 1.3675836521527904,
      "grad_norm": 4.397889137268066,
      "learning_rate": 5.691110586084143e-05,
      "loss": 2.7418,
      "step": 22520
    },
    {
      "epoch": 1.368190927309164,
      "grad_norm": 4.3157782554626465,
      "learning_rate": 5.687961282252619e-05,
      "loss": 2.8386,
      "step": 22530
    },
    {
      "epoch": 1.3687982024655372,
      "grad_norm": 2.317596912384033,
      "learning_rate": 5.6848117001985955e-05,
      "loss": 2.566,
      "step": 22540
    },
    {
      "epoch": 1.3694054776219104,
      "grad_norm": 3.6683709621429443,
      "learning_rate": 5.681661841195816e-05,
      "loss": 3.032,
      "step": 22550
    },
    {
      "epoch": 1.3700127527782837,
      "grad_norm": 2.8482370376586914,
      "learning_rate": 5.678511706518129e-05,
      "loss": 2.6506,
      "step": 22560
    },
    {
      "epoch": 1.3706200279346572,
      "grad_norm": 4.204015731811523,
      "learning_rate": 5.6753612974395035e-05,
      "loss": 2.3081,
      "step": 22570
    },
    {
      "epoch": 1.3712273030910305,
      "grad_norm": 3.239100933074951,
      "learning_rate": 5.6722106152340116e-05,
      "loss": 2.907,
      "step": 22580
    },
    {
      "epoch": 1.371834578247404,
      "grad_norm": 2.7926976680755615,
      "learning_rate": 5.669059661175842e-05,
      "loss": 2.5801,
      "step": 22590
    },
    {
      "epoch": 1.3724418534037772,
      "grad_norm": 3.859246253967285,
      "learning_rate": 5.66590843653929e-05,
      "loss": 2.652,
      "step": 22600
    },
    {
      "epoch": 1.3730491285601505,
      "grad_norm": 2.2194583415985107,
      "learning_rate": 5.66275694259876e-05,
      "loss": 2.3921,
      "step": 22610
    },
    {
      "epoch": 1.373656403716524,
      "grad_norm": 2.5100624561309814,
      "learning_rate": 5.6596051806287665e-05,
      "loss": 2.2813,
      "step": 22620
    },
    {
      "epoch": 1.3742636788728972,
      "grad_norm": 2.6881730556488037,
      "learning_rate": 5.6564531519039356e-05,
      "loss": 2.3414,
      "step": 22630
    },
    {
      "epoch": 1.3748709540292707,
      "grad_norm": 3.3396410942077637,
      "learning_rate": 5.6533008576989924e-05,
      "loss": 2.5374,
      "step": 22640
    },
    {
      "epoch": 1.375478229185644,
      "grad_norm": 3.886131525039673,
      "learning_rate": 5.650148299288778e-05,
      "loss": 2.6182,
      "step": 22650
    },
    {
      "epoch": 1.3760855043420173,
      "grad_norm": 3.2587342262268066,
      "learning_rate": 5.646995477948238e-05,
      "loss": 2.4669,
      "step": 22660
    },
    {
      "epoch": 1.3766927794983908,
      "grad_norm": 3.275846242904663,
      "learning_rate": 5.643842394952422e-05,
      "loss": 2.8841,
      "step": 22670
    },
    {
      "epoch": 1.377300054654764,
      "grad_norm": 2.8572263717651367,
      "learning_rate": 5.640689051576486e-05,
      "loss": 2.7942,
      "step": 22680
    },
    {
      "epoch": 1.3779073298111375,
      "grad_norm": 3.24812650680542,
      "learning_rate": 5.637535449095694e-05,
      "loss": 3.1105,
      "step": 22690
    },
    {
      "epoch": 1.3785146049675108,
      "grad_norm": 4.361527919769287,
      "learning_rate": 5.6343815887854144e-05,
      "loss": 2.9648,
      "step": 22700
    },
    {
      "epoch": 1.379121880123884,
      "grad_norm": 4.190904140472412,
      "learning_rate": 5.631227471921116e-05,
      "loss": 2.476,
      "step": 22710
    },
    {
      "epoch": 1.3797291552802575,
      "grad_norm": 2.374239683151245,
      "learning_rate": 5.628073099778375e-05,
      "loss": 2.321,
      "step": 22720
    },
    {
      "epoch": 1.3803364304366308,
      "grad_norm": 3.3326830863952637,
      "learning_rate": 5.624918473632869e-05,
      "loss": 2.1257,
      "step": 22730
    },
    {
      "epoch": 1.3809437055930043,
      "grad_norm": 3.190699815750122,
      "learning_rate": 5.6217635947603796e-05,
      "loss": 2.5816,
      "step": 22740
    },
    {
      "epoch": 1.3815509807493775,
      "grad_norm": 3.31198787689209,
      "learning_rate": 5.61860846443679e-05,
      "loss": 2.8688,
      "step": 22750
    },
    {
      "epoch": 1.3821582559057508,
      "grad_norm": 2.889683961868286,
      "learning_rate": 5.615453083938086e-05,
      "loss": 2.6307,
      "step": 22760
    },
    {
      "epoch": 1.3827655310621243,
      "grad_norm": 2.310194253921509,
      "learning_rate": 5.612297454540352e-05,
      "loss": 2.2568,
      "step": 22770
    },
    {
      "epoch": 1.3833728062184976,
      "grad_norm": 4.910935401916504,
      "learning_rate": 5.609141577519777e-05,
      "loss": 2.8208,
      "step": 22780
    },
    {
      "epoch": 1.383980081374871,
      "grad_norm": 2.309549570083618,
      "learning_rate": 5.6059854541526435e-05,
      "loss": 2.8654,
      "step": 22790
    },
    {
      "epoch": 1.3845873565312443,
      "grad_norm": 3.3394947052001953,
      "learning_rate": 5.602829085715345e-05,
      "loss": 2.7005,
      "step": 22800
    },
    {
      "epoch": 1.3851946316876176,
      "grad_norm": 2.223846673965454,
      "learning_rate": 5.599672473484361e-05,
      "loss": 2.6381,
      "step": 22810
    },
    {
      "epoch": 1.385801906843991,
      "grad_norm": 2.9601211547851562,
      "learning_rate": 5.596515618736279e-05,
      "loss": 2.5668,
      "step": 22820
    },
    {
      "epoch": 1.3864091820003643,
      "grad_norm": 4.610170841217041,
      "learning_rate": 5.59335852274778e-05,
      "loss": 2.483,
      "step": 22830
    },
    {
      "epoch": 1.3870164571567378,
      "grad_norm": 3.2419135570526123,
      "learning_rate": 5.590201186795645e-05,
      "loss": 2.4331,
      "step": 22840
    },
    {
      "epoch": 1.387623732313111,
      "grad_norm": 2.7021000385284424,
      "learning_rate": 5.5870436121567495e-05,
      "loss": 2.2563,
      "step": 22850
    },
    {
      "epoch": 1.3882310074694844,
      "grad_norm": 3.003574848175049,
      "learning_rate": 5.5838858001080675e-05,
      "loss": 2.6297,
      "step": 22860
    },
    {
      "epoch": 1.3888382826258578,
      "grad_norm": 2.439497232437134,
      "learning_rate": 5.58072775192667e-05,
      "loss": 2.2348,
      "step": 22870
    },
    {
      "epoch": 1.389445557782231,
      "grad_norm": 2.485365867614746,
      "learning_rate": 5.5775694688897194e-05,
      "loss": 2.6057,
      "step": 22880
    },
    {
      "epoch": 1.3900528329386046,
      "grad_norm": 3.0672545433044434,
      "learning_rate": 5.574410952274477e-05,
      "loss": 2.7939,
      "step": 22890
    },
    {
      "epoch": 1.3906601080949779,
      "grad_norm": 5.338139533996582,
      "learning_rate": 5.571252203358298e-05,
      "loss": 2.8262,
      "step": 22900
    },
    {
      "epoch": 1.3912673832513511,
      "grad_norm": 4.712933540344238,
      "learning_rate": 5.56809322341863e-05,
      "loss": 2.7533,
      "step": 22910
    },
    {
      "epoch": 1.3918746584077246,
      "grad_norm": 3.139730215072632,
      "learning_rate": 5.564934013733014e-05,
      "loss": 2.5945,
      "step": 22920
    },
    {
      "epoch": 1.3924819335640979,
      "grad_norm": 2.5465805530548096,
      "learning_rate": 5.561774575579086e-05,
      "loss": 2.4756,
      "step": 22930
    },
    {
      "epoch": 1.3930892087204714,
      "grad_norm": 2.771050214767456,
      "learning_rate": 5.558614910234573e-05,
      "loss": 2.8132,
      "step": 22940
    },
    {
      "epoch": 1.3936964838768446,
      "grad_norm": 2.7988040447235107,
      "learning_rate": 5.5554550189772936e-05,
      "loss": 3.0058,
      "step": 22950
    },
    {
      "epoch": 1.394303759033218,
      "grad_norm": 2.2938296794891357,
      "learning_rate": 5.5522949030851566e-05,
      "loss": 2.4954,
      "step": 22960
    },
    {
      "epoch": 1.3949110341895912,
      "grad_norm": 1.9713258743286133,
      "learning_rate": 5.549134563836167e-05,
      "loss": 2.6005,
      "step": 22970
    },
    {
      "epoch": 1.3955183093459647,
      "grad_norm": 2.2807722091674805,
      "learning_rate": 5.545974002508412e-05,
      "loss": 2.4194,
      "step": 22980
    },
    {
      "epoch": 1.3961255845023381,
      "grad_norm": 2.9356327056884766,
      "learning_rate": 5.542813220380076e-05,
      "loss": 2.6523,
      "step": 22990
    },
    {
      "epoch": 1.3967328596587114,
      "grad_norm": 2.448540210723877,
      "learning_rate": 5.539652218729429e-05,
      "loss": 2.3983,
      "step": 23000
    },
    {
      "epoch": 1.3973401348150847,
      "grad_norm": 4.499155044555664,
      "learning_rate": 5.53649099883483e-05,
      "loss": 2.4543,
      "step": 23010
    },
    {
      "epoch": 1.397947409971458,
      "grad_norm": 2.0778086185455322,
      "learning_rate": 5.533329561974725e-05,
      "loss": 2.5156,
      "step": 23020
    },
    {
      "epoch": 1.3985546851278314,
      "grad_norm": 2.725215435028076,
      "learning_rate": 5.5301679094276524e-05,
      "loss": 2.9794,
      "step": 23030
    },
    {
      "epoch": 1.3991619602842047,
      "grad_norm": 2.812419891357422,
      "learning_rate": 5.527006042472234e-05,
      "loss": 2.9854,
      "step": 23040
    },
    {
      "epoch": 1.3997692354405782,
      "grad_norm": 4.447711944580078,
      "learning_rate": 5.5238439623871786e-05,
      "loss": 2.7851,
      "step": 23050
    },
    {
      "epoch": 1.4003765105969515,
      "grad_norm": 4.091264247894287,
      "learning_rate": 5.52068167045128e-05,
      "loss": 2.9773,
      "step": 23060
    },
    {
      "epoch": 1.4009837857533247,
      "grad_norm": 3.225407361984253,
      "learning_rate": 5.517519167943423e-05,
      "loss": 2.9345,
      "step": 23070
    },
    {
      "epoch": 1.4015910609096982,
      "grad_norm": 3.8771302700042725,
      "learning_rate": 5.514356456142572e-05,
      "loss": 2.9982,
      "step": 23080
    },
    {
      "epoch": 1.4021983360660715,
      "grad_norm": 3.7528936862945557,
      "learning_rate": 5.511193536327779e-05,
      "loss": 2.7677,
      "step": 23090
    },
    {
      "epoch": 1.402805611222445,
      "grad_norm": 5.968296051025391,
      "learning_rate": 5.508030409778177e-05,
      "loss": 2.7794,
      "step": 23100
    },
    {
      "epoch": 1.4034128863788182,
      "grad_norm": 3.158583641052246,
      "learning_rate": 5.504867077772988e-05,
      "loss": 2.6046,
      "step": 23110
    },
    {
      "epoch": 1.4040201615351915,
      "grad_norm": 4.139980316162109,
      "learning_rate": 5.50170354159151e-05,
      "loss": 2.754,
      "step": 23120
    },
    {
      "epoch": 1.404627436691565,
      "grad_norm": 2.6695451736450195,
      "learning_rate": 5.498539802513131e-05,
      "loss": 2.515,
      "step": 23130
    },
    {
      "epoch": 1.4052347118479382,
      "grad_norm": 3.537353754043579,
      "learning_rate": 5.4953758618173157e-05,
      "loss": 2.3141,
      "step": 23140
    },
    {
      "epoch": 1.4058419870043117,
      "grad_norm": 3.2646067142486572,
      "learning_rate": 5.492211720783611e-05,
      "loss": 2.7207,
      "step": 23150
    },
    {
      "epoch": 1.406449262160685,
      "grad_norm": 2.819648027420044,
      "learning_rate": 5.489047380691649e-05,
      "loss": 2.5025,
      "step": 23160
    },
    {
      "epoch": 1.4070565373170583,
      "grad_norm": 2.9913530349731445,
      "learning_rate": 5.485882842821136e-05,
      "loss": 2.3724,
      "step": 23170
    },
    {
      "epoch": 1.4076638124734318,
      "grad_norm": 3.3318161964416504,
      "learning_rate": 5.482718108451864e-05,
      "loss": 3.1968,
      "step": 23180
    },
    {
      "epoch": 1.408271087629805,
      "grad_norm": 2.7337393760681152,
      "learning_rate": 5.4795531788637e-05,
      "loss": 3.0549,
      "step": 23190
    },
    {
      "epoch": 1.4088783627861785,
      "grad_norm": 3.377821922302246,
      "learning_rate": 5.4763880553365945e-05,
      "loss": 2.5495,
      "step": 23200
    },
    {
      "epoch": 1.4094856379425518,
      "grad_norm": 3.5983150005340576,
      "learning_rate": 5.473222739150571e-05,
      "loss": 2.4126,
      "step": 23210
    },
    {
      "epoch": 1.410092913098925,
      "grad_norm": 2.923947334289551,
      "learning_rate": 5.470057231585736e-05,
      "loss": 2.3791,
      "step": 23220
    },
    {
      "epoch": 1.4107001882552985,
      "grad_norm": 2.9697744846343994,
      "learning_rate": 5.4668915339222684e-05,
      "loss": 2.7246,
      "step": 23230
    },
    {
      "epoch": 1.4113074634116718,
      "grad_norm": 4.365800857543945,
      "learning_rate": 5.4637256474404295e-05,
      "loss": 2.6353,
      "step": 23240
    },
    {
      "epoch": 1.4119147385680453,
      "grad_norm": 3.9990456104278564,
      "learning_rate": 5.460559573420554e-05,
      "loss": 2.9563,
      "step": 23250
    },
    {
      "epoch": 1.4125220137244185,
      "grad_norm": 3.0615992546081543,
      "learning_rate": 5.4573933131430524e-05,
      "loss": 3.1092,
      "step": 23260
    },
    {
      "epoch": 1.4131292888807918,
      "grad_norm": 2.4767332077026367,
      "learning_rate": 5.454226867888408e-05,
      "loss": 3.0401,
      "step": 23270
    },
    {
      "epoch": 1.4137365640371653,
      "grad_norm": 3.6153626441955566,
      "learning_rate": 5.451060238937187e-05,
      "loss": 2.9688,
      "step": 23280
    },
    {
      "epoch": 1.4143438391935386,
      "grad_norm": 3.4548134803771973,
      "learning_rate": 5.447893427570019e-05,
      "loss": 2.9016,
      "step": 23290
    },
    {
      "epoch": 1.414951114349912,
      "grad_norm": 5.641505718231201,
      "learning_rate": 5.444726435067618e-05,
      "loss": 2.9373,
      "step": 23300
    },
    {
      "epoch": 1.4155583895062853,
      "grad_norm": 2.6285817623138428,
      "learning_rate": 5.4415592627107625e-05,
      "loss": 2.8395,
      "step": 23310
    },
    {
      "epoch": 1.4161656646626586,
      "grad_norm": 2.7234792709350586,
      "learning_rate": 5.4383919117803084e-05,
      "loss": 2.6758,
      "step": 23320
    },
    {
      "epoch": 1.416772939819032,
      "grad_norm": 2.222658157348633,
      "learning_rate": 5.435224383557183e-05,
      "loss": 2.8533,
      "step": 23330
    },
    {
      "epoch": 1.4173802149754053,
      "grad_norm": 3.0123705863952637,
      "learning_rate": 5.432056679322386e-05,
      "loss": 2.6085,
      "step": 23340
    },
    {
      "epoch": 1.4179874901317788,
      "grad_norm": 3.527031898498535,
      "learning_rate": 5.428888800356986e-05,
      "loss": 2.6642,
      "step": 23350
    },
    {
      "epoch": 1.418594765288152,
      "grad_norm": 3.5415754318237305,
      "learning_rate": 5.425720747942126e-05,
      "loss": 2.782,
      "step": 23360
    },
    {
      "epoch": 1.4192020404445254,
      "grad_norm": 4.127784252166748,
      "learning_rate": 5.422552523359014e-05,
      "loss": 2.4474,
      "step": 23370
    },
    {
      "epoch": 1.4198093156008988,
      "grad_norm": 3.933048963546753,
      "learning_rate": 5.419384127888931e-05,
      "loss": 2.4443,
      "step": 23380
    },
    {
      "epoch": 1.4204165907572721,
      "grad_norm": 4.0830535888671875,
      "learning_rate": 5.4162155628132296e-05,
      "loss": 2.3658,
      "step": 23390
    },
    {
      "epoch": 1.4210238659136456,
      "grad_norm": 4.245389461517334,
      "learning_rate": 5.413046829413324e-05,
      "loss": 3.0285,
      "step": 23400
    },
    {
      "epoch": 1.4216311410700189,
      "grad_norm": 4.5657057762146,
      "learning_rate": 5.409877928970703e-05,
      "loss": 2.7018,
      "step": 23410
    },
    {
      "epoch": 1.4222384162263921,
      "grad_norm": 2.4825048446655273,
      "learning_rate": 5.406708862766919e-05,
      "loss": 2.6678,
      "step": 23420
    },
    {
      "epoch": 1.4228456913827654,
      "grad_norm": 2.3897950649261475,
      "learning_rate": 5.403539632083595e-05,
      "loss": 2.5731,
      "step": 23430
    },
    {
      "epoch": 1.4234529665391389,
      "grad_norm": 3.257643699645996,
      "learning_rate": 5.4003702382024156e-05,
      "loss": 2.5704,
      "step": 23440
    },
    {
      "epoch": 1.4240602416955124,
      "grad_norm": 5.677215576171875,
      "learning_rate": 5.397200682405137e-05,
      "loss": 2.7207,
      "step": 23450
    },
    {
      "epoch": 1.4246675168518856,
      "grad_norm": 4.155137062072754,
      "learning_rate": 5.394030965973574e-05,
      "loss": 2.4783,
      "step": 23460
    },
    {
      "epoch": 1.425274792008259,
      "grad_norm": 3.0498602390289307,
      "learning_rate": 5.3908610901896154e-05,
      "loss": 2.5404,
      "step": 23470
    },
    {
      "epoch": 1.4258820671646322,
      "grad_norm": 3.124971628189087,
      "learning_rate": 5.3876910563352045e-05,
      "loss": 2.5905,
      "step": 23480
    },
    {
      "epoch": 1.4264893423210057,
      "grad_norm": 3.880244016647339,
      "learning_rate": 5.384520865692357e-05,
      "loss": 2.7028,
      "step": 23490
    },
    {
      "epoch": 1.427096617477379,
      "grad_norm": 2.8303518295288086,
      "learning_rate": 5.3813505195431476e-05,
      "loss": 2.5579,
      "step": 23500
    },
    {
      "epoch": 1.4277038926337524,
      "grad_norm": 3.776660442352295,
      "learning_rate": 5.378180019169714e-05,
      "loss": 2.3749,
      "step": 23510
    },
    {
      "epoch": 1.4283111677901257,
      "grad_norm": 3.2253026962280273,
      "learning_rate": 5.375009365854259e-05,
      "loss": 2.7703,
      "step": 23520
    },
    {
      "epoch": 1.428918442946499,
      "grad_norm": 5.648983955383301,
      "learning_rate": 5.371838560879043e-05,
      "loss": 2.6349,
      "step": 23530
    },
    {
      "epoch": 1.4295257181028724,
      "grad_norm": 4.365876197814941,
      "learning_rate": 5.3686676055263895e-05,
      "loss": 2.3229,
      "step": 23540
    },
    {
      "epoch": 1.4301329932592457,
      "grad_norm": 3.1979358196258545,
      "learning_rate": 5.365496501078686e-05,
      "loss": 2.2998,
      "step": 23550
    },
    {
      "epoch": 1.4307402684156192,
      "grad_norm": 2.840728998184204,
      "learning_rate": 5.3623252488183774e-05,
      "loss": 2.6275,
      "step": 23560
    },
    {
      "epoch": 1.4313475435719925,
      "grad_norm": 2.9732038974761963,
      "learning_rate": 5.359153850027967e-05,
      "loss": 2.4593,
      "step": 23570
    },
    {
      "epoch": 1.4319548187283657,
      "grad_norm": 5.267054080963135,
      "learning_rate": 5.355982305990018e-05,
      "loss": 2.6173,
      "step": 23580
    },
    {
      "epoch": 1.4325620938847392,
      "grad_norm": 5.439442157745361,
      "learning_rate": 5.352810617987158e-05,
      "loss": 2.543,
      "step": 23590
    },
    {
      "epoch": 1.4331693690411125,
      "grad_norm": 6.223703861236572,
      "learning_rate": 5.3496387873020626e-05,
      "loss": 2.7027,
      "step": 23600
    },
    {
      "epoch": 1.433776644197486,
      "grad_norm": 3.710362195968628,
      "learning_rate": 5.346466815217472e-05,
      "loss": 2.4899,
      "step": 23610
    },
    {
      "epoch": 1.4343839193538592,
      "grad_norm": 3.4337494373321533,
      "learning_rate": 5.3432947030161864e-05,
      "loss": 2.4245,
      "step": 23620
    },
    {
      "epoch": 1.4349911945102325,
      "grad_norm": 4.225805282592773,
      "learning_rate": 5.3401224519810545e-05,
      "loss": 2.5807,
      "step": 23630
    },
    {
      "epoch": 1.435598469666606,
      "grad_norm": 3.163536310195923,
      "learning_rate": 5.336950063394987e-05,
      "loss": 2.4791,
      "step": 23640
    },
    {
      "epoch": 1.4362057448229792,
      "grad_norm": 3.8375821113586426,
      "learning_rate": 5.333777538540945e-05,
      "loss": 2.5135,
      "step": 23650
    },
    {
      "epoch": 1.4368130199793527,
      "grad_norm": 2.823673725128174,
      "learning_rate": 5.330604878701954e-05,
      "loss": 2.3629,
      "step": 23660
    },
    {
      "epoch": 1.437420295135726,
      "grad_norm": 2.3321962356567383,
      "learning_rate": 5.32743208516108e-05,
      "loss": 2.2944,
      "step": 23670
    },
    {
      "epoch": 1.4380275702920993,
      "grad_norm": 3.1436328887939453,
      "learning_rate": 5.32425915920146e-05,
      "loss": 2.4153,
      "step": 23680
    },
    {
      "epoch": 1.4386348454484728,
      "grad_norm": 3.2955377101898193,
      "learning_rate": 5.3210861021062696e-05,
      "loss": 2.448,
      "step": 23690
    },
    {
      "epoch": 1.439242120604846,
      "grad_norm": 1.8531161546707153,
      "learning_rate": 5.317912915158747e-05,
      "loss": 2.4746,
      "step": 23700
    },
    {
      "epoch": 1.4398493957612195,
      "grad_norm": 3.502438545227051,
      "learning_rate": 5.314739599642177e-05,
      "loss": 2.7581,
      "step": 23710
    },
    {
      "epoch": 1.4404566709175928,
      "grad_norm": 4.054166793823242,
      "learning_rate": 5.311566156839901e-05,
      "loss": 2.5015,
      "step": 23720
    },
    {
      "epoch": 1.441063946073966,
      "grad_norm": 1.7059959173202515,
      "learning_rate": 5.30839258803531e-05,
      "loss": 2.4251,
      "step": 23730
    },
    {
      "epoch": 1.4416712212303395,
      "grad_norm": 2.35099458694458,
      "learning_rate": 5.305218894511844e-05,
      "loss": 2.4914,
      "step": 23740
    },
    {
      "epoch": 1.4422784963867128,
      "grad_norm": 1.6656383275985718,
      "learning_rate": 5.302045077552995e-05,
      "loss": 2.4668,
      "step": 23750
    },
    {
      "epoch": 1.4428857715430863,
      "grad_norm": 2.65592885017395,
      "learning_rate": 5.298871138442307e-05,
      "loss": 2.3966,
      "step": 23760
    },
    {
      "epoch": 1.4434930466994595,
      "grad_norm": 2.8593344688415527,
      "learning_rate": 5.295697078463371e-05,
      "loss": 3.0781,
      "step": 23770
    },
    {
      "epoch": 1.4441003218558328,
      "grad_norm": 4.503179550170898,
      "learning_rate": 5.2925228988998274e-05,
      "loss": 3.2282,
      "step": 23780
    },
    {
      "epoch": 1.4447075970122063,
      "grad_norm": 3.5114948749542236,
      "learning_rate": 5.2893486010353635e-05,
      "loss": 2.9563,
      "step": 23790
    },
    {
      "epoch": 1.4453148721685796,
      "grad_norm": 2.8225271701812744,
      "learning_rate": 5.286174186153718e-05,
      "loss": 3.0823,
      "step": 23800
    },
    {
      "epoch": 1.445922147324953,
      "grad_norm": 2.87205171585083,
      "learning_rate": 5.282999655538673e-05,
      "loss": 2.7979,
      "step": 23810
    },
    {
      "epoch": 1.4465294224813263,
      "grad_norm": 3.1437220573425293,
      "learning_rate": 5.279825010474061e-05,
      "loss": 2.6079,
      "step": 23820
    },
    {
      "epoch": 1.4471366976376996,
      "grad_norm": 2.7542357444763184,
      "learning_rate": 5.276650252243758e-05,
      "loss": 2.8153,
      "step": 23830
    },
    {
      "epoch": 1.447743972794073,
      "grad_norm": 2.6482369899749756,
      "learning_rate": 5.2734753821316864e-05,
      "loss": 2.7499,
      "step": 23840
    },
    {
      "epoch": 1.4483512479504463,
      "grad_norm": 2.545438051223755,
      "learning_rate": 5.270300401421815e-05,
      "loss": 2.4722,
      "step": 23850
    },
    {
      "epoch": 1.4489585231068198,
      "grad_norm": 3.781752586364746,
      "learning_rate": 5.267125311398157e-05,
      "loss": 2.5123,
      "step": 23860
    },
    {
      "epoch": 1.449565798263193,
      "grad_norm": 2.730027914047241,
      "learning_rate": 5.2639501133447675e-05,
      "loss": 2.3984,
      "step": 23870
    },
    {
      "epoch": 1.4501730734195664,
      "grad_norm": 2.512303352355957,
      "learning_rate": 5.2607748085457485e-05,
      "loss": 2.5847,
      "step": 23880
    },
    {
      "epoch": 1.4507803485759396,
      "grad_norm": 2.252962112426758,
      "learning_rate": 5.2575993982852444e-05,
      "loss": 2.3999,
      "step": 23890
    },
    {
      "epoch": 1.4513876237323131,
      "grad_norm": 3.5600736141204834,
      "learning_rate": 5.254423883847441e-05,
      "loss": 2.4533,
      "step": 23900
    },
    {
      "epoch": 1.4519948988886866,
      "grad_norm": 2.956249713897705,
      "learning_rate": 5.2512482665165676e-05,
      "loss": 2.671,
      "step": 23910
    },
    {
      "epoch": 1.4526021740450599,
      "grad_norm": 3.6604995727539062,
      "learning_rate": 5.248072547576892e-05,
      "loss": 2.5766,
      "step": 23920
    },
    {
      "epoch": 1.4532094492014331,
      "grad_norm": 3.255581855773926,
      "learning_rate": 5.244896728312728e-05,
      "loss": 2.3249,
      "step": 23930
    },
    {
      "epoch": 1.4538167243578064,
      "grad_norm": 3.5870609283447266,
      "learning_rate": 5.241720810008428e-05,
      "loss": 2.5512,
      "step": 23940
    },
    {
      "epoch": 1.4544239995141799,
      "grad_norm": 5.158060073852539,
      "learning_rate": 5.238544793948381e-05,
      "loss": 2.7278,
      "step": 23950
    },
    {
      "epoch": 1.4550312746705532,
      "grad_norm": 3.7116143703460693,
      "learning_rate": 5.235368681417021e-05,
      "loss": 2.6557,
      "step": 23960
    },
    {
      "epoch": 1.4556385498269266,
      "grad_norm": 3.3980963230133057,
      "learning_rate": 5.232192473698818e-05,
      "loss": 2.6861,
      "step": 23970
    },
    {
      "epoch": 1.4562458249833,
      "grad_norm": 2.979290008544922,
      "learning_rate": 5.229016172078279e-05,
      "loss": 2.9793,
      "step": 23980
    },
    {
      "epoch": 1.4568531001396732,
      "grad_norm": 4.744499683380127,
      "learning_rate": 5.225839777839955e-05,
      "loss": 2.918,
      "step": 23990
    },
    {
      "epoch": 1.4574603752960467,
      "grad_norm": 2.8974339962005615,
      "learning_rate": 5.222663292268427e-05,
      "loss": 2.8285,
      "step": 24000
    },
    {
      "epoch": 1.45806765045242,
      "grad_norm": 2.997828722000122,
      "learning_rate": 5.2194867166483174e-05,
      "loss": 2.8542,
      "step": 24010
    },
    {
      "epoch": 1.4586749256087934,
      "grad_norm": 3.128255605697632,
      "learning_rate": 5.2163100522642824e-05,
      "loss": 2.4031,
      "step": 24020
    },
    {
      "epoch": 1.4592822007651667,
      "grad_norm": 2.4191768169403076,
      "learning_rate": 5.213133300401018e-05,
      "loss": 2.339,
      "step": 24030
    },
    {
      "epoch": 1.45988947592154,
      "grad_norm": 2.6434662342071533,
      "learning_rate": 5.209956462343254e-05,
      "loss": 2.8909,
      "step": 24040
    },
    {
      "epoch": 1.4604967510779134,
      "grad_norm": 4.050110340118408,
      "learning_rate": 5.2067795393757526e-05,
      "loss": 2.3631,
      "step": 24050
    },
    {
      "epoch": 1.4611040262342867,
      "grad_norm": 2.698751211166382,
      "learning_rate": 5.203602532783311e-05,
      "loss": 2.5189,
      "step": 24060
    },
    {
      "epoch": 1.4617113013906602,
      "grad_norm": 3.3157639503479004,
      "learning_rate": 5.200425443850763e-05,
      "loss": 2.5914,
      "step": 24070
    },
    {
      "epoch": 1.4623185765470335,
      "grad_norm": 5.706690788269043,
      "learning_rate": 5.1972482738629727e-05,
      "loss": 2.5983,
      "step": 24080
    },
    {
      "epoch": 1.4629258517034067,
      "grad_norm": 3.875051975250244,
      "learning_rate": 5.194071024104838e-05,
      "loss": 3.0021,
      "step": 24090
    },
    {
      "epoch": 1.4635331268597802,
      "grad_norm": 5.696118354797363,
      "learning_rate": 5.190893695861293e-05,
      "loss": 2.7638,
      "step": 24100
    },
    {
      "epoch": 1.4641404020161535,
      "grad_norm": 3.8624379634857178,
      "learning_rate": 5.187716290417296e-05,
      "loss": 3.0143,
      "step": 24110
    },
    {
      "epoch": 1.464747677172527,
      "grad_norm": 2.4775025844573975,
      "learning_rate": 5.1845388090578414e-05,
      "loss": 2.9233,
      "step": 24120
    },
    {
      "epoch": 1.4653549523289002,
      "grad_norm": 3.248776912689209,
      "learning_rate": 5.181361253067953e-05,
      "loss": 2.6236,
      "step": 24130
    },
    {
      "epoch": 1.4659622274852735,
      "grad_norm": 3.5227699279785156,
      "learning_rate": 5.178183623732687e-05,
      "loss": 2.5407,
      "step": 24140
    },
    {
      "epoch": 1.466569502641647,
      "grad_norm": 2.619075059890747,
      "learning_rate": 5.175005922337125e-05,
      "loss": 2.8088,
      "step": 24150
    },
    {
      "epoch": 1.4671767777980202,
      "grad_norm": 4.221052169799805,
      "learning_rate": 5.17182815016638e-05,
      "loss": 2.7806,
      "step": 24160
    },
    {
      "epoch": 1.4677840529543937,
      "grad_norm": 2.699749231338501,
      "learning_rate": 5.168650308505596e-05,
      "loss": 2.7384,
      "step": 24170
    },
    {
      "epoch": 1.468391328110767,
      "grad_norm": 4.7734808921813965,
      "learning_rate": 5.16547239863994e-05,
      "loss": 2.7004,
      "step": 24180
    },
    {
      "epoch": 1.4689986032671403,
      "grad_norm": 2.492122173309326,
      "learning_rate": 5.16229442185461e-05,
      "loss": 2.385,
      "step": 24190
    },
    {
      "epoch": 1.4696058784235138,
      "grad_norm": 2.268583059310913,
      "learning_rate": 5.159116379434833e-05,
      "loss": 2.3504,
      "step": 24200
    },
    {
      "epoch": 1.470213153579887,
      "grad_norm": 3.466386318206787,
      "learning_rate": 5.1559382726658565e-05,
      "loss": 2.7144,
      "step": 24210
    },
    {
      "epoch": 1.4708204287362605,
      "grad_norm": 1.983106017112732,
      "learning_rate": 5.15276010283296e-05,
      "loss": 2.7619,
      "step": 24220
    },
    {
      "epoch": 1.4714277038926338,
      "grad_norm": 2.51230788230896,
      "learning_rate": 5.1495818712214425e-05,
      "loss": 2.4425,
      "step": 24230
    },
    {
      "epoch": 1.472034979049007,
      "grad_norm": 2.055626153945923,
      "learning_rate": 5.1464035791166364e-05,
      "loss": 2.6024,
      "step": 24240
    },
    {
      "epoch": 1.4726422542053805,
      "grad_norm": 3.7614388465881348,
      "learning_rate": 5.143225227803892e-05,
      "loss": 2.8989,
      "step": 24250
    },
    {
      "epoch": 1.4732495293617538,
      "grad_norm": 4.618579864501953,
      "learning_rate": 5.140046818568583e-05,
      "loss": 3.0318,
      "step": 24260
    },
    {
      "epoch": 1.4738568045181273,
      "grad_norm": 4.359683990478516,
      "learning_rate": 5.136868352696109e-05,
      "loss": 2.8029,
      "step": 24270
    },
    {
      "epoch": 1.4744640796745005,
      "grad_norm": 2.885856866836548,
      "learning_rate": 5.133689831471896e-05,
      "loss": 2.6743,
      "step": 24280
    },
    {
      "epoch": 1.4750713548308738,
      "grad_norm": 3.1591360569000244,
      "learning_rate": 5.130511256181384e-05,
      "loss": 2.6074,
      "step": 24290
    },
    {
      "epoch": 1.4756786299872473,
      "grad_norm": 4.167646408081055,
      "learning_rate": 5.1273326281100406e-05,
      "loss": 2.7382,
      "step": 24300
    },
    {
      "epoch": 1.4762859051436206,
      "grad_norm": 2.7892942428588867,
      "learning_rate": 5.124153948543357e-05,
      "loss": 2.3801,
      "step": 24310
    },
    {
      "epoch": 1.476893180299994,
      "grad_norm": 2.6137893199920654,
      "learning_rate": 5.1209752187668384e-05,
      "loss": 2.5492,
      "step": 24320
    },
    {
      "epoch": 1.4775004554563673,
      "grad_norm": 1.8234256505966187,
      "learning_rate": 5.117796440066014e-05,
      "loss": 2.3409,
      "step": 24330
    },
    {
      "epoch": 1.4781077306127406,
      "grad_norm": 1.534180998802185,
      "learning_rate": 5.1146176137264335e-05,
      "loss": 2.4581,
      "step": 24340
    },
    {
      "epoch": 1.4787150057691139,
      "grad_norm": 2.6788690090179443,
      "learning_rate": 5.111438741033665e-05,
      "loss": 2.8122,
      "step": 24350
    },
    {
      "epoch": 1.4793222809254873,
      "grad_norm": 3.371476650238037,
      "learning_rate": 5.108259823273294e-05,
      "loss": 2.7526,
      "step": 24360
    },
    {
      "epoch": 1.4799295560818606,
      "grad_norm": 3.2371907234191895,
      "learning_rate": 5.105080861730927e-05,
      "loss": 2.8737,
      "step": 24370
    },
    {
      "epoch": 1.480536831238234,
      "grad_norm": 4.255894660949707,
      "learning_rate": 5.101901857692185e-05,
      "loss": 2.9211,
      "step": 24380
    },
    {
      "epoch": 1.4811441063946074,
      "grad_norm": 5.121058940887451,
      "learning_rate": 5.09872281244271e-05,
      "loss": 2.8106,
      "step": 24390
    },
    {
      "epoch": 1.4817513815509806,
      "grad_norm": 3.264969825744629,
      "learning_rate": 5.095543727268156e-05,
      "loss": 2.1683,
      "step": 24400
    },
    {
      "epoch": 1.4823586567073541,
      "grad_norm": 5.355094909667969,
      "learning_rate": 5.092364603454197e-05,
      "loss": 2.3408,
      "step": 24410
    },
    {
      "epoch": 1.4829659318637274,
      "grad_norm": 2.2232139110565186,
      "learning_rate": 5.089185442286523e-05,
      "loss": 2.6251,
      "step": 24420
    },
    {
      "epoch": 1.4835732070201009,
      "grad_norm": 3.0518126487731934,
      "learning_rate": 5.086006245050835e-05,
      "loss": 2.3868,
      "step": 24430
    },
    {
      "epoch": 1.4841804821764741,
      "grad_norm": 5.281515121459961,
      "learning_rate": 5.08282701303285e-05,
      "loss": 2.7609,
      "step": 24440
    },
    {
      "epoch": 1.4847877573328474,
      "grad_norm": 3.015838384628296,
      "learning_rate": 5.0796477475183055e-05,
      "loss": 2.5682,
      "step": 24450
    },
    {
      "epoch": 1.485395032489221,
      "grad_norm": 4.418484210968018,
      "learning_rate": 5.0764684497929404e-05,
      "loss": 2.839,
      "step": 24460
    },
    {
      "epoch": 1.4860023076455942,
      "grad_norm": 4.456687927246094,
      "learning_rate": 5.073289121142517e-05,
      "loss": 2.7402,
      "step": 24470
    },
    {
      "epoch": 1.4866095828019676,
      "grad_norm": 3.847278594970703,
      "learning_rate": 5.070109762852807e-05,
      "loss": 2.6637,
      "step": 24480
    },
    {
      "epoch": 1.487216857958341,
      "grad_norm": 2.8207809925079346,
      "learning_rate": 5.066930376209591e-05,
      "loss": 2.6498,
      "step": 24490
    },
    {
      "epoch": 1.4878241331147142,
      "grad_norm": 6.004297256469727,
      "learning_rate": 5.0637509624986644e-05,
      "loss": 2.5486,
      "step": 24500
    },
    {
      "epoch": 1.4884314082710877,
      "grad_norm": 5.5984272956848145,
      "learning_rate": 5.0605715230058346e-05,
      "loss": 2.6955,
      "step": 24510
    },
    {
      "epoch": 1.489038683427461,
      "grad_norm": 4.0564470291137695,
      "learning_rate": 5.057392059016916e-05,
      "loss": 3.0652,
      "step": 24520
    },
    {
      "epoch": 1.4896459585838344,
      "grad_norm": 2.4993441104888916,
      "learning_rate": 5.0542125718177335e-05,
      "loss": 2.9813,
      "step": 24530
    },
    {
      "epoch": 1.4902532337402077,
      "grad_norm": 3.1311378479003906,
      "learning_rate": 5.0510330626941247e-05,
      "loss": 2.7028,
      "step": 24540
    },
    {
      "epoch": 1.490860508896581,
      "grad_norm": 3.9925994873046875,
      "learning_rate": 5.047853532931932e-05,
      "loss": 3.2113,
      "step": 24550
    },
    {
      "epoch": 1.4914677840529544,
      "grad_norm": 4.130398750305176,
      "learning_rate": 5.044673983817009e-05,
      "loss": 2.7605,
      "step": 24560
    },
    {
      "epoch": 1.4920750592093277,
      "grad_norm": 4.3842010498046875,
      "learning_rate": 5.041494416635214e-05,
      "loss": 2.4546,
      "step": 24570
    },
    {
      "epoch": 1.4926823343657012,
      "grad_norm": 3.6023011207580566,
      "learning_rate": 5.038314832672418e-05,
      "loss": 2.7045,
      "step": 24580
    },
    {
      "epoch": 1.4932896095220745,
      "grad_norm": 4.342766284942627,
      "learning_rate": 5.035135233214493e-05,
      "loss": 2.8217,
      "step": 24590
    },
    {
      "epoch": 1.4938968846784477,
      "grad_norm": 3.1555309295654297,
      "learning_rate": 5.031955619547319e-05,
      "loss": 3.1151,
      "step": 24600
    },
    {
      "epoch": 1.4945041598348212,
      "grad_norm": 3.3972487449645996,
      "learning_rate": 5.028775992956785e-05,
      "loss": 2.7063,
      "step": 24610
    },
    {
      "epoch": 1.4951114349911945,
      "grad_norm": 3.5034148693084717,
      "learning_rate": 5.025596354728781e-05,
      "loss": 2.6491,
      "step": 24620
    },
    {
      "epoch": 1.495718710147568,
      "grad_norm": 3.619065523147583,
      "learning_rate": 5.022416706149202e-05,
      "loss": 2.7286,
      "step": 24630
    },
    {
      "epoch": 1.4963259853039412,
      "grad_norm": 5.868706703186035,
      "learning_rate": 5.019237048503951e-05,
      "loss": 2.8256,
      "step": 24640
    },
    {
      "epoch": 1.4969332604603145,
      "grad_norm": 4.6873860359191895,
      "learning_rate": 5.01605738307893e-05,
      "loss": 2.3023,
      "step": 24650
    },
    {
      "epoch": 1.497540535616688,
      "grad_norm": 6.013185977935791,
      "learning_rate": 5.012877711160051e-05,
      "loss": 2.8877,
      "step": 24660
    },
    {
      "epoch": 1.4981478107730613,
      "grad_norm": 3.154848337173462,
      "learning_rate": 5.009698034033217e-05,
      "loss": 2.8082,
      "step": 24670
    },
    {
      "epoch": 1.4987550859294347,
      "grad_norm": 3.8865597248077393,
      "learning_rate": 5.006518352984345e-05,
      "loss": 2.8634,
      "step": 24680
    },
    {
      "epoch": 1.499362361085808,
      "grad_norm": 3.0450868606567383,
      "learning_rate": 5.003338669299347e-05,
      "loss": 3.0027,
      "step": 24690
    },
    {
      "epoch": 1.4999696362421813,
      "grad_norm": 4.637424468994141,
      "learning_rate": 5.000158984264138e-05,
      "loss": 2.9455,
      "step": 24700
    },
    {
      "epoch": 1.5005769113985545,
      "grad_norm": 3.9370996952056885,
      "learning_rate": 4.996979299164634e-05,
      "loss": 3.2395,
      "step": 24710
    },
    {
      "epoch": 1.501184186554928,
      "grad_norm": 2.9319684505462646,
      "learning_rate": 4.9937996152867476e-05,
      "loss": 3.0637,
      "step": 24720
    },
    {
      "epoch": 1.5017914617113015,
      "grad_norm": 2.440178632736206,
      "learning_rate": 4.990619933916397e-05,
      "loss": 2.3427,
      "step": 24730
    },
    {
      "epoch": 1.5023987368676748,
      "grad_norm": 2.7182157039642334,
      "learning_rate": 4.987440256339494e-05,
      "loss": 2.6292,
      "step": 24740
    },
    {
      "epoch": 1.503006012024048,
      "grad_norm": 3.4302566051483154,
      "learning_rate": 4.984260583841953e-05,
      "loss": 2.6682,
      "step": 24750
    },
    {
      "epoch": 1.5036132871804213,
      "grad_norm": 3.2626194953918457,
      "learning_rate": 4.9810809177096794e-05,
      "loss": 2.3776,
      "step": 24760
    },
    {
      "epoch": 1.5042205623367948,
      "grad_norm": 2.604231834411621,
      "learning_rate": 4.977901259228586e-05,
      "loss": 2.3854,
      "step": 24770
    },
    {
      "epoch": 1.5048278374931683,
      "grad_norm": 1.7375884056091309,
      "learning_rate": 4.974721609684576e-05,
      "loss": 2.1262,
      "step": 24780
    },
    {
      "epoch": 1.5054351126495416,
      "grad_norm": 3.0584042072296143,
      "learning_rate": 4.9715419703635474e-05,
      "loss": 2.4638,
      "step": 24790
    },
    {
      "epoch": 1.5060423878059148,
      "grad_norm": 2.54044508934021,
      "learning_rate": 4.968362342551401e-05,
      "loss": 2.6323,
      "step": 24800
    },
    {
      "epoch": 1.506649662962288,
      "grad_norm": 3.422015428543091,
      "learning_rate": 4.965182727534027e-05,
      "loss": 2.5856,
      "step": 24810
    },
    {
      "epoch": 1.5072569381186616,
      "grad_norm": 3.281851291656494,
      "learning_rate": 4.9620031265973123e-05,
      "loss": 2.6361,
      "step": 24820
    },
    {
      "epoch": 1.507864213275035,
      "grad_norm": 3.320404052734375,
      "learning_rate": 4.9588235410271375e-05,
      "loss": 2.2646,
      "step": 24830
    },
    {
      "epoch": 1.5084714884314083,
      "grad_norm": 2.964063882827759,
      "learning_rate": 4.955643972109379e-05,
      "loss": 2.2059,
      "step": 24840
    },
    {
      "epoch": 1.5090787635877816,
      "grad_norm": 2.0236384868621826,
      "learning_rate": 4.952464421129905e-05,
      "loss": 2.1905,
      "step": 24850
    },
    {
      "epoch": 1.5096860387441549,
      "grad_norm": 3.352855920791626,
      "learning_rate": 4.9492848893745763e-05,
      "loss": 2.6052,
      "step": 24860
    },
    {
      "epoch": 1.5102933139005283,
      "grad_norm": 3.632373571395874,
      "learning_rate": 4.946105378129245e-05,
      "loss": 2.6665,
      "step": 24870
    },
    {
      "epoch": 1.5109005890569018,
      "grad_norm": 4.902778625488281,
      "learning_rate": 4.942925888679758e-05,
      "loss": 2.8469,
      "step": 24880
    },
    {
      "epoch": 1.511507864213275,
      "grad_norm": 3.2187845706939697,
      "learning_rate": 4.939746422311951e-05,
      "loss": 2.6338,
      "step": 24890
    },
    {
      "epoch": 1.5121151393696484,
      "grad_norm": 4.7900285720825195,
      "learning_rate": 4.936566980311651e-05,
      "loss": 2.5757,
      "step": 24900
    },
    {
      "epoch": 1.5127224145260216,
      "grad_norm": 3.0750203132629395,
      "learning_rate": 4.933387563964671e-05,
      "loss": 2.3904,
      "step": 24910
    },
    {
      "epoch": 1.5133296896823951,
      "grad_norm": 3.4806251525878906,
      "learning_rate": 4.930208174556824e-05,
      "loss": 2.1825,
      "step": 24920
    },
    {
      "epoch": 1.5139369648387686,
      "grad_norm": 2.89782452583313,
      "learning_rate": 4.9270288133739023e-05,
      "loss": 2.5434,
      "step": 24930
    },
    {
      "epoch": 1.5145442399951419,
      "grad_norm": 3.480597972869873,
      "learning_rate": 4.9238494817016915e-05,
      "loss": 2.647,
      "step": 24940
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 3.1704938411712646,
      "learning_rate": 4.92067018082596e-05,
      "loss": 2.6513,
      "step": 24950
    },
    {
      "epoch": 1.5157587903078884,
      "grad_norm": 2.80157208442688,
      "learning_rate": 4.917490912032473e-05,
      "loss": 2.6134,
      "step": 24960
    },
    {
      "epoch": 1.516366065464262,
      "grad_norm": 3.074295997619629,
      "learning_rate": 4.914311676606977e-05,
      "loss": 3.0771,
      "step": 24970
    },
    {
      "epoch": 1.5169733406206352,
      "grad_norm": 5.057108402252197,
      "learning_rate": 4.911132475835202e-05,
      "loss": 2.8956,
      "step": 24980
    },
    {
      "epoch": 1.5175806157770086,
      "grad_norm": 4.123636722564697,
      "learning_rate": 4.9079533110028665e-05,
      "loss": 2.8489,
      "step": 24990
    },
    {
      "epoch": 1.518187890933382,
      "grad_norm": 4.162281036376953,
      "learning_rate": 4.904774183395682e-05,
      "loss": 2.5579,
      "step": 25000
    },
    {
      "epoch": 1.518187890933382,
      "eval_loss": 4.615926265716553,
      "eval_runtime": 2103.5859,
      "eval_samples_per_second": 7.828,
      "eval_steps_per_second": 1.957,
      "step": 25000
    },
    {
      "epoch": 1.5187951660897552,
      "grad_norm": 4.2774529457092285,
      "learning_rate": 4.9015950942993326e-05,
      "loss": 4.0094,
      "step": 25010
    },
    {
      "epoch": 1.5194024412461287,
      "grad_norm": 3.6216506958007812,
      "learning_rate": 4.898416044999493e-05,
      "loss": 3.5299,
      "step": 25020
    },
    {
      "epoch": 1.520009716402502,
      "grad_norm": 4.892371654510498,
      "learning_rate": 4.895237036781825e-05,
      "loss": 2.9202,
      "step": 25030
    },
    {
      "epoch": 1.5206169915588754,
      "grad_norm": 4.139711856842041,
      "learning_rate": 4.8920580709319673e-05,
      "loss": 2.9688,
      "step": 25040
    },
    {
      "epoch": 1.5212242667152487,
      "grad_norm": 4.56417179107666,
      "learning_rate": 4.888879148735544e-05,
      "loss": 2.9161,
      "step": 25050
    },
    {
      "epoch": 1.521831541871622,
      "grad_norm": 3.7848215103149414,
      "learning_rate": 4.885700271478163e-05,
      "loss": 2.6984,
      "step": 25060
    },
    {
      "epoch": 1.5224388170279954,
      "grad_norm": 4.076253890991211,
      "learning_rate": 4.8825214404454126e-05,
      "loss": 2.8979,
      "step": 25070
    },
    {
      "epoch": 1.5230460921843687,
      "grad_norm": 3.5544426441192627,
      "learning_rate": 4.8793426569228635e-05,
      "loss": 3.1522,
      "step": 25080
    },
    {
      "epoch": 1.5236533673407422,
      "grad_norm": 2.8214545249938965,
      "learning_rate": 4.876163922196065e-05,
      "loss": 2.9369,
      "step": 25090
    },
    {
      "epoch": 1.5242606424971155,
      "grad_norm": 3.300182580947876,
      "learning_rate": 4.872985237550548e-05,
      "loss": 2.7572,
      "step": 25100
    },
    {
      "epoch": 1.5248679176534887,
      "grad_norm": 3.961308002471924,
      "learning_rate": 4.8698066042718244e-05,
      "loss": 2.4243,
      "step": 25110
    },
    {
      "epoch": 1.525475192809862,
      "grad_norm": 2.779069662094116,
      "learning_rate": 4.866628023645384e-05,
      "loss": 2.4098,
      "step": 25120
    },
    {
      "epoch": 1.5260824679662355,
      "grad_norm": 3.5349457263946533,
      "learning_rate": 4.863449496956694e-05,
      "loss": 2.8426,
      "step": 25130
    },
    {
      "epoch": 1.526689743122609,
      "grad_norm": 6.299428939819336,
      "learning_rate": 4.8602710254912026e-05,
      "loss": 2.7981,
      "step": 25140
    },
    {
      "epoch": 1.5272970182789822,
      "grad_norm": 3.37083101272583,
      "learning_rate": 4.857092610534334e-05,
      "loss": 2.5172,
      "step": 25150
    },
    {
      "epoch": 1.5279042934353555,
      "grad_norm": 2.9515652656555176,
      "learning_rate": 4.85391425337149e-05,
      "loss": 2.6933,
      "step": 25160
    },
    {
      "epoch": 1.5285115685917288,
      "grad_norm": 2.8891067504882812,
      "learning_rate": 4.8507359552880485e-05,
      "loss": 2.5029,
      "step": 25170
    },
    {
      "epoch": 1.5291188437481023,
      "grad_norm": 2.734715461730957,
      "learning_rate": 4.8475577175693625e-05,
      "loss": 2.4065,
      "step": 25180
    },
    {
      "epoch": 1.5297261189044757,
      "grad_norm": 2.3697891235351562,
      "learning_rate": 4.8443795415007646e-05,
      "loss": 2.3409,
      "step": 25190
    },
    {
      "epoch": 1.530333394060849,
      "grad_norm": 3.582399368286133,
      "learning_rate": 4.841201428367559e-05,
      "loss": 2.952,
      "step": 25200
    },
    {
      "epoch": 1.5309406692172223,
      "grad_norm": 4.377269268035889,
      "learning_rate": 4.838023379455025e-05,
      "loss": 2.7533,
      "step": 25210
    },
    {
      "epoch": 1.5315479443735955,
      "grad_norm": 3.247922658920288,
      "learning_rate": 4.834845396048414e-05,
      "loss": 2.9065,
      "step": 25220
    },
    {
      "epoch": 1.532155219529969,
      "grad_norm": 3.9525723457336426,
      "learning_rate": 4.831667479432957e-05,
      "loss": 3.0969,
      "step": 25230
    },
    {
      "epoch": 1.5327624946863425,
      "grad_norm": 6.652679920196533,
      "learning_rate": 4.828489630893852e-05,
      "loss": 2.7275,
      "step": 25240
    },
    {
      "epoch": 1.5333697698427158,
      "grad_norm": 6.256941795349121,
      "learning_rate": 4.825311851716271e-05,
      "loss": 2.8972,
      "step": 25250
    },
    {
      "epoch": 1.533977044999089,
      "grad_norm": 4.800395488739014,
      "learning_rate": 4.82213414318536e-05,
      "loss": 3.0931,
      "step": 25260
    },
    {
      "epoch": 1.5345843201554623,
      "grad_norm": 2.210146903991699,
      "learning_rate": 4.818956506586235e-05,
      "loss": 2.5302,
      "step": 25270
    },
    {
      "epoch": 1.5351915953118358,
      "grad_norm": 3.7936818599700928,
      "learning_rate": 4.8157789432039826e-05,
      "loss": 2.8619,
      "step": 25280
    },
    {
      "epoch": 1.5357988704682093,
      "grad_norm": 2.9433605670928955,
      "learning_rate": 4.8126014543236556e-05,
      "loss": 2.8913,
      "step": 25290
    },
    {
      "epoch": 1.5364061456245826,
      "grad_norm": 3.797196388244629,
      "learning_rate": 4.809424041230288e-05,
      "loss": 2.8167,
      "step": 25300
    },
    {
      "epoch": 1.5370134207809558,
      "grad_norm": 5.519953727722168,
      "learning_rate": 4.806246705208875e-05,
      "loss": 3.1111,
      "step": 25310
    },
    {
      "epoch": 1.537620695937329,
      "grad_norm": 3.732125997543335,
      "learning_rate": 4.8030694475443785e-05,
      "loss": 2.73,
      "step": 25320
    },
    {
      "epoch": 1.5382279710937026,
      "grad_norm": 5.185771942138672,
      "learning_rate": 4.7998922695217313e-05,
      "loss": 2.9354,
      "step": 25330
    },
    {
      "epoch": 1.538835246250076,
      "grad_norm": 3.350294351577759,
      "learning_rate": 4.7967151724258405e-05,
      "loss": 2.7622,
      "step": 25340
    },
    {
      "epoch": 1.5394425214064493,
      "grad_norm": 2.3217833042144775,
      "learning_rate": 4.793538157541571e-05,
      "loss": 2.6391,
      "step": 25350
    },
    {
      "epoch": 1.5400497965628226,
      "grad_norm": 2.6068100929260254,
      "learning_rate": 4.790361226153759e-05,
      "loss": 2.7159,
      "step": 25360
    },
    {
      "epoch": 1.5406570717191959,
      "grad_norm": 3.0556576251983643,
      "learning_rate": 4.787184379547204e-05,
      "loss": 2.5288,
      "step": 25370
    },
    {
      "epoch": 1.5412643468755693,
      "grad_norm": 3.0050463676452637,
      "learning_rate": 4.784007619006677e-05,
      "loss": 2.6016,
      "step": 25380
    },
    {
      "epoch": 1.5418716220319428,
      "grad_norm": 2.8166651725769043,
      "learning_rate": 4.780830945816909e-05,
      "loss": 2.341,
      "step": 25390
    },
    {
      "epoch": 1.542478897188316,
      "grad_norm": 2.0059049129486084,
      "learning_rate": 4.777654361262597e-05,
      "loss": 2.4805,
      "step": 25400
    },
    {
      "epoch": 1.5430861723446894,
      "grad_norm": 2.582124948501587,
      "learning_rate": 4.7744778666284015e-05,
      "loss": 2.3325,
      "step": 25410
    },
    {
      "epoch": 1.5436934475010626,
      "grad_norm": 3.103250026702881,
      "learning_rate": 4.771301463198949e-05,
      "loss": 2.782,
      "step": 25420
    },
    {
      "epoch": 1.5443007226574361,
      "grad_norm": 3.206402540206909,
      "learning_rate": 4.768125152258827e-05,
      "loss": 2.9236,
      "step": 25430
    },
    {
      "epoch": 1.5449079978138094,
      "grad_norm": 2.9969120025634766,
      "learning_rate": 4.764948935092587e-05,
      "loss": 3.0093,
      "step": 25440
    },
    {
      "epoch": 1.5455152729701829,
      "grad_norm": 4.219132423400879,
      "learning_rate": 4.76177281298474e-05,
      "loss": 2.9854,
      "step": 25450
    },
    {
      "epoch": 1.5461225481265561,
      "grad_norm": 2.525869369506836,
      "learning_rate": 4.758596787219762e-05,
      "loss": 2.9312,
      "step": 25460
    },
    {
      "epoch": 1.5467298232829294,
      "grad_norm": 3.25061297416687,
      "learning_rate": 4.7554208590820885e-05,
      "loss": 2.7645,
      "step": 25470
    },
    {
      "epoch": 1.547337098439303,
      "grad_norm": 2.843360185623169,
      "learning_rate": 4.752245029856114e-05,
      "loss": 2.7641,
      "step": 25480
    },
    {
      "epoch": 1.5479443735956762,
      "grad_norm": 3.3281490802764893,
      "learning_rate": 4.7490693008261956e-05,
      "loss": 2.6828,
      "step": 25490
    },
    {
      "epoch": 1.5485516487520496,
      "grad_norm": 2.903304100036621,
      "learning_rate": 4.745893673276649e-05,
      "loss": 2.6601,
      "step": 25500
    },
    {
      "epoch": 1.549158923908423,
      "grad_norm": 2.899745225906372,
      "learning_rate": 4.742718148491748e-05,
      "loss": 2.9624,
      "step": 25510
    },
    {
      "epoch": 1.5497661990647962,
      "grad_norm": 3.9908339977264404,
      "learning_rate": 4.739542727755724e-05,
      "loss": 3.0013,
      "step": 25520
    },
    {
      "epoch": 1.5503734742211697,
      "grad_norm": 3.7535531520843506,
      "learning_rate": 4.73636741235277e-05,
      "loss": 2.8256,
      "step": 25530
    },
    {
      "epoch": 1.550980749377543,
      "grad_norm": 3.7175509929656982,
      "learning_rate": 4.7331922035670326e-05,
      "loss": 2.4994,
      "step": 25540
    },
    {
      "epoch": 1.5515880245339164,
      "grad_norm": 2.955920696258545,
      "learning_rate": 4.730017102682618e-05,
      "loss": 2.3086,
      "step": 25550
    },
    {
      "epoch": 1.5521952996902897,
      "grad_norm": 3.2000246047973633,
      "learning_rate": 4.726842110983586e-05,
      "loss": 3.01,
      "step": 25560
    },
    {
      "epoch": 1.552802574846663,
      "grad_norm": 4.0339508056640625,
      "learning_rate": 4.723667229753956e-05,
      "loss": 3.1149,
      "step": 25570
    },
    {
      "epoch": 1.5534098500030362,
      "grad_norm": 5.416073322296143,
      "learning_rate": 4.720492460277699e-05,
      "loss": 2.6753,
      "step": 25580
    },
    {
      "epoch": 1.5540171251594097,
      "grad_norm": 5.229828357696533,
      "learning_rate": 4.7173178038387436e-05,
      "loss": 2.4883,
      "step": 25590
    },
    {
      "epoch": 1.5546244003157832,
      "grad_norm": 2.7399356365203857,
      "learning_rate": 4.714143261720967e-05,
      "loss": 2.0886,
      "step": 25600
    },
    {
      "epoch": 1.5552316754721565,
      "grad_norm": 3.3889708518981934,
      "learning_rate": 4.710968835208211e-05,
      "loss": 2.5899,
      "step": 25610
    },
    {
      "epoch": 1.5558389506285297,
      "grad_norm": 5.304402828216553,
      "learning_rate": 4.707794525584262e-05,
      "loss": 2.8347,
      "step": 25620
    },
    {
      "epoch": 1.556446225784903,
      "grad_norm": 5.743016242980957,
      "learning_rate": 4.704620334132859e-05,
      "loss": 2.6285,
      "step": 25630
    },
    {
      "epoch": 1.5570535009412765,
      "grad_norm": 4.608743667602539,
      "learning_rate": 4.701446262137695e-05,
      "loss": 2.5193,
      "step": 25640
    },
    {
      "epoch": 1.55766077609765,
      "grad_norm": 2.928345203399658,
      "learning_rate": 4.6982723108824214e-05,
      "loss": 2.4538,
      "step": 25650
    },
    {
      "epoch": 1.5582680512540232,
      "grad_norm": 2.9279322624206543,
      "learning_rate": 4.695098481650629e-05,
      "loss": 2.1983,
      "step": 25660
    },
    {
      "epoch": 1.5588753264103965,
      "grad_norm": 2.837273120880127,
      "learning_rate": 4.691924775725867e-05,
      "loss": 2.2703,
      "step": 25670
    },
    {
      "epoch": 1.5594826015667698,
      "grad_norm": 4.031105041503906,
      "learning_rate": 4.6887511943916296e-05,
      "loss": 2.7066,
      "step": 25680
    },
    {
      "epoch": 1.5600898767231433,
      "grad_norm": 3.54443097114563,
      "learning_rate": 4.685577738931371e-05,
      "loss": 2.6965,
      "step": 25690
    },
    {
      "epoch": 1.5606971518795167,
      "grad_norm": 4.417652606964111,
      "learning_rate": 4.682404410628479e-05,
      "loss": 3.0966,
      "step": 25700
    },
    {
      "epoch": 1.56130442703589,
      "grad_norm": 4.251106262207031,
      "learning_rate": 4.679231210766301e-05,
      "loss": 2.6973,
      "step": 25710
    },
    {
      "epoch": 1.5619117021922633,
      "grad_norm": 5.0929083824157715,
      "learning_rate": 4.676058140628133e-05,
      "loss": 2.971,
      "step": 25720
    },
    {
      "epoch": 1.5625189773486365,
      "grad_norm": 3.4077978134155273,
      "learning_rate": 4.672885201497211e-05,
      "loss": 2.9882,
      "step": 25730
    },
    {
      "epoch": 1.56312625250501,
      "grad_norm": 2.3830580711364746,
      "learning_rate": 4.6697123946567227e-05,
      "loss": 2.7517,
      "step": 25740
    },
    {
      "epoch": 1.5637335276613835,
      "grad_norm": 3.0908873081207275,
      "learning_rate": 4.666539721389802e-05,
      "loss": 2.4158,
      "step": 25750
    },
    {
      "epoch": 1.5643408028177568,
      "grad_norm": 3.7426533699035645,
      "learning_rate": 4.663367182979529e-05,
      "loss": 3.042,
      "step": 25760
    },
    {
      "epoch": 1.56494807797413,
      "grad_norm": 3.3383491039276123,
      "learning_rate": 4.66019478070893e-05,
      "loss": 3.0248,
      "step": 25770
    },
    {
      "epoch": 1.5655553531305033,
      "grad_norm": 6.740508079528809,
      "learning_rate": 4.657022515860972e-05,
      "loss": 2.7967,
      "step": 25780
    },
    {
      "epoch": 1.5661626282868768,
      "grad_norm": 5.395181655883789,
      "learning_rate": 4.653850389718571e-05,
      "loss": 2.9174,
      "step": 25790
    },
    {
      "epoch": 1.5667699034432503,
      "grad_norm": 5.015252113342285,
      "learning_rate": 4.650678403564585e-05,
      "loss": 2.7341,
      "step": 25800
    },
    {
      "epoch": 1.5673771785996236,
      "grad_norm": 5.23283576965332,
      "learning_rate": 4.647506558681818e-05,
      "loss": 2.4005,
      "step": 25810
    },
    {
      "epoch": 1.5679844537559968,
      "grad_norm": 4.1798295974731445,
      "learning_rate": 4.644334856353011e-05,
      "loss": 2.6106,
      "step": 25820
    },
    {
      "epoch": 1.56859172891237,
      "grad_norm": 5.71796989440918,
      "learning_rate": 4.641163297860853e-05,
      "loss": 2.5353,
      "step": 25830
    },
    {
      "epoch": 1.5691990040687436,
      "grad_norm": 3.498880386352539,
      "learning_rate": 4.6379918844879724e-05,
      "loss": 2.5039,
      "step": 25840
    },
    {
      "epoch": 1.569806279225117,
      "grad_norm": 3.278676748275757,
      "learning_rate": 4.634820617516939e-05,
      "loss": 2.389,
      "step": 25850
    },
    {
      "epoch": 1.5704135543814903,
      "grad_norm": 3.921434164047241,
      "learning_rate": 4.6316494982302645e-05,
      "loss": 2.5311,
      "step": 25860
    },
    {
      "epoch": 1.5710208295378636,
      "grad_norm": 2.1929984092712402,
      "learning_rate": 4.6284785279103977e-05,
      "loss": 2.3236,
      "step": 25870
    },
    {
      "epoch": 1.5716281046942369,
      "grad_norm": 2.669698715209961,
      "learning_rate": 4.625307707839733e-05,
      "loss": 2.3653,
      "step": 25880
    },
    {
      "epoch": 1.5722353798506103,
      "grad_norm": 2.4584999084472656,
      "learning_rate": 4.622137039300598e-05,
      "loss": 2.622,
      "step": 25890
    },
    {
      "epoch": 1.5728426550069836,
      "grad_norm": 3.896324872970581,
      "learning_rate": 4.618966523575264e-05,
      "loss": 3.0313,
      "step": 25900
    },
    {
      "epoch": 1.573449930163357,
      "grad_norm": 3.8089447021484375,
      "learning_rate": 4.6157961619459325e-05,
      "loss": 2.7463,
      "step": 25910
    },
    {
      "epoch": 1.5740572053197304,
      "grad_norm": 6.116644382476807,
      "learning_rate": 4.612625955694755e-05,
      "loss": 2.7352,
      "step": 25920
    },
    {
      "epoch": 1.5746644804761036,
      "grad_norm": 5.319202899932861,
      "learning_rate": 4.6094559061038114e-05,
      "loss": 2.8181,
      "step": 25930
    },
    {
      "epoch": 1.5752717556324771,
      "grad_norm": 4.874117851257324,
      "learning_rate": 4.606286014455116e-05,
      "loss": 2.5933,
      "step": 25940
    },
    {
      "epoch": 1.5758790307888504,
      "grad_norm": 5.939822196960449,
      "learning_rate": 4.603116282030628e-05,
      "loss": 2.4738,
      "step": 25950
    },
    {
      "epoch": 1.5764863059452239,
      "grad_norm": 4.498966693878174,
      "learning_rate": 4.599946710112237e-05,
      "loss": 2.7136,
      "step": 25960
    },
    {
      "epoch": 1.5770935811015971,
      "grad_norm": 3.314690589904785,
      "learning_rate": 4.59677729998177e-05,
      "loss": 2.4994,
      "step": 25970
    },
    {
      "epoch": 1.5777008562579704,
      "grad_norm": 6.0083770751953125,
      "learning_rate": 4.593608052920981e-05,
      "loss": 2.4785,
      "step": 25980
    },
    {
      "epoch": 1.5783081314143437,
      "grad_norm": 3.236705780029297,
      "learning_rate": 4.590438970211569e-05,
      "loss": 2.2813,
      "step": 25990
    },
    {
      "epoch": 1.5789154065707172,
      "grad_norm": 3.2854321002960205,
      "learning_rate": 4.587270053135163e-05,
      "loss": 2.8867,
      "step": 26000
    },
    {
      "epoch": 1.5795226817270906,
      "grad_norm": 5.675291061401367,
      "learning_rate": 4.584101302973318e-05,
      "loss": 2.676,
      "step": 26010
    },
    {
      "epoch": 1.580129956883464,
      "grad_norm": 6.536532402038574,
      "learning_rate": 4.5809327210075287e-05,
      "loss": 2.8837,
      "step": 26020
    },
    {
      "epoch": 1.5807372320398372,
      "grad_norm": 5.378554344177246,
      "learning_rate": 4.577764308519224e-05,
      "loss": 3.2821,
      "step": 26030
    },
    {
      "epoch": 1.5813445071962104,
      "grad_norm": 4.077751159667969,
      "learning_rate": 4.574596066789757e-05,
      "loss": 3.1484,
      "step": 26040
    },
    {
      "epoch": 1.581951782352584,
      "grad_norm": 4.262429237365723,
      "learning_rate": 4.5714279971004154e-05,
      "loss": 3.0226,
      "step": 26050
    },
    {
      "epoch": 1.5825590575089574,
      "grad_norm": 4.640654563903809,
      "learning_rate": 4.568260100732416e-05,
      "loss": 2.8568,
      "step": 26060
    },
    {
      "epoch": 1.5831663326653307,
      "grad_norm": 3.7063937187194824,
      "learning_rate": 4.56509237896691e-05,
      "loss": 2.7134,
      "step": 26070
    },
    {
      "epoch": 1.583773607821704,
      "grad_norm": 2.9973220825195312,
      "learning_rate": 4.561924833084972e-05,
      "loss": 2.5893,
      "step": 26080
    },
    {
      "epoch": 1.5843808829780772,
      "grad_norm": 5.579086780548096,
      "learning_rate": 4.5587574643676084e-05,
      "loss": 2.7747,
      "step": 26090
    },
    {
      "epoch": 1.5849881581344507,
      "grad_norm": 3.8867783546447754,
      "learning_rate": 4.555590274095752e-05,
      "loss": 2.3974,
      "step": 26100
    },
    {
      "epoch": 1.5855954332908242,
      "grad_norm": 2.8875510692596436,
      "learning_rate": 4.5524232635502685e-05,
      "loss": 2.554,
      "step": 26110
    },
    {
      "epoch": 1.5862027084471975,
      "grad_norm": 4.227319240570068,
      "learning_rate": 4.549256434011945e-05,
      "loss": 2.8316,
      "step": 26120
    },
    {
      "epoch": 1.5868099836035707,
      "grad_norm": 3.5819220542907715,
      "learning_rate": 4.546089786761499e-05,
      "loss": 2.604,
      "step": 26130
    },
    {
      "epoch": 1.587417258759944,
      "grad_norm": 5.8330464363098145,
      "learning_rate": 4.542923323079571e-05,
      "loss": 2.6473,
      "step": 26140
    },
    {
      "epoch": 1.5880245339163175,
      "grad_norm": 4.893011093139648,
      "learning_rate": 4.5397570442467316e-05,
      "loss": 2.7687,
      "step": 26150
    },
    {
      "epoch": 1.588631809072691,
      "grad_norm": 6.774825572967529,
      "learning_rate": 4.536590951543474e-05,
      "loss": 2.4462,
      "step": 26160
    },
    {
      "epoch": 1.5892390842290642,
      "grad_norm": 4.068716049194336,
      "learning_rate": 4.5334250462502135e-05,
      "loss": 2.722,
      "step": 26170
    },
    {
      "epoch": 1.5898463593854375,
      "grad_norm": 4.974623203277588,
      "learning_rate": 4.530259329647297e-05,
      "loss": 3.002,
      "step": 26180
    },
    {
      "epoch": 1.5904536345418108,
      "grad_norm": 3.578117847442627,
      "learning_rate": 4.52709380301499e-05,
      "loss": 2.9391,
      "step": 26190
    },
    {
      "epoch": 1.5910609096981843,
      "grad_norm": 2.5750954151153564,
      "learning_rate": 4.523928467633479e-05,
      "loss": 2.6708,
      "step": 26200
    },
    {
      "epoch": 1.5916681848545577,
      "grad_norm": 4.889318466186523,
      "learning_rate": 4.520763324782878e-05,
      "loss": 3.3907,
      "step": 26210
    },
    {
      "epoch": 1.592275460010931,
      "grad_norm": 3.7750463485717773,
      "learning_rate": 4.517598375743222e-05,
      "loss": 2.601,
      "step": 26220
    },
    {
      "epoch": 1.5928827351673043,
      "grad_norm": 4.3021721839904785,
      "learning_rate": 4.5144336217944646e-05,
      "loss": 2.434,
      "step": 26230
    },
    {
      "epoch": 1.5934900103236775,
      "grad_norm": 5.510955333709717,
      "learning_rate": 4.511269064216485e-05,
      "loss": 2.3416,
      "step": 26240
    },
    {
      "epoch": 1.594097285480051,
      "grad_norm": 4.606334686279297,
      "learning_rate": 4.508104704289079e-05,
      "loss": 2.751,
      "step": 26250
    },
    {
      "epoch": 1.5947045606364245,
      "grad_norm": 3.5379178524017334,
      "learning_rate": 4.504940543291965e-05,
      "loss": 2.9626,
      "step": 26260
    },
    {
      "epoch": 1.5953118357927978,
      "grad_norm": 3.899902820587158,
      "learning_rate": 4.501776582504779e-05,
      "loss": 2.5107,
      "step": 26270
    },
    {
      "epoch": 1.595919110949171,
      "grad_norm": 3.0583367347717285,
      "learning_rate": 4.49861282320708e-05,
      "loss": 2.6166,
      "step": 26280
    },
    {
      "epoch": 1.5965263861055443,
      "grad_norm": 3.086071729660034,
      "learning_rate": 4.495449266678337e-05,
      "loss": 2.8283,
      "step": 26290
    },
    {
      "epoch": 1.5971336612619178,
      "grad_norm": 3.6281564235687256,
      "learning_rate": 4.4922859141979484e-05,
      "loss": 2.7507,
      "step": 26300
    },
    {
      "epoch": 1.5977409364182913,
      "grad_norm": 3.2351322174072266,
      "learning_rate": 4.4891227670452235e-05,
      "loss": 2.4318,
      "step": 26310
    },
    {
      "epoch": 1.5983482115746646,
      "grad_norm": 5.308643817901611,
      "learning_rate": 4.4859598264993866e-05,
      "loss": 2.6988,
      "step": 26320
    },
    {
      "epoch": 1.5989554867310378,
      "grad_norm": 3.926219940185547,
      "learning_rate": 4.482797093839581e-05,
      "loss": 2.8868,
      "step": 26330
    },
    {
      "epoch": 1.599562761887411,
      "grad_norm": 4.301018238067627,
      "learning_rate": 4.4796345703448706e-05,
      "loss": 2.9063,
      "step": 26340
    },
    {
      "epoch": 1.6001700370437846,
      "grad_norm": 4.594023704528809,
      "learning_rate": 4.476472257294226e-05,
      "loss": 2.7167,
      "step": 26350
    },
    {
      "epoch": 1.6007773122001578,
      "grad_norm": 4.4105610847473145,
      "learning_rate": 4.4733101559665395e-05,
      "loss": 2.2908,
      "step": 26360
    },
    {
      "epoch": 1.6013845873565313,
      "grad_norm": 3.5422446727752686,
      "learning_rate": 4.4701482676406125e-05,
      "loss": 2.2199,
      "step": 26370
    },
    {
      "epoch": 1.6019918625129046,
      "grad_norm": 2.6991615295410156,
      "learning_rate": 4.466986593595167e-05,
      "loss": 2.2599,
      "step": 26380
    },
    {
      "epoch": 1.6025991376692779,
      "grad_norm": 3.2722890377044678,
      "learning_rate": 4.46382513510883e-05,
      "loss": 2.3716,
      "step": 26390
    },
    {
      "epoch": 1.6032064128256514,
      "grad_norm": 2.8385009765625,
      "learning_rate": 4.460663893460147e-05,
      "loss": 2.3876,
      "step": 26400
    },
    {
      "epoch": 1.6038136879820246,
      "grad_norm": 2.6696248054504395,
      "learning_rate": 4.457502869927578e-05,
      "loss": 2.4248,
      "step": 26410
    },
    {
      "epoch": 1.604420963138398,
      "grad_norm": 2.937730073928833,
      "learning_rate": 4.4543420657894874e-05,
      "loss": 2.4316,
      "step": 26420
    },
    {
      "epoch": 1.6050282382947714,
      "grad_norm": 4.408176422119141,
      "learning_rate": 4.4511814823241555e-05,
      "loss": 3.1779,
      "step": 26430
    },
    {
      "epoch": 1.6056355134511446,
      "grad_norm": 4.34859037399292,
      "learning_rate": 4.448021120809772e-05,
      "loss": 2.923,
      "step": 26440
    },
    {
      "epoch": 1.606242788607518,
      "grad_norm": 3.8603267669677734,
      "learning_rate": 4.444860982524439e-05,
      "loss": 2.7256,
      "step": 26450
    },
    {
      "epoch": 1.6068500637638914,
      "grad_norm": 3.822958469390869,
      "learning_rate": 4.441701068746166e-05,
      "loss": 2.9498,
      "step": 26460
    },
    {
      "epoch": 1.6074573389202649,
      "grad_norm": 3.5089223384857178,
      "learning_rate": 4.438541380752873e-05,
      "loss": 2.6383,
      "step": 26470
    },
    {
      "epoch": 1.6080646140766381,
      "grad_norm": 3.4761292934417725,
      "learning_rate": 4.4353819198223864e-05,
      "loss": 2.6109,
      "step": 26480
    },
    {
      "epoch": 1.6086718892330114,
      "grad_norm": 2.9353506565093994,
      "learning_rate": 4.4322226872324455e-05,
      "loss": 2.4348,
      "step": 26490
    },
    {
      "epoch": 1.6092791643893847,
      "grad_norm": 3.263211250305176,
      "learning_rate": 4.429063684260691e-05,
      "loss": 2.1784,
      "step": 26500
    },
    {
      "epoch": 1.6098864395457582,
      "grad_norm": 3.6860904693603516,
      "learning_rate": 4.4259049121846765e-05,
      "loss": 2.8402,
      "step": 26510
    },
    {
      "epoch": 1.6104937147021317,
      "grad_norm": 4.558617115020752,
      "learning_rate": 4.422746372281858e-05,
      "loss": 2.9342,
      "step": 26520
    },
    {
      "epoch": 1.611100989858505,
      "grad_norm": 4.711281776428223,
      "learning_rate": 4.419588065829601e-05,
      "loss": 2.6433,
      "step": 26530
    },
    {
      "epoch": 1.6117082650148782,
      "grad_norm": 4.887182235717773,
      "learning_rate": 4.416429994105173e-05,
      "loss": 2.3336,
      "step": 26540
    },
    {
      "epoch": 1.6123155401712514,
      "grad_norm": 4.7282395362854,
      "learning_rate": 4.413272158385751e-05,
      "loss": 2.712,
      "step": 26550
    },
    {
      "epoch": 1.612922815327625,
      "grad_norm": 2.869692325592041,
      "learning_rate": 4.4101145599484106e-05,
      "loss": 2.4955,
      "step": 26560
    },
    {
      "epoch": 1.6135300904839984,
      "grad_norm": 3.6404435634613037,
      "learning_rate": 4.406957200070139e-05,
      "loss": 2.7298,
      "step": 26570
    },
    {
      "epoch": 1.6141373656403717,
      "grad_norm": 4.207278728485107,
      "learning_rate": 4.403800080027819e-05,
      "loss": 2.4373,
      "step": 26580
    },
    {
      "epoch": 1.614744640796745,
      "grad_norm": 3.3395843505859375,
      "learning_rate": 4.4006432010982446e-05,
      "loss": 2.5583,
      "step": 26590
    },
    {
      "epoch": 1.6153519159531182,
      "grad_norm": 3.307216167449951,
      "learning_rate": 4.397486564558102e-05,
      "loss": 2.4616,
      "step": 26600
    },
    {
      "epoch": 1.6159591911094917,
      "grad_norm": 4.037144660949707,
      "learning_rate": 4.3943301716839896e-05,
      "loss": 2.7805,
      "step": 26610
    },
    {
      "epoch": 1.6165664662658652,
      "grad_norm": 3.908339262008667,
      "learning_rate": 4.3911740237524035e-05,
      "loss": 2.6693,
      "step": 26620
    },
    {
      "epoch": 1.6171737414222385,
      "grad_norm": 3.403404474258423,
      "learning_rate": 4.388018122039735e-05,
      "loss": 2.8727,
      "step": 26630
    },
    {
      "epoch": 1.6177810165786117,
      "grad_norm": 3.289381265640259,
      "learning_rate": 4.384862467822288e-05,
      "loss": 2.7476,
      "step": 26640
    },
    {
      "epoch": 1.618388291734985,
      "grad_norm": 2.430338144302368,
      "learning_rate": 4.3817070623762566e-05,
      "loss": 2.4385,
      "step": 26650
    },
    {
      "epoch": 1.6189955668913585,
      "grad_norm": 2.534905195236206,
      "learning_rate": 4.378551906977735e-05,
      "loss": 2.652,
      "step": 26660
    },
    {
      "epoch": 1.619602842047732,
      "grad_norm": 2.8804566860198975,
      "learning_rate": 4.375397002902717e-05,
      "loss": 2.5378,
      "step": 26670
    },
    {
      "epoch": 1.6202101172041052,
      "grad_norm": 2.660921812057495,
      "learning_rate": 4.372242351427103e-05,
      "loss": 3.1724,
      "step": 26680
    },
    {
      "epoch": 1.6208173923604785,
      "grad_norm": 2.241351842880249,
      "learning_rate": 4.369087953826678e-05,
      "loss": 2.444,
      "step": 26690
    },
    {
      "epoch": 1.6214246675168518,
      "grad_norm": 2.5492448806762695,
      "learning_rate": 4.3659338113771325e-05,
      "loss": 2.2293,
      "step": 26700
    },
    {
      "epoch": 1.6220319426732253,
      "grad_norm": 1.7604079246520996,
      "learning_rate": 4.36277992535405e-05,
      "loss": 2.3723,
      "step": 26710
    },
    {
      "epoch": 1.6226392178295987,
      "grad_norm": 2.581834077835083,
      "learning_rate": 4.359626297032918e-05,
      "loss": 2.4119,
      "step": 26720
    },
    {
      "epoch": 1.623246492985972,
      "grad_norm": 3.312464952468872,
      "learning_rate": 4.356472927689109e-05,
      "loss": 2.6842,
      "step": 26730
    },
    {
      "epoch": 1.6238537681423453,
      "grad_norm": 3.340165853500366,
      "learning_rate": 4.353319818597898e-05,
      "loss": 2.8428,
      "step": 26740
    },
    {
      "epoch": 1.6244610432987185,
      "grad_norm": 4.215907573699951,
      "learning_rate": 4.3501669710344495e-05,
      "loss": 2.4507,
      "step": 26750
    },
    {
      "epoch": 1.625068318455092,
      "grad_norm": 4.948910236358643,
      "learning_rate": 4.347014386273829e-05,
      "loss": 2.6206,
      "step": 26760
    },
    {
      "epoch": 1.6256755936114655,
      "grad_norm": 3.8683671951293945,
      "learning_rate": 4.34386206559099e-05,
      "loss": 2.9269,
      "step": 26770
    },
    {
      "epoch": 1.6262828687678388,
      "grad_norm": 4.019731521606445,
      "learning_rate": 4.3407100102607816e-05,
      "loss": 2.5696,
      "step": 26780
    },
    {
      "epoch": 1.626890143924212,
      "grad_norm": 3.7143447399139404,
      "learning_rate": 4.337558221557945e-05,
      "loss": 2.5328,
      "step": 26790
    },
    {
      "epoch": 1.6274974190805853,
      "grad_norm": 3.837233781814575,
      "learning_rate": 4.334406700757114e-05,
      "loss": 3.1571,
      "step": 26800
    },
    {
      "epoch": 1.6281046942369588,
      "grad_norm": 5.494791030883789,
      "learning_rate": 4.331255449132814e-05,
      "loss": 3.4385,
      "step": 26810
    },
    {
      "epoch": 1.628711969393332,
      "grad_norm": 8.256538391113281,
      "learning_rate": 4.3281044679594606e-05,
      "loss": 3.235,
      "step": 26820
    },
    {
      "epoch": 1.6293192445497056,
      "grad_norm": 5.048996925354004,
      "learning_rate": 4.324953758511361e-05,
      "loss": 2.8459,
      "step": 26830
    },
    {
      "epoch": 1.6299265197060788,
      "grad_norm": 7.627274036407471,
      "learning_rate": 4.321803322062713e-05,
      "loss": 2.6789,
      "step": 26840
    },
    {
      "epoch": 1.630533794862452,
      "grad_norm": 2.6294219493865967,
      "learning_rate": 4.318653159887603e-05,
      "loss": 2.4083,
      "step": 26850
    },
    {
      "epoch": 1.6311410700188256,
      "grad_norm": 4.514567852020264,
      "learning_rate": 4.315503273260005e-05,
      "loss": 2.5157,
      "step": 26860
    },
    {
      "epoch": 1.6317483451751988,
      "grad_norm": 5.02113151550293,
      "learning_rate": 4.312353663453787e-05,
      "loss": 2.6481,
      "step": 26870
    },
    {
      "epoch": 1.6323556203315723,
      "grad_norm": 4.982553958892822,
      "learning_rate": 4.309204331742699e-05,
      "loss": 2.5719,
      "step": 26880
    },
    {
      "epoch": 1.6329628954879456,
      "grad_norm": 4.615222930908203,
      "learning_rate": 4.306055279400381e-05,
      "loss": 2.4556,
      "step": 26890
    },
    {
      "epoch": 1.6335701706443189,
      "grad_norm": 4.070384979248047,
      "learning_rate": 4.3029065077003584e-05,
      "loss": 2.8585,
      "step": 26900
    },
    {
      "epoch": 1.6341774458006921,
      "grad_norm": 4.047118186950684,
      "learning_rate": 4.2997580179160474e-05,
      "loss": 2.56,
      "step": 26910
    },
    {
      "epoch": 1.6347847209570656,
      "grad_norm": 4.2883524894714355,
      "learning_rate": 4.296609811320747e-05,
      "loss": 2.5802,
      "step": 26920
    },
    {
      "epoch": 1.635391996113439,
      "grad_norm": 3.253359079360962,
      "learning_rate": 4.293461889187641e-05,
      "loss": 2.515,
      "step": 26930
    },
    {
      "epoch": 1.6359992712698124,
      "grad_norm": 4.303258895874023,
      "learning_rate": 4.2903142527897984e-05,
      "loss": 2.9626,
      "step": 26940
    },
    {
      "epoch": 1.6366065464261856,
      "grad_norm": 3.241668701171875,
      "learning_rate": 4.287166903400175e-05,
      "loss": 2.9591,
      "step": 26950
    },
    {
      "epoch": 1.637213821582559,
      "grad_norm": 5.863216876983643,
      "learning_rate": 4.284019842291611e-05,
      "loss": 2.6524,
      "step": 26960
    },
    {
      "epoch": 1.6378210967389324,
      "grad_norm": 4.994816303253174,
      "learning_rate": 4.280873070736825e-05,
      "loss": 2.5425,
      "step": 26970
    },
    {
      "epoch": 1.6384283718953059,
      "grad_norm": 3.555097818374634,
      "learning_rate": 4.27772659000842e-05,
      "loss": 2.5132,
      "step": 26980
    },
    {
      "epoch": 1.6390356470516791,
      "grad_norm": 3.8313050270080566,
      "learning_rate": 4.274580401378886e-05,
      "loss": 2.8346,
      "step": 26990
    },
    {
      "epoch": 1.6396429222080524,
      "grad_norm": 4.319934368133545,
      "learning_rate": 4.271434506120593e-05,
      "loss": 2.7881,
      "step": 27000
    },
    {
      "epoch": 1.6402501973644257,
      "grad_norm": 5.116018772125244,
      "learning_rate": 4.268288905505788e-05,
      "loss": 2.8199,
      "step": 27010
    },
    {
      "epoch": 1.6408574725207992,
      "grad_norm": 3.847355842590332,
      "learning_rate": 4.265143600806601e-05,
      "loss": 3.0775,
      "step": 27020
    },
    {
      "epoch": 1.6414647476771727,
      "grad_norm": 3.122429609298706,
      "learning_rate": 4.261998593295049e-05,
      "loss": 2.7685,
      "step": 27030
    },
    {
      "epoch": 1.642072022833546,
      "grad_norm": 3.913689374923706,
      "learning_rate": 4.258853884243018e-05,
      "loss": 2.305,
      "step": 27040
    },
    {
      "epoch": 1.6426792979899192,
      "grad_norm": 4.389955997467041,
      "learning_rate": 4.25570947492228e-05,
      "loss": 2.7056,
      "step": 27050
    },
    {
      "epoch": 1.6432865731462925,
      "grad_norm": 2.6011581420898438,
      "learning_rate": 4.252565366604484e-05,
      "loss": 2.7328,
      "step": 27060
    },
    {
      "epoch": 1.643893848302666,
      "grad_norm": 2.6803104877471924,
      "learning_rate": 4.2494215605611574e-05,
      "loss": 2.4248,
      "step": 27070
    },
    {
      "epoch": 1.6445011234590394,
      "grad_norm": 2.9420387744903564,
      "learning_rate": 4.246278058063707e-05,
      "loss": 2.3528,
      "step": 27080
    },
    {
      "epoch": 1.6451083986154127,
      "grad_norm": 2.7898409366607666,
      "learning_rate": 4.243134860383412e-05,
      "loss": 2.6035,
      "step": 27090
    },
    {
      "epoch": 1.645715673771786,
      "grad_norm": 3.1426496505737305,
      "learning_rate": 4.239991968791435e-05,
      "loss": 2.7774,
      "step": 27100
    },
    {
      "epoch": 1.6463229489281592,
      "grad_norm": 4.00977087020874,
      "learning_rate": 4.2368493845588106e-05,
      "loss": 2.5421,
      "step": 27110
    },
    {
      "epoch": 1.6469302240845327,
      "grad_norm": 3.059678554534912,
      "learning_rate": 4.2337071089564483e-05,
      "loss": 2.431,
      "step": 27120
    },
    {
      "epoch": 1.6475374992409062,
      "grad_norm": 1.609276533126831,
      "learning_rate": 4.230565143255135e-05,
      "loss": 2.217,
      "step": 27130
    },
    {
      "epoch": 1.6481447743972795,
      "grad_norm": 2.0301527976989746,
      "learning_rate": 4.227423488725533e-05,
      "loss": 2.088,
      "step": 27140
    },
    {
      "epoch": 1.6487520495536527,
      "grad_norm": 3.5318074226379395,
      "learning_rate": 4.224282146638177e-05,
      "loss": 2.5361,
      "step": 27150
    },
    {
      "epoch": 1.649359324710026,
      "grad_norm": 3.578608274459839,
      "learning_rate": 4.221141118263474e-05,
      "loss": 2.9092,
      "step": 27160
    },
    {
      "epoch": 1.6499665998663995,
      "grad_norm": 3.038562297821045,
      "learning_rate": 4.218000404871707e-05,
      "loss": 2.7193,
      "step": 27170
    },
    {
      "epoch": 1.650573875022773,
      "grad_norm": 3.7608072757720947,
      "learning_rate": 4.214860007733032e-05,
      "loss": 2.8982,
      "step": 27180
    },
    {
      "epoch": 1.6511811501791462,
      "grad_norm": 3.56156063079834,
      "learning_rate": 4.211719928117474e-05,
      "loss": 2.5877,
      "step": 27190
    },
    {
      "epoch": 1.6517884253355195,
      "grad_norm": 4.069881439208984,
      "learning_rate": 4.208580167294932e-05,
      "loss": 2.6177,
      "step": 27200
    },
    {
      "epoch": 1.6523957004918928,
      "grad_norm": 2.9021358489990234,
      "learning_rate": 4.2054407265351733e-05,
      "loss": 2.5882,
      "step": 27210
    },
    {
      "epoch": 1.6530029756482663,
      "grad_norm": 2.3236188888549805,
      "learning_rate": 4.20230160710784e-05,
      "loss": 2.6225,
      "step": 27220
    },
    {
      "epoch": 1.6536102508046397,
      "grad_norm": 3.194990396499634,
      "learning_rate": 4.1991628102824416e-05,
      "loss": 2.4572,
      "step": 27230
    },
    {
      "epoch": 1.654217525961013,
      "grad_norm": 3.9834656715393066,
      "learning_rate": 4.196024337328358e-05,
      "loss": 2.5137,
      "step": 27240
    },
    {
      "epoch": 1.6548248011173863,
      "grad_norm": 4.627569675445557,
      "learning_rate": 4.192886189514834e-05,
      "loss": 2.3769,
      "step": 27250
    },
    {
      "epoch": 1.6554320762737595,
      "grad_norm": 4.827144622802734,
      "learning_rate": 4.1897483681109916e-05,
      "loss": 2.5674,
      "step": 27260
    },
    {
      "epoch": 1.656039351430133,
      "grad_norm": 3.7593445777893066,
      "learning_rate": 4.186610874385813e-05,
      "loss": 2.4576,
      "step": 27270
    },
    {
      "epoch": 1.6566466265865063,
      "grad_norm": 4.889360427856445,
      "learning_rate": 4.1834737096081525e-05,
      "loss": 2.6698,
      "step": 27280
    },
    {
      "epoch": 1.6572539017428798,
      "grad_norm": 3.3748815059661865,
      "learning_rate": 4.1803368750467254e-05,
      "loss": 2.5894,
      "step": 27290
    },
    {
      "epoch": 1.657861176899253,
      "grad_norm": 5.622072696685791,
      "learning_rate": 4.1772003719701216e-05,
      "loss": 2.4958,
      "step": 27300
    },
    {
      "epoch": 1.6584684520556263,
      "grad_norm": 3.9305732250213623,
      "learning_rate": 4.174064201646793e-05,
      "loss": 2.6491,
      "step": 27310
    },
    {
      "epoch": 1.6590757272119998,
      "grad_norm": 4.178013801574707,
      "learning_rate": 4.1709283653450524e-05,
      "loss": 2.8983,
      "step": 27320
    },
    {
      "epoch": 1.659683002368373,
      "grad_norm": 5.049219131469727,
      "learning_rate": 4.1677928643330874e-05,
      "loss": 2.4051,
      "step": 27330
    },
    {
      "epoch": 1.6602902775247466,
      "grad_norm": 5.457857608795166,
      "learning_rate": 4.1646576998789426e-05,
      "loss": 2.7801,
      "step": 27340
    },
    {
      "epoch": 1.6608975526811198,
      "grad_norm": 4.096324920654297,
      "learning_rate": 4.1615228732505274e-05,
      "loss": 2.9763,
      "step": 27350
    },
    {
      "epoch": 1.661504827837493,
      "grad_norm": 5.047778606414795,
      "learning_rate": 4.1583883857156136e-05,
      "loss": 2.7078,
      "step": 27360
    },
    {
      "epoch": 1.6621121029938664,
      "grad_norm": 5.53115177154541,
      "learning_rate": 4.1552542385418437e-05,
      "loss": 2.6983,
      "step": 27370
    },
    {
      "epoch": 1.6627193781502398,
      "grad_norm": 4.218850135803223,
      "learning_rate": 4.152120432996713e-05,
      "loss": 2.6987,
      "step": 27380
    },
    {
      "epoch": 1.6633266533066133,
      "grad_norm": 3.7328405380249023,
      "learning_rate": 4.148986970347582e-05,
      "loss": 2.8595,
      "step": 27390
    },
    {
      "epoch": 1.6639339284629866,
      "grad_norm": 3.3986735343933105,
      "learning_rate": 4.145853851861673e-05,
      "loss": 2.7511,
      "step": 27400
    },
    {
      "epoch": 1.6645412036193599,
      "grad_norm": 5.863999843597412,
      "learning_rate": 4.1427210788060714e-05,
      "loss": 2.4526,
      "step": 27410
    },
    {
      "epoch": 1.6651484787757331,
      "grad_norm": 3.79203462600708,
      "learning_rate": 4.1395886524477186e-05,
      "loss": 2.8285,
      "step": 27420
    },
    {
      "epoch": 1.6657557539321066,
      "grad_norm": 3.03691029548645,
      "learning_rate": 4.136456574053418e-05,
      "loss": 2.6086,
      "step": 27430
    },
    {
      "epoch": 1.66636302908848,
      "grad_norm": 2.9840340614318848,
      "learning_rate": 4.13332484488983e-05,
      "loss": 3.1108,
      "step": 27440
    },
    {
      "epoch": 1.6669703042448534,
      "grad_norm": 4.401215553283691,
      "learning_rate": 4.13019346622348e-05,
      "loss": 2.9278,
      "step": 27450
    },
    {
      "epoch": 1.6675775794012266,
      "grad_norm": 6.342116832733154,
      "learning_rate": 4.127062439320745e-05,
      "loss": 2.9858,
      "step": 27460
    },
    {
      "epoch": 1.6681848545576,
      "grad_norm": 4.030030250549316,
      "learning_rate": 4.123931765447862e-05,
      "loss": 3.0378,
      "step": 27470
    },
    {
      "epoch": 1.6687921297139734,
      "grad_norm": 4.014825344085693,
      "learning_rate": 4.120801445870924e-05,
      "loss": 2.8871,
      "step": 27480
    },
    {
      "epoch": 1.6693994048703469,
      "grad_norm": 3.359982490539551,
      "learning_rate": 4.117671481855886e-05,
      "loss": 2.5179,
      "step": 27490
    },
    {
      "epoch": 1.6700066800267201,
      "grad_norm": 2.722805976867676,
      "learning_rate": 4.114541874668553e-05,
      "loss": 2.5965,
      "step": 27500
    },
    {
      "epoch": 1.6706139551830934,
      "grad_norm": 2.1368825435638428,
      "learning_rate": 4.111412625574587e-05,
      "loss": 2.2753,
      "step": 27510
    },
    {
      "epoch": 1.6712212303394667,
      "grad_norm": 2.480086088180542,
      "learning_rate": 4.108283735839506e-05,
      "loss": 2.6033,
      "step": 27520
    },
    {
      "epoch": 1.6718285054958402,
      "grad_norm": 2.498894691467285,
      "learning_rate": 4.105155206728687e-05,
      "loss": 2.5575,
      "step": 27530
    },
    {
      "epoch": 1.6724357806522137,
      "grad_norm": 2.515348196029663,
      "learning_rate": 4.102027039507352e-05,
      "loss": 2.4773,
      "step": 27540
    },
    {
      "epoch": 1.673043055808587,
      "grad_norm": 2.150516986846924,
      "learning_rate": 4.0988992354405844e-05,
      "loss": 2.4382,
      "step": 27550
    },
    {
      "epoch": 1.6736503309649602,
      "grad_norm": 3.2215471267700195,
      "learning_rate": 4.095771795793317e-05,
      "loss": 2.3841,
      "step": 27560
    },
    {
      "epoch": 1.6742576061213335,
      "grad_norm": 2.7030835151672363,
      "learning_rate": 4.092644721830337e-05,
      "loss": 2.7932,
      "step": 27570
    },
    {
      "epoch": 1.674864881277707,
      "grad_norm": 3.168769359588623,
      "learning_rate": 4.089518014816283e-05,
      "loss": 2.8899,
      "step": 27580
    },
    {
      "epoch": 1.6754721564340804,
      "grad_norm": 5.506172180175781,
      "learning_rate": 4.086391676015642e-05,
      "loss": 2.9128,
      "step": 27590
    },
    {
      "epoch": 1.6760794315904537,
      "grad_norm": 2.622122287750244,
      "learning_rate": 4.08326570669276e-05,
      "loss": 2.5941,
      "step": 27600
    },
    {
      "epoch": 1.676686706746827,
      "grad_norm": 2.8541290760040283,
      "learning_rate": 4.080140108111826e-05,
      "loss": 2.4785,
      "step": 27610
    },
    {
      "epoch": 1.6772939819032002,
      "grad_norm": 2.3498694896698,
      "learning_rate": 4.077014881536884e-05,
      "loss": 2.4742,
      "step": 27620
    },
    {
      "epoch": 1.6779012570595737,
      "grad_norm": 3.8213422298431396,
      "learning_rate": 4.073890028231819e-05,
      "loss": 2.5797,
      "step": 27630
    },
    {
      "epoch": 1.6785085322159472,
      "grad_norm": 2.0353567600250244,
      "learning_rate": 4.0707655494603795e-05,
      "loss": 2.4971,
      "step": 27640
    },
    {
      "epoch": 1.6791158073723205,
      "grad_norm": 3.0119311809539795,
      "learning_rate": 4.067641446486152e-05,
      "loss": 3.1422,
      "step": 27650
    },
    {
      "epoch": 1.6797230825286937,
      "grad_norm": 5.166798114776611,
      "learning_rate": 4.064517720572572e-05,
      "loss": 2.9914,
      "step": 27660
    },
    {
      "epoch": 1.680330357685067,
      "grad_norm": 4.372562408447266,
      "learning_rate": 4.061394372982922e-05,
      "loss": 2.5708,
      "step": 27670
    },
    {
      "epoch": 1.6809376328414405,
      "grad_norm": 4.470961570739746,
      "learning_rate": 4.0582714049803395e-05,
      "loss": 2.3962,
      "step": 27680
    },
    {
      "epoch": 1.681544907997814,
      "grad_norm": 4.16790246963501,
      "learning_rate": 4.0551488178277986e-05,
      "loss": 2.7371,
      "step": 27690
    },
    {
      "epoch": 1.6821521831541872,
      "grad_norm": 5.004640102386475,
      "learning_rate": 4.052026612788123e-05,
      "loss": 2.6268,
      "step": 27700
    },
    {
      "epoch": 1.6827594583105605,
      "grad_norm": 4.161397933959961,
      "learning_rate": 4.048904791123983e-05,
      "loss": 2.5986,
      "step": 27710
    },
    {
      "epoch": 1.6833667334669338,
      "grad_norm": 2.1820168495178223,
      "learning_rate": 4.045783354097893e-05,
      "loss": 2.3893,
      "step": 27720
    },
    {
      "epoch": 1.6839740086233073,
      "grad_norm": 4.315838813781738,
      "learning_rate": 4.042662302972212e-05,
      "loss": 2.4258,
      "step": 27730
    },
    {
      "epoch": 1.6845812837796805,
      "grad_norm": 3.905935525894165,
      "learning_rate": 4.039541639009143e-05,
      "loss": 2.5626,
      "step": 27740
    },
    {
      "epoch": 1.685188558936054,
      "grad_norm": 4.666691303253174,
      "learning_rate": 4.0364213634707305e-05,
      "loss": 2.8863,
      "step": 27750
    },
    {
      "epoch": 1.6857958340924273,
      "grad_norm": 3.2546896934509277,
      "learning_rate": 4.033301477618866e-05,
      "loss": 2.6341,
      "step": 27760
    },
    {
      "epoch": 1.6864031092488005,
      "grad_norm": 3.86275315284729,
      "learning_rate": 4.03018198271528e-05,
      "loss": 2.6764,
      "step": 27770
    },
    {
      "epoch": 1.687010384405174,
      "grad_norm": 2.7341396808624268,
      "learning_rate": 4.027062880021545e-05,
      "loss": 2.6561,
      "step": 27780
    },
    {
      "epoch": 1.6876176595615473,
      "grad_norm": 3.1364052295684814,
      "learning_rate": 4.023944170799078e-05,
      "loss": 2.4931,
      "step": 27790
    },
    {
      "epoch": 1.6882249347179208,
      "grad_norm": 2.735231876373291,
      "learning_rate": 4.0208258563091326e-05,
      "loss": 2.6908,
      "step": 27800
    },
    {
      "epoch": 1.688832209874294,
      "grad_norm": 2.6808457374572754,
      "learning_rate": 4.0177079378128065e-05,
      "loss": 3.014,
      "step": 27810
    },
    {
      "epoch": 1.6894394850306673,
      "grad_norm": 4.469793319702148,
      "learning_rate": 4.014590416571033e-05,
      "loss": 2.6186,
      "step": 27820
    },
    {
      "epoch": 1.6900467601870406,
      "grad_norm": 3.9745914936065674,
      "learning_rate": 4.0114732938445907e-05,
      "loss": 2.6828,
      "step": 27830
    },
    {
      "epoch": 1.690654035343414,
      "grad_norm": 3.18310809135437,
      "learning_rate": 4.0083565708940916e-05,
      "loss": 2.6349,
      "step": 27840
    },
    {
      "epoch": 1.6912613104997876,
      "grad_norm": 3.6476502418518066,
      "learning_rate": 4.005240248979989e-05,
      "loss": 2.6474,
      "step": 27850
    },
    {
      "epoch": 1.6918685856561608,
      "grad_norm": 2.117406129837036,
      "learning_rate": 4.002124329362571e-05,
      "loss": 2.5076,
      "step": 27860
    },
    {
      "epoch": 1.692475860812534,
      "grad_norm": 3.4306626319885254,
      "learning_rate": 3.999008813301968e-05,
      "loss": 2.6783,
      "step": 27870
    },
    {
      "epoch": 1.6930831359689074,
      "grad_norm": 4.4688401222229,
      "learning_rate": 3.995893702058143e-05,
      "loss": 2.6253,
      "step": 27880
    },
    {
      "epoch": 1.6936904111252808,
      "grad_norm": 2.4076597690582275,
      "learning_rate": 3.992778996890897e-05,
      "loss": 2.4055,
      "step": 27890
    },
    {
      "epoch": 1.6942976862816543,
      "grad_norm": 2.3639402389526367,
      "learning_rate": 3.9896646990598655e-05,
      "loss": 2.2037,
      "step": 27900
    },
    {
      "epoch": 1.6949049614380276,
      "grad_norm": 3.5590837001800537,
      "learning_rate": 3.9865508098245205e-05,
      "loss": 2.5286,
      "step": 27910
    },
    {
      "epoch": 1.6955122365944009,
      "grad_norm": 3.3661367893218994,
      "learning_rate": 3.98343733044417e-05,
      "loss": 2.7618,
      "step": 27920
    },
    {
      "epoch": 1.6961195117507741,
      "grad_norm": 3.745000123977661,
      "learning_rate": 3.980324262177953e-05,
      "loss": 2.8846,
      "step": 27930
    },
    {
      "epoch": 1.6967267869071476,
      "grad_norm": 4.53298807144165,
      "learning_rate": 3.977211606284841e-05,
      "loss": 2.9813,
      "step": 27940
    },
    {
      "epoch": 1.697334062063521,
      "grad_norm": 4.022564888000488,
      "learning_rate": 3.974099364023648e-05,
      "loss": 2.6897,
      "step": 27950
    },
    {
      "epoch": 1.6979413372198944,
      "grad_norm": 4.234374523162842,
      "learning_rate": 3.970987536653011e-05,
      "loss": 2.878,
      "step": 27960
    },
    {
      "epoch": 1.6985486123762676,
      "grad_norm": 3.800853729248047,
      "learning_rate": 3.967876125431402e-05,
      "loss": 2.3537,
      "step": 27970
    },
    {
      "epoch": 1.699155887532641,
      "grad_norm": 2.5656650066375732,
      "learning_rate": 3.964765131617124e-05,
      "loss": 2.4212,
      "step": 27980
    },
    {
      "epoch": 1.6997631626890144,
      "grad_norm": 3.4810826778411865,
      "learning_rate": 3.961654556468318e-05,
      "loss": 2.8498,
      "step": 27990
    },
    {
      "epoch": 1.7003704378453879,
      "grad_norm": 5.180968761444092,
      "learning_rate": 3.9585444012429445e-05,
      "loss": 2.9166,
      "step": 28000
    },
    {
      "epoch": 1.7009777130017611,
      "grad_norm": 4.820973873138428,
      "learning_rate": 3.9554346671988e-05,
      "loss": 2.9159,
      "step": 28010
    },
    {
      "epoch": 1.7015849881581344,
      "grad_norm": 3.9332971572875977,
      "learning_rate": 3.9523253555935146e-05,
      "loss": 2.7849,
      "step": 28020
    },
    {
      "epoch": 1.7021922633145077,
      "grad_norm": 3.772594690322876,
      "learning_rate": 3.9492164676845416e-05,
      "loss": 2.2493,
      "step": 28030
    },
    {
      "epoch": 1.7027995384708812,
      "grad_norm": 4.236151695251465,
      "learning_rate": 3.946108004729164e-05,
      "loss": 2.4857,
      "step": 28040
    },
    {
      "epoch": 1.7034068136272547,
      "grad_norm": 3.805453300476074,
      "learning_rate": 3.942999967984491e-05,
      "loss": 2.909,
      "step": 28050
    },
    {
      "epoch": 1.704014088783628,
      "grad_norm": 4.320736885070801,
      "learning_rate": 3.939892358707469e-05,
      "loss": 3.0615,
      "step": 28060
    },
    {
      "epoch": 1.7046213639400012,
      "grad_norm": 4.407611846923828,
      "learning_rate": 3.936785178154859e-05,
      "loss": 3.0696,
      "step": 28070
    },
    {
      "epoch": 1.7052286390963745,
      "grad_norm": 6.596492290496826,
      "learning_rate": 3.9336784275832575e-05,
      "loss": 2.6419,
      "step": 28080
    },
    {
      "epoch": 1.705835914252748,
      "grad_norm": 4.226678371429443,
      "learning_rate": 3.93057210824908e-05,
      "loss": 2.5871,
      "step": 28090
    },
    {
      "epoch": 1.7064431894091214,
      "grad_norm": 6.649904251098633,
      "learning_rate": 3.9274662214085756e-05,
      "loss": 3.4008,
      "step": 28100
    },
    {
      "epoch": 1.7070504645654947,
      "grad_norm": 4.317543029785156,
      "learning_rate": 3.924360768317813e-05,
      "loss": 2.9244,
      "step": 28110
    },
    {
      "epoch": 1.707657739721868,
      "grad_norm": 4.556155204772949,
      "learning_rate": 3.9212557502326866e-05,
      "loss": 2.7629,
      "step": 28120
    },
    {
      "epoch": 1.7082650148782412,
      "grad_norm": 4.762858867645264,
      "learning_rate": 3.9181511684089146e-05,
      "loss": 2.8117,
      "step": 28130
    },
    {
      "epoch": 1.7088722900346147,
      "grad_norm": 3.8381948471069336,
      "learning_rate": 3.915047024102041e-05,
      "loss": 3.0028,
      "step": 28140
    },
    {
      "epoch": 1.709479565190988,
      "grad_norm": 4.775230884552002,
      "learning_rate": 3.9119433185674305e-05,
      "loss": 2.8988,
      "step": 28150
    },
    {
      "epoch": 1.7100868403473615,
      "grad_norm": 2.5276968479156494,
      "learning_rate": 3.9088400530602705e-05,
      "loss": 2.6851,
      "step": 28160
    },
    {
      "epoch": 1.7106941155037347,
      "grad_norm": 3.2333931922912598,
      "learning_rate": 3.90573722883557e-05,
      "loss": 2.7632,
      "step": 28170
    },
    {
      "epoch": 1.711301390660108,
      "grad_norm": 3.961015224456787,
      "learning_rate": 3.902634847148163e-05,
      "loss": 2.8292,
      "step": 28180
    },
    {
      "epoch": 1.7119086658164815,
      "grad_norm": 6.070415019989014,
      "learning_rate": 3.899532909252701e-05,
      "loss": 2.6105,
      "step": 28190
    },
    {
      "epoch": 1.7125159409728548,
      "grad_norm": 2.9697365760803223,
      "learning_rate": 3.896431416403657e-05,
      "loss": 2.1573,
      "step": 28200
    },
    {
      "epoch": 1.7131232161292282,
      "grad_norm": 2.024301052093506,
      "learning_rate": 3.893330369855323e-05,
      "loss": 1.9399,
      "step": 28210
    },
    {
      "epoch": 1.7137304912856015,
      "grad_norm": 3.162966728210449,
      "learning_rate": 3.8902297708618135e-05,
      "loss": 2.254,
      "step": 28220
    },
    {
      "epoch": 1.7143377664419748,
      "grad_norm": 3.9674994945526123,
      "learning_rate": 3.88712962067706e-05,
      "loss": 2.3717,
      "step": 28230
    },
    {
      "epoch": 1.7149450415983483,
      "grad_norm": 5.534243583679199,
      "learning_rate": 3.88402992055481e-05,
      "loss": 2.6853,
      "step": 28240
    },
    {
      "epoch": 1.7155523167547215,
      "grad_norm": 3.7690041065216064,
      "learning_rate": 3.880930671748635e-05,
      "loss": 2.7572,
      "step": 28250
    },
    {
      "epoch": 1.716159591911095,
      "grad_norm": 3.7703278064727783,
      "learning_rate": 3.877831875511919e-05,
      "loss": 2.7518,
      "step": 28260
    },
    {
      "epoch": 1.7167668670674683,
      "grad_norm": 3.6631505489349365,
      "learning_rate": 3.8747335330978656e-05,
      "loss": 2.6614,
      "step": 28270
    },
    {
      "epoch": 1.7173741422238415,
      "grad_norm": 5.682994365692139,
      "learning_rate": 3.87163564575949e-05,
      "loss": 2.6824,
      "step": 28280
    },
    {
      "epoch": 1.7179814173802148,
      "grad_norm": 4.361665725708008,
      "learning_rate": 3.868538214749632e-05,
      "loss": 2.3698,
      "step": 28290
    },
    {
      "epoch": 1.7185886925365883,
      "grad_norm": 4.622548580169678,
      "learning_rate": 3.8654412413209404e-05,
      "loss": 2.7621,
      "step": 28300
    },
    {
      "epoch": 1.7191959676929618,
      "grad_norm": 3.6140596866607666,
      "learning_rate": 3.8623447267258806e-05,
      "loss": 2.75,
      "step": 28310
    },
    {
      "epoch": 1.719803242849335,
      "grad_norm": 3.892381191253662,
      "learning_rate": 3.8592486722167296e-05,
      "loss": 2.8202,
      "step": 28320
    },
    {
      "epoch": 1.7204105180057083,
      "grad_norm": 2.1758713722229004,
      "learning_rate": 3.856153079045585e-05,
      "loss": 2.3337,
      "step": 28330
    },
    {
      "epoch": 1.7210177931620816,
      "grad_norm": 2.237229108810425,
      "learning_rate": 3.8530579484643544e-05,
      "loss": 2.172,
      "step": 28340
    },
    {
      "epoch": 1.721625068318455,
      "grad_norm": 2.469071865081787,
      "learning_rate": 3.849963281724754e-05,
      "loss": 1.9794,
      "step": 28350
    },
    {
      "epoch": 1.7222323434748286,
      "grad_norm": 3.94492506980896,
      "learning_rate": 3.846869080078317e-05,
      "loss": 2.1319,
      "step": 28360
    },
    {
      "epoch": 1.7228396186312018,
      "grad_norm": 2.507493734359741,
      "learning_rate": 3.843775344776393e-05,
      "loss": 2.4468,
      "step": 28370
    },
    {
      "epoch": 1.723446893787575,
      "grad_norm": 4.133434772491455,
      "learning_rate": 3.840682077070132e-05,
      "loss": 2.3982,
      "step": 28380
    },
    {
      "epoch": 1.7240541689439484,
      "grad_norm": 3.3627731800079346,
      "learning_rate": 3.837589278210503e-05,
      "loss": 2.3391,
      "step": 28390
    },
    {
      "epoch": 1.7246614441003218,
      "grad_norm": 3.6260578632354736,
      "learning_rate": 3.834496949448283e-05,
      "loss": 2.851,
      "step": 28400
    },
    {
      "epoch": 1.7252687192566953,
      "grad_norm": 3.1252307891845703,
      "learning_rate": 3.831405092034058e-05,
      "loss": 2.6649,
      "step": 28410
    },
    {
      "epoch": 1.7258759944130686,
      "grad_norm": 3.4378507137298584,
      "learning_rate": 3.8283137072182265e-05,
      "loss": 2.7308,
      "step": 28420
    },
    {
      "epoch": 1.7264832695694419,
      "grad_norm": 3.328465223312378,
      "learning_rate": 3.825222796250993e-05,
      "loss": 2.53,
      "step": 28430
    },
    {
      "epoch": 1.7270905447258151,
      "grad_norm": 3.8264060020446777,
      "learning_rate": 3.822132360382369e-05,
      "loss": 2.6188,
      "step": 28440
    },
    {
      "epoch": 1.7276978198821886,
      "grad_norm": 2.845705509185791,
      "learning_rate": 3.8190424008621784e-05,
      "loss": 3.0209,
      "step": 28450
    },
    {
      "epoch": 1.7283050950385621,
      "grad_norm": 3.680612802505493,
      "learning_rate": 3.8159529189400496e-05,
      "loss": 2.4754,
      "step": 28460
    },
    {
      "epoch": 1.7289123701949354,
      "grad_norm": 2.965176582336426,
      "learning_rate": 3.812863915865417e-05,
      "loss": 2.3303,
      "step": 28470
    },
    {
      "epoch": 1.7295196453513086,
      "grad_norm": 4.8215413093566895,
      "learning_rate": 3.809775392887525e-05,
      "loss": 3.256,
      "step": 28480
    },
    {
      "epoch": 1.730126920507682,
      "grad_norm": 5.622900009155273,
      "learning_rate": 3.806687351255419e-05,
      "loss": 3.291,
      "step": 28490
    },
    {
      "epoch": 1.7307341956640554,
      "grad_norm": 4.840456485748291,
      "learning_rate": 3.803599792217955e-05,
      "loss": 2.8086,
      "step": 28500
    },
    {
      "epoch": 1.7313414708204289,
      "grad_norm": 4.118255138397217,
      "learning_rate": 3.800512717023786e-05,
      "loss": 2.4894,
      "step": 28510
    },
    {
      "epoch": 1.7319487459768022,
      "grad_norm": 3.9063878059387207,
      "learning_rate": 3.797426126921381e-05,
      "loss": 2.7617,
      "step": 28520
    },
    {
      "epoch": 1.7325560211331754,
      "grad_norm": 1.5950480699539185,
      "learning_rate": 3.794340023159002e-05,
      "loss": 2.4454,
      "step": 28530
    },
    {
      "epoch": 1.7331632962895487,
      "grad_norm": 3.4988720417022705,
      "learning_rate": 3.79125440698472e-05,
      "loss": 2.5814,
      "step": 28540
    },
    {
      "epoch": 1.7337705714459222,
      "grad_norm": 4.107630729675293,
      "learning_rate": 3.788169279646405e-05,
      "loss": 2.4766,
      "step": 28550
    },
    {
      "epoch": 1.7343778466022957,
      "grad_norm": 5.045661926269531,
      "learning_rate": 3.785084642391734e-05,
      "loss": 2.9566,
      "step": 28560
    },
    {
      "epoch": 1.734985121758669,
      "grad_norm": 3.6112380027770996,
      "learning_rate": 3.782000496468184e-05,
      "loss": 3.0485,
      "step": 28570
    },
    {
      "epoch": 1.7355923969150422,
      "grad_norm": 3.5577712059020996,
      "learning_rate": 3.778916843123031e-05,
      "loss": 2.4158,
      "step": 28580
    },
    {
      "epoch": 1.7361996720714155,
      "grad_norm": 2.5918102264404297,
      "learning_rate": 3.775833683603353e-05,
      "loss": 2.2513,
      "step": 28590
    },
    {
      "epoch": 1.736806947227789,
      "grad_norm": 3.1822402477264404,
      "learning_rate": 3.772751019156031e-05,
      "loss": 2.7366,
      "step": 28600
    },
    {
      "epoch": 1.7374142223841622,
      "grad_norm": 3.8234097957611084,
      "learning_rate": 3.769668851027744e-05,
      "loss": 2.7548,
      "step": 28610
    },
    {
      "epoch": 1.7380214975405357,
      "grad_norm": 4.514443397521973,
      "learning_rate": 3.766587180464967e-05,
      "loss": 2.6574,
      "step": 28620
    },
    {
      "epoch": 1.738628772696909,
      "grad_norm": 3.838338613510132,
      "learning_rate": 3.763506008713976e-05,
      "loss": 2.3753,
      "step": 28630
    },
    {
      "epoch": 1.7392360478532822,
      "grad_norm": 3.6534335613250732,
      "learning_rate": 3.76042533702085e-05,
      "loss": 2.4366,
      "step": 28640
    },
    {
      "epoch": 1.7398433230096557,
      "grad_norm": 4.129878997802734,
      "learning_rate": 3.7573451666314614e-05,
      "loss": 3.0524,
      "step": 28650
    },
    {
      "epoch": 1.740450598166029,
      "grad_norm": 4.192127704620361,
      "learning_rate": 3.7542654987914763e-05,
      "loss": 2.7356,
      "step": 28660
    },
    {
      "epoch": 1.7410578733224025,
      "grad_norm": 5.615862846374512,
      "learning_rate": 3.751186334746362e-05,
      "loss": 2.611,
      "step": 28670
    },
    {
      "epoch": 1.7416651484787757,
      "grad_norm": 5.156880855560303,
      "learning_rate": 3.748107675741386e-05,
      "loss": 2.8122,
      "step": 28680
    },
    {
      "epoch": 1.742272423635149,
      "grad_norm": 4.424083232879639,
      "learning_rate": 3.7450295230216036e-05,
      "loss": 2.7138,
      "step": 28690
    },
    {
      "epoch": 1.7428796987915225,
      "grad_norm": 4.299456596374512,
      "learning_rate": 3.7419518778318666e-05,
      "loss": 2.8167,
      "step": 28700
    },
    {
      "epoch": 1.7434869739478958,
      "grad_norm": 5.744271755218506,
      "learning_rate": 3.73887474141683e-05,
      "loss": 3.1993,
      "step": 28710
    },
    {
      "epoch": 1.7440942491042692,
      "grad_norm": 4.984640598297119,
      "learning_rate": 3.735798115020932e-05,
      "loss": 2.7603,
      "step": 28720
    },
    {
      "epoch": 1.7447015242606425,
      "grad_norm": 3.363652467727661,
      "learning_rate": 3.73272199988841e-05,
      "loss": 2.3137,
      "step": 28730
    },
    {
      "epoch": 1.7453087994170158,
      "grad_norm": 4.100825309753418,
      "learning_rate": 3.729646397263292e-05,
      "loss": 2.4325,
      "step": 28740
    },
    {
      "epoch": 1.745916074573389,
      "grad_norm": 3.4923834800720215,
      "learning_rate": 3.7265713083894074e-05,
      "loss": 3.2088,
      "step": 28750
    },
    {
      "epoch": 1.7465233497297625,
      "grad_norm": 3.082257032394409,
      "learning_rate": 3.723496734510365e-05,
      "loss": 2.8712,
      "step": 28760
    },
    {
      "epoch": 1.747130624886136,
      "grad_norm": 2.629753828048706,
      "learning_rate": 3.720422676869573e-05,
      "loss": 2.6769,
      "step": 28770
    },
    {
      "epoch": 1.7477379000425093,
      "grad_norm": 3.338813543319702,
      "learning_rate": 3.717349136710229e-05,
      "loss": 2.4733,
      "step": 28780
    },
    {
      "epoch": 1.7483451751988826,
      "grad_norm": 1.931459665298462,
      "learning_rate": 3.714276115275324e-05,
      "loss": 2.2201,
      "step": 28790
    },
    {
      "epoch": 1.7489524503552558,
      "grad_norm": 2.40714955329895,
      "learning_rate": 3.7112036138076336e-05,
      "loss": 2.1701,
      "step": 28800
    },
    {
      "epoch": 1.7495597255116293,
      "grad_norm": 2.384894371032715,
      "learning_rate": 3.708131633549727e-05,
      "loss": 2.4645,
      "step": 28810
    },
    {
      "epoch": 1.7501670006680028,
      "grad_norm": 3.400449514389038,
      "learning_rate": 3.705060175743963e-05,
      "loss": 2.9958,
      "step": 28820
    },
    {
      "epoch": 1.750774275824376,
      "grad_norm": 5.960484027862549,
      "learning_rate": 3.7019892416324885e-05,
      "loss": 3.3114,
      "step": 28830
    },
    {
      "epoch": 1.7513815509807493,
      "grad_norm": 3.6600091457366943,
      "learning_rate": 3.6989188324572375e-05,
      "loss": 2.8327,
      "step": 28840
    },
    {
      "epoch": 1.7519888261371226,
      "grad_norm": 3.7318639755249023,
      "learning_rate": 3.695848949459932e-05,
      "loss": 2.5951,
      "step": 28850
    },
    {
      "epoch": 1.752596101293496,
      "grad_norm": 6.5652899742126465,
      "learning_rate": 3.69277959388208e-05,
      "loss": 2.9806,
      "step": 28860
    },
    {
      "epoch": 1.7532033764498696,
      "grad_norm": 4.131858825683594,
      "learning_rate": 3.689710766964982e-05,
      "loss": 2.7332,
      "step": 28870
    },
    {
      "epoch": 1.7538106516062428,
      "grad_norm": 5.74480676651001,
      "learning_rate": 3.686642469949717e-05,
      "loss": 3.1877,
      "step": 28880
    },
    {
      "epoch": 1.754417926762616,
      "grad_norm": 4.225172519683838,
      "learning_rate": 3.683574704077154e-05,
      "loss": 3.1763,
      "step": 28890
    },
    {
      "epoch": 1.7550252019189894,
      "grad_norm": 4.771342754364014,
      "learning_rate": 3.680507470587946e-05,
      "loss": 2.9559,
      "step": 28900
    },
    {
      "epoch": 1.7556324770753629,
      "grad_norm": 4.7298665046691895,
      "learning_rate": 3.6774407707225325e-05,
      "loss": 2.8404,
      "step": 28910
    },
    {
      "epoch": 1.7562397522317363,
      "grad_norm": 4.226207256317139,
      "learning_rate": 3.6743746057211354e-05,
      "loss": 2.7879,
      "step": 28920
    },
    {
      "epoch": 1.7568470273881096,
      "grad_norm": 6.00656795501709,
      "learning_rate": 3.671308976823759e-05,
      "loss": 2.6794,
      "step": 28930
    },
    {
      "epoch": 1.7574543025444829,
      "grad_norm": 6.661191463470459,
      "learning_rate": 3.6682438852701946e-05,
      "loss": 2.8371,
      "step": 28940
    },
    {
      "epoch": 1.7580615777008561,
      "grad_norm": 3.608820915222168,
      "learning_rate": 3.6651793323000125e-05,
      "loss": 2.8928,
      "step": 28950
    },
    {
      "epoch": 1.7586688528572296,
      "grad_norm": 4.0009236335754395,
      "learning_rate": 3.6621153191525695e-05,
      "loss": 2.7695,
      "step": 28960
    },
    {
      "epoch": 1.7592761280136031,
      "grad_norm": 4.230514049530029,
      "learning_rate": 3.659051847066995e-05,
      "loss": 2.9555,
      "step": 28970
    },
    {
      "epoch": 1.7598834031699764,
      "grad_norm": 3.224668502807617,
      "learning_rate": 3.655988917282212e-05,
      "loss": 2.8344,
      "step": 28980
    },
    {
      "epoch": 1.7604906783263496,
      "grad_norm": 3.5642096996307373,
      "learning_rate": 3.6529265310369176e-05,
      "loss": 2.8916,
      "step": 28990
    },
    {
      "epoch": 1.761097953482723,
      "grad_norm": 3.625626564025879,
      "learning_rate": 3.649864689569587e-05,
      "loss": 2.6976,
      "step": 29000
    },
    {
      "epoch": 1.7617052286390964,
      "grad_norm": 4.502725124359131,
      "learning_rate": 3.646803394118477e-05,
      "loss": 2.547,
      "step": 29010
    },
    {
      "epoch": 1.7623125037954699,
      "grad_norm": 4.4761881828308105,
      "learning_rate": 3.643742645921629e-05,
      "loss": 2.4204,
      "step": 29020
    },
    {
      "epoch": 1.7629197789518432,
      "grad_norm": 4.649190902709961,
      "learning_rate": 3.640682446216854e-05,
      "loss": 3.0074,
      "step": 29030
    },
    {
      "epoch": 1.7635270541082164,
      "grad_norm": 2.7490601539611816,
      "learning_rate": 3.637622796241746e-05,
      "loss": 2.816,
      "step": 29040
    },
    {
      "epoch": 1.7641343292645897,
      "grad_norm": 4.295554161071777,
      "learning_rate": 3.634563697233676e-05,
      "loss": 2.5082,
      "step": 29050
    },
    {
      "epoch": 1.7647416044209632,
      "grad_norm": 4.582057952880859,
      "learning_rate": 3.6315051504297965e-05,
      "loss": 2.4892,
      "step": 29060
    },
    {
      "epoch": 1.7653488795773364,
      "grad_norm": 4.758692264556885,
      "learning_rate": 3.628447157067028e-05,
      "loss": 2.7224,
      "step": 29070
    },
    {
      "epoch": 1.76595615473371,
      "grad_norm": 4.042190074920654,
      "learning_rate": 3.6253897183820726e-05,
      "loss": 2.6888,
      "step": 29080
    },
    {
      "epoch": 1.7665634298900832,
      "grad_norm": 5.619490146636963,
      "learning_rate": 3.622332835611407e-05,
      "loss": 2.5149,
      "step": 29090
    },
    {
      "epoch": 1.7671707050464565,
      "grad_norm": 3.9777028560638428,
      "learning_rate": 3.6192765099912857e-05,
      "loss": 2.556,
      "step": 29100
    },
    {
      "epoch": 1.76777798020283,
      "grad_norm": 5.683965682983398,
      "learning_rate": 3.6162207427577336e-05,
      "loss": 2.5363,
      "step": 29110
    },
    {
      "epoch": 1.7683852553592032,
      "grad_norm": 4.776590824127197,
      "learning_rate": 3.613165535146551e-05,
      "loss": 2.5103,
      "step": 29120
    },
    {
      "epoch": 1.7689925305155767,
      "grad_norm": 8.418567657470703,
      "learning_rate": 3.610110888393312e-05,
      "loss": 3.0111,
      "step": 29130
    },
    {
      "epoch": 1.76959980567195,
      "grad_norm": 6.376906871795654,
      "learning_rate": 3.6070568037333674e-05,
      "loss": 3.094,
      "step": 29140
    },
    {
      "epoch": 1.7702070808283232,
      "grad_norm": 5.779107570648193,
      "learning_rate": 3.604003282401835e-05,
      "loss": 2.7777,
      "step": 29150
    },
    {
      "epoch": 1.7708143559846967,
      "grad_norm": 4.865358352661133,
      "learning_rate": 3.6009503256336086e-05,
      "loss": 2.4488,
      "step": 29160
    },
    {
      "epoch": 1.77142163114107,
      "grad_norm": 4.561474800109863,
      "learning_rate": 3.597897934663353e-05,
      "loss": 2.7188,
      "step": 29170
    },
    {
      "epoch": 1.7720289062974435,
      "grad_norm": 4.0833659172058105,
      "learning_rate": 3.594846110725503e-05,
      "loss": 2.7045,
      "step": 29180
    },
    {
      "epoch": 1.7726361814538167,
      "grad_norm": 5.548903942108154,
      "learning_rate": 3.5917948550542645e-05,
      "loss": 3.099,
      "step": 29190
    },
    {
      "epoch": 1.77324345661019,
      "grad_norm": 5.508554935455322,
      "learning_rate": 3.588744168883613e-05,
      "loss": 3.0305,
      "step": 29200
    },
    {
      "epoch": 1.7738507317665633,
      "grad_norm": 3.8001582622528076,
      "learning_rate": 3.585694053447298e-05,
      "loss": 3.1154,
      "step": 29210
    },
    {
      "epoch": 1.7744580069229368,
      "grad_norm": 2.861966848373413,
      "learning_rate": 3.582644509978832e-05,
      "loss": 2.7621,
      "step": 29220
    },
    {
      "epoch": 1.7750652820793102,
      "grad_norm": 4.636146068572998,
      "learning_rate": 3.5795955397114996e-05,
      "loss": 2.454,
      "step": 29230
    },
    {
      "epoch": 1.7756725572356835,
      "grad_norm": 4.6878767013549805,
      "learning_rate": 3.576547143878351e-05,
      "loss": 2.6452,
      "step": 29240
    },
    {
      "epoch": 1.7762798323920568,
      "grad_norm": 3.7410542964935303,
      "learning_rate": 3.57349932371221e-05,
      "loss": 2.7513,
      "step": 29250
    },
    {
      "epoch": 1.77688710754843,
      "grad_norm": 4.595264434814453,
      "learning_rate": 3.570452080445661e-05,
      "loss": 2.576,
      "step": 29260
    },
    {
      "epoch": 1.7774943827048035,
      "grad_norm": 3.830388307571411,
      "learning_rate": 3.5674054153110596e-05,
      "loss": 2.6569,
      "step": 29270
    },
    {
      "epoch": 1.778101657861177,
      "grad_norm": 7.213352203369141,
      "learning_rate": 3.5643593295405206e-05,
      "loss": 2.595,
      "step": 29280
    },
    {
      "epoch": 1.7787089330175503,
      "grad_norm": 3.7417523860931396,
      "learning_rate": 3.561313824365936e-05,
      "loss": 2.3066,
      "step": 29290
    },
    {
      "epoch": 1.7793162081739236,
      "grad_norm": 4.834421634674072,
      "learning_rate": 3.558268901018955e-05,
      "loss": 2.5255,
      "step": 29300
    },
    {
      "epoch": 1.7799234833302968,
      "grad_norm": 4.578987121582031,
      "learning_rate": 3.555224560730991e-05,
      "loss": 2.8251,
      "step": 29310
    },
    {
      "epoch": 1.7805307584866703,
      "grad_norm": 3.6105237007141113,
      "learning_rate": 3.552180804733222e-05,
      "loss": 3.025,
      "step": 29320
    },
    {
      "epoch": 1.7811380336430438,
      "grad_norm": 2.807901382446289,
      "learning_rate": 3.549137634256596e-05,
      "loss": 2.7193,
      "step": 29330
    },
    {
      "epoch": 1.781745308799417,
      "grad_norm": 2.9730753898620605,
      "learning_rate": 3.54609505053182e-05,
      "loss": 2.5492,
      "step": 29340
    },
    {
      "epoch": 1.7823525839557903,
      "grad_norm": 3.606858253479004,
      "learning_rate": 3.543053054789359e-05,
      "loss": 2.4433,
      "step": 29350
    },
    {
      "epoch": 1.7829598591121636,
      "grad_norm": 3.053027629852295,
      "learning_rate": 3.540011648259445e-05,
      "loss": 2.7429,
      "step": 29360
    },
    {
      "epoch": 1.783567134268537,
      "grad_norm": 3.6357715129852295,
      "learning_rate": 3.536970832172076e-05,
      "loss": 2.9466,
      "step": 29370
    },
    {
      "epoch": 1.7841744094249106,
      "grad_norm": 4.698935508728027,
      "learning_rate": 3.533930607757002e-05,
      "loss": 2.8815,
      "step": 29380
    },
    {
      "epoch": 1.7847816845812838,
      "grad_norm": 4.793560028076172,
      "learning_rate": 3.530890976243737e-05,
      "loss": 3.2392,
      "step": 29390
    },
    {
      "epoch": 1.785388959737657,
      "grad_norm": 2.824784517288208,
      "learning_rate": 3.527851938861563e-05,
      "loss": 2.8833,
      "step": 29400
    },
    {
      "epoch": 1.7859962348940304,
      "grad_norm": 5.068221092224121,
      "learning_rate": 3.524813496839509e-05,
      "loss": 2.769,
      "step": 29410
    },
    {
      "epoch": 1.7866035100504039,
      "grad_norm": 3.839327812194824,
      "learning_rate": 3.521775651406371e-05,
      "loss": 2.6594,
      "step": 29420
    },
    {
      "epoch": 1.7872107852067773,
      "grad_norm": 5.169712066650391,
      "learning_rate": 3.518738403790702e-05,
      "loss": 2.9444,
      "step": 29430
    },
    {
      "epoch": 1.7878180603631506,
      "grad_norm": 6.201050758361816,
      "learning_rate": 3.515701755220814e-05,
      "loss": 2.774,
      "step": 29440
    },
    {
      "epoch": 1.7884253355195239,
      "grad_norm": 6.57234525680542,
      "learning_rate": 3.512665706924776e-05,
      "loss": 2.9865,
      "step": 29450
    },
    {
      "epoch": 1.7890326106758971,
      "grad_norm": 7.808908939361572,
      "learning_rate": 3.5096302601304146e-05,
      "loss": 2.8633,
      "step": 29460
    },
    {
      "epoch": 1.7896398858322706,
      "grad_norm": 5.2389020919799805,
      "learning_rate": 3.506595416065311e-05,
      "loss": 2.5052,
      "step": 29470
    },
    {
      "epoch": 1.7902471609886441,
      "grad_norm": 5.1758131980896,
      "learning_rate": 3.5035611759568065e-05,
      "loss": 2.3563,
      "step": 29480
    },
    {
      "epoch": 1.7908544361450174,
      "grad_norm": 6.20626163482666,
      "learning_rate": 3.500527541031995e-05,
      "loss": 2.6525,
      "step": 29490
    },
    {
      "epoch": 1.7914617113013906,
      "grad_norm": 8.217997550964355,
      "learning_rate": 3.497494512517727e-05,
      "loss": 2.647,
      "step": 29500
    },
    {
      "epoch": 1.792068986457764,
      "grad_norm": 6.880894184112549,
      "learning_rate": 3.4944620916406076e-05,
      "loss": 2.4434,
      "step": 29510
    },
    {
      "epoch": 1.7926762616141374,
      "grad_norm": 6.3537516593933105,
      "learning_rate": 3.491430279626996e-05,
      "loss": 2.513,
      "step": 29520
    },
    {
      "epoch": 1.7932835367705107,
      "grad_norm": 5.358026504516602,
      "learning_rate": 3.4883990777030065e-05,
      "loss": 2.3305,
      "step": 29530
    },
    {
      "epoch": 1.7938908119268842,
      "grad_norm": 4.4615159034729,
      "learning_rate": 3.4853684870945044e-05,
      "loss": 2.4593,
      "step": 29540
    },
    {
      "epoch": 1.7944980870832574,
      "grad_norm": 5.618409156799316,
      "learning_rate": 3.482338509027107e-05,
      "loss": 2.6163,
      "step": 29550
    },
    {
      "epoch": 1.7951053622396307,
      "grad_norm": 4.536451816558838,
      "learning_rate": 3.479309144726188e-05,
      "loss": 2.8525,
      "step": 29560
    },
    {
      "epoch": 1.7957126373960042,
      "grad_norm": 4.465053081512451,
      "learning_rate": 3.476280395416871e-05,
      "loss": 2.9176,
      "step": 29570
    },
    {
      "epoch": 1.7963199125523774,
      "grad_norm": 4.294246673583984,
      "learning_rate": 3.4732522623240304e-05,
      "loss": 2.9683,
      "step": 29580
    },
    {
      "epoch": 1.796927187708751,
      "grad_norm": 5.584254741668701,
      "learning_rate": 3.470224746672287e-05,
      "loss": 2.9092,
      "step": 29590
    },
    {
      "epoch": 1.7975344628651242,
      "grad_norm": 6.586606979370117,
      "learning_rate": 3.4671978496860226e-05,
      "loss": 3.1424,
      "step": 29600
    },
    {
      "epoch": 1.7981417380214975,
      "grad_norm": 5.640864849090576,
      "learning_rate": 3.464171572589359e-05,
      "loss": 3.0948,
      "step": 29610
    },
    {
      "epoch": 1.798749013177871,
      "grad_norm": 5.945687770843506,
      "learning_rate": 3.461145916606171e-05,
      "loss": 2.7618,
      "step": 29620
    },
    {
      "epoch": 1.7993562883342442,
      "grad_norm": 5.099071502685547,
      "learning_rate": 3.458120882960083e-05,
      "loss": 2.6759,
      "step": 29630
    },
    {
      "epoch": 1.7999635634906177,
      "grad_norm": 6.348816871643066,
      "learning_rate": 3.455096472874467e-05,
      "loss": 2.6468,
      "step": 29640
    },
    {
      "epoch": 1.800570838646991,
      "grad_norm": 5.54618501663208,
      "learning_rate": 3.452072687572444e-05,
      "loss": 2.6273,
      "step": 29650
    },
    {
      "epoch": 1.8011781138033642,
      "grad_norm": 5.340479850769043,
      "learning_rate": 3.4490495282768754e-05,
      "loss": 2.8092,
      "step": 29660
    },
    {
      "epoch": 1.8017853889597375,
      "grad_norm": 3.6599040031433105,
      "learning_rate": 3.446026996210381e-05,
      "loss": 2.6081,
      "step": 29670
    },
    {
      "epoch": 1.802392664116111,
      "grad_norm": 4.355531215667725,
      "learning_rate": 3.4430050925953186e-05,
      "loss": 2.7059,
      "step": 29680
    },
    {
      "epoch": 1.8029999392724845,
      "grad_norm": 4.65760612487793,
      "learning_rate": 3.439983818653794e-05,
      "loss": 2.5428,
      "step": 29690
    },
    {
      "epoch": 1.8036072144288577,
      "grad_norm": 4.079263210296631,
      "learning_rate": 3.436963175607656e-05,
      "loss": 2.5344,
      "step": 29700
    },
    {
      "epoch": 1.804214489585231,
      "grad_norm": 3.9137539863586426,
      "learning_rate": 3.433943164678506e-05,
      "loss": 3.0951,
      "step": 29710
    },
    {
      "epoch": 1.8048217647416043,
      "grad_norm": 4.077544689178467,
      "learning_rate": 3.430923787087682e-05,
      "loss": 2.9165,
      "step": 29720
    },
    {
      "epoch": 1.8054290398979778,
      "grad_norm": 5.13355016708374,
      "learning_rate": 3.427905044056267e-05,
      "loss": 3.0773,
      "step": 29730
    },
    {
      "epoch": 1.8060363150543512,
      "grad_norm": 4.940144062042236,
      "learning_rate": 3.42488693680509e-05,
      "loss": 2.9901,
      "step": 29740
    },
    {
      "epoch": 1.8066435902107245,
      "grad_norm": 6.610537528991699,
      "learning_rate": 3.421869466554722e-05,
      "loss": 3.1017,
      "step": 29750
    },
    {
      "epoch": 1.8072508653670978,
      "grad_norm": 4.936845302581787,
      "learning_rate": 3.4188526345254754e-05,
      "loss": 3.0686,
      "step": 29760
    },
    {
      "epoch": 1.807858140523471,
      "grad_norm": 4.810458660125732,
      "learning_rate": 3.4158364419374065e-05,
      "loss": 2.9865,
      "step": 29770
    },
    {
      "epoch": 1.8084654156798445,
      "grad_norm": 3.0742433071136475,
      "learning_rate": 3.412820890010309e-05,
      "loss": 3.0113,
      "step": 29780
    },
    {
      "epoch": 1.809072690836218,
      "grad_norm": 4.62926721572876,
      "learning_rate": 3.4098059799637225e-05,
      "loss": 3.0469,
      "step": 29790
    },
    {
      "epoch": 1.8096799659925913,
      "grad_norm": 5.055771827697754,
      "learning_rate": 3.406791713016924e-05,
      "loss": 3.0781,
      "step": 29800
    },
    {
      "epoch": 1.8102872411489646,
      "grad_norm": 3.7082629203796387,
      "learning_rate": 3.40377809038893e-05,
      "loss": 2.8248,
      "step": 29810
    },
    {
      "epoch": 1.8108945163053378,
      "grad_norm": 2.8642492294311523,
      "learning_rate": 3.400765113298498e-05,
      "loss": 2.3404,
      "step": 29820
    },
    {
      "epoch": 1.8115017914617113,
      "grad_norm": 3.4676554203033447,
      "learning_rate": 3.397752782964125e-05,
      "loss": 2.6354,
      "step": 29830
    },
    {
      "epoch": 1.8121090666180848,
      "grad_norm": 3.91117787361145,
      "learning_rate": 3.394741100604045e-05,
      "loss": 2.9165,
      "step": 29840
    },
    {
      "epoch": 1.812716341774458,
      "grad_norm": 4.521233558654785,
      "learning_rate": 3.391730067436229e-05,
      "loss": 3.0498,
      "step": 29850
    },
    {
      "epoch": 1.8133236169308313,
      "grad_norm": 4.291633129119873,
      "learning_rate": 3.388719684678388e-05,
      "loss": 3.1595,
      "step": 29860
    },
    {
      "epoch": 1.8139308920872046,
      "grad_norm": 2.749617576599121,
      "learning_rate": 3.385709953547968e-05,
      "loss": 2.635,
      "step": 29870
    },
    {
      "epoch": 1.814538167243578,
      "grad_norm": 3.362908124923706,
      "learning_rate": 3.382700875262154e-05,
      "loss": 2.8095,
      "step": 29880
    },
    {
      "epoch": 1.8151454423999516,
      "grad_norm": 4.2432756423950195,
      "learning_rate": 3.3796924510378626e-05,
      "loss": 2.7818,
      "step": 29890
    },
    {
      "epoch": 1.8157527175563248,
      "grad_norm": 3.0496022701263428,
      "learning_rate": 3.3766846820917494e-05,
      "loss": 2.4466,
      "step": 29900
    },
    {
      "epoch": 1.816359992712698,
      "grad_norm": 3.577486276626587,
      "learning_rate": 3.373677569640206e-05,
      "loss": 2.5235,
      "step": 29910
    },
    {
      "epoch": 1.8169672678690714,
      "grad_norm": 3.325106620788574,
      "learning_rate": 3.370671114899353e-05,
      "loss": 2.4762,
      "step": 29920
    },
    {
      "epoch": 1.8175745430254449,
      "grad_norm": 3.0283567905426025,
      "learning_rate": 3.367665319085051e-05,
      "loss": 2.3766,
      "step": 29930
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 5.382801532745361,
      "learning_rate": 3.364660183412892e-05,
      "loss": 3.1724,
      "step": 29940
    },
    {
      "epoch": 1.8187890933381916,
      "grad_norm": 4.392666339874268,
      "learning_rate": 3.361655709098199e-05,
      "loss": 2.7823,
      "step": 29950
    },
    {
      "epoch": 1.8193963684945649,
      "grad_norm": 3.389409065246582,
      "learning_rate": 3.358651897356032e-05,
      "loss": 2.5033,
      "step": 29960
    },
    {
      "epoch": 1.8200036436509381,
      "grad_norm": 5.159205913543701,
      "learning_rate": 3.3556487494011754e-05,
      "loss": 2.6101,
      "step": 29970
    },
    {
      "epoch": 1.8206109188073116,
      "grad_norm": 7.5537004470825195,
      "learning_rate": 3.352646266448155e-05,
      "loss": 2.6996,
      "step": 29980
    },
    {
      "epoch": 1.821218193963685,
      "grad_norm": 4.617079257965088,
      "learning_rate": 3.3496444497112226e-05,
      "loss": 2.6299,
      "step": 29990
    },
    {
      "epoch": 1.8218254691200584,
      "grad_norm": 5.0709123611450195,
      "learning_rate": 3.3466433004043565e-05,
      "loss": 3.0853,
      "step": 30000
    },
    {
      "epoch": 1.8218254691200584,
      "eval_loss": 4.4011712074279785,
      "eval_runtime": 2103.882,
      "eval_samples_per_second": 7.827,
      "eval_steps_per_second": 1.957,
      "step": 30000
    },
    {
      "epoch": 1.8224327442764316,
      "grad_norm": 3.7059147357940674,
      "learning_rate": 3.3436428197412704e-05,
      "loss": 4.0158,
      "step": 30010
    },
    {
      "epoch": 1.823040019432805,
      "grad_norm": 3.8361828327178955,
      "learning_rate": 3.340643008935411e-05,
      "loss": 3.5035,
      "step": 30020
    },
    {
      "epoch": 1.8236472945891784,
      "grad_norm": 3.871936798095703,
      "learning_rate": 3.337643869199945e-05,
      "loss": 2.8462,
      "step": 30030
    },
    {
      "epoch": 1.8242545697455517,
      "grad_norm": 3.197228193283081,
      "learning_rate": 3.334645401747774e-05,
      "loss": 2.5324,
      "step": 30040
    },
    {
      "epoch": 1.8248618449019252,
      "grad_norm": 4.116153717041016,
      "learning_rate": 3.331647607791525e-05,
      "loss": 2.3769,
      "step": 30050
    },
    {
      "epoch": 1.8254691200582984,
      "grad_norm": 3.637352228164673,
      "learning_rate": 3.328650488543554e-05,
      "loss": 2.5832,
      "step": 30060
    },
    {
      "epoch": 1.8260763952146717,
      "grad_norm": 4.543855667114258,
      "learning_rate": 3.3256540452159454e-05,
      "loss": 2.7935,
      "step": 30070
    },
    {
      "epoch": 1.826683670371045,
      "grad_norm": 4.197878837585449,
      "learning_rate": 3.322658279020506e-05,
      "loss": 3.0128,
      "step": 30080
    },
    {
      "epoch": 1.8272909455274184,
      "grad_norm": 3.673283576965332,
      "learning_rate": 3.319663191168772e-05,
      "loss": 2.6313,
      "step": 30090
    },
    {
      "epoch": 1.827898220683792,
      "grad_norm": 2.9314768314361572,
      "learning_rate": 3.316668782872007e-05,
      "loss": 2.76,
      "step": 30100
    },
    {
      "epoch": 1.8285054958401652,
      "grad_norm": 3.352224111557007,
      "learning_rate": 3.313675055341195e-05,
      "loss": 2.9236,
      "step": 30110
    },
    {
      "epoch": 1.8291127709965385,
      "grad_norm": 3.5995113849639893,
      "learning_rate": 3.3106820097870463e-05,
      "loss": 3.1096,
      "step": 30120
    },
    {
      "epoch": 1.8297200461529117,
      "grad_norm": 5.092935085296631,
      "learning_rate": 3.3076896474199995e-05,
      "loss": 2.7382,
      "step": 30130
    },
    {
      "epoch": 1.8303273213092852,
      "grad_norm": 3.4167444705963135,
      "learning_rate": 3.3046979694502113e-05,
      "loss": 2.4706,
      "step": 30140
    },
    {
      "epoch": 1.8309345964656587,
      "grad_norm": 4.99505090713501,
      "learning_rate": 3.301706977087564e-05,
      "loss": 2.5954,
      "step": 30150
    },
    {
      "epoch": 1.831541871622032,
      "grad_norm": 3.08146071434021,
      "learning_rate": 3.298716671541662e-05,
      "loss": 2.7878,
      "step": 30160
    },
    {
      "epoch": 1.8321491467784052,
      "grad_norm": 5.704779624938965,
      "learning_rate": 3.2957270540218344e-05,
      "loss": 3.0586,
      "step": 30170
    },
    {
      "epoch": 1.8327564219347785,
      "grad_norm": 7.455875396728516,
      "learning_rate": 3.2927381257371294e-05,
      "loss": 2.8981,
      "step": 30180
    },
    {
      "epoch": 1.833363697091152,
      "grad_norm": 5.361733436584473,
      "learning_rate": 3.2897498878963165e-05,
      "loss": 2.845,
      "step": 30190
    },
    {
      "epoch": 1.8339709722475255,
      "grad_norm": 6.656473159790039,
      "learning_rate": 3.286762341707886e-05,
      "loss": 2.6933,
      "step": 30200
    },
    {
      "epoch": 1.8345782474038987,
      "grad_norm": 4.46694803237915,
      "learning_rate": 3.2837754883800506e-05,
      "loss": 2.8817,
      "step": 30210
    },
    {
      "epoch": 1.835185522560272,
      "grad_norm": 4.002302646636963,
      "learning_rate": 3.280789329120742e-05,
      "loss": 2.9404,
      "step": 30220
    },
    {
      "epoch": 1.8357927977166453,
      "grad_norm": 4.660049915313721,
      "learning_rate": 3.2778038651376096e-05,
      "loss": 3.4621,
      "step": 30230
    },
    {
      "epoch": 1.8364000728730188,
      "grad_norm": 6.47898530960083,
      "learning_rate": 3.274819097638021e-05,
      "loss": 3.2787,
      "step": 30240
    },
    {
      "epoch": 1.8370073480293923,
      "grad_norm": 8.121679306030273,
      "learning_rate": 3.271835027829067e-05,
      "loss": 3.0771,
      "step": 30250
    },
    {
      "epoch": 1.8376146231857655,
      "grad_norm": 8.155464172363281,
      "learning_rate": 3.2688516569175506e-05,
      "loss": 3.049,
      "step": 30260
    },
    {
      "epoch": 1.8382218983421388,
      "grad_norm": 7.535719394683838,
      "learning_rate": 3.265868986109996e-05,
      "loss": 2.9219,
      "step": 30270
    },
    {
      "epoch": 1.838829173498512,
      "grad_norm": 5.734094142913818,
      "learning_rate": 3.262887016612639e-05,
      "loss": 2.3347,
      "step": 30280
    },
    {
      "epoch": 1.8394364486548855,
      "grad_norm": 6.770988941192627,
      "learning_rate": 3.25990574963144e-05,
      "loss": 2.9091,
      "step": 30290
    },
    {
      "epoch": 1.840043723811259,
      "grad_norm": 5.926705837249756,
      "learning_rate": 3.256925186372071e-05,
      "loss": 2.7285,
      "step": 30300
    },
    {
      "epoch": 1.8406509989676323,
      "grad_norm": 5.281815528869629,
      "learning_rate": 3.2539453280399165e-05,
      "loss": 2.8084,
      "step": 30310
    },
    {
      "epoch": 1.8412582741240056,
      "grad_norm": 6.4658708572387695,
      "learning_rate": 3.250966175840077e-05,
      "loss": 2.913,
      "step": 30320
    },
    {
      "epoch": 1.8418655492803788,
      "grad_norm": 8.554198265075684,
      "learning_rate": 3.2479877309773756e-05,
      "loss": 2.7419,
      "step": 30330
    },
    {
      "epoch": 1.8424728244367523,
      "grad_norm": 4.853321075439453,
      "learning_rate": 3.245009994656337e-05,
      "loss": 3.229,
      "step": 30340
    },
    {
      "epoch": 1.8430800995931258,
      "grad_norm": 5.391592502593994,
      "learning_rate": 3.242032968081207e-05,
      "loss": 3.3862,
      "step": 30350
    },
    {
      "epoch": 1.843687374749499,
      "grad_norm": 3.6856253147125244,
      "learning_rate": 3.239056652455943e-05,
      "loss": 2.8598,
      "step": 30360
    },
    {
      "epoch": 1.8442946499058723,
      "grad_norm": 4.676469326019287,
      "learning_rate": 3.2360810489842166e-05,
      "loss": 2.7247,
      "step": 30370
    },
    {
      "epoch": 1.8449019250622456,
      "grad_norm": 4.277629852294922,
      "learning_rate": 3.233106158869405e-05,
      "loss": 2.7169,
      "step": 30380
    },
    {
      "epoch": 1.845509200218619,
      "grad_norm": 5.1827569007873535,
      "learning_rate": 3.230131983314601e-05,
      "loss": 2.9874,
      "step": 30390
    },
    {
      "epoch": 1.8461164753749926,
      "grad_norm": 3.7904417514801025,
      "learning_rate": 3.227158523522614e-05,
      "loss": 2.9181,
      "step": 30400
    },
    {
      "epoch": 1.8467237505313658,
      "grad_norm": 3.2295961380004883,
      "learning_rate": 3.2241857806959544e-05,
      "loss": 2.3508,
      "step": 30410
    },
    {
      "epoch": 1.847331025687739,
      "grad_norm": 3.4774317741394043,
      "learning_rate": 3.221213756036847e-05,
      "loss": 2.3635,
      "step": 30420
    },
    {
      "epoch": 1.8479383008441124,
      "grad_norm": 2.636749029159546,
      "learning_rate": 3.218242450747225e-05,
      "loss": 2.2147,
      "step": 30430
    },
    {
      "epoch": 1.8485455760004859,
      "grad_norm": 3.4085867404937744,
      "learning_rate": 3.215271866028734e-05,
      "loss": 2.6574,
      "step": 30440
    },
    {
      "epoch": 1.8491528511568591,
      "grad_norm": 2.3655245304107666,
      "learning_rate": 3.212302003082725e-05,
      "loss": 2.2158,
      "step": 30450
    },
    {
      "epoch": 1.8497601263132326,
      "grad_norm": 5.169524192810059,
      "learning_rate": 3.209332863110257e-05,
      "loss": 2.9003,
      "step": 30460
    },
    {
      "epoch": 1.8503674014696059,
      "grad_norm": 3.5785906314849854,
      "learning_rate": 3.2063644473120976e-05,
      "loss": 3.0483,
      "step": 30470
    },
    {
      "epoch": 1.8509746766259791,
      "grad_norm": 4.278904438018799,
      "learning_rate": 3.2033967568887224e-05,
      "loss": 2.8811,
      "step": 30480
    },
    {
      "epoch": 1.8515819517823526,
      "grad_norm": 4.442675590515137,
      "learning_rate": 3.200429793040313e-05,
      "loss": 2.5785,
      "step": 30490
    },
    {
      "epoch": 1.852189226938726,
      "grad_norm": 2.8824405670166016,
      "learning_rate": 3.197463556966755e-05,
      "loss": 2.6642,
      "step": 30500
    },
    {
      "epoch": 1.8527965020950994,
      "grad_norm": 3.46451735496521,
      "learning_rate": 3.1944980498676416e-05,
      "loss": 2.4831,
      "step": 30510
    },
    {
      "epoch": 1.8534037772514727,
      "grad_norm": 3.0357022285461426,
      "learning_rate": 3.191533272942273e-05,
      "loss": 2.5849,
      "step": 30520
    },
    {
      "epoch": 1.854011052407846,
      "grad_norm": 5.520260334014893,
      "learning_rate": 3.1885692273896516e-05,
      "loss": 2.9054,
      "step": 30530
    },
    {
      "epoch": 1.8546183275642192,
      "grad_norm": 6.519630432128906,
      "learning_rate": 3.185605914408484e-05,
      "loss": 2.8626,
      "step": 30540
    },
    {
      "epoch": 1.8552256027205927,
      "grad_norm": 5.267477989196777,
      "learning_rate": 3.182643335197181e-05,
      "loss": 3.1732,
      "step": 30550
    },
    {
      "epoch": 1.8558328778769662,
      "grad_norm": 3.7975478172302246,
      "learning_rate": 3.1796814909538577e-05,
      "loss": 2.6358,
      "step": 30560
    },
    {
      "epoch": 1.8564401530333394,
      "grad_norm": 3.416346788406372,
      "learning_rate": 3.1767203828763305e-05,
      "loss": 2.3931,
      "step": 30570
    },
    {
      "epoch": 1.8570474281897127,
      "grad_norm": 3.0401766300201416,
      "learning_rate": 3.173760012162117e-05,
      "loss": 2.593,
      "step": 30580
    },
    {
      "epoch": 1.857654703346086,
      "grad_norm": 3.18760085105896,
      "learning_rate": 3.1708003800084415e-05,
      "loss": 2.3473,
      "step": 30590
    },
    {
      "epoch": 1.8582619785024594,
      "grad_norm": 3.9256091117858887,
      "learning_rate": 3.167841487612225e-05,
      "loss": 2.3707,
      "step": 30600
    },
    {
      "epoch": 1.858869253658833,
      "grad_norm": 3.6521670818328857,
      "learning_rate": 3.164883336170092e-05,
      "loss": 2.4194,
      "step": 30610
    },
    {
      "epoch": 1.8594765288152062,
      "grad_norm": 4.633058071136475,
      "learning_rate": 3.161925926878362e-05,
      "loss": 2.4826,
      "step": 30620
    },
    {
      "epoch": 1.8600838039715795,
      "grad_norm": 4.867528438568115,
      "learning_rate": 3.1589692609330625e-05,
      "loss": 2.6141,
      "step": 30630
    },
    {
      "epoch": 1.8606910791279527,
      "grad_norm": 3.8052711486816406,
      "learning_rate": 3.156013339529914e-05,
      "loss": 2.4818,
      "step": 30640
    },
    {
      "epoch": 1.8612983542843262,
      "grad_norm": 3.5755863189697266,
      "learning_rate": 3.1530581638643415e-05,
      "loss": 2.338,
      "step": 30650
    },
    {
      "epoch": 1.8619056294406997,
      "grad_norm": 3.8988635540008545,
      "learning_rate": 3.150103735131459e-05,
      "loss": 2.4848,
      "step": 30660
    },
    {
      "epoch": 1.862512904597073,
      "grad_norm": 4.2038164138793945,
      "learning_rate": 3.1471500545260904e-05,
      "loss": 2.9367,
      "step": 30670
    },
    {
      "epoch": 1.8631201797534462,
      "grad_norm": 4.837455749511719,
      "learning_rate": 3.1441971232427496e-05,
      "loss": 3.1084,
      "step": 30680
    },
    {
      "epoch": 1.8637274549098195,
      "grad_norm": 4.778217792510986,
      "learning_rate": 3.1412449424756474e-05,
      "loss": 2.7547,
      "step": 30690
    },
    {
      "epoch": 1.864334730066193,
      "grad_norm": 4.7167487144470215,
      "learning_rate": 3.138293513418692e-05,
      "loss": 2.6849,
      "step": 30700
    },
    {
      "epoch": 1.8649420052225665,
      "grad_norm": 4.554112911224365,
      "learning_rate": 3.135342837265494e-05,
      "loss": 2.8578,
      "step": 30710
    },
    {
      "epoch": 1.8655492803789397,
      "grad_norm": 5.817078590393066,
      "learning_rate": 3.132392915209347e-05,
      "loss": 2.4612,
      "step": 30720
    },
    {
      "epoch": 1.866156555535313,
      "grad_norm": 5.629991054534912,
      "learning_rate": 3.1294437484432506e-05,
      "loss": 3.1314,
      "step": 30730
    },
    {
      "epoch": 1.8667638306916863,
      "grad_norm": 4.061916828155518,
      "learning_rate": 3.126495338159892e-05,
      "loss": 3.1383,
      "step": 30740
    },
    {
      "epoch": 1.8673711058480598,
      "grad_norm": 4.450559139251709,
      "learning_rate": 3.123547685551658e-05,
      "loss": 3.0726,
      "step": 30750
    },
    {
      "epoch": 1.8679783810044333,
      "grad_norm": 5.569741249084473,
      "learning_rate": 3.1206007918106254e-05,
      "loss": 2.9764,
      "step": 30760
    },
    {
      "epoch": 1.8685856561608065,
      "grad_norm": 5.374441623687744,
      "learning_rate": 3.1176546581285645e-05,
      "loss": 2.8952,
      "step": 30770
    },
    {
      "epoch": 1.8691929313171798,
      "grad_norm": 5.745995998382568,
      "learning_rate": 3.114709285696937e-05,
      "loss": 2.8843,
      "step": 30780
    },
    {
      "epoch": 1.869800206473553,
      "grad_norm": 4.075222492218018,
      "learning_rate": 3.111764675706903e-05,
      "loss": 3.0639,
      "step": 30790
    },
    {
      "epoch": 1.8704074816299265,
      "grad_norm": 4.876399517059326,
      "learning_rate": 3.108820829349306e-05,
      "loss": 2.7185,
      "step": 30800
    },
    {
      "epoch": 1.8710147567863,
      "grad_norm": 4.890158653259277,
      "learning_rate": 3.105877747814684e-05,
      "loss": 2.7585,
      "step": 30810
    },
    {
      "epoch": 1.8716220319426733,
      "grad_norm": 4.2703118324279785,
      "learning_rate": 3.102935432293268e-05,
      "loss": 2.489,
      "step": 30820
    },
    {
      "epoch": 1.8722293070990466,
      "grad_norm": 4.169798374176025,
      "learning_rate": 3.099993883974978e-05,
      "loss": 2.4417,
      "step": 30830
    },
    {
      "epoch": 1.8728365822554198,
      "grad_norm": 5.064050197601318,
      "learning_rate": 3.09705310404942e-05,
      "loss": 3.1658,
      "step": 30840
    },
    {
      "epoch": 1.8734438574117933,
      "grad_norm": 5.575483798980713,
      "learning_rate": 3.094113093705894e-05,
      "loss": 2.885,
      "step": 30850
    },
    {
      "epoch": 1.8740511325681668,
      "grad_norm": 4.492085933685303,
      "learning_rate": 3.0911738541333866e-05,
      "loss": 2.6994,
      "step": 30860
    },
    {
      "epoch": 1.87465840772454,
      "grad_norm": 5.733485698699951,
      "learning_rate": 3.088235386520575e-05,
      "loss": 2.9987,
      "step": 30870
    },
    {
      "epoch": 1.8752656828809133,
      "grad_norm": 3.195108413696289,
      "learning_rate": 3.08529769205582e-05,
      "loss": 2.6677,
      "step": 30880
    },
    {
      "epoch": 1.8758729580372866,
      "grad_norm": 4.961599826812744,
      "learning_rate": 3.082360771927171e-05,
      "loss": 2.8551,
      "step": 30890
    },
    {
      "epoch": 1.87648023319366,
      "grad_norm": 4.449210166931152,
      "learning_rate": 3.079424627322368e-05,
      "loss": 2.9581,
      "step": 30900
    },
    {
      "epoch": 1.8770875083500334,
      "grad_norm": 3.7517073154449463,
      "learning_rate": 3.0764892594288344e-05,
      "loss": 2.5095,
      "step": 30910
    },
    {
      "epoch": 1.8776947835064068,
      "grad_norm": 4.944154262542725,
      "learning_rate": 3.073554669433678e-05,
      "loss": 2.6138,
      "step": 30920
    },
    {
      "epoch": 1.87830205866278,
      "grad_norm": 6.0787763595581055,
      "learning_rate": 3.070620858523694e-05,
      "loss": 2.7807,
      "step": 30930
    },
    {
      "epoch": 1.8789093338191534,
      "grad_norm": 3.7440171241760254,
      "learning_rate": 3.067687827885364e-05,
      "loss": 2.5091,
      "step": 30940
    },
    {
      "epoch": 1.8795166089755269,
      "grad_norm": 4.118960380554199,
      "learning_rate": 3.0647555787048505e-05,
      "loss": 2.4324,
      "step": 30950
    },
    {
      "epoch": 1.8801238841319001,
      "grad_norm": 2.5345895290374756,
      "learning_rate": 3.061824112168003e-05,
      "loss": 2.4464,
      "step": 30960
    },
    {
      "epoch": 1.8807311592882736,
      "grad_norm": 3.82553768157959,
      "learning_rate": 3.0588934294603515e-05,
      "loss": 2.745,
      "step": 30970
    },
    {
      "epoch": 1.8813384344446469,
      "grad_norm": 4.498579025268555,
      "learning_rate": 3.055963531767113e-05,
      "loss": 2.8959,
      "step": 30980
    },
    {
      "epoch": 1.8819457096010201,
      "grad_norm": 4.599263668060303,
      "learning_rate": 3.0530344202731856e-05,
      "loss": 2.9042,
      "step": 30990
    },
    {
      "epoch": 1.8825529847573934,
      "grad_norm": 2.59539532661438,
      "learning_rate": 3.0501060961631446e-05,
      "loss": 2.737,
      "step": 31000
    },
    {
      "epoch": 1.883160259913767,
      "grad_norm": 4.739022254943848,
      "learning_rate": 3.0471785606212516e-05,
      "loss": 2.3536,
      "step": 31010
    },
    {
      "epoch": 1.8837675350701404,
      "grad_norm": 3.9387216567993164,
      "learning_rate": 3.0442518148314532e-05,
      "loss": 2.6014,
      "step": 31020
    },
    {
      "epoch": 1.8843748102265137,
      "grad_norm": 4.757291793823242,
      "learning_rate": 3.0413258599773677e-05,
      "loss": 2.8119,
      "step": 31030
    },
    {
      "epoch": 1.884982085382887,
      "grad_norm": 4.997286796569824,
      "learning_rate": 3.0384006972422974e-05,
      "loss": 2.5681,
      "step": 31040
    },
    {
      "epoch": 1.8855893605392602,
      "grad_norm": 4.476750373840332,
      "learning_rate": 3.0354763278092292e-05,
      "loss": 3.0612,
      "step": 31050
    },
    {
      "epoch": 1.8861966356956337,
      "grad_norm": 5.163238525390625,
      "learning_rate": 3.03255275286082e-05,
      "loss": 2.8449,
      "step": 31060
    },
    {
      "epoch": 1.8868039108520072,
      "grad_norm": 6.362392425537109,
      "learning_rate": 3.029629973579413e-05,
      "loss": 2.9289,
      "step": 31070
    },
    {
      "epoch": 1.8874111860083804,
      "grad_norm": 4.5981645584106445,
      "learning_rate": 3.026707991147023e-05,
      "loss": 2.7992,
      "step": 31080
    },
    {
      "epoch": 1.8880184611647537,
      "grad_norm": 5.271345615386963,
      "learning_rate": 3.0237868067453523e-05,
      "loss": 2.6215,
      "step": 31090
    },
    {
      "epoch": 1.888625736321127,
      "grad_norm": 7.034597396850586,
      "learning_rate": 3.020866421555769e-05,
      "loss": 2.8894,
      "step": 31100
    },
    {
      "epoch": 1.8892330114775004,
      "grad_norm": 7.1208295822143555,
      "learning_rate": 3.017946836759326e-05,
      "loss": 2.92,
      "step": 31110
    },
    {
      "epoch": 1.889840286633874,
      "grad_norm": 4.238949298858643,
      "learning_rate": 3.015028053536748e-05,
      "loss": 2.7753,
      "step": 31120
    },
    {
      "epoch": 1.8904475617902472,
      "grad_norm": 4.563117980957031,
      "learning_rate": 3.01211007306844e-05,
      "loss": 2.8257,
      "step": 31130
    },
    {
      "epoch": 1.8910548369466205,
      "grad_norm": 3.505448579788208,
      "learning_rate": 3.0091928965344784e-05,
      "loss": 2.6809,
      "step": 31140
    },
    {
      "epoch": 1.8916621121029937,
      "grad_norm": 6.623416900634766,
      "learning_rate": 3.0062765251146154e-05,
      "loss": 2.5623,
      "step": 31150
    },
    {
      "epoch": 1.8922693872593672,
      "grad_norm": 5.193199157714844,
      "learning_rate": 3.003360959988277e-05,
      "loss": 2.7318,
      "step": 31160
    },
    {
      "epoch": 1.8928766624157407,
      "grad_norm": 2.976412773132324,
      "learning_rate": 3.0004462023345675e-05,
      "loss": 2.2233,
      "step": 31170
    },
    {
      "epoch": 1.893483937572114,
      "grad_norm": 4.19972562789917,
      "learning_rate": 2.997532253332259e-05,
      "loss": 2.5601,
      "step": 31180
    },
    {
      "epoch": 1.8940912127284872,
      "grad_norm": 4.415539741516113,
      "learning_rate": 2.9946191141598e-05,
      "loss": 2.8905,
      "step": 31190
    },
    {
      "epoch": 1.8946984878848605,
      "grad_norm": 5.09467887878418,
      "learning_rate": 2.9917067859953085e-05,
      "loss": 2.8425,
      "step": 31200
    },
    {
      "epoch": 1.895305763041234,
      "grad_norm": 4.390669345855713,
      "learning_rate": 2.9887952700165777e-05,
      "loss": 2.655,
      "step": 31210
    },
    {
      "epoch": 1.8959130381976075,
      "grad_norm": 4.146218776702881,
      "learning_rate": 2.9858845674010722e-05,
      "loss": 2.7935,
      "step": 31220
    },
    {
      "epoch": 1.8965203133539807,
      "grad_norm": 3.1910998821258545,
      "learning_rate": 2.982974679325925e-05,
      "loss": 2.8292,
      "step": 31230
    },
    {
      "epoch": 1.897127588510354,
      "grad_norm": 3.564682722091675,
      "learning_rate": 2.98006560696794e-05,
      "loss": 2.8534,
      "step": 31240
    },
    {
      "epoch": 1.8977348636667273,
      "grad_norm": 3.9953107833862305,
      "learning_rate": 2.9771573515035945e-05,
      "loss": 2.6707,
      "step": 31250
    },
    {
      "epoch": 1.8983421388231008,
      "grad_norm": 4.568257808685303,
      "learning_rate": 2.9742499141090335e-05,
      "loss": 2.5328,
      "step": 31260
    },
    {
      "epoch": 1.8989494139794743,
      "grad_norm": 3.7825353145599365,
      "learning_rate": 2.9713432959600685e-05,
      "loss": 2.5659,
      "step": 31270
    },
    {
      "epoch": 1.8995566891358475,
      "grad_norm": 5.277776718139648,
      "learning_rate": 2.968437498232185e-05,
      "loss": 2.765,
      "step": 31280
    },
    {
      "epoch": 1.9001639642922208,
      "grad_norm": 4.458937644958496,
      "learning_rate": 2.965532522100533e-05,
      "loss": 2.7147,
      "step": 31290
    },
    {
      "epoch": 1.900771239448594,
      "grad_norm": 4.929209232330322,
      "learning_rate": 2.9626283687399314e-05,
      "loss": 3.1742,
      "step": 31300
    },
    {
      "epoch": 1.9013785146049675,
      "grad_norm": 6.253803253173828,
      "learning_rate": 2.959725039324863e-05,
      "loss": 2.9506,
      "step": 31310
    },
    {
      "epoch": 1.901985789761341,
      "grad_norm": 5.133046627044678,
      "learning_rate": 2.9568225350294844e-05,
      "loss": 3.035,
      "step": 31320
    },
    {
      "epoch": 1.9025930649177143,
      "grad_norm": 5.800815582275391,
      "learning_rate": 2.9539208570276145e-05,
      "loss": 2.9553,
      "step": 31330
    },
    {
      "epoch": 1.9032003400740876,
      "grad_norm": 3.8433101177215576,
      "learning_rate": 2.951020006492735e-05,
      "loss": 2.9736,
      "step": 31340
    },
    {
      "epoch": 1.9038076152304608,
      "grad_norm": 4.23875617980957,
      "learning_rate": 2.948119984597997e-05,
      "loss": 3.0291,
      "step": 31350
    },
    {
      "epoch": 1.9044148903868343,
      "grad_norm": 2.6833341121673584,
      "learning_rate": 2.9452207925162177e-05,
      "loss": 2.8142,
      "step": 31360
    },
    {
      "epoch": 1.9050221655432076,
      "grad_norm": 3.931788444519043,
      "learning_rate": 2.9423224314198756e-05,
      "loss": 2.3647,
      "step": 31370
    },
    {
      "epoch": 1.905629440699581,
      "grad_norm": 4.355130195617676,
      "learning_rate": 2.9394249024811128e-05,
      "loss": 2.6063,
      "step": 31380
    },
    {
      "epoch": 1.9062367158559543,
      "grad_norm": 4.004887104034424,
      "learning_rate": 2.9365282068717353e-05,
      "loss": 2.5009,
      "step": 31390
    },
    {
      "epoch": 1.9068439910123276,
      "grad_norm": 2.99853777885437,
      "learning_rate": 2.9336323457632174e-05,
      "loss": 2.4583,
      "step": 31400
    },
    {
      "epoch": 1.907451266168701,
      "grad_norm": 3.809448003768921,
      "learning_rate": 2.9307373203266873e-05,
      "loss": 2.3129,
      "step": 31410
    },
    {
      "epoch": 1.9080585413250744,
      "grad_norm": 3.2395131587982178,
      "learning_rate": 2.927843131732942e-05,
      "loss": 2.6872,
      "step": 31420
    },
    {
      "epoch": 1.9086658164814478,
      "grad_norm": 4.187257766723633,
      "learning_rate": 2.924949781152434e-05,
      "loss": 2.8029,
      "step": 31430
    },
    {
      "epoch": 1.909273091637821,
      "grad_norm": 3.343498468399048,
      "learning_rate": 2.9220572697552838e-05,
      "loss": 2.819,
      "step": 31440
    },
    {
      "epoch": 1.9098803667941944,
      "grad_norm": 3.804870367050171,
      "learning_rate": 2.919165598711269e-05,
      "loss": 2.5446,
      "step": 31450
    },
    {
      "epoch": 1.9104876419505676,
      "grad_norm": 4.08945369720459,
      "learning_rate": 2.9162747691898253e-05,
      "loss": 2.5002,
      "step": 31460
    },
    {
      "epoch": 1.9110949171069411,
      "grad_norm": 3.671698808670044,
      "learning_rate": 2.913384782360049e-05,
      "loss": 2.5809,
      "step": 31470
    },
    {
      "epoch": 1.9117021922633146,
      "grad_norm": 3.646005868911743,
      "learning_rate": 2.9104956393907014e-05,
      "loss": 2.6961,
      "step": 31480
    },
    {
      "epoch": 1.9123094674196879,
      "grad_norm": 3.678928852081299,
      "learning_rate": 2.907607341450197e-05,
      "loss": 2.6599,
      "step": 31490
    },
    {
      "epoch": 1.9129167425760611,
      "grad_norm": 2.9153997898101807,
      "learning_rate": 2.904719889706604e-05,
      "loss": 2.5495,
      "step": 31500
    },
    {
      "epoch": 1.9135240177324344,
      "grad_norm": 2.7865073680877686,
      "learning_rate": 2.9018332853276607e-05,
      "loss": 2.4894,
      "step": 31510
    },
    {
      "epoch": 1.914131292888808,
      "grad_norm": 4.603034019470215,
      "learning_rate": 2.8989475294807523e-05,
      "loss": 3.0249,
      "step": 31520
    },
    {
      "epoch": 1.9147385680451814,
      "grad_norm": 3.7879087924957275,
      "learning_rate": 2.896062623332926e-05,
      "loss": 3.0139,
      "step": 31530
    },
    {
      "epoch": 1.9153458432015547,
      "grad_norm": 3.6453351974487305,
      "learning_rate": 2.8931785680508806e-05,
      "loss": 2.6087,
      "step": 31540
    },
    {
      "epoch": 1.915953118357928,
      "grad_norm": 3.940544605255127,
      "learning_rate": 2.89029536480098e-05,
      "loss": 3.0119,
      "step": 31550
    },
    {
      "epoch": 1.9165603935143012,
      "grad_norm": 3.5910286903381348,
      "learning_rate": 2.8874130147492316e-05,
      "loss": 3.2281,
      "step": 31560
    },
    {
      "epoch": 1.9171676686706747,
      "grad_norm": 4.229496955871582,
      "learning_rate": 2.8845315190613066e-05,
      "loss": 2.7497,
      "step": 31570
    },
    {
      "epoch": 1.9177749438270482,
      "grad_norm": 4.1679253578186035,
      "learning_rate": 2.8816508789025247e-05,
      "loss": 2.7536,
      "step": 31580
    },
    {
      "epoch": 1.9183822189834214,
      "grad_norm": 4.030735969543457,
      "learning_rate": 2.8787710954378676e-05,
      "loss": 2.7374,
      "step": 31590
    },
    {
      "epoch": 1.9189894941397947,
      "grad_norm": 4.385236740112305,
      "learning_rate": 2.8758921698319624e-05,
      "loss": 2.7872,
      "step": 31600
    },
    {
      "epoch": 1.919596769296168,
      "grad_norm": 6.4368205070495605,
      "learning_rate": 2.873014103249093e-05,
      "loss": 2.4775,
      "step": 31610
    },
    {
      "epoch": 1.9202040444525414,
      "grad_norm": 5.3680806159973145,
      "learning_rate": 2.870136896853196e-05,
      "loss": 2.8802,
      "step": 31620
    },
    {
      "epoch": 1.920811319608915,
      "grad_norm": 5.924291610717773,
      "learning_rate": 2.8672605518078592e-05,
      "loss": 2.9177,
      "step": 31630
    },
    {
      "epoch": 1.9214185947652882,
      "grad_norm": 6.167788028717041,
      "learning_rate": 2.8643850692763218e-05,
      "loss": 2.8154,
      "step": 31640
    },
    {
      "epoch": 1.9220258699216615,
      "grad_norm": 8.117122650146484,
      "learning_rate": 2.861510450421475e-05,
      "loss": 2.7689,
      "step": 31650
    },
    {
      "epoch": 1.9226331450780347,
      "grad_norm": 7.326465129852295,
      "learning_rate": 2.8586366964058587e-05,
      "loss": 2.7186,
      "step": 31660
    },
    {
      "epoch": 1.9232404202344082,
      "grad_norm": 5.971794605255127,
      "learning_rate": 2.8557638083916682e-05,
      "loss": 3.1969,
      "step": 31670
    },
    {
      "epoch": 1.9238476953907817,
      "grad_norm": 3.720010995864868,
      "learning_rate": 2.8528917875407433e-05,
      "loss": 3.1189,
      "step": 31680
    },
    {
      "epoch": 1.924454970547155,
      "grad_norm": 6.064222812652588,
      "learning_rate": 2.850020635014576e-05,
      "loss": 3.2663,
      "step": 31690
    },
    {
      "epoch": 1.9250622457035282,
      "grad_norm": 5.496344566345215,
      "learning_rate": 2.8471503519743047e-05,
      "loss": 2.7555,
      "step": 31700
    },
    {
      "epoch": 1.9256695208599015,
      "grad_norm": 4.56443452835083,
      "learning_rate": 2.844280939580719e-05,
      "loss": 2.693,
      "step": 31710
    },
    {
      "epoch": 1.926276796016275,
      "grad_norm": 3.9801931381225586,
      "learning_rate": 2.8414123989942533e-05,
      "loss": 2.8868,
      "step": 31720
    },
    {
      "epoch": 1.9268840711726485,
      "grad_norm": 3.9124057292938232,
      "learning_rate": 2.8385447313749904e-05,
      "loss": 2.5717,
      "step": 31730
    },
    {
      "epoch": 1.9274913463290217,
      "grad_norm": 3.827208995819092,
      "learning_rate": 2.835677937882664e-05,
      "loss": 3.0211,
      "step": 31740
    },
    {
      "epoch": 1.928098621485395,
      "grad_norm": 6.041252613067627,
      "learning_rate": 2.8328120196766494e-05,
      "loss": 3.1326,
      "step": 31750
    },
    {
      "epoch": 1.9287058966417683,
      "grad_norm": 3.3382604122161865,
      "learning_rate": 2.8299469779159703e-05,
      "loss": 2.9039,
      "step": 31760
    },
    {
      "epoch": 1.9293131717981418,
      "grad_norm": 4.6285858154296875,
      "learning_rate": 2.827082813759294e-05,
      "loss": 3.1015,
      "step": 31770
    },
    {
      "epoch": 1.9299204469545153,
      "grad_norm": 4.63464879989624,
      "learning_rate": 2.8242195283649343e-05,
      "loss": 2.9437,
      "step": 31780
    },
    {
      "epoch": 1.9305277221108885,
      "grad_norm": 5.000597953796387,
      "learning_rate": 2.8213571228908497e-05,
      "loss": 2.752,
      "step": 31790
    },
    {
      "epoch": 1.9311349972672618,
      "grad_norm": 3.167459487915039,
      "learning_rate": 2.8184955984946427e-05,
      "loss": 2.7809,
      "step": 31800
    },
    {
      "epoch": 1.931742272423635,
      "grad_norm": 3.965614080429077,
      "learning_rate": 2.8156349563335573e-05,
      "loss": 2.6841,
      "step": 31810
    },
    {
      "epoch": 1.9323495475800085,
      "grad_norm": 3.327393054962158,
      "learning_rate": 2.8127751975644866e-05,
      "loss": 2.7524,
      "step": 31820
    },
    {
      "epoch": 1.9329568227363818,
      "grad_norm": 3.0651586055755615,
      "learning_rate": 2.809916323343963e-05,
      "loss": 2.7316,
      "step": 31830
    },
    {
      "epoch": 1.9335640978927553,
      "grad_norm": 3.613797664642334,
      "learning_rate": 2.8070583348281552e-05,
      "loss": 2.6955,
      "step": 31840
    },
    {
      "epoch": 1.9341713730491286,
      "grad_norm": 4.367508888244629,
      "learning_rate": 2.804201233172882e-05,
      "loss": 3.0549,
      "step": 31850
    },
    {
      "epoch": 1.9347786482055018,
      "grad_norm": 4.216620445251465,
      "learning_rate": 2.801345019533602e-05,
      "loss": 3.0167,
      "step": 31860
    },
    {
      "epoch": 1.9353859233618753,
      "grad_norm": 4.142899036407471,
      "learning_rate": 2.7984896950654133e-05,
      "loss": 3.0214,
      "step": 31870
    },
    {
      "epoch": 1.9359931985182486,
      "grad_norm": 5.22513484954834,
      "learning_rate": 2.7956352609230542e-05,
      "loss": 2.7651,
      "step": 31880
    },
    {
      "epoch": 1.936600473674622,
      "grad_norm": 4.73373556137085,
      "learning_rate": 2.7927817182609008e-05,
      "loss": 2.7662,
      "step": 31890
    },
    {
      "epoch": 1.9372077488309953,
      "grad_norm": 7.078933238983154,
      "learning_rate": 2.7899290682329778e-05,
      "loss": 2.7237,
      "step": 31900
    },
    {
      "epoch": 1.9378150239873686,
      "grad_norm": 4.484838485717773,
      "learning_rate": 2.7870773119929362e-05,
      "loss": 3.4549,
      "step": 31910
    },
    {
      "epoch": 1.9384222991437419,
      "grad_norm": 5.022948265075684,
      "learning_rate": 2.784226450694073e-05,
      "loss": 2.9981,
      "step": 31920
    },
    {
      "epoch": 1.9390295743001154,
      "grad_norm": 5.254807472229004,
      "learning_rate": 2.781376485489321e-05,
      "loss": 2.9563,
      "step": 31930
    },
    {
      "epoch": 1.9396368494564888,
      "grad_norm": 6.996189117431641,
      "learning_rate": 2.778527417531255e-05,
      "loss": 2.9609,
      "step": 31940
    },
    {
      "epoch": 1.940244124612862,
      "grad_norm": 6.054628372192383,
      "learning_rate": 2.7756792479720806e-05,
      "loss": 2.8784,
      "step": 31950
    },
    {
      "epoch": 1.9408513997692354,
      "grad_norm": 5.196822166442871,
      "learning_rate": 2.7728319779636445e-05,
      "loss": 2.6637,
      "step": 31960
    },
    {
      "epoch": 1.9414586749256086,
      "grad_norm": 3.1453664302825928,
      "learning_rate": 2.769985608657427e-05,
      "loss": 2.4518,
      "step": 31970
    },
    {
      "epoch": 1.9420659500819821,
      "grad_norm": 3.6761112213134766,
      "learning_rate": 2.7671401412045456e-05,
      "loss": 2.4329,
      "step": 31980
    },
    {
      "epoch": 1.9426732252383556,
      "grad_norm": 3.375551462173462,
      "learning_rate": 2.7642955767557523e-05,
      "loss": 2.4814,
      "step": 31990
    },
    {
      "epoch": 1.9432805003947289,
      "grad_norm": 3.834174633026123,
      "learning_rate": 2.7614519164614328e-05,
      "loss": 2.2169,
      "step": 32000
    },
    {
      "epoch": 1.9438877755511021,
      "grad_norm": 4.599134922027588,
      "learning_rate": 2.758609161471612e-05,
      "loss": 2.4596,
      "step": 32010
    },
    {
      "epoch": 1.9444950507074754,
      "grad_norm": 5.054767608642578,
      "learning_rate": 2.7557673129359436e-05,
      "loss": 2.4051,
      "step": 32020
    },
    {
      "epoch": 1.945102325863849,
      "grad_norm": 3.5896193981170654,
      "learning_rate": 2.752926372003717e-05,
      "loss": 2.8867,
      "step": 32030
    },
    {
      "epoch": 1.9457096010202224,
      "grad_norm": 4.329637050628662,
      "learning_rate": 2.7500863398238525e-05,
      "loss": 2.9766,
      "step": 32040
    },
    {
      "epoch": 1.9463168761765957,
      "grad_norm": 4.789530277252197,
      "learning_rate": 2.7472472175449053e-05,
      "loss": 2.4732,
      "step": 32050
    },
    {
      "epoch": 1.946924151332969,
      "grad_norm": 4.9626030921936035,
      "learning_rate": 2.744409006315062e-05,
      "loss": 2.6223,
      "step": 32060
    },
    {
      "epoch": 1.9475314264893422,
      "grad_norm": 4.410284519195557,
      "learning_rate": 2.74157170728214e-05,
      "loss": 2.9682,
      "step": 32070
    },
    {
      "epoch": 1.9481387016457157,
      "grad_norm": 3.5237369537353516,
      "learning_rate": 2.7387353215935857e-05,
      "loss": 2.7463,
      "step": 32080
    },
    {
      "epoch": 1.9487459768020892,
      "grad_norm": 9.022725105285645,
      "learning_rate": 2.735899850396484e-05,
      "loss": 2.6346,
      "step": 32090
    },
    {
      "epoch": 1.9493532519584624,
      "grad_norm": 7.544192314147949,
      "learning_rate": 2.7330652948375406e-05,
      "loss": 2.8588,
      "step": 32100
    },
    {
      "epoch": 1.9499605271148357,
      "grad_norm": 6.646838188171387,
      "learning_rate": 2.730231656063099e-05,
      "loss": 2.7378,
      "step": 32110
    },
    {
      "epoch": 1.950567802271209,
      "grad_norm": 5.296668529510498,
      "learning_rate": 2.7273989352191204e-05,
      "loss": 2.7934,
      "step": 32120
    },
    {
      "epoch": 1.9511750774275824,
      "grad_norm": 7.100170612335205,
      "learning_rate": 2.724567133451209e-05,
      "loss": 3.089,
      "step": 32130
    },
    {
      "epoch": 1.951782352583956,
      "grad_norm": 7.597975730895996,
      "learning_rate": 2.7217362519045887e-05,
      "loss": 3.0446,
      "step": 32140
    },
    {
      "epoch": 1.9523896277403292,
      "grad_norm": 6.614585876464844,
      "learning_rate": 2.7189062917241116e-05,
      "loss": 3.0415,
      "step": 32150
    },
    {
      "epoch": 1.9529969028967025,
      "grad_norm": 5.868378639221191,
      "learning_rate": 2.716077254054259e-05,
      "loss": 3.0372,
      "step": 32160
    },
    {
      "epoch": 1.9536041780530757,
      "grad_norm": 5.255078315734863,
      "learning_rate": 2.7132491400391404e-05,
      "loss": 3.0866,
      "step": 32170
    },
    {
      "epoch": 1.9542114532094492,
      "grad_norm": 4.766270160675049,
      "learning_rate": 2.710421950822492e-05,
      "loss": 2.4622,
      "step": 32180
    },
    {
      "epoch": 1.9548187283658227,
      "grad_norm": 4.464132308959961,
      "learning_rate": 2.7075956875476672e-05,
      "loss": 2.6848,
      "step": 32190
    },
    {
      "epoch": 1.955426003522196,
      "grad_norm": 4.495061874389648,
      "learning_rate": 2.7047703513576584e-05,
      "loss": 2.779,
      "step": 32200
    },
    {
      "epoch": 1.9560332786785692,
      "grad_norm": 4.439495086669922,
      "learning_rate": 2.7019459433950733e-05,
      "loss": 3.0757,
      "step": 32210
    },
    {
      "epoch": 1.9566405538349425,
      "grad_norm": 6.841809272766113,
      "learning_rate": 2.6991224648021495e-05,
      "loss": 3.1083,
      "step": 32220
    },
    {
      "epoch": 1.957247828991316,
      "grad_norm": 6.387080669403076,
      "learning_rate": 2.696299916720743e-05,
      "loss": 2.6392,
      "step": 32230
    },
    {
      "epoch": 1.9578551041476895,
      "grad_norm": 5.038681507110596,
      "learning_rate": 2.693478300292344e-05,
      "loss": 2.8423,
      "step": 32240
    },
    {
      "epoch": 1.9584623793040628,
      "grad_norm": 5.632120132446289,
      "learning_rate": 2.6906576166580534e-05,
      "loss": 2.9573,
      "step": 32250
    },
    {
      "epoch": 1.959069654460436,
      "grad_norm": 2.9539670944213867,
      "learning_rate": 2.6878378669586013e-05,
      "loss": 2.6555,
      "step": 32260
    },
    {
      "epoch": 1.9596769296168093,
      "grad_norm": 4.011358737945557,
      "learning_rate": 2.6850190523343388e-05,
      "loss": 2.6552,
      "step": 32270
    },
    {
      "epoch": 1.9602842047731828,
      "grad_norm": 4.42551851272583,
      "learning_rate": 2.6822011739252428e-05,
      "loss": 2.6289,
      "step": 32280
    },
    {
      "epoch": 1.960891479929556,
      "grad_norm": 3.1826913356781006,
      "learning_rate": 2.679384232870906e-05,
      "loss": 2.6979,
      "step": 32290
    },
    {
      "epoch": 1.9614987550859295,
      "grad_norm": 4.577463150024414,
      "learning_rate": 2.6765682303105442e-05,
      "loss": 2.6633,
      "step": 32300
    },
    {
      "epoch": 1.9621060302423028,
      "grad_norm": 2.9815526008605957,
      "learning_rate": 2.6737531673829954e-05,
      "loss": 2.4767,
      "step": 32310
    },
    {
      "epoch": 1.962713305398676,
      "grad_norm": 2.7464656829833984,
      "learning_rate": 2.670939045226713e-05,
      "loss": 2.8526,
      "step": 32320
    },
    {
      "epoch": 1.9633205805550495,
      "grad_norm": 3.2134830951690674,
      "learning_rate": 2.6681258649797748e-05,
      "loss": 3.0052,
      "step": 32330
    },
    {
      "epoch": 1.9639278557114228,
      "grad_norm": 3.476374864578247,
      "learning_rate": 2.6653136277798752e-05,
      "loss": 2.6447,
      "step": 32340
    },
    {
      "epoch": 1.9645351308677963,
      "grad_norm": 4.135629177093506,
      "learning_rate": 2.6625023347643253e-05,
      "loss": 3.0894,
      "step": 32350
    },
    {
      "epoch": 1.9651424060241696,
      "grad_norm": 3.7765328884124756,
      "learning_rate": 2.659691987070061e-05,
      "loss": 2.6296,
      "step": 32360
    },
    {
      "epoch": 1.9657496811805428,
      "grad_norm": 2.8356850147247314,
      "learning_rate": 2.6568825858336304e-05,
      "loss": 2.5192,
      "step": 32370
    },
    {
      "epoch": 1.966356956336916,
      "grad_norm": 3.8458805084228516,
      "learning_rate": 2.6540741321911995e-05,
      "loss": 2.4646,
      "step": 32380
    },
    {
      "epoch": 1.9669642314932896,
      "grad_norm": 2.8645622730255127,
      "learning_rate": 2.6512666272785512e-05,
      "loss": 2.4466,
      "step": 32390
    },
    {
      "epoch": 1.967571506649663,
      "grad_norm": 5.0589070320129395,
      "learning_rate": 2.648460072231086e-05,
      "loss": 2.6764,
      "step": 32400
    },
    {
      "epoch": 1.9681787818060363,
      "grad_norm": 5.496134281158447,
      "learning_rate": 2.6456544681838192e-05,
      "loss": 2.8856,
      "step": 32410
    },
    {
      "epoch": 1.9687860569624096,
      "grad_norm": 6.025566577911377,
      "learning_rate": 2.64284981627138e-05,
      "loss": 2.7894,
      "step": 32420
    },
    {
      "epoch": 1.9693933321187829,
      "grad_norm": 6.131012439727783,
      "learning_rate": 2.640046117628018e-05,
      "loss": 2.853,
      "step": 32430
    },
    {
      "epoch": 1.9700006072751564,
      "grad_norm": 8.52871036529541,
      "learning_rate": 2.637243373387591e-05,
      "loss": 2.8365,
      "step": 32440
    },
    {
      "epoch": 1.9706078824315298,
      "grad_norm": 7.962235927581787,
      "learning_rate": 2.634441584683574e-05,
      "loss": 2.8842,
      "step": 32450
    },
    {
      "epoch": 1.971215157587903,
      "grad_norm": 6.246546745300293,
      "learning_rate": 2.6316407526490554e-05,
      "loss": 2.9322,
      "step": 32460
    },
    {
      "epoch": 1.9718224327442764,
      "grad_norm": 5.145756721496582,
      "learning_rate": 2.6288408784167357e-05,
      "loss": 2.7127,
      "step": 32470
    },
    {
      "epoch": 1.9724297079006496,
      "grad_norm": 3.0537109375,
      "learning_rate": 2.6260419631189293e-05,
      "loss": 2.6172,
      "step": 32480
    },
    {
      "epoch": 1.9730369830570231,
      "grad_norm": 3.8714544773101807,
      "learning_rate": 2.623244007887561e-05,
      "loss": 2.6878,
      "step": 32490
    },
    {
      "epoch": 1.9736442582133966,
      "grad_norm": 2.739659070968628,
      "learning_rate": 2.620447013854167e-05,
      "loss": 2.6192,
      "step": 32500
    },
    {
      "epoch": 1.9742515333697699,
      "grad_norm": 5.997098922729492,
      "learning_rate": 2.6176509821499007e-05,
      "loss": 2.9206,
      "step": 32510
    },
    {
      "epoch": 1.9748588085261432,
      "grad_norm": 5.073155403137207,
      "learning_rate": 2.614855913905521e-05,
      "loss": 2.7862,
      "step": 32520
    },
    {
      "epoch": 1.9754660836825164,
      "grad_norm": 5.090256690979004,
      "learning_rate": 2.612061810251395e-05,
      "loss": 2.8633,
      "step": 32530
    },
    {
      "epoch": 1.97607335883889,
      "grad_norm": 4.287500858306885,
      "learning_rate": 2.609268672317502e-05,
      "loss": 2.971,
      "step": 32540
    },
    {
      "epoch": 1.9766806339952634,
      "grad_norm": 4.704253196716309,
      "learning_rate": 2.6064765012334357e-05,
      "loss": 3.0689,
      "step": 32550
    },
    {
      "epoch": 1.9772879091516367,
      "grad_norm": 4.490828037261963,
      "learning_rate": 2.6036852981283922e-05,
      "loss": 3.0927,
      "step": 32560
    },
    {
      "epoch": 1.97789518430801,
      "grad_norm": 6.557298183441162,
      "learning_rate": 2.600895064131179e-05,
      "loss": 3.0799,
      "step": 32570
    },
    {
      "epoch": 1.9785024594643832,
      "grad_norm": 5.932257175445557,
      "learning_rate": 2.598105800370211e-05,
      "loss": 3.1948,
      "step": 32580
    },
    {
      "epoch": 1.9791097346207567,
      "grad_norm": 6.20795202255249,
      "learning_rate": 2.595317507973511e-05,
      "loss": 2.8984,
      "step": 32590
    },
    {
      "epoch": 1.9797170097771302,
      "grad_norm": 5.968320369720459,
      "learning_rate": 2.5925301880687088e-05,
      "loss": 2.9876,
      "step": 32600
    },
    {
      "epoch": 1.9803242849335034,
      "grad_norm": 5.509295463562012,
      "learning_rate": 2.5897438417830407e-05,
      "loss": 2.4869,
      "step": 32610
    },
    {
      "epoch": 1.9809315600898767,
      "grad_norm": 4.933558464050293,
      "learning_rate": 2.5869584702433476e-05,
      "loss": 2.585,
      "step": 32620
    },
    {
      "epoch": 1.98153883524625,
      "grad_norm": 4.256415367126465,
      "learning_rate": 2.5841740745760824e-05,
      "loss": 2.9602,
      "step": 32630
    },
    {
      "epoch": 1.9821461104026235,
      "grad_norm": 3.2448925971984863,
      "learning_rate": 2.5813906559072963e-05,
      "loss": 2.9309,
      "step": 32640
    },
    {
      "epoch": 1.982753385558997,
      "grad_norm": 4.040432929992676,
      "learning_rate": 2.578608215362649e-05,
      "loss": 2.6966,
      "step": 32650
    },
    {
      "epoch": 1.9833606607153702,
      "grad_norm": 3.6344430446624756,
      "learning_rate": 2.5758267540674042e-05,
      "loss": 2.7302,
      "step": 32660
    },
    {
      "epoch": 1.9839679358717435,
      "grad_norm": 2.925574541091919,
      "learning_rate": 2.5730462731464273e-05,
      "loss": 2.3953,
      "step": 32670
    },
    {
      "epoch": 1.9845752110281167,
      "grad_norm": 3.6267874240875244,
      "learning_rate": 2.5702667737241902e-05,
      "loss": 3.1173,
      "step": 32680
    },
    {
      "epoch": 1.9851824861844902,
      "grad_norm": 3.1984024047851562,
      "learning_rate": 2.5674882569247653e-05,
      "loss": 2.9561,
      "step": 32690
    },
    {
      "epoch": 1.9857897613408635,
      "grad_norm": 3.578991174697876,
      "learning_rate": 2.5647107238718326e-05,
      "loss": 2.4361,
      "step": 32700
    },
    {
      "epoch": 1.986397036497237,
      "grad_norm": 4.402180194854736,
      "learning_rate": 2.5619341756886682e-05,
      "loss": 2.2091,
      "step": 32710
    },
    {
      "epoch": 1.9870043116536102,
      "grad_norm": 4.9030842781066895,
      "learning_rate": 2.5591586134981526e-05,
      "loss": 2.6225,
      "step": 32720
    },
    {
      "epoch": 1.9876115868099835,
      "grad_norm": 5.270265579223633,
      "learning_rate": 2.5563840384227677e-05,
      "loss": 2.8619,
      "step": 32730
    },
    {
      "epoch": 1.988218861966357,
      "grad_norm": 6.129726409912109,
      "learning_rate": 2.553610451584596e-05,
      "loss": 2.9704,
      "step": 32740
    },
    {
      "epoch": 1.9888261371227303,
      "grad_norm": 4.284210205078125,
      "learning_rate": 2.5508378541053196e-05,
      "loss": 2.7324,
      "step": 32750
    },
    {
      "epoch": 1.9894334122791038,
      "grad_norm": 6.354742050170898,
      "learning_rate": 2.5480662471062213e-05,
      "loss": 2.7939,
      "step": 32760
    },
    {
      "epoch": 1.990040687435477,
      "grad_norm": 3.071451425552368,
      "learning_rate": 2.545295631708181e-05,
      "loss": 2.3896,
      "step": 32770
    },
    {
      "epoch": 1.9906479625918503,
      "grad_norm": 3.938683032989502,
      "learning_rate": 2.542526009031684e-05,
      "loss": 2.4841,
      "step": 32780
    },
    {
      "epoch": 1.9912552377482238,
      "grad_norm": 3.8939433097839355,
      "learning_rate": 2.539757380196808e-05,
      "loss": 2.7133,
      "step": 32790
    },
    {
      "epoch": 1.991862512904597,
      "grad_norm": 8.138978958129883,
      "learning_rate": 2.5369897463232318e-05,
      "loss": 2.5925,
      "step": 32800
    },
    {
      "epoch": 1.9924697880609705,
      "grad_norm": 4.1997904777526855,
      "learning_rate": 2.534223108530225e-05,
      "loss": 2.4538,
      "step": 32810
    },
    {
      "epoch": 1.9930770632173438,
      "grad_norm": 5.573452949523926,
      "learning_rate": 2.5314574679366664e-05,
      "loss": 2.78,
      "step": 32820
    },
    {
      "epoch": 1.993684338373717,
      "grad_norm": 4.513126373291016,
      "learning_rate": 2.528692825661022e-05,
      "loss": 2.6218,
      "step": 32830
    },
    {
      "epoch": 1.9942916135300903,
      "grad_norm": 4.888145446777344,
      "learning_rate": 2.525929182821359e-05,
      "loss": 2.6831,
      "step": 32840
    },
    {
      "epoch": 1.9948988886864638,
      "grad_norm": 4.882471561431885,
      "learning_rate": 2.5231665405353348e-05,
      "loss": 2.955,
      "step": 32850
    },
    {
      "epoch": 1.9955061638428373,
      "grad_norm": 3.494654893875122,
      "learning_rate": 2.5204048999202127e-05,
      "loss": 2.9157,
      "step": 32860
    },
    {
      "epoch": 1.9961134389992106,
      "grad_norm": 3.7554662227630615,
      "learning_rate": 2.5176442620928388e-05,
      "loss": 3.2001,
      "step": 32870
    },
    {
      "epoch": 1.9967207141555838,
      "grad_norm": 5.302505970001221,
      "learning_rate": 2.5148846281696576e-05,
      "loss": 3.1816,
      "step": 32880
    },
    {
      "epoch": 1.997327989311957,
      "grad_norm": 5.763834476470947,
      "learning_rate": 2.5121259992667134e-05,
      "loss": 2.922,
      "step": 32890
    },
    {
      "epoch": 1.9979352644683306,
      "grad_norm": 4.214088439941406,
      "learning_rate": 2.509368376499639e-05,
      "loss": 2.9223,
      "step": 32900
    },
    {
      "epoch": 1.998542539624704,
      "grad_norm": 4.978677272796631,
      "learning_rate": 2.5066117609836594e-05,
      "loss": 2.9672,
      "step": 32910
    },
    {
      "epoch": 1.9991498147810773,
      "grad_norm": 6.320485591888428,
      "learning_rate": 2.5038561538335924e-05,
      "loss": 2.6222,
      "step": 32920
    },
    {
      "epoch": 1.9997570899374506,
      "grad_norm": 6.204697132110596,
      "learning_rate": 2.501101556163855e-05,
      "loss": 2.6607,
      "step": 32930
    },
    {
      "epoch": 2.000364365093824,
      "grad_norm": 5.234920501708984,
      "learning_rate": 2.4983479690884448e-05,
      "loss": 2.6145,
      "step": 32940
    },
    {
      "epoch": 2.0009716402501976,
      "grad_norm": 3.30338454246521,
      "learning_rate": 2.4955953937209587e-05,
      "loss": 2.7561,
      "step": 32950
    },
    {
      "epoch": 2.001578915406571,
      "grad_norm": 4.1347336769104,
      "learning_rate": 2.4928438311745795e-05,
      "loss": 3.1037,
      "step": 32960
    },
    {
      "epoch": 2.002186190562944,
      "grad_norm": 5.024414539337158,
      "learning_rate": 2.4900932825620864e-05,
      "loss": 2.7359,
      "step": 32970
    },
    {
      "epoch": 2.0027934657193174,
      "grad_norm": 5.028374671936035,
      "learning_rate": 2.487343748995844e-05,
      "loss": 2.8799,
      "step": 32980
    },
    {
      "epoch": 2.0034007408756906,
      "grad_norm": 4.869287014007568,
      "learning_rate": 2.4845952315878072e-05,
      "loss": 2.5343,
      "step": 32990
    },
    {
      "epoch": 2.004008016032064,
      "grad_norm": 6.541067600250244,
      "learning_rate": 2.48184773144952e-05,
      "loss": 2.6952,
      "step": 33000
    },
    {
      "epoch": 2.0046152911884376,
      "grad_norm": 5.511374473571777,
      "learning_rate": 2.479101249692115e-05,
      "loss": 2.5869,
      "step": 33010
    },
    {
      "epoch": 2.005222566344811,
      "grad_norm": 5.15678596496582,
      "learning_rate": 2.476355787426314e-05,
      "loss": 2.7969,
      "step": 33020
    },
    {
      "epoch": 2.005829841501184,
      "grad_norm": 6.245672702789307,
      "learning_rate": 2.4736113457624245e-05,
      "loss": 3.2589,
      "step": 33030
    },
    {
      "epoch": 2.0064371166575574,
      "grad_norm": 4.554827690124512,
      "learning_rate": 2.4708679258103416e-05,
      "loss": 3.2421,
      "step": 33040
    },
    {
      "epoch": 2.0070443918139307,
      "grad_norm": 4.2445502281188965,
      "learning_rate": 2.4681255286795514e-05,
      "loss": 3.1729,
      "step": 33050
    },
    {
      "epoch": 2.0076516669703044,
      "grad_norm": 5.984968185424805,
      "learning_rate": 2.465384155479121e-05,
      "loss": 3.0966,
      "step": 33060
    },
    {
      "epoch": 2.0082589421266777,
      "grad_norm": 5.12469482421875,
      "learning_rate": 2.462643807317705e-05,
      "loss": 2.8469,
      "step": 33070
    },
    {
      "epoch": 2.008866217283051,
      "grad_norm": 4.378252983093262,
      "learning_rate": 2.4599044853035446e-05,
      "loss": 2.6369,
      "step": 33080
    },
    {
      "epoch": 2.009473492439424,
      "grad_norm": 5.684182167053223,
      "learning_rate": 2.4571661905444648e-05,
      "loss": 2.6638,
      "step": 33090
    },
    {
      "epoch": 2.0100807675957975,
      "grad_norm": 4.476592540740967,
      "learning_rate": 2.4544289241478757e-05,
      "loss": 2.6283,
      "step": 33100
    },
    {
      "epoch": 2.010688042752171,
      "grad_norm": 6.16377067565918,
      "learning_rate": 2.4516926872207695e-05,
      "loss": 2.8017,
      "step": 33110
    },
    {
      "epoch": 2.0112953179085444,
      "grad_norm": 5.549210548400879,
      "learning_rate": 2.4489574808697285e-05,
      "loss": 2.6967,
      "step": 33120
    },
    {
      "epoch": 2.0119025930649177,
      "grad_norm": 3.9430482387542725,
      "learning_rate": 2.4462233062009105e-05,
      "loss": 2.3385,
      "step": 33130
    },
    {
      "epoch": 2.012509868221291,
      "grad_norm": 4.714114189147949,
      "learning_rate": 2.443490164320062e-05,
      "loss": 2.3561,
      "step": 33140
    },
    {
      "epoch": 2.0131171433776642,
      "grad_norm": 3.6941962242126465,
      "learning_rate": 2.4407580563325033e-05,
      "loss": 2.3742,
      "step": 33150
    },
    {
      "epoch": 2.013724418534038,
      "grad_norm": 6.187869071960449,
      "learning_rate": 2.4380269833431478e-05,
      "loss": 2.8899,
      "step": 33160
    },
    {
      "epoch": 2.014331693690411,
      "grad_norm": 5.54863977432251,
      "learning_rate": 2.435296946456484e-05,
      "loss": 2.9757,
      "step": 33170
    },
    {
      "epoch": 2.0149389688467845,
      "grad_norm": 4.354215621948242,
      "learning_rate": 2.4325679467765806e-05,
      "loss": 3.2319,
      "step": 33180
    },
    {
      "epoch": 2.0155462440031577,
      "grad_norm": 3.7731711864471436,
      "learning_rate": 2.429839985407088e-05,
      "loss": 2.7131,
      "step": 33190
    },
    {
      "epoch": 2.016153519159531,
      "grad_norm": 3.3932337760925293,
      "learning_rate": 2.4271130634512405e-05,
      "loss": 2.5678,
      "step": 33200
    },
    {
      "epoch": 2.0167607943159047,
      "grad_norm": 4.913022518157959,
      "learning_rate": 2.424387182011849e-05,
      "loss": 2.6173,
      "step": 33210
    },
    {
      "epoch": 2.017368069472278,
      "grad_norm": 4.273904800415039,
      "learning_rate": 2.4216623421912994e-05,
      "loss": 3.075,
      "step": 33220
    },
    {
      "epoch": 2.0179753446286512,
      "grad_norm": 4.749651908874512,
      "learning_rate": 2.4189385450915602e-05,
      "loss": 3.0231,
      "step": 33230
    },
    {
      "epoch": 2.0185826197850245,
      "grad_norm": 3.6490602493286133,
      "learning_rate": 2.4162157918141827e-05,
      "loss": 2.6792,
      "step": 33240
    },
    {
      "epoch": 2.019189894941398,
      "grad_norm": 6.85235595703125,
      "learning_rate": 2.4134940834602885e-05,
      "loss": 2.6781,
      "step": 33250
    },
    {
      "epoch": 2.0197971700977715,
      "grad_norm": 5.681461811065674,
      "learning_rate": 2.4107734211305815e-05,
      "loss": 2.8025,
      "step": 33260
    },
    {
      "epoch": 2.0204044452541448,
      "grad_norm": 5.301532745361328,
      "learning_rate": 2.40805380592534e-05,
      "loss": 2.5069,
      "step": 33270
    },
    {
      "epoch": 2.021011720410518,
      "grad_norm": 4.841687202453613,
      "learning_rate": 2.4053352389444195e-05,
      "loss": 2.8247,
      "step": 33280
    },
    {
      "epoch": 2.0216189955668913,
      "grad_norm": 5.194425106048584,
      "learning_rate": 2.402617721287252e-05,
      "loss": 2.6721,
      "step": 33290
    },
    {
      "epoch": 2.0222262707232646,
      "grad_norm": 3.227538585662842,
      "learning_rate": 2.3999012540528452e-05,
      "loss": 2.2993,
      "step": 33300
    },
    {
      "epoch": 2.0228335458796383,
      "grad_norm": 3.9483802318573,
      "learning_rate": 2.3971858383397793e-05,
      "loss": 2.5612,
      "step": 33310
    },
    {
      "epoch": 2.0234408210360115,
      "grad_norm": 4.37691593170166,
      "learning_rate": 2.3944714752462162e-05,
      "loss": 2.7099,
      "step": 33320
    },
    {
      "epoch": 2.024048096192385,
      "grad_norm": 3.967750310897827,
      "learning_rate": 2.3917581658698845e-05,
      "loss": 2.8894,
      "step": 33330
    },
    {
      "epoch": 2.024655371348758,
      "grad_norm": 5.935337543487549,
      "learning_rate": 2.389045911308091e-05,
      "loss": 2.6831,
      "step": 33340
    },
    {
      "epoch": 2.0252626465051313,
      "grad_norm": 4.9094343185424805,
      "learning_rate": 2.386334712657714e-05,
      "loss": 2.7889,
      "step": 33350
    },
    {
      "epoch": 2.025869921661505,
      "grad_norm": 3.4020886421203613,
      "learning_rate": 2.383624571015205e-05,
      "loss": 2.5996,
      "step": 33360
    },
    {
      "epoch": 2.0264771968178783,
      "grad_norm": 3.7556254863739014,
      "learning_rate": 2.3809154874765893e-05,
      "loss": 2.5302,
      "step": 33370
    },
    {
      "epoch": 2.0270844719742516,
      "grad_norm": 5.100698947906494,
      "learning_rate": 2.378207463137461e-05,
      "loss": 3.0748,
      "step": 33380
    },
    {
      "epoch": 2.027691747130625,
      "grad_norm": 4.210476875305176,
      "learning_rate": 2.3755004990929923e-05,
      "loss": 3.1182,
      "step": 33390
    },
    {
      "epoch": 2.028299022286998,
      "grad_norm": 3.032315969467163,
      "learning_rate": 2.3727945964379205e-05,
      "loss": 2.809,
      "step": 33400
    },
    {
      "epoch": 2.028906297443372,
      "grad_norm": 3.577256202697754,
      "learning_rate": 2.3700897562665563e-05,
      "loss": 2.8166,
      "step": 33410
    },
    {
      "epoch": 2.029513572599745,
      "grad_norm": 4.927359104156494,
      "learning_rate": 2.3673859796727794e-05,
      "loss": 3.2787,
      "step": 33420
    },
    {
      "epoch": 2.0301208477561183,
      "grad_norm": 4.338995456695557,
      "learning_rate": 2.3646832677500404e-05,
      "loss": 2.972,
      "step": 33430
    },
    {
      "epoch": 2.0307281229124916,
      "grad_norm": 5.618282794952393,
      "learning_rate": 2.3619816215913588e-05,
      "loss": 2.8954,
      "step": 33440
    },
    {
      "epoch": 2.031335398068865,
      "grad_norm": 3.445662021636963,
      "learning_rate": 2.3592810422893236e-05,
      "loss": 2.5055,
      "step": 33450
    },
    {
      "epoch": 2.031942673225238,
      "grad_norm": 4.499686241149902,
      "learning_rate": 2.35658153093609e-05,
      "loss": 2.654,
      "step": 33460
    },
    {
      "epoch": 2.032549948381612,
      "grad_norm": 4.947421073913574,
      "learning_rate": 2.3538830886233877e-05,
      "loss": 2.8329,
      "step": 33470
    },
    {
      "epoch": 2.033157223537985,
      "grad_norm": 4.579370021820068,
      "learning_rate": 2.351185716442507e-05,
      "loss": 2.7225,
      "step": 33480
    },
    {
      "epoch": 2.0337644986943584,
      "grad_norm": 3.7926316261291504,
      "learning_rate": 2.348489415484311e-05,
      "loss": 2.5891,
      "step": 33490
    },
    {
      "epoch": 2.0343717738507316,
      "grad_norm": 2.956869125366211,
      "learning_rate": 2.3457941868392208e-05,
      "loss": 2.6457,
      "step": 33500
    },
    {
      "epoch": 2.034979049007105,
      "grad_norm": 3.7830965518951416,
      "learning_rate": 2.3431000315972353e-05,
      "loss": 2.8209,
      "step": 33510
    },
    {
      "epoch": 2.0355863241634786,
      "grad_norm": 3.6935505867004395,
      "learning_rate": 2.340406950847911e-05,
      "loss": 2.8691,
      "step": 33520
    },
    {
      "epoch": 2.036193599319852,
      "grad_norm": 4.692933559417725,
      "learning_rate": 2.3377149456803744e-05,
      "loss": 2.7741,
      "step": 33530
    },
    {
      "epoch": 2.036800874476225,
      "grad_norm": 6.028061866760254,
      "learning_rate": 2.335024017183312e-05,
      "loss": 3.0063,
      "step": 33540
    },
    {
      "epoch": 2.0374081496325984,
      "grad_norm": 5.239374160766602,
      "learning_rate": 2.3323341664449845e-05,
      "loss": 2.939,
      "step": 33550
    },
    {
      "epoch": 2.0380154247889717,
      "grad_norm": 5.847793102264404,
      "learning_rate": 2.329645394553204e-05,
      "loss": 2.8375,
      "step": 33560
    },
    {
      "epoch": 2.0386226999453454,
      "grad_norm": 5.885931491851807,
      "learning_rate": 2.3269577025953542e-05,
      "loss": 3.0304,
      "step": 33570
    },
    {
      "epoch": 2.0392299751017187,
      "grad_norm": 3.4151206016540527,
      "learning_rate": 2.3242710916583826e-05,
      "loss": 2.649,
      "step": 33580
    },
    {
      "epoch": 2.039837250258092,
      "grad_norm": 3.6355881690979004,
      "learning_rate": 2.3215855628287962e-05,
      "loss": 2.8895,
      "step": 33590
    },
    {
      "epoch": 2.040444525414465,
      "grad_norm": 6.291988372802734,
      "learning_rate": 2.3189011171926663e-05,
      "loss": 3.1138,
      "step": 33600
    },
    {
      "epoch": 2.0410518005708385,
      "grad_norm": 2.693162202835083,
      "learning_rate": 2.3162177558356245e-05,
      "loss": 2.9467,
      "step": 33610
    },
    {
      "epoch": 2.041659075727212,
      "grad_norm": 2.9511799812316895,
      "learning_rate": 2.3135354798428648e-05,
      "loss": 2.3842,
      "step": 33620
    },
    {
      "epoch": 2.0422663508835854,
      "grad_norm": 4.52343225479126,
      "learning_rate": 2.3108542902991436e-05,
      "loss": 2.8811,
      "step": 33630
    },
    {
      "epoch": 2.0428736260399587,
      "grad_norm": 4.344634056091309,
      "learning_rate": 2.3081741882887752e-05,
      "loss": 3.2059,
      "step": 33640
    },
    {
      "epoch": 2.043480901196332,
      "grad_norm": 4.118124485015869,
      "learning_rate": 2.3054951748956345e-05,
      "loss": 3.2759,
      "step": 33650
    },
    {
      "epoch": 2.0440881763527052,
      "grad_norm": 4.386122226715088,
      "learning_rate": 2.3028172512031604e-05,
      "loss": 2.8384,
      "step": 33660
    },
    {
      "epoch": 2.044695451509079,
      "grad_norm": 4.347466945648193,
      "learning_rate": 2.300140418294347e-05,
      "loss": 2.9405,
      "step": 33670
    },
    {
      "epoch": 2.045302726665452,
      "grad_norm": 5.402634620666504,
      "learning_rate": 2.2974646772517468e-05,
      "loss": 3.0883,
      "step": 33680
    },
    {
      "epoch": 2.0459100018218255,
      "grad_norm": 4.338550567626953,
      "learning_rate": 2.2947900291574732e-05,
      "loss": 3.0941,
      "step": 33690
    },
    {
      "epoch": 2.0465172769781987,
      "grad_norm": 3.9887454509735107,
      "learning_rate": 2.2921164750931955e-05,
      "loss": 3.366,
      "step": 33700
    },
    {
      "epoch": 2.047124552134572,
      "grad_norm": 5.6027421951293945,
      "learning_rate": 2.2894440161401425e-05,
      "loss": 3.0127,
      "step": 33710
    },
    {
      "epoch": 2.0477318272909457,
      "grad_norm": 8.08435344696045,
      "learning_rate": 2.286772653379099e-05,
      "loss": 2.9872,
      "step": 33720
    },
    {
      "epoch": 2.048339102447319,
      "grad_norm": 4.369473934173584,
      "learning_rate": 2.2841023878904046e-05,
      "loss": 2.846,
      "step": 33730
    },
    {
      "epoch": 2.0489463776036922,
      "grad_norm": 4.369890213012695,
      "learning_rate": 2.2814332207539603e-05,
      "loss": 2.9335,
      "step": 33740
    },
    {
      "epoch": 2.0495536527600655,
      "grad_norm": 5.311707019805908,
      "learning_rate": 2.278765153049219e-05,
      "loss": 2.7163,
      "step": 33750
    },
    {
      "epoch": 2.050160927916439,
      "grad_norm": 7.528745651245117,
      "learning_rate": 2.276098185855191e-05,
      "loss": 3.0568,
      "step": 33760
    },
    {
      "epoch": 2.0507682030728125,
      "grad_norm": 6.511457443237305,
      "learning_rate": 2.2734323202504347e-05,
      "loss": 3.077,
      "step": 33770
    },
    {
      "epoch": 2.0513754782291858,
      "grad_norm": 5.542037010192871,
      "learning_rate": 2.2707675573130743e-05,
      "loss": 2.9128,
      "step": 33780
    },
    {
      "epoch": 2.051982753385559,
      "grad_norm": 6.670073509216309,
      "learning_rate": 2.2681038981207807e-05,
      "loss": 3.1248,
      "step": 33790
    },
    {
      "epoch": 2.0525900285419323,
      "grad_norm": 5.934410572052002,
      "learning_rate": 2.265441343750778e-05,
      "loss": 3.0924,
      "step": 33800
    },
    {
      "epoch": 2.0531973036983056,
      "grad_norm": 6.063572883605957,
      "learning_rate": 2.26277989527985e-05,
      "loss": 3.4311,
      "step": 33810
    },
    {
      "epoch": 2.0538045788546793,
      "grad_norm": 6.501346588134766,
      "learning_rate": 2.260119553784326e-05,
      "loss": 3.1571,
      "step": 33820
    },
    {
      "epoch": 2.0544118540110525,
      "grad_norm": 6.418333530426025,
      "learning_rate": 2.257460320340093e-05,
      "loss": 3.366,
      "step": 33830
    },
    {
      "epoch": 2.055019129167426,
      "grad_norm": 3.421818494796753,
      "learning_rate": 2.254802196022581e-05,
      "loss": 3.1087,
      "step": 33840
    },
    {
      "epoch": 2.055626404323799,
      "grad_norm": 7.218165397644043,
      "learning_rate": 2.252145181906784e-05,
      "loss": 2.979,
      "step": 33850
    },
    {
      "epoch": 2.0562336794801723,
      "grad_norm": 6.7346086502075195,
      "learning_rate": 2.2494892790672397e-05,
      "loss": 3.052,
      "step": 33860
    },
    {
      "epoch": 2.056840954636546,
      "grad_norm": 6.5269389152526855,
      "learning_rate": 2.2468344885780358e-05,
      "loss": 2.8291,
      "step": 33870
    },
    {
      "epoch": 2.0574482297929193,
      "grad_norm": 3.8209962844848633,
      "learning_rate": 2.244180811512811e-05,
      "loss": 2.9844,
      "step": 33880
    },
    {
      "epoch": 2.0580555049492926,
      "grad_norm": 5.9879150390625,
      "learning_rate": 2.2415282489447597e-05,
      "loss": 2.7434,
      "step": 33890
    },
    {
      "epoch": 2.058662780105666,
      "grad_norm": 6.165341377258301,
      "learning_rate": 2.238876801946616e-05,
      "loss": 2.7847,
      "step": 33900
    },
    {
      "epoch": 2.059270055262039,
      "grad_norm": 5.797549247741699,
      "learning_rate": 2.236226471590668e-05,
      "loss": 2.9579,
      "step": 33910
    },
    {
      "epoch": 2.0598773304184124,
      "grad_norm": 5.96799898147583,
      "learning_rate": 2.2335772589487498e-05,
      "loss": 2.4083,
      "step": 33920
    },
    {
      "epoch": 2.060484605574786,
      "grad_norm": 5.779932975769043,
      "learning_rate": 2.230929165092249e-05,
      "loss": 2.6611,
      "step": 33930
    },
    {
      "epoch": 2.0610918807311593,
      "grad_norm": 6.237419128417969,
      "learning_rate": 2.2282821910920953e-05,
      "loss": 2.5735,
      "step": 33940
    },
    {
      "epoch": 2.0616991558875326,
      "grad_norm": 5.260836601257324,
      "learning_rate": 2.2256363380187673e-05,
      "loss": 3.0447,
      "step": 33950
    },
    {
      "epoch": 2.062306431043906,
      "grad_norm": 6.3372087478637695,
      "learning_rate": 2.2229916069422897e-05,
      "loss": 2.7866,
      "step": 33960
    },
    {
      "epoch": 2.062913706200279,
      "grad_norm": 4.771143913269043,
      "learning_rate": 2.220347998932234e-05,
      "loss": 2.9521,
      "step": 33970
    },
    {
      "epoch": 2.063520981356653,
      "grad_norm": 3.5806643962860107,
      "learning_rate": 2.2177055150577174e-05,
      "loss": 3.0099,
      "step": 33980
    },
    {
      "epoch": 2.064128256513026,
      "grad_norm": 5.284003257751465,
      "learning_rate": 2.215064156387402e-05,
      "loss": 2.6833,
      "step": 33990
    },
    {
      "epoch": 2.0647355316693994,
      "grad_norm": 5.227338790893555,
      "learning_rate": 2.2124239239894944e-05,
      "loss": 2.8346,
      "step": 34000
    },
    {
      "epoch": 2.0653428068257726,
      "grad_norm": 5.16695499420166,
      "learning_rate": 2.20978481893175e-05,
      "loss": 3.0087,
      "step": 34010
    },
    {
      "epoch": 2.065950081982146,
      "grad_norm": 6.41165018081665,
      "learning_rate": 2.207146842281462e-05,
      "loss": 3.0727,
      "step": 34020
    },
    {
      "epoch": 2.0665573571385196,
      "grad_norm": 6.767673492431641,
      "learning_rate": 2.204509995105472e-05,
      "loss": 2.9358,
      "step": 34030
    },
    {
      "epoch": 2.067164632294893,
      "grad_norm": 6.288267612457275,
      "learning_rate": 2.2018742784701617e-05,
      "loss": 2.6063,
      "step": 34040
    },
    {
      "epoch": 2.067771907451266,
      "grad_norm": 3.8179969787597656,
      "learning_rate": 2.1992396934414576e-05,
      "loss": 2.4864,
      "step": 34050
    },
    {
      "epoch": 2.0683791826076394,
      "grad_norm": 7.221653938293457,
      "learning_rate": 2.196606241084827e-05,
      "loss": 2.8711,
      "step": 34060
    },
    {
      "epoch": 2.0689864577640127,
      "grad_norm": 6.1353254318237305,
      "learning_rate": 2.1939739224652788e-05,
      "loss": 2.7556,
      "step": 34070
    },
    {
      "epoch": 2.0695937329203864,
      "grad_norm": 3.7130537033081055,
      "learning_rate": 2.1913427386473683e-05,
      "loss": 2.8046,
      "step": 34080
    },
    {
      "epoch": 2.0702010080767597,
      "grad_norm": 4.51097297668457,
      "learning_rate": 2.1887126906951862e-05,
      "loss": 2.6464,
      "step": 34090
    },
    {
      "epoch": 2.070808283233133,
      "grad_norm": 4.731729507446289,
      "learning_rate": 2.186083779672365e-05,
      "loss": 2.8223,
      "step": 34100
    },
    {
      "epoch": 2.071415558389506,
      "grad_norm": 4.841809272766113,
      "learning_rate": 2.1834560066420797e-05,
      "loss": 2.5313,
      "step": 34110
    },
    {
      "epoch": 2.0720228335458795,
      "grad_norm": 4.994941711425781,
      "learning_rate": 2.180829372667042e-05,
      "loss": 2.7777,
      "step": 34120
    },
    {
      "epoch": 2.072630108702253,
      "grad_norm": 6.034902095794678,
      "learning_rate": 2.1782038788095056e-05,
      "loss": 2.6928,
      "step": 34130
    },
    {
      "epoch": 2.0732373838586264,
      "grad_norm": 4.437280178070068,
      "learning_rate": 2.1755795261312618e-05,
      "loss": 2.373,
      "step": 34140
    },
    {
      "epoch": 2.0738446590149997,
      "grad_norm": 5.08991813659668,
      "learning_rate": 2.1729563156936382e-05,
      "loss": 2.3603,
      "step": 34150
    },
    {
      "epoch": 2.074451934171373,
      "grad_norm": 3.818814516067505,
      "learning_rate": 2.1703342485575072e-05,
      "loss": 3.0328,
      "step": 34160
    },
    {
      "epoch": 2.0750592093277462,
      "grad_norm": 4.63964319229126,
      "learning_rate": 2.1677133257832743e-05,
      "loss": 2.6933,
      "step": 34170
    },
    {
      "epoch": 2.07566648448412,
      "grad_norm": 5.730715274810791,
      "learning_rate": 2.1650935484308786e-05,
      "loss": 2.8991,
      "step": 34180
    },
    {
      "epoch": 2.076273759640493,
      "grad_norm": 6.494025230407715,
      "learning_rate": 2.1624749175598e-05,
      "loss": 2.5559,
      "step": 34190
    },
    {
      "epoch": 2.0768810347968665,
      "grad_norm": 4.8157806396484375,
      "learning_rate": 2.1598574342290584e-05,
      "loss": 2.3818,
      "step": 34200
    },
    {
      "epoch": 2.0774883099532397,
      "grad_norm": 5.291860580444336,
      "learning_rate": 2.1572410994972037e-05,
      "loss": 2.7248,
      "step": 34210
    },
    {
      "epoch": 2.078095585109613,
      "grad_norm": 5.035621166229248,
      "learning_rate": 2.154625914422323e-05,
      "loss": 2.68,
      "step": 34220
    },
    {
      "epoch": 2.0787028602659867,
      "grad_norm": 6.439980506896973,
      "learning_rate": 2.152011880062037e-05,
      "loss": 3.127,
      "step": 34230
    },
    {
      "epoch": 2.07931013542236,
      "grad_norm": 6.706729412078857,
      "learning_rate": 2.149398997473509e-05,
      "loss": 3.2825,
      "step": 34240
    },
    {
      "epoch": 2.0799174105787333,
      "grad_norm": 6.577686309814453,
      "learning_rate": 2.1467872677134243e-05,
      "loss": 2.8426,
      "step": 34250
    },
    {
      "epoch": 2.0805246857351065,
      "grad_norm": 4.810701370239258,
      "learning_rate": 2.144176691838008e-05,
      "loss": 2.4988,
      "step": 34260
    },
    {
      "epoch": 2.08113196089148,
      "grad_norm": 5.379055023193359,
      "learning_rate": 2.1415672709030225e-05,
      "loss": 2.9631,
      "step": 34270
    },
    {
      "epoch": 2.0817392360478535,
      "grad_norm": 4.46151065826416,
      "learning_rate": 2.1389590059637572e-05,
      "loss": 2.6806,
      "step": 34280
    },
    {
      "epoch": 2.0823465112042268,
      "grad_norm": 4.529811382293701,
      "learning_rate": 2.1363518980750358e-05,
      "loss": 2.7293,
      "step": 34290
    },
    {
      "epoch": 2.0829537863606,
      "grad_norm": 5.998034954071045,
      "learning_rate": 2.1337459482912143e-05,
      "loss": 2.6608,
      "step": 34300
    },
    {
      "epoch": 2.0835610615169733,
      "grad_norm": 5.6179375648498535,
      "learning_rate": 2.1311411576661807e-05,
      "loss": 2.514,
      "step": 34310
    },
    {
      "epoch": 2.0841683366733466,
      "grad_norm": 5.264997959136963,
      "learning_rate": 2.1285375272533538e-05,
      "loss": 2.7018,
      "step": 34320
    },
    {
      "epoch": 2.08477561182972,
      "grad_norm": 6.2104387283325195,
      "learning_rate": 2.1259350581056824e-05,
      "loss": 2.8902,
      "step": 34330
    },
    {
      "epoch": 2.0853828869860935,
      "grad_norm": 6.2650651931762695,
      "learning_rate": 2.1233337512756453e-05,
      "loss": 2.8136,
      "step": 34340
    },
    {
      "epoch": 2.085990162142467,
      "grad_norm": 4.321043968200684,
      "learning_rate": 2.1207336078152563e-05,
      "loss": 3.0358,
      "step": 34350
    },
    {
      "epoch": 2.08659743729884,
      "grad_norm": 5.419412612915039,
      "learning_rate": 2.1181346287760517e-05,
      "loss": 2.8057,
      "step": 34360
    },
    {
      "epoch": 2.0872047124552133,
      "grad_norm": 4.111314296722412,
      "learning_rate": 2.115536815209102e-05,
      "loss": 3.027,
      "step": 34370
    },
    {
      "epoch": 2.0878119876115866,
      "grad_norm": 5.6675124168396,
      "learning_rate": 2.112940168165003e-05,
      "loss": 2.6406,
      "step": 34380
    },
    {
      "epoch": 2.0884192627679603,
      "grad_norm": 3.5854239463806152,
      "learning_rate": 2.1103446886938805e-05,
      "loss": 2.5946,
      "step": 34390
    },
    {
      "epoch": 2.0890265379243336,
      "grad_norm": 3.5546746253967285,
      "learning_rate": 2.107750377845387e-05,
      "loss": 2.5688,
      "step": 34400
    },
    {
      "epoch": 2.089633813080707,
      "grad_norm": 4.186774730682373,
      "learning_rate": 2.105157236668705e-05,
      "loss": 2.9099,
      "step": 34410
    },
    {
      "epoch": 2.09024108823708,
      "grad_norm": 5.069873332977295,
      "learning_rate": 2.1025652662125383e-05,
      "loss": 3.0931,
      "step": 34420
    },
    {
      "epoch": 2.0908483633934534,
      "grad_norm": 4.548136234283447,
      "learning_rate": 2.099974467525126e-05,
      "loss": 2.437,
      "step": 34430
    },
    {
      "epoch": 2.091455638549827,
      "grad_norm": 4.309502601623535,
      "learning_rate": 2.097384841654226e-05,
      "loss": 2.5017,
      "step": 34440
    },
    {
      "epoch": 2.0920629137062003,
      "grad_norm": 4.224390029907227,
      "learning_rate": 2.094796389647125e-05,
      "loss": 2.4675,
      "step": 34450
    },
    {
      "epoch": 2.0926701888625736,
      "grad_norm": 6.659058570861816,
      "learning_rate": 2.092209112550631e-05,
      "loss": 3.0531,
      "step": 34460
    },
    {
      "epoch": 2.093277464018947,
      "grad_norm": 5.563099384307861,
      "learning_rate": 2.089623011411084e-05,
      "loss": 3.2847,
      "step": 34470
    },
    {
      "epoch": 2.09388473917532,
      "grad_norm": 5.889889717102051,
      "learning_rate": 2.0870380872743427e-05,
      "loss": 3.2747,
      "step": 34480
    },
    {
      "epoch": 2.094492014331694,
      "grad_norm": 5.3425703048706055,
      "learning_rate": 2.0844543411857903e-05,
      "loss": 2.931,
      "step": 34490
    },
    {
      "epoch": 2.095099289488067,
      "grad_norm": 5.0573811531066895,
      "learning_rate": 2.0818717741903386e-05,
      "loss": 3.1073,
      "step": 34500
    },
    {
      "epoch": 2.0957065646444404,
      "grad_norm": 4.574842929840088,
      "learning_rate": 2.0792903873324166e-05,
      "loss": 3.2051,
      "step": 34510
    },
    {
      "epoch": 2.0963138398008136,
      "grad_norm": 5.193909168243408,
      "learning_rate": 2.0767101816559798e-05,
      "loss": 3.186,
      "step": 34520
    },
    {
      "epoch": 2.096921114957187,
      "grad_norm": 5.139163970947266,
      "learning_rate": 2.0741311582045e-05,
      "loss": 3.0268,
      "step": 34530
    },
    {
      "epoch": 2.0975283901135606,
      "grad_norm": 4.805534362792969,
      "learning_rate": 2.0715533180209795e-05,
      "loss": 2.9133,
      "step": 34540
    },
    {
      "epoch": 2.098135665269934,
      "grad_norm": 4.236606597900391,
      "learning_rate": 2.0689766621479366e-05,
      "loss": 2.8281,
      "step": 34550
    },
    {
      "epoch": 2.098742940426307,
      "grad_norm": 5.791839122772217,
      "learning_rate": 2.0664011916274127e-05,
      "loss": 3.1344,
      "step": 34560
    },
    {
      "epoch": 2.0993502155826804,
      "grad_norm": 4.618990898132324,
      "learning_rate": 2.063826907500967e-05,
      "loss": 3.0926,
      "step": 34570
    },
    {
      "epoch": 2.0999574907390537,
      "grad_norm": 4.420037269592285,
      "learning_rate": 2.0612538108096858e-05,
      "loss": 2.7624,
      "step": 34580
    },
    {
      "epoch": 2.1005647658954274,
      "grad_norm": 5.60624361038208,
      "learning_rate": 2.0586819025941657e-05,
      "loss": 2.6224,
      "step": 34590
    },
    {
      "epoch": 2.1011720410518007,
      "grad_norm": 5.752581596374512,
      "learning_rate": 2.0561111838945286e-05,
      "loss": 2.8983,
      "step": 34600
    },
    {
      "epoch": 2.101779316208174,
      "grad_norm": 5.000877857208252,
      "learning_rate": 2.0535416557504123e-05,
      "loss": 2.9205,
      "step": 34610
    },
    {
      "epoch": 2.102386591364547,
      "grad_norm": 5.226465225219727,
      "learning_rate": 2.050973319200979e-05,
      "loss": 3.0387,
      "step": 34620
    },
    {
      "epoch": 2.1029938665209205,
      "grad_norm": 4.815652847290039,
      "learning_rate": 2.0484061752849025e-05,
      "loss": 3.1065,
      "step": 34630
    },
    {
      "epoch": 2.103601141677294,
      "grad_norm": 6.283012390136719,
      "learning_rate": 2.045840225040377e-05,
      "loss": 2.983,
      "step": 34640
    },
    {
      "epoch": 2.1042084168336674,
      "grad_norm": 5.606648921966553,
      "learning_rate": 2.0432754695051136e-05,
      "loss": 2.7022,
      "step": 34650
    },
    {
      "epoch": 2.1048156919900407,
      "grad_norm": 3.901597023010254,
      "learning_rate": 2.040711909716341e-05,
      "loss": 2.7229,
      "step": 34660
    },
    {
      "epoch": 2.105422967146414,
      "grad_norm": 5.263944149017334,
      "learning_rate": 2.038149546710802e-05,
      "loss": 2.9706,
      "step": 34670
    },
    {
      "epoch": 2.1060302423027872,
      "grad_norm": 6.236720085144043,
      "learning_rate": 2.0355883815247585e-05,
      "loss": 2.7272,
      "step": 34680
    },
    {
      "epoch": 2.106637517459161,
      "grad_norm": 6.759106159210205,
      "learning_rate": 2.033028415193984e-05,
      "loss": 2.5709,
      "step": 34690
    },
    {
      "epoch": 2.107244792615534,
      "grad_norm": 6.776601791381836,
      "learning_rate": 2.030469648753773e-05,
      "loss": 3.0061,
      "step": 34700
    },
    {
      "epoch": 2.1078520677719075,
      "grad_norm": 6.017334461212158,
      "learning_rate": 2.0279120832389305e-05,
      "loss": 2.9849,
      "step": 34710
    },
    {
      "epoch": 2.1084593429282807,
      "grad_norm": 4.689187526702881,
      "learning_rate": 2.0253557196837753e-05,
      "loss": 3.024,
      "step": 34720
    },
    {
      "epoch": 2.109066618084654,
      "grad_norm": 4.612195014953613,
      "learning_rate": 2.0228005591221432e-05,
      "loss": 3.1365,
      "step": 34730
    },
    {
      "epoch": 2.1096738932410277,
      "grad_norm": 3.820722818374634,
      "learning_rate": 2.0202466025873813e-05,
      "loss": 3.3293,
      "step": 34740
    },
    {
      "epoch": 2.110281168397401,
      "grad_norm": 3.7657790184020996,
      "learning_rate": 2.0176938511123493e-05,
      "loss": 3.1861,
      "step": 34750
    },
    {
      "epoch": 2.1108884435537743,
      "grad_norm": 4.553177833557129,
      "learning_rate": 2.01514230572942e-05,
      "loss": 3.1274,
      "step": 34760
    },
    {
      "epoch": 2.1114957187101475,
      "grad_norm": 5.2511420249938965,
      "learning_rate": 2.0125919674704818e-05,
      "loss": 3.0134,
      "step": 34770
    },
    {
      "epoch": 2.112102993866521,
      "grad_norm": 4.696007251739502,
      "learning_rate": 2.010042837366931e-05,
      "loss": 3.159,
      "step": 34780
    },
    {
      "epoch": 2.1127102690228945,
      "grad_norm": 4.168705463409424,
      "learning_rate": 2.007494916449676e-05,
      "loss": 2.8999,
      "step": 34790
    },
    {
      "epoch": 2.1133175441792678,
      "grad_norm": 2.9561078548431396,
      "learning_rate": 2.0049482057491364e-05,
      "loss": 2.5389,
      "step": 34800
    },
    {
      "epoch": 2.113924819335641,
      "grad_norm": 3.5137224197387695,
      "learning_rate": 2.0024027062952428e-05,
      "loss": 2.7699,
      "step": 34810
    },
    {
      "epoch": 2.1145320944920143,
      "grad_norm": 3.1366560459136963,
      "learning_rate": 1.999858419117436e-05,
      "loss": 2.6239,
      "step": 34820
    },
    {
      "epoch": 2.1151393696483876,
      "grad_norm": 3.960590362548828,
      "learning_rate": 1.9973153452446652e-05,
      "loss": 2.8041,
      "step": 34830
    },
    {
      "epoch": 2.115746644804761,
      "grad_norm": 4.372802257537842,
      "learning_rate": 1.9947734857053894e-05,
      "loss": 3.0218,
      "step": 34840
    },
    {
      "epoch": 2.1163539199611345,
      "grad_norm": 7.3590006828308105,
      "learning_rate": 1.9922328415275802e-05,
      "loss": 3.24,
      "step": 34850
    },
    {
      "epoch": 2.116961195117508,
      "grad_norm": 4.756282806396484,
      "learning_rate": 1.9896934137387147e-05,
      "loss": 3.2789,
      "step": 34860
    },
    {
      "epoch": 2.117568470273881,
      "grad_norm": 4.339788913726807,
      "learning_rate": 1.9871552033657742e-05,
      "loss": 2.7876,
      "step": 34870
    },
    {
      "epoch": 2.1181757454302543,
      "grad_norm": 3.521895170211792,
      "learning_rate": 1.9846182114352518e-05,
      "loss": 2.9162,
      "step": 34880
    },
    {
      "epoch": 2.1187830205866276,
      "grad_norm": 4.907527446746826,
      "learning_rate": 1.98208243897315e-05,
      "loss": 2.9572,
      "step": 34890
    },
    {
      "epoch": 2.1193902957430013,
      "grad_norm": 4.210888385772705,
      "learning_rate": 1.979547887004975e-05,
      "loss": 2.6092,
      "step": 34900
    },
    {
      "epoch": 2.1199975708993746,
      "grad_norm": 4.694028854370117,
      "learning_rate": 1.9770145565557392e-05,
      "loss": 2.95,
      "step": 34910
    },
    {
      "epoch": 2.120604846055748,
      "grad_norm": 4.2854204177856445,
      "learning_rate": 1.9744824486499618e-05,
      "loss": 2.916,
      "step": 34920
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 4.083286285400391,
      "learning_rate": 1.9719515643116674e-05,
      "loss": 2.9252,
      "step": 34930
    },
    {
      "epoch": 2.1218193963684944,
      "grad_norm": 4.28009033203125,
      "learning_rate": 1.9694219045643875e-05,
      "loss": 2.6507,
      "step": 34940
    },
    {
      "epoch": 2.122426671524868,
      "grad_norm": 2.9745073318481445,
      "learning_rate": 1.966893470431153e-05,
      "loss": 2.2932,
      "step": 34950
    },
    {
      "epoch": 2.1230339466812413,
      "grad_norm": 3.220839262008667,
      "learning_rate": 1.9643662629345072e-05,
      "loss": 2.7898,
      "step": 34960
    },
    {
      "epoch": 2.1236412218376146,
      "grad_norm": 3.8246822357177734,
      "learning_rate": 1.9618402830964916e-05,
      "loss": 2.6056,
      "step": 34970
    },
    {
      "epoch": 2.124248496993988,
      "grad_norm": 3.8856070041656494,
      "learning_rate": 1.9593155319386523e-05,
      "loss": 2.7531,
      "step": 34980
    },
    {
      "epoch": 2.124855772150361,
      "grad_norm": 3.667757749557495,
      "learning_rate": 1.9567920104820396e-05,
      "loss": 2.6907,
      "step": 34990
    },
    {
      "epoch": 2.125463047306735,
      "grad_norm": 3.882417917251587,
      "learning_rate": 1.9542697197472047e-05,
      "loss": 2.674,
      "step": 35000
    },
    {
      "epoch": 2.125463047306735,
      "eval_loss": 4.44534158706665,
      "eval_runtime": 2083.0171,
      "eval_samples_per_second": 7.905,
      "eval_steps_per_second": 1.976,
      "step": 35000
    },
    {
      "epoch": 2.126070322463108,
      "grad_norm": 5.0159759521484375,
      "learning_rate": 1.951748660754203e-05,
      "loss": 3.971,
      "step": 35010
    },
    {
      "epoch": 2.1266775976194814,
      "grad_norm": 6.113588809967041,
      "learning_rate": 1.94922883452259e-05,
      "loss": 3.6054,
      "step": 35020
    },
    {
      "epoch": 2.1272848727758547,
      "grad_norm": 5.438748836517334,
      "learning_rate": 1.946710242071423e-05,
      "loss": 3.5854,
      "step": 35030
    },
    {
      "epoch": 2.127892147932228,
      "grad_norm": 4.155613899230957,
      "learning_rate": 1.9441928844192637e-05,
      "loss": 3.2662,
      "step": 35040
    },
    {
      "epoch": 2.1284994230886016,
      "grad_norm": 4.152950763702393,
      "learning_rate": 1.941676762584169e-05,
      "loss": 3.113,
      "step": 35050
    },
    {
      "epoch": 2.129106698244975,
      "grad_norm": 4.733211994171143,
      "learning_rate": 1.9391618775837007e-05,
      "loss": 2.9835,
      "step": 35060
    },
    {
      "epoch": 2.129713973401348,
      "grad_norm": 4.703424453735352,
      "learning_rate": 1.936648230434917e-05,
      "loss": 2.8957,
      "step": 35070
    },
    {
      "epoch": 2.1303212485577214,
      "grad_norm": 5.4436774253845215,
      "learning_rate": 1.934135822154377e-05,
      "loss": 2.658,
      "step": 35080
    },
    {
      "epoch": 2.1309285237140947,
      "grad_norm": 5.507932186126709,
      "learning_rate": 1.9316246537581385e-05,
      "loss": 2.6443,
      "step": 35090
    },
    {
      "epoch": 2.1315357988704684,
      "grad_norm": 5.089256763458252,
      "learning_rate": 1.9291147262617582e-05,
      "loss": 3.348,
      "step": 35100
    },
    {
      "epoch": 2.1321430740268417,
      "grad_norm": 7.114605903625488,
      "learning_rate": 1.9266060406802893e-05,
      "loss": 3.1182,
      "step": 35110
    },
    {
      "epoch": 2.132750349183215,
      "grad_norm": 9.460821151733398,
      "learning_rate": 1.924098598028287e-05,
      "loss": 3.3637,
      "step": 35120
    },
    {
      "epoch": 2.133357624339588,
      "grad_norm": 7.659095764160156,
      "learning_rate": 1.9215923993198e-05,
      "loss": 3.2163,
      "step": 35130
    },
    {
      "epoch": 2.1339648994959615,
      "grad_norm": 6.784200668334961,
      "learning_rate": 1.9190874455683765e-05,
      "loss": 2.8851,
      "step": 35140
    },
    {
      "epoch": 2.134572174652335,
      "grad_norm": 8.514810562133789,
      "learning_rate": 1.9165837377870542e-05,
      "loss": 3.4038,
      "step": 35150
    },
    {
      "epoch": 2.1351794498087084,
      "grad_norm": 8.51009464263916,
      "learning_rate": 1.9140812769883775e-05,
      "loss": 3.3666,
      "step": 35160
    },
    {
      "epoch": 2.1357867249650817,
      "grad_norm": 8.760727882385254,
      "learning_rate": 1.9115800641843808e-05,
      "loss": 3.2198,
      "step": 35170
    },
    {
      "epoch": 2.136394000121455,
      "grad_norm": 11.010499000549316,
      "learning_rate": 1.909080100386591e-05,
      "loss": 3.0849,
      "step": 35180
    },
    {
      "epoch": 2.1370012752778282,
      "grad_norm": 6.341734409332275,
      "learning_rate": 1.906581386606038e-05,
      "loss": 3.37,
      "step": 35190
    },
    {
      "epoch": 2.1376085504342015,
      "grad_norm": 6.762129783630371,
      "learning_rate": 1.9040839238532405e-05,
      "loss": 3.0997,
      "step": 35200
    },
    {
      "epoch": 2.138215825590575,
      "grad_norm": 6.094871520996094,
      "learning_rate": 1.9015877131382097e-05,
      "loss": 2.9953,
      "step": 35210
    },
    {
      "epoch": 2.1388231007469485,
      "grad_norm": 7.697209358215332,
      "learning_rate": 1.8990927554704525e-05,
      "loss": 3.0852,
      "step": 35220
    },
    {
      "epoch": 2.1394303759033217,
      "grad_norm": 5.670214653015137,
      "learning_rate": 1.8965990518589725e-05,
      "loss": 2.8771,
      "step": 35230
    },
    {
      "epoch": 2.140037651059695,
      "grad_norm": 3.5569546222686768,
      "learning_rate": 1.8941066033122628e-05,
      "loss": 2.551,
      "step": 35240
    },
    {
      "epoch": 2.1406449262160683,
      "grad_norm": 7.935906887054443,
      "learning_rate": 1.8916154108383084e-05,
      "loss": 3.1319,
      "step": 35250
    },
    {
      "epoch": 2.141252201372442,
      "grad_norm": 6.969381809234619,
      "learning_rate": 1.8891254754445847e-05,
      "loss": 3.1636,
      "step": 35260
    },
    {
      "epoch": 2.1418594765288153,
      "grad_norm": 6.8508710861206055,
      "learning_rate": 1.886636798138068e-05,
      "loss": 3.0337,
      "step": 35270
    },
    {
      "epoch": 2.1424667516851885,
      "grad_norm": 8.390077590942383,
      "learning_rate": 1.8841493799252124e-05,
      "loss": 2.8914,
      "step": 35280
    },
    {
      "epoch": 2.143074026841562,
      "grad_norm": 7.7623066902160645,
      "learning_rate": 1.881663221811972e-05,
      "loss": 2.9605,
      "step": 35290
    },
    {
      "epoch": 2.143681301997935,
      "grad_norm": 9.496501922607422,
      "learning_rate": 1.879178324803787e-05,
      "loss": 2.8899,
      "step": 35300
    },
    {
      "epoch": 2.1442885771543088,
      "grad_norm": 6.40179443359375,
      "learning_rate": 1.8766946899055926e-05,
      "loss": 3.187,
      "step": 35310
    },
    {
      "epoch": 2.144895852310682,
      "grad_norm": 6.1403608322143555,
      "learning_rate": 1.8742123181218085e-05,
      "loss": 2.9614,
      "step": 35320
    },
    {
      "epoch": 2.1455031274670553,
      "grad_norm": 9.697779655456543,
      "learning_rate": 1.8717312104563457e-05,
      "loss": 2.9454,
      "step": 35330
    },
    {
      "epoch": 2.1461104026234286,
      "grad_norm": 9.378396034240723,
      "learning_rate": 1.8692513679126033e-05,
      "loss": 3.2223,
      "step": 35340
    },
    {
      "epoch": 2.146717677779802,
      "grad_norm": 7.031903266906738,
      "learning_rate": 1.8667727914934698e-05,
      "loss": 3.336,
      "step": 35350
    },
    {
      "epoch": 2.1473249529361755,
      "grad_norm": 6.834940433502197,
      "learning_rate": 1.86429548220132e-05,
      "loss": 3.1274,
      "step": 35360
    },
    {
      "epoch": 2.147932228092549,
      "grad_norm": 6.929764747619629,
      "learning_rate": 1.8618194410380173e-05,
      "loss": 3.0017,
      "step": 35370
    },
    {
      "epoch": 2.148539503248922,
      "grad_norm": 6.1568379402160645,
      "learning_rate": 1.8593446690049116e-05,
      "loss": 2.8559,
      "step": 35380
    },
    {
      "epoch": 2.1491467784052953,
      "grad_norm": 5.371661186218262,
      "learning_rate": 1.8568711671028422e-05,
      "loss": 2.7759,
      "step": 35390
    },
    {
      "epoch": 2.1497540535616686,
      "grad_norm": 7.163914680480957,
      "learning_rate": 1.854398936332132e-05,
      "loss": 2.8258,
      "step": 35400
    },
    {
      "epoch": 2.1503613287180423,
      "grad_norm": 6.0833635330200195,
      "learning_rate": 1.8519279776925897e-05,
      "loss": 2.6706,
      "step": 35410
    },
    {
      "epoch": 2.1509686038744156,
      "grad_norm": 7.412637710571289,
      "learning_rate": 1.8494582921835106e-05,
      "loss": 2.6411,
      "step": 35420
    },
    {
      "epoch": 2.151575879030789,
      "grad_norm": 6.978858470916748,
      "learning_rate": 1.8469898808036752e-05,
      "loss": 3.1036,
      "step": 35430
    },
    {
      "epoch": 2.152183154187162,
      "grad_norm": 5.729442596435547,
      "learning_rate": 1.844522744551348e-05,
      "loss": 3.0364,
      "step": 35440
    },
    {
      "epoch": 2.1527904293435354,
      "grad_norm": 10.576695442199707,
      "learning_rate": 1.8420568844242763e-05,
      "loss": 2.9071,
      "step": 35450
    },
    {
      "epoch": 2.153397704499909,
      "grad_norm": 9.184857368469238,
      "learning_rate": 1.8395923014196974e-05,
      "loss": 3.0844,
      "step": 35460
    },
    {
      "epoch": 2.1540049796562823,
      "grad_norm": 8.25998306274414,
      "learning_rate": 1.8371289965343258e-05,
      "loss": 3.1174,
      "step": 35470
    },
    {
      "epoch": 2.1546122548126556,
      "grad_norm": 8.188396453857422,
      "learning_rate": 1.8346669707643627e-05,
      "loss": 3.1095,
      "step": 35480
    },
    {
      "epoch": 2.155219529969029,
      "grad_norm": 8.016458511352539,
      "learning_rate": 1.8322062251054855e-05,
      "loss": 3.0387,
      "step": 35490
    },
    {
      "epoch": 2.155826805125402,
      "grad_norm": 5.706003665924072,
      "learning_rate": 1.829746760552864e-05,
      "loss": 3.1429,
      "step": 35500
    },
    {
      "epoch": 2.156434080281776,
      "grad_norm": 7.00698184967041,
      "learning_rate": 1.8272885781011433e-05,
      "loss": 2.9458,
      "step": 35510
    },
    {
      "epoch": 2.157041355438149,
      "grad_norm": 5.7850141525268555,
      "learning_rate": 1.8248316787444515e-05,
      "loss": 2.7546,
      "step": 35520
    },
    {
      "epoch": 2.1576486305945224,
      "grad_norm": 5.570592403411865,
      "learning_rate": 1.822376063476396e-05,
      "loss": 2.9067,
      "step": 35530
    },
    {
      "epoch": 2.1582559057508957,
      "grad_norm": 8.15234661102295,
      "learning_rate": 1.8199217332900703e-05,
      "loss": 3.0032,
      "step": 35540
    },
    {
      "epoch": 2.158863180907269,
      "grad_norm": 7.672235012054443,
      "learning_rate": 1.817468689178045e-05,
      "loss": 2.7443,
      "step": 35550
    },
    {
      "epoch": 2.1594704560636426,
      "grad_norm": 5.923000812530518,
      "learning_rate": 1.8150169321323656e-05,
      "loss": 2.597,
      "step": 35560
    },
    {
      "epoch": 2.160077731220016,
      "grad_norm": 3.4794809818267822,
      "learning_rate": 1.8125664631445633e-05,
      "loss": 2.8039,
      "step": 35570
    },
    {
      "epoch": 2.160685006376389,
      "grad_norm": 4.542021751403809,
      "learning_rate": 1.810117283205649e-05,
      "loss": 2.5351,
      "step": 35580
    },
    {
      "epoch": 2.1612922815327624,
      "grad_norm": 4.921670913696289,
      "learning_rate": 1.8076693933061083e-05,
      "loss": 2.7077,
      "step": 35590
    },
    {
      "epoch": 2.1618995566891357,
      "grad_norm": 4.894698619842529,
      "learning_rate": 1.805222794435908e-05,
      "loss": 2.8105,
      "step": 35600
    },
    {
      "epoch": 2.1625068318455094,
      "grad_norm": 6.29503059387207,
      "learning_rate": 1.8027774875844904e-05,
      "loss": 2.9978,
      "step": 35610
    },
    {
      "epoch": 2.1631141070018827,
      "grad_norm": 6.797706604003906,
      "learning_rate": 1.8003334737407772e-05,
      "loss": 3.1097,
      "step": 35620
    },
    {
      "epoch": 2.163721382158256,
      "grad_norm": 4.59125280380249,
      "learning_rate": 1.797890753893166e-05,
      "loss": 2.7915,
      "step": 35630
    },
    {
      "epoch": 2.164328657314629,
      "grad_norm": 4.771632671356201,
      "learning_rate": 1.795449329029531e-05,
      "loss": 3.0302,
      "step": 35640
    },
    {
      "epoch": 2.1649359324710025,
      "grad_norm": 5.058710098266602,
      "learning_rate": 1.7930092001372255e-05,
      "loss": 2.8116,
      "step": 35650
    },
    {
      "epoch": 2.165543207627376,
      "grad_norm": 5.339875221252441,
      "learning_rate": 1.7905703682030755e-05,
      "loss": 3.0169,
      "step": 35660
    },
    {
      "epoch": 2.1661504827837494,
      "grad_norm": 5.748322010040283,
      "learning_rate": 1.7881328342133834e-05,
      "loss": 2.8745,
      "step": 35670
    },
    {
      "epoch": 2.1667577579401227,
      "grad_norm": 6.468901634216309,
      "learning_rate": 1.7856965991539265e-05,
      "loss": 2.7976,
      "step": 35680
    },
    {
      "epoch": 2.167365033096496,
      "grad_norm": 5.214787483215332,
      "learning_rate": 1.783261664009957e-05,
      "loss": 2.548,
      "step": 35690
    },
    {
      "epoch": 2.1679723082528692,
      "grad_norm": 5.735241889953613,
      "learning_rate": 1.7808280297662017e-05,
      "loss": 2.8791,
      "step": 35700
    },
    {
      "epoch": 2.168579583409243,
      "grad_norm": 5.548679828643799,
      "learning_rate": 1.778395697406861e-05,
      "loss": 2.6504,
      "step": 35710
    },
    {
      "epoch": 2.169186858565616,
      "grad_norm": 4.474346160888672,
      "learning_rate": 1.7759646679156073e-05,
      "loss": 2.8083,
      "step": 35720
    },
    {
      "epoch": 2.1697941337219895,
      "grad_norm": 4.806942462921143,
      "learning_rate": 1.773534942275591e-05,
      "loss": 3.0803,
      "step": 35730
    },
    {
      "epoch": 2.1704014088783627,
      "grad_norm": 5.49811315536499,
      "learning_rate": 1.7711065214694295e-05,
      "loss": 2.8636,
      "step": 35740
    },
    {
      "epoch": 2.171008684034736,
      "grad_norm": 5.67785120010376,
      "learning_rate": 1.7686794064792157e-05,
      "loss": 3.4069,
      "step": 35750
    },
    {
      "epoch": 2.1716159591911097,
      "grad_norm": 8.565950393676758,
      "learning_rate": 1.766253598286513e-05,
      "loss": 2.8536,
      "step": 35760
    },
    {
      "epoch": 2.172223234347483,
      "grad_norm": 8.924613952636719,
      "learning_rate": 1.7638290978723575e-05,
      "loss": 2.876,
      "step": 35770
    },
    {
      "epoch": 2.1728305095038563,
      "grad_norm": 6.832515239715576,
      "learning_rate": 1.761405906217255e-05,
      "loss": 2.7792,
      "step": 35780
    },
    {
      "epoch": 2.1734377846602295,
      "grad_norm": 6.468482494354248,
      "learning_rate": 1.7589840243011833e-05,
      "loss": 2.9714,
      "step": 35790
    },
    {
      "epoch": 2.174045059816603,
      "grad_norm": 5.473161220550537,
      "learning_rate": 1.7565634531035885e-05,
      "loss": 3.1253,
      "step": 35800
    },
    {
      "epoch": 2.174652334972976,
      "grad_norm": 6.481754779815674,
      "learning_rate": 1.7541441936033908e-05,
      "loss": 2.7512,
      "step": 35810
    },
    {
      "epoch": 2.1752596101293498,
      "grad_norm": 4.279179096221924,
      "learning_rate": 1.7517262467789763e-05,
      "loss": 2.4268,
      "step": 35820
    },
    {
      "epoch": 2.175866885285723,
      "grad_norm": 3.5325815677642822,
      "learning_rate": 1.7493096136082022e-05,
      "loss": 2.6814,
      "step": 35830
    },
    {
      "epoch": 2.1764741604420963,
      "grad_norm": 3.8223307132720947,
      "learning_rate": 1.746894295068389e-05,
      "loss": 2.8199,
      "step": 35840
    },
    {
      "epoch": 2.1770814355984696,
      "grad_norm": 3.963649272918701,
      "learning_rate": 1.7444802921363346e-05,
      "loss": 2.6257,
      "step": 35850
    },
    {
      "epoch": 2.177688710754843,
      "grad_norm": 3.3613667488098145,
      "learning_rate": 1.742067605788299e-05,
      "loss": 2.7265,
      "step": 35860
    },
    {
      "epoch": 2.1782959859112165,
      "grad_norm": 6.6729254722595215,
      "learning_rate": 1.739656237000009e-05,
      "loss": 2.6932,
      "step": 35870
    },
    {
      "epoch": 2.17890326106759,
      "grad_norm": 3.365273952484131,
      "learning_rate": 1.7372461867466645e-05,
      "loss": 2.781,
      "step": 35880
    },
    {
      "epoch": 2.179510536223963,
      "grad_norm": 6.976356029510498,
      "learning_rate": 1.7348374560029267e-05,
      "loss": 2.8263,
      "step": 35890
    },
    {
      "epoch": 2.1801178113803363,
      "grad_norm": 4.3726348876953125,
      "learning_rate": 1.7324300457429233e-05,
      "loss": 3.0692,
      "step": 35900
    },
    {
      "epoch": 2.1807250865367096,
      "grad_norm": 6.364043235778809,
      "learning_rate": 1.7300239569402477e-05,
      "loss": 3.1007,
      "step": 35910
    },
    {
      "epoch": 2.1813323616930833,
      "grad_norm": 3.5136802196502686,
      "learning_rate": 1.7276191905679646e-05,
      "loss": 3.2853,
      "step": 35920
    },
    {
      "epoch": 2.1819396368494566,
      "grad_norm": 4.519925594329834,
      "learning_rate": 1.725215747598598e-05,
      "loss": 3.1825,
      "step": 35930
    },
    {
      "epoch": 2.18254691200583,
      "grad_norm": 5.192319393157959,
      "learning_rate": 1.722813629004139e-05,
      "loss": 3.0699,
      "step": 35940
    },
    {
      "epoch": 2.183154187162203,
      "grad_norm": 5.236526012420654,
      "learning_rate": 1.7204128357560417e-05,
      "loss": 3.0392,
      "step": 35950
    },
    {
      "epoch": 2.1837614623185764,
      "grad_norm": 4.650314807891846,
      "learning_rate": 1.7180133688252266e-05,
      "loss": 3.035,
      "step": 35960
    },
    {
      "epoch": 2.18436873747495,
      "grad_norm": 6.562033653259277,
      "learning_rate": 1.7156152291820742e-05,
      "loss": 2.6526,
      "step": 35970
    },
    {
      "epoch": 2.1849760126313233,
      "grad_norm": 6.755687236785889,
      "learning_rate": 1.7132184177964324e-05,
      "loss": 3.1373,
      "step": 35980
    },
    {
      "epoch": 2.1855832877876966,
      "grad_norm": 5.326009750366211,
      "learning_rate": 1.7108229356376064e-05,
      "loss": 3.0678,
      "step": 35990
    },
    {
      "epoch": 2.18619056294407,
      "grad_norm": 3.855588436126709,
      "learning_rate": 1.708428783674373e-05,
      "loss": 2.7096,
      "step": 36000
    },
    {
      "epoch": 2.186797838100443,
      "grad_norm": 3.992675304412842,
      "learning_rate": 1.706035962874961e-05,
      "loss": 3.0744,
      "step": 36010
    },
    {
      "epoch": 2.187405113256817,
      "grad_norm": 6.327816009521484,
      "learning_rate": 1.7036444742070673e-05,
      "loss": 2.5167,
      "step": 36020
    },
    {
      "epoch": 2.18801238841319,
      "grad_norm": 4.412105083465576,
      "learning_rate": 1.7012543186378478e-05,
      "loss": 2.2986,
      "step": 36030
    },
    {
      "epoch": 2.1886196635695634,
      "grad_norm": 4.752479076385498,
      "learning_rate": 1.698865497133918e-05,
      "loss": 2.6729,
      "step": 36040
    },
    {
      "epoch": 2.1892269387259367,
      "grad_norm": 4.734127998352051,
      "learning_rate": 1.696478010661357e-05,
      "loss": 2.8184,
      "step": 36050
    },
    {
      "epoch": 2.18983421388231,
      "grad_norm": 3.6098828315734863,
      "learning_rate": 1.6940918601857016e-05,
      "loss": 2.9269,
      "step": 36060
    },
    {
      "epoch": 2.1904414890386836,
      "grad_norm": 6.936905384063721,
      "learning_rate": 1.691707046671947e-05,
      "loss": 3.0281,
      "step": 36070
    },
    {
      "epoch": 2.191048764195057,
      "grad_norm": 6.273223876953125,
      "learning_rate": 1.689323571084554e-05,
      "loss": 2.9036,
      "step": 36080
    },
    {
      "epoch": 2.19165603935143,
      "grad_norm": 5.193957805633545,
      "learning_rate": 1.6869414343874356e-05,
      "loss": 3.3236,
      "step": 36090
    },
    {
      "epoch": 2.1922633145078034,
      "grad_norm": 5.9615936279296875,
      "learning_rate": 1.684560637543967e-05,
      "loss": 3.2022,
      "step": 36100
    },
    {
      "epoch": 2.1928705896641767,
      "grad_norm": 4.0804266929626465,
      "learning_rate": 1.6821811815169793e-05,
      "loss": 3.0808,
      "step": 36110
    },
    {
      "epoch": 2.19347786482055,
      "grad_norm": 5.139610767364502,
      "learning_rate": 1.6798030672687627e-05,
      "loss": 3.3448,
      "step": 36120
    },
    {
      "epoch": 2.1940851399769237,
      "grad_norm": 4.555859565734863,
      "learning_rate": 1.677426295761064e-05,
      "loss": 3.2651,
      "step": 36130
    },
    {
      "epoch": 2.194692415133297,
      "grad_norm": 5.347294330596924,
      "learning_rate": 1.6750508679550864e-05,
      "loss": 3.3011,
      "step": 36140
    },
    {
      "epoch": 2.19529969028967,
      "grad_norm": 5.830401420593262,
      "learning_rate": 1.6726767848114944e-05,
      "loss": 3.1401,
      "step": 36150
    },
    {
      "epoch": 2.1959069654460435,
      "grad_norm": 4.0594916343688965,
      "learning_rate": 1.6703040472904026e-05,
      "loss": 2.5306,
      "step": 36160
    },
    {
      "epoch": 2.1965142406024167,
      "grad_norm": 4.1961350440979,
      "learning_rate": 1.6679326563513858e-05,
      "loss": 2.8336,
      "step": 36170
    },
    {
      "epoch": 2.1971215157587904,
      "grad_norm": 3.6618800163269043,
      "learning_rate": 1.6655626129534673e-05,
      "loss": 3.2261,
      "step": 36180
    },
    {
      "epoch": 2.1977287909151637,
      "grad_norm": 4.20866584777832,
      "learning_rate": 1.6631939180551353e-05,
      "loss": 3.1599,
      "step": 36190
    },
    {
      "epoch": 2.198336066071537,
      "grad_norm": 6.653391361236572,
      "learning_rate": 1.6608265726143262e-05,
      "loss": 3.0302,
      "step": 36200
    },
    {
      "epoch": 2.1989433412279102,
      "grad_norm": 6.093746185302734,
      "learning_rate": 1.6584605775884325e-05,
      "loss": 3.0314,
      "step": 36210
    },
    {
      "epoch": 2.1995506163842835,
      "grad_norm": 6.274200439453125,
      "learning_rate": 1.656095933934298e-05,
      "loss": 3.2419,
      "step": 36220
    },
    {
      "epoch": 2.200157891540657,
      "grad_norm": 5.1716108322143555,
      "learning_rate": 1.653732642608227e-05,
      "loss": 3.1308,
      "step": 36230
    },
    {
      "epoch": 2.2007651666970305,
      "grad_norm": 4.184174060821533,
      "learning_rate": 1.6513707045659687e-05,
      "loss": 2.7485,
      "step": 36240
    },
    {
      "epoch": 2.2013724418534037,
      "grad_norm": 4.537614345550537,
      "learning_rate": 1.6490101207627284e-05,
      "loss": 2.7859,
      "step": 36250
    },
    {
      "epoch": 2.201979717009777,
      "grad_norm": 4.2512526512146,
      "learning_rate": 1.6466508921531627e-05,
      "loss": 2.6812,
      "step": 36260
    },
    {
      "epoch": 2.2025869921661503,
      "grad_norm": 5.224819660186768,
      "learning_rate": 1.6442930196913846e-05,
      "loss": 2.8067,
      "step": 36270
    },
    {
      "epoch": 2.203194267322524,
      "grad_norm": 4.370588302612305,
      "learning_rate": 1.641936504330954e-05,
      "loss": 3.4724,
      "step": 36280
    },
    {
      "epoch": 2.2038015424788973,
      "grad_norm": 4.95741605758667,
      "learning_rate": 1.6395813470248832e-05,
      "loss": 3.145,
      "step": 36290
    },
    {
      "epoch": 2.2044088176352705,
      "grad_norm": 2.9093804359436035,
      "learning_rate": 1.6372275487256344e-05,
      "loss": 3.2297,
      "step": 36300
    },
    {
      "epoch": 2.205016092791644,
      "grad_norm": 5.576838493347168,
      "learning_rate": 1.6348751103851224e-05,
      "loss": 3.2466,
      "step": 36310
    },
    {
      "epoch": 2.205623367948017,
      "grad_norm": 4.819037437438965,
      "learning_rate": 1.63252403295471e-05,
      "loss": 3.1906,
      "step": 36320
    },
    {
      "epoch": 2.2062306431043908,
      "grad_norm": 4.644280910491943,
      "learning_rate": 1.630174317385208e-05,
      "loss": 3.2277,
      "step": 36330
    },
    {
      "epoch": 2.206837918260764,
      "grad_norm": 5.417575836181641,
      "learning_rate": 1.6278259646268833e-05,
      "loss": 3.4807,
      "step": 36340
    },
    {
      "epoch": 2.2074451934171373,
      "grad_norm": 4.287461280822754,
      "learning_rate": 1.6254789756294454e-05,
      "loss": 3.3334,
      "step": 36350
    },
    {
      "epoch": 2.2080524685735106,
      "grad_norm": 4.959292411804199,
      "learning_rate": 1.6231333513420532e-05,
      "loss": 3.3701,
      "step": 36360
    },
    {
      "epoch": 2.208659743729884,
      "grad_norm": 4.081213474273682,
      "learning_rate": 1.6207890927133157e-05,
      "loss": 3.1833,
      "step": 36370
    },
    {
      "epoch": 2.2092670188862575,
      "grad_norm": 4.439948558807373,
      "learning_rate": 1.618446200691287e-05,
      "loss": 3.3822,
      "step": 36380
    },
    {
      "epoch": 2.209874294042631,
      "grad_norm": 5.805119037628174,
      "learning_rate": 1.6161046762234716e-05,
      "loss": 3.3191,
      "step": 36390
    },
    {
      "epoch": 2.210481569199004,
      "grad_norm": 4.050114631652832,
      "learning_rate": 1.613764520256818e-05,
      "loss": 2.8292,
      "step": 36400
    },
    {
      "epoch": 2.2110888443553773,
      "grad_norm": 4.53665018081665,
      "learning_rate": 1.611425733737721e-05,
      "loss": 3.0336,
      "step": 36410
    },
    {
      "epoch": 2.2116961195117506,
      "grad_norm": 4.81351900100708,
      "learning_rate": 1.609088317612027e-05,
      "loss": 3.0938,
      "step": 36420
    },
    {
      "epoch": 2.2123033946681243,
      "grad_norm": 6.484923362731934,
      "learning_rate": 1.6067522728250222e-05,
      "loss": 2.8076,
      "step": 36430
    },
    {
      "epoch": 2.2129106698244976,
      "grad_norm": 5.540999412536621,
      "learning_rate": 1.6044176003214408e-05,
      "loss": 2.7643,
      "step": 36440
    },
    {
      "epoch": 2.213517944980871,
      "grad_norm": 5.385491371154785,
      "learning_rate": 1.602084301045461e-05,
      "loss": 2.7157,
      "step": 36450
    },
    {
      "epoch": 2.214125220137244,
      "grad_norm": 4.568080902099609,
      "learning_rate": 1.5997523759407064e-05,
      "loss": 2.8685,
      "step": 36460
    },
    {
      "epoch": 2.2147324952936174,
      "grad_norm": 6.365224361419678,
      "learning_rate": 1.5974218259502448e-05,
      "loss": 3.5954,
      "step": 36470
    },
    {
      "epoch": 2.215339770449991,
      "grad_norm": 5.913025379180908,
      "learning_rate": 1.5950926520165875e-05,
      "loss": 3.3986,
      "step": 36480
    },
    {
      "epoch": 2.2159470456063644,
      "grad_norm": 5.899466037750244,
      "learning_rate": 1.592764855081688e-05,
      "loss": 3.4071,
      "step": 36490
    },
    {
      "epoch": 2.2165543207627376,
      "grad_norm": 4.6339030265808105,
      "learning_rate": 1.590438436086948e-05,
      "loss": 3.3404,
      "step": 36500
    },
    {
      "epoch": 2.217161595919111,
      "grad_norm": 4.715567588806152,
      "learning_rate": 1.588113395973208e-05,
      "loss": 3.4485,
      "step": 36510
    },
    {
      "epoch": 2.217768871075484,
      "grad_norm": 4.515633583068848,
      "learning_rate": 1.5857897356807477e-05,
      "loss": 3.0952,
      "step": 36520
    },
    {
      "epoch": 2.218376146231858,
      "grad_norm": 5.402049541473389,
      "learning_rate": 1.5834674561492935e-05,
      "loss": 2.8523,
      "step": 36530
    },
    {
      "epoch": 2.218983421388231,
      "grad_norm": 3.646864891052246,
      "learning_rate": 1.581146558318014e-05,
      "loss": 2.9173,
      "step": 36540
    },
    {
      "epoch": 2.2195906965446044,
      "grad_norm": 3.246690034866333,
      "learning_rate": 1.5788270431255164e-05,
      "loss": 2.8877,
      "step": 36550
    },
    {
      "epoch": 2.2201979717009777,
      "grad_norm": 7.019157409667969,
      "learning_rate": 1.5765089115098485e-05,
      "loss": 2.7417,
      "step": 36560
    },
    {
      "epoch": 2.220805246857351,
      "grad_norm": 6.023280620574951,
      "learning_rate": 1.574192164408502e-05,
      "loss": 2.6927,
      "step": 36570
    },
    {
      "epoch": 2.2214125220137246,
      "grad_norm": 4.9813666343688965,
      "learning_rate": 1.5718768027584063e-05,
      "loss": 3.4407,
      "step": 36580
    },
    {
      "epoch": 2.222019797170098,
      "grad_norm": 5.191766262054443,
      "learning_rate": 1.5695628274959278e-05,
      "loss": 3.3947,
      "step": 36590
    },
    {
      "epoch": 2.222627072326471,
      "grad_norm": 4.493548393249512,
      "learning_rate": 1.567250239556875e-05,
      "loss": 3.2306,
      "step": 36600
    },
    {
      "epoch": 2.2232343474828444,
      "grad_norm": 6.570965766906738,
      "learning_rate": 1.564939039876498e-05,
      "loss": 3.546,
      "step": 36610
    },
    {
      "epoch": 2.2238416226392177,
      "grad_norm": 5.969123363494873,
      "learning_rate": 1.5626292293894816e-05,
      "loss": 3.4261,
      "step": 36620
    },
    {
      "epoch": 2.2244488977955914,
      "grad_norm": 4.912861347198486,
      "learning_rate": 1.5603208090299498e-05,
      "loss": 3.0216,
      "step": 36630
    },
    {
      "epoch": 2.2250561729519647,
      "grad_norm": 7.030879020690918,
      "learning_rate": 1.5580137797314642e-05,
      "loss": 3.0012,
      "step": 36640
    },
    {
      "epoch": 2.225663448108338,
      "grad_norm": 7.439868927001953,
      "learning_rate": 1.5557081424270247e-05,
      "loss": 3.2352,
      "step": 36650
    },
    {
      "epoch": 2.226270723264711,
      "grad_norm": 8.916692733764648,
      "learning_rate": 1.5534038980490677e-05,
      "loss": 3.4988,
      "step": 36660
    },
    {
      "epoch": 2.2268779984210845,
      "grad_norm": 6.1974873542785645,
      "learning_rate": 1.5511010475294664e-05,
      "loss": 3.3812,
      "step": 36670
    },
    {
      "epoch": 2.227485273577458,
      "grad_norm": 5.617547988891602,
      "learning_rate": 1.5487995917995278e-05,
      "loss": 3.2344,
      "step": 36680
    },
    {
      "epoch": 2.2280925487338314,
      "grad_norm": 8.099358558654785,
      "learning_rate": 1.5464995317900015e-05,
      "loss": 3.1989,
      "step": 36690
    },
    {
      "epoch": 2.2286998238902047,
      "grad_norm": 5.947533130645752,
      "learning_rate": 1.5442008684310665e-05,
      "loss": 3.16,
      "step": 36700
    },
    {
      "epoch": 2.229307099046578,
      "grad_norm": 6.238855361938477,
      "learning_rate": 1.5419036026523388e-05,
      "loss": 3.2888,
      "step": 36710
    },
    {
      "epoch": 2.2299143742029512,
      "grad_norm": 5.1791911125183105,
      "learning_rate": 1.5396077353828685e-05,
      "loss": 3.692,
      "step": 36720
    },
    {
      "epoch": 2.2305216493593245,
      "grad_norm": 6.052880764007568,
      "learning_rate": 1.537313267551142e-05,
      "loss": 3.5058,
      "step": 36730
    },
    {
      "epoch": 2.231128924515698,
      "grad_norm": 5.128438949584961,
      "learning_rate": 1.5350202000850783e-05,
      "loss": 3.379,
      "step": 36740
    },
    {
      "epoch": 2.2317361996720715,
      "grad_norm": 8.685992240905762,
      "learning_rate": 1.5327285339120307e-05,
      "loss": 3.1974,
      "step": 36750
    },
    {
      "epoch": 2.2323434748284448,
      "grad_norm": 8.993016242980957,
      "learning_rate": 1.5304382699587828e-05,
      "loss": 3.3331,
      "step": 36760
    },
    {
      "epoch": 2.232950749984818,
      "grad_norm": 7.429642200469971,
      "learning_rate": 1.528149409151558e-05,
      "loss": 3.2001,
      "step": 36770
    },
    {
      "epoch": 2.2335580251411913,
      "grad_norm": 7.030888080596924,
      "learning_rate": 1.5258619524160066e-05,
      "loss": 3.2903,
      "step": 36780
    },
    {
      "epoch": 2.234165300297565,
      "grad_norm": 6.163439750671387,
      "learning_rate": 1.5235759006772132e-05,
      "loss": 3.2986,
      "step": 36790
    },
    {
      "epoch": 2.2347725754539383,
      "grad_norm": 5.849786758422852,
      "learning_rate": 1.5212912548596896e-05,
      "loss": 3.3679,
      "step": 36800
    },
    {
      "epoch": 2.2353798506103115,
      "grad_norm": 4.69745397567749,
      "learning_rate": 1.5190080158873876e-05,
      "loss": 3.1246,
      "step": 36810
    },
    {
      "epoch": 2.235987125766685,
      "grad_norm": 8.084734916687012,
      "learning_rate": 1.5167261846836833e-05,
      "loss": 3.205,
      "step": 36820
    },
    {
      "epoch": 2.236594400923058,
      "grad_norm": 9.929105758666992,
      "learning_rate": 1.5144457621713848e-05,
      "loss": 3.6819,
      "step": 36830
    },
    {
      "epoch": 2.2372016760794318,
      "grad_norm": 6.661068439483643,
      "learning_rate": 1.5121667492727337e-05,
      "loss": 3.5145,
      "step": 36840
    },
    {
      "epoch": 2.237808951235805,
      "grad_norm": 9.77655029296875,
      "learning_rate": 1.5098891469093979e-05,
      "loss": 3.4923,
      "step": 36850
    },
    {
      "epoch": 2.2384162263921783,
      "grad_norm": 7.0052289962768555,
      "learning_rate": 1.5076129560024777e-05,
      "loss": 3.5693,
      "step": 36860
    },
    {
      "epoch": 2.2390235015485516,
      "grad_norm": 5.10051965713501,
      "learning_rate": 1.5053381774724957e-05,
      "loss": 3.2173,
      "step": 36870
    },
    {
      "epoch": 2.239630776704925,
      "grad_norm": 8.658079147338867,
      "learning_rate": 1.5030648122394136e-05,
      "loss": 3.2537,
      "step": 36880
    },
    {
      "epoch": 2.2402380518612985,
      "grad_norm": 10.201066970825195,
      "learning_rate": 1.5007928612226152e-05,
      "loss": 3.5713,
      "step": 36890
    },
    {
      "epoch": 2.240845327017672,
      "grad_norm": 10.055798530578613,
      "learning_rate": 1.4985223253409137e-05,
      "loss": 3.5831,
      "step": 36900
    },
    {
      "epoch": 2.241452602174045,
      "grad_norm": 6.988338947296143,
      "learning_rate": 1.4962532055125484e-05,
      "loss": 3.3582,
      "step": 36910
    },
    {
      "epoch": 2.2420598773304183,
      "grad_norm": 9.783836364746094,
      "learning_rate": 1.4939855026551914e-05,
      "loss": 3.5635,
      "step": 36920
    },
    {
      "epoch": 2.2426671524867916,
      "grad_norm": 9.901494026184082,
      "learning_rate": 1.491719217685934e-05,
      "loss": 3.331,
      "step": 36930
    },
    {
      "epoch": 2.2432744276431653,
      "grad_norm": 6.966732978820801,
      "learning_rate": 1.4894543515212995e-05,
      "loss": 3.2876,
      "step": 36940
    },
    {
      "epoch": 2.2438817027995386,
      "grad_norm": 9.41213607788086,
      "learning_rate": 1.4871909050772338e-05,
      "loss": 3.2285,
      "step": 36950
    },
    {
      "epoch": 2.244488977955912,
      "grad_norm": 6.621315956115723,
      "learning_rate": 1.4849288792691141e-05,
      "loss": 3.6322,
      "step": 36960
    },
    {
      "epoch": 2.245096253112285,
      "grad_norm": 5.481005668640137,
      "learning_rate": 1.4826682750117376e-05,
      "loss": 3.1178,
      "step": 36970
    },
    {
      "epoch": 2.2457035282686584,
      "grad_norm": 6.108675003051758,
      "learning_rate": 1.4804090932193298e-05,
      "loss": 2.9607,
      "step": 36980
    },
    {
      "epoch": 2.246310803425032,
      "grad_norm": 5.05885648727417,
      "learning_rate": 1.4781513348055387e-05,
      "loss": 3.6247,
      "step": 36990
    },
    {
      "epoch": 2.2469180785814054,
      "grad_norm": 4.930550575256348,
      "learning_rate": 1.4758950006834382e-05,
      "loss": 3.8889,
      "step": 37000
    },
    {
      "epoch": 2.2475253537377786,
      "grad_norm": 5.209481239318848,
      "learning_rate": 1.4736400917655263e-05,
      "loss": 3.639,
      "step": 37010
    },
    {
      "epoch": 2.248132628894152,
      "grad_norm": 5.594690799713135,
      "learning_rate": 1.471386608963723e-05,
      "loss": 3.4405,
      "step": 37020
    },
    {
      "epoch": 2.248739904050525,
      "grad_norm": 4.980207920074463,
      "learning_rate": 1.4691345531893713e-05,
      "loss": 3.1631,
      "step": 37030
    },
    {
      "epoch": 2.2493471792068984,
      "grad_norm": 6.61152458190918,
      "learning_rate": 1.4668839253532424e-05,
      "loss": 2.9658,
      "step": 37040
    },
    {
      "epoch": 2.249954454363272,
      "grad_norm": 6.803511142730713,
      "learning_rate": 1.4646347263655235e-05,
      "loss": 3.0907,
      "step": 37050
    },
    {
      "epoch": 2.2505617295196454,
      "grad_norm": 5.330347061157227,
      "learning_rate": 1.4623869571358273e-05,
      "loss": 2.9975,
      "step": 37060
    },
    {
      "epoch": 2.2511690046760187,
      "grad_norm": 4.647728443145752,
      "learning_rate": 1.4601406185731869e-05,
      "loss": 3.3961,
      "step": 37070
    },
    {
      "epoch": 2.251776279832392,
      "grad_norm": 5.944721698760986,
      "learning_rate": 1.4578957115860575e-05,
      "loss": 3.2734,
      "step": 37080
    },
    {
      "epoch": 2.252383554988765,
      "grad_norm": 6.376841068267822,
      "learning_rate": 1.455652237082315e-05,
      "loss": 3.4858,
      "step": 37090
    },
    {
      "epoch": 2.252990830145139,
      "grad_norm": 4.66516637802124,
      "learning_rate": 1.4534101959692554e-05,
      "loss": 3.2784,
      "step": 37100
    },
    {
      "epoch": 2.253598105301512,
      "grad_norm": 7.0099873542785645,
      "learning_rate": 1.4511695891535987e-05,
      "loss": 3.3037,
      "step": 37110
    },
    {
      "epoch": 2.2542053804578854,
      "grad_norm": 6.979880332946777,
      "learning_rate": 1.4489304175414797e-05,
      "loss": 3.2656,
      "step": 37120
    },
    {
      "epoch": 2.2548126556142587,
      "grad_norm": 6.573182582855225,
      "learning_rate": 1.4466926820384557e-05,
      "loss": 3.4233,
      "step": 37130
    },
    {
      "epoch": 2.255419930770632,
      "grad_norm": 4.712101936340332,
      "learning_rate": 1.4444563835495028e-05,
      "loss": 3.2575,
      "step": 37140
    },
    {
      "epoch": 2.2560272059270057,
      "grad_norm": 6.4685845375061035,
      "learning_rate": 1.4422215229790154e-05,
      "loss": 3.4274,
      "step": 37150
    },
    {
      "epoch": 2.256634481083379,
      "grad_norm": 5.917227745056152,
      "learning_rate": 1.4399881012308063e-05,
      "loss": 2.9502,
      "step": 37160
    },
    {
      "epoch": 2.257241756239752,
      "grad_norm": 4.778731822967529,
      "learning_rate": 1.4377561192081073e-05,
      "loss": 3.2248,
      "step": 37170
    },
    {
      "epoch": 2.2578490313961255,
      "grad_norm": 5.229501247406006,
      "learning_rate": 1.4355255778135662e-05,
      "loss": 3.2615,
      "step": 37180
    },
    {
      "epoch": 2.2584563065524987,
      "grad_norm": 6.651784896850586,
      "learning_rate": 1.433296477949252e-05,
      "loss": 3.1399,
      "step": 37190
    },
    {
      "epoch": 2.2590635817088724,
      "grad_norm": 7.815605640411377,
      "learning_rate": 1.4310688205166483e-05,
      "loss": 3.2373,
      "step": 37200
    },
    {
      "epoch": 2.2596708568652457,
      "grad_norm": 6.633792877197266,
      "learning_rate": 1.4288426064166527e-05,
      "loss": 3.3931,
      "step": 37210
    },
    {
      "epoch": 2.260278132021619,
      "grad_norm": 7.128925323486328,
      "learning_rate": 1.4266178365495808e-05,
      "loss": 3.26,
      "step": 37220
    },
    {
      "epoch": 2.2608854071779922,
      "grad_norm": 8.639233589172363,
      "learning_rate": 1.4243945118151686e-05,
      "loss": 3.2345,
      "step": 37230
    },
    {
      "epoch": 2.2614926823343655,
      "grad_norm": 6.665154933929443,
      "learning_rate": 1.4221726331125624e-05,
      "loss": 3.3855,
      "step": 37240
    },
    {
      "epoch": 2.262099957490739,
      "grad_norm": 5.336401462554932,
      "learning_rate": 1.419952201340325e-05,
      "loss": 3.2956,
      "step": 37250
    },
    {
      "epoch": 2.2627072326471125,
      "grad_norm": 5.922295093536377,
      "learning_rate": 1.4177332173964346e-05,
      "loss": 3.4317,
      "step": 37260
    },
    {
      "epoch": 2.2633145078034858,
      "grad_norm": 7.454500198364258,
      "learning_rate": 1.4155156821782845e-05,
      "loss": 3.1801,
      "step": 37270
    },
    {
      "epoch": 2.263921782959859,
      "grad_norm": 7.787052631378174,
      "learning_rate": 1.4132995965826795e-05,
      "loss": 3.0438,
      "step": 37280
    },
    {
      "epoch": 2.2645290581162323,
      "grad_norm": 4.796835899353027,
      "learning_rate": 1.4110849615058397e-05,
      "loss": 3.097,
      "step": 37290
    },
    {
      "epoch": 2.265136333272606,
      "grad_norm": 7.689883708953857,
      "learning_rate": 1.4088717778434018e-05,
      "loss": 3.6052,
      "step": 37300
    },
    {
      "epoch": 2.2657436084289793,
      "grad_norm": 7.741863250732422,
      "learning_rate": 1.4066600464904106e-05,
      "loss": 3.2702,
      "step": 37310
    },
    {
      "epoch": 2.2663508835853525,
      "grad_norm": 6.795001029968262,
      "learning_rate": 1.4044497683413249e-05,
      "loss": 3.7572,
      "step": 37320
    },
    {
      "epoch": 2.266958158741726,
      "grad_norm": 10.225038528442383,
      "learning_rate": 1.4022409442900175e-05,
      "loss": 3.4132,
      "step": 37330
    },
    {
      "epoch": 2.267565433898099,
      "grad_norm": 6.024172306060791,
      "learning_rate": 1.4000335752297716e-05,
      "loss": 3.1274,
      "step": 37340
    },
    {
      "epoch": 2.2681727090544728,
      "grad_norm": 9.740255355834961,
      "learning_rate": 1.3978276620532826e-05,
      "loss": 3.2974,
      "step": 37350
    },
    {
      "epoch": 2.268779984210846,
      "grad_norm": 7.618648052215576,
      "learning_rate": 1.3956232056526564e-05,
      "loss": 3.6167,
      "step": 37360
    },
    {
      "epoch": 2.2693872593672193,
      "grad_norm": 8.034468650817871,
      "learning_rate": 1.3934202069194091e-05,
      "loss": 3.6137,
      "step": 37370
    },
    {
      "epoch": 2.2699945345235926,
      "grad_norm": 6.396188735961914,
      "learning_rate": 1.3912186667444716e-05,
      "loss": 3.5424,
      "step": 37380
    },
    {
      "epoch": 2.270601809679966,
      "grad_norm": 6.212900161743164,
      "learning_rate": 1.3890185860181804e-05,
      "loss": 3.4022,
      "step": 37390
    },
    {
      "epoch": 2.2712090848363395,
      "grad_norm": 7.013267517089844,
      "learning_rate": 1.3868199656302828e-05,
      "loss": 3.4321,
      "step": 37400
    },
    {
      "epoch": 2.271816359992713,
      "grad_norm": 7.778470993041992,
      "learning_rate": 1.3846228064699358e-05,
      "loss": 3.5713,
      "step": 37410
    },
    {
      "epoch": 2.272423635149086,
      "grad_norm": 6.481118202209473,
      "learning_rate": 1.3824271094257057e-05,
      "loss": 3.2041,
      "step": 37420
    },
    {
      "epoch": 2.2730309103054593,
      "grad_norm": 7.109966278076172,
      "learning_rate": 1.3802328753855676e-05,
      "loss": 3.4497,
      "step": 37430
    },
    {
      "epoch": 2.2736381854618326,
      "grad_norm": 7.392847061157227,
      "learning_rate": 1.3780401052369046e-05,
      "loss": 3.5669,
      "step": 37440
    },
    {
      "epoch": 2.2742454606182063,
      "grad_norm": 8.988951683044434,
      "learning_rate": 1.3758487998665065e-05,
      "loss": 3.4061,
      "step": 37450
    },
    {
      "epoch": 2.2748527357745796,
      "grad_norm": 7.882608890533447,
      "learning_rate": 1.3736589601605749e-05,
      "loss": 3.0101,
      "step": 37460
    },
    {
      "epoch": 2.275460010930953,
      "grad_norm": 7.840122699737549,
      "learning_rate": 1.371470587004714e-05,
      "loss": 2.8105,
      "step": 37470
    },
    {
      "epoch": 2.276067286087326,
      "grad_norm": 5.815730094909668,
      "learning_rate": 1.3692836812839388e-05,
      "loss": 2.9622,
      "step": 37480
    },
    {
      "epoch": 2.2766745612436994,
      "grad_norm": 6.8313140869140625,
      "learning_rate": 1.367098243882664e-05,
      "loss": 3.4416,
      "step": 37490
    },
    {
      "epoch": 2.277281836400073,
      "grad_norm": 5.136789321899414,
      "learning_rate": 1.3649142756847206e-05,
      "loss": 3.3391,
      "step": 37500
    },
    {
      "epoch": 2.2778891115564464,
      "grad_norm": 5.612852096557617,
      "learning_rate": 1.3627317775733372e-05,
      "loss": 3.4364,
      "step": 37510
    },
    {
      "epoch": 2.2784963867128196,
      "grad_norm": 4.917140007019043,
      "learning_rate": 1.3605507504311499e-05,
      "loss": 3.1501,
      "step": 37520
    },
    {
      "epoch": 2.279103661869193,
      "grad_norm": 5.7945051193237305,
      "learning_rate": 1.3583711951402045e-05,
      "loss": 3.4324,
      "step": 37530
    },
    {
      "epoch": 2.279710937025566,
      "grad_norm": 5.099568843841553,
      "learning_rate": 1.3561931125819471e-05,
      "loss": 3.5164,
      "step": 37540
    },
    {
      "epoch": 2.28031821218194,
      "grad_norm": 4.408764839172363,
      "learning_rate": 1.3540165036372266e-05,
      "loss": 3.3275,
      "step": 37550
    },
    {
      "epoch": 2.280925487338313,
      "grad_norm": 5.676374435424805,
      "learning_rate": 1.3518413691862974e-05,
      "loss": 3.2051,
      "step": 37560
    },
    {
      "epoch": 2.2815327624946864,
      "grad_norm": 4.748423099517822,
      "learning_rate": 1.3496677101088223e-05,
      "loss": 2.8376,
      "step": 37570
    },
    {
      "epoch": 2.2821400376510597,
      "grad_norm": 4.834485054016113,
      "learning_rate": 1.3474955272838619e-05,
      "loss": 3.0267,
      "step": 37580
    },
    {
      "epoch": 2.282747312807433,
      "grad_norm": 5.02852201461792,
      "learning_rate": 1.3453248215898812e-05,
      "loss": 3.0365,
      "step": 37590
    },
    {
      "epoch": 2.2833545879638066,
      "grad_norm": 5.23722505569458,
      "learning_rate": 1.3431555939047468e-05,
      "loss": 3.5456,
      "step": 37600
    },
    {
      "epoch": 2.28396186312018,
      "grad_norm": 5.412703990936279,
      "learning_rate": 1.340987845105734e-05,
      "loss": 3.1678,
      "step": 37610
    },
    {
      "epoch": 2.284569138276553,
      "grad_norm": 5.867762565612793,
      "learning_rate": 1.33882157606951e-05,
      "loss": 3.4464,
      "step": 37620
    },
    {
      "epoch": 2.2851764134329264,
      "grad_norm": 6.004922866821289,
      "learning_rate": 1.3366567876721503e-05,
      "loss": 3.3071,
      "step": 37630
    },
    {
      "epoch": 2.2857836885892997,
      "grad_norm": 5.411060810089111,
      "learning_rate": 1.3344934807891273e-05,
      "loss": 3.4246,
      "step": 37640
    },
    {
      "epoch": 2.2863909637456734,
      "grad_norm": 5.692368507385254,
      "learning_rate": 1.3323316562953215e-05,
      "loss": 3.3856,
      "step": 37650
    },
    {
      "epoch": 2.2869982389020467,
      "grad_norm": 5.266292572021484,
      "learning_rate": 1.3301713150650059e-05,
      "loss": 3.3122,
      "step": 37660
    },
    {
      "epoch": 2.28760551405842,
      "grad_norm": 9.26701831817627,
      "learning_rate": 1.3280124579718577e-05,
      "loss": 3.6092,
      "step": 37670
    },
    {
      "epoch": 2.288212789214793,
      "grad_norm": 6.160646915435791,
      "learning_rate": 1.3258550858889535e-05,
      "loss": 3.2153,
      "step": 37680
    },
    {
      "epoch": 2.2888200643711665,
      "grad_norm": 7.22013521194458,
      "learning_rate": 1.3236991996887676e-05,
      "loss": 3.7207,
      "step": 37690
    },
    {
      "epoch": 2.2894273395275397,
      "grad_norm": 6.3926239013671875,
      "learning_rate": 1.3215448002431762e-05,
      "loss": 3.7288,
      "step": 37700
    },
    {
      "epoch": 2.2900346146839134,
      "grad_norm": 5.804327011108398,
      "learning_rate": 1.3193918884234524e-05,
      "loss": 3.6323,
      "step": 37710
    },
    {
      "epoch": 2.2906418898402867,
      "grad_norm": 7.665761470794678,
      "learning_rate": 1.3172404651002657e-05,
      "loss": 3.466,
      "step": 37720
    },
    {
      "epoch": 2.29124916499666,
      "grad_norm": 6.043303489685059,
      "learning_rate": 1.3150905311436895e-05,
      "loss": 3.3691,
      "step": 37730
    },
    {
      "epoch": 2.2918564401530332,
      "grad_norm": 5.512628078460693,
      "learning_rate": 1.3129420874231902e-05,
      "loss": 3.2599,
      "step": 37740
    },
    {
      "epoch": 2.2924637153094065,
      "grad_norm": 7.491353988647461,
      "learning_rate": 1.3107951348076324e-05,
      "loss": 3.2497,
      "step": 37750
    },
    {
      "epoch": 2.2930709904657802,
      "grad_norm": 7.9470109939575195,
      "learning_rate": 1.3086496741652777e-05,
      "loss": 3.5311,
      "step": 37760
    },
    {
      "epoch": 2.2936782656221535,
      "grad_norm": 6.798891067504883,
      "learning_rate": 1.3065057063637841e-05,
      "loss": 3.4054,
      "step": 37770
    },
    {
      "epoch": 2.2942855407785268,
      "grad_norm": 7.692149639129639,
      "learning_rate": 1.3043632322702076e-05,
      "loss": 3.2838,
      "step": 37780
    },
    {
      "epoch": 2.2948928159349,
      "grad_norm": 7.148983001708984,
      "learning_rate": 1.3022222527509963e-05,
      "loss": 3.178,
      "step": 37790
    },
    {
      "epoch": 2.2955000910912733,
      "grad_norm": 5.314471244812012,
      "learning_rate": 1.3000827686720002e-05,
      "loss": 3.4348,
      "step": 37800
    },
    {
      "epoch": 2.296107366247647,
      "grad_norm": 6.819664001464844,
      "learning_rate": 1.2979447808984585e-05,
      "loss": 3.5368,
      "step": 37810
    },
    {
      "epoch": 2.2967146414040203,
      "grad_norm": 6.0334248542785645,
      "learning_rate": 1.2958082902950091e-05,
      "loss": 3.415,
      "step": 37820
    },
    {
      "epoch": 2.2973219165603935,
      "grad_norm": 4.516618728637695,
      "learning_rate": 1.2936732977256787e-05,
      "loss": 3.092,
      "step": 37830
    },
    {
      "epoch": 2.297929191716767,
      "grad_norm": 4.3100504875183105,
      "learning_rate": 1.2915398040538973e-05,
      "loss": 2.7033,
      "step": 37840
    },
    {
      "epoch": 2.29853646687314,
      "grad_norm": 5.269129753112793,
      "learning_rate": 1.2894078101424807e-05,
      "loss": 3.3653,
      "step": 37850
    },
    {
      "epoch": 2.2991437420295133,
      "grad_norm": 5.6334638595581055,
      "learning_rate": 1.2872773168536423e-05,
      "loss": 3.1625,
      "step": 37860
    },
    {
      "epoch": 2.299751017185887,
      "grad_norm": 4.235183238983154,
      "learning_rate": 1.285148325048986e-05,
      "loss": 3.2105,
      "step": 37870
    },
    {
      "epoch": 2.3003582923422603,
      "grad_norm": 6.455000877380371,
      "learning_rate": 1.2830208355895124e-05,
      "loss": 3.3332,
      "step": 37880
    },
    {
      "epoch": 2.3009655674986336,
      "grad_norm": 7.9344706535339355,
      "learning_rate": 1.2808948493356127e-05,
      "loss": 3.4815,
      "step": 37890
    },
    {
      "epoch": 2.301572842655007,
      "grad_norm": 7.606700897216797,
      "learning_rate": 1.2787703671470664e-05,
      "loss": 3.041,
      "step": 37900
    },
    {
      "epoch": 2.30218011781138,
      "grad_norm": 6.821325302124023,
      "learning_rate": 1.2766473898830477e-05,
      "loss": 3.0993,
      "step": 37910
    },
    {
      "epoch": 2.302787392967754,
      "grad_norm": 8.78971004486084,
      "learning_rate": 1.2745259184021263e-05,
      "loss": 3.185,
      "step": 37920
    },
    {
      "epoch": 2.303394668124127,
      "grad_norm": 10.726692199707031,
      "learning_rate": 1.2724059535622562e-05,
      "loss": 3.4981,
      "step": 37930
    },
    {
      "epoch": 2.3040019432805003,
      "grad_norm": 8.664220809936523,
      "learning_rate": 1.2702874962207861e-05,
      "loss": 3.6235,
      "step": 37940
    },
    {
      "epoch": 2.3046092184368736,
      "grad_norm": 6.6081695556640625,
      "learning_rate": 1.268170547234453e-05,
      "loss": 3.6974,
      "step": 37950
    },
    {
      "epoch": 2.305216493593247,
      "grad_norm": 6.105552673339844,
      "learning_rate": 1.2660551074593851e-05,
      "loss": 3.5371,
      "step": 37960
    },
    {
      "epoch": 2.3058237687496206,
      "grad_norm": 7.048062801361084,
      "learning_rate": 1.2639411777511007e-05,
      "loss": 3.4707,
      "step": 37970
    },
    {
      "epoch": 2.306431043905994,
      "grad_norm": 6.794731140136719,
      "learning_rate": 1.2618287589645039e-05,
      "loss": 3.5054,
      "step": 37980
    },
    {
      "epoch": 2.307038319062367,
      "grad_norm": 7.510888576507568,
      "learning_rate": 1.2597178519538944e-05,
      "loss": 3.1221,
      "step": 37990
    },
    {
      "epoch": 2.3076455942187404,
      "grad_norm": 9.007030487060547,
      "learning_rate": 1.2576084575729547e-05,
      "loss": 3.3384,
      "step": 38000
    },
    {
      "epoch": 2.3082528693751136,
      "grad_norm": 6.294415473937988,
      "learning_rate": 1.2555005766747574e-05,
      "loss": 3.5839,
      "step": 38010
    },
    {
      "epoch": 2.3088601445314874,
      "grad_norm": 6.945226669311523,
      "learning_rate": 1.2533942101117635e-05,
      "loss": 3.3244,
      "step": 38020
    },
    {
      "epoch": 2.3094674196878606,
      "grad_norm": 5.461344242095947,
      "learning_rate": 1.251289358735821e-05,
      "loss": 2.8982,
      "step": 38030
    },
    {
      "epoch": 2.310074694844234,
      "grad_norm": 6.2151198387146,
      "learning_rate": 1.2491860233981651e-05,
      "loss": 3.0507,
      "step": 38040
    },
    {
      "epoch": 2.310681970000607,
      "grad_norm": 6.680407524108887,
      "learning_rate": 1.2470842049494186e-05,
      "loss": 3.3494,
      "step": 38050
    },
    {
      "epoch": 2.3112892451569804,
      "grad_norm": 6.214541435241699,
      "learning_rate": 1.2449839042395883e-05,
      "loss": 3.7796,
      "step": 38060
    },
    {
      "epoch": 2.311896520313354,
      "grad_norm": 5.109645843505859,
      "learning_rate": 1.2428851221180726e-05,
      "loss": 3.5768,
      "step": 38070
    },
    {
      "epoch": 2.3125037954697274,
      "grad_norm": 5.531761169433594,
      "learning_rate": 1.2407878594336509e-05,
      "loss": 3.7223,
      "step": 38080
    },
    {
      "epoch": 2.3131110706261007,
      "grad_norm": 5.665055274963379,
      "learning_rate": 1.238692117034489e-05,
      "loss": 3.6792,
      "step": 38090
    },
    {
      "epoch": 2.313718345782474,
      "grad_norm": 6.18168830871582,
      "learning_rate": 1.2365978957681396e-05,
      "loss": 3.6133,
      "step": 38100
    },
    {
      "epoch": 2.314325620938847,
      "grad_norm": 4.868929862976074,
      "learning_rate": 1.2345051964815385e-05,
      "loss": 3.4406,
      "step": 38110
    },
    {
      "epoch": 2.314932896095221,
      "grad_norm": 4.986720561981201,
      "learning_rate": 1.2324140200210071e-05,
      "loss": 3.2765,
      "step": 38120
    },
    {
      "epoch": 2.315540171251594,
      "grad_norm": 5.592357158660889,
      "learning_rate": 1.2303243672322495e-05,
      "loss": 3.7866,
      "step": 38130
    },
    {
      "epoch": 2.3161474464079674,
      "grad_norm": 6.255531311035156,
      "learning_rate": 1.2282362389603542e-05,
      "loss": 3.5385,
      "step": 38140
    },
    {
      "epoch": 2.3167547215643407,
      "grad_norm": 6.4513702392578125,
      "learning_rate": 1.2261496360497954e-05,
      "loss": 3.698,
      "step": 38150
    },
    {
      "epoch": 2.317361996720714,
      "grad_norm": 8.03873348236084,
      "learning_rate": 1.2240645593444277e-05,
      "loss": 3.05,
      "step": 38160
    },
    {
      "epoch": 2.3179692718770877,
      "grad_norm": 5.102801322937012,
      "learning_rate": 1.2219810096874913e-05,
      "loss": 3.3202,
      "step": 38170
    },
    {
      "epoch": 2.318576547033461,
      "grad_norm": 4.682106971740723,
      "learning_rate": 1.2198989879216011e-05,
      "loss": 3.3037,
      "step": 38180
    },
    {
      "epoch": 2.319183822189834,
      "grad_norm": 6.462068557739258,
      "learning_rate": 1.2178184948887656e-05,
      "loss": 3.4245,
      "step": 38190
    },
    {
      "epoch": 2.3197910973462075,
      "grad_norm": 5.001280784606934,
      "learning_rate": 1.2157395314303672e-05,
      "loss": 3.3029,
      "step": 38200
    },
    {
      "epoch": 2.3203983725025807,
      "grad_norm": 5.570512294769287,
      "learning_rate": 1.2136620983871706e-05,
      "loss": 3.3674,
      "step": 38210
    },
    {
      "epoch": 2.3210056476589545,
      "grad_norm": 4.7310872077941895,
      "learning_rate": 1.2115861965993253e-05,
      "loss": 3.7419,
      "step": 38220
    },
    {
      "epoch": 2.3216129228153277,
      "grad_norm": 6.0501909255981445,
      "learning_rate": 1.2095118269063593e-05,
      "loss": 3.5093,
      "step": 38230
    },
    {
      "epoch": 2.322220197971701,
      "grad_norm": 6.187516212463379,
      "learning_rate": 1.2074389901471783e-05,
      "loss": 3.2551,
      "step": 38240
    },
    {
      "epoch": 2.3228274731280742,
      "grad_norm": 5.662460803985596,
      "learning_rate": 1.20536768716007e-05,
      "loss": 3.1228,
      "step": 38250
    },
    {
      "epoch": 2.3234347482844475,
      "grad_norm": 6.962975978851318,
      "learning_rate": 1.2032979187827048e-05,
      "loss": 3.4888,
      "step": 38260
    },
    {
      "epoch": 2.3240420234408212,
      "grad_norm": 9.938179016113281,
      "learning_rate": 1.2012296858521293e-05,
      "loss": 3.1964,
      "step": 38270
    },
    {
      "epoch": 2.3246492985971945,
      "grad_norm": 7.27351713180542,
      "learning_rate": 1.1991629892047695e-05,
      "loss": 3.4199,
      "step": 38280
    },
    {
      "epoch": 2.3252565737535678,
      "grad_norm": 7.2438154220581055,
      "learning_rate": 1.1970978296764313e-05,
      "loss": 3.5071,
      "step": 38290
    },
    {
      "epoch": 2.325863848909941,
      "grad_norm": 11.508769989013672,
      "learning_rate": 1.1950342081022963e-05,
      "loss": 3.8856,
      "step": 38300
    },
    {
      "epoch": 2.3264711240663143,
      "grad_norm": 9.560524940490723,
      "learning_rate": 1.1929721253169274e-05,
      "loss": 3.6796,
      "step": 38310
    },
    {
      "epoch": 2.327078399222688,
      "grad_norm": 11.971531867980957,
      "learning_rate": 1.1909115821542633e-05,
      "loss": 3.4873,
      "step": 38320
    },
    {
      "epoch": 2.3276856743790613,
      "grad_norm": 12.692415237426758,
      "learning_rate": 1.1888525794476186e-05,
      "loss": 3.4364,
      "step": 38330
    },
    {
      "epoch": 2.3282929495354345,
      "grad_norm": 8.526453971862793,
      "learning_rate": 1.1867951180296904e-05,
      "loss": 3.3134,
      "step": 38340
    },
    {
      "epoch": 2.328900224691808,
      "grad_norm": 9.312342643737793,
      "learning_rate": 1.1847391987325474e-05,
      "loss": 3.7778,
      "step": 38350
    },
    {
      "epoch": 2.329507499848181,
      "grad_norm": 9.887785911560059,
      "learning_rate": 1.1826848223876352e-05,
      "loss": 3.8398,
      "step": 38360
    },
    {
      "epoch": 2.3301147750045548,
      "grad_norm": 8.214704513549805,
      "learning_rate": 1.1806319898257772e-05,
      "loss": 3.8427,
      "step": 38370
    },
    {
      "epoch": 2.330722050160928,
      "grad_norm": 8.139571189880371,
      "learning_rate": 1.1785807018771711e-05,
      "loss": 3.6077,
      "step": 38380
    },
    {
      "epoch": 2.3313293253173013,
      "grad_norm": 8.684921264648438,
      "learning_rate": 1.1765309593713908e-05,
      "loss": 3.2509,
      "step": 38390
    },
    {
      "epoch": 2.3319366004736746,
      "grad_norm": 6.546315670013428,
      "learning_rate": 1.174482763137385e-05,
      "loss": 3.4937,
      "step": 38400
    },
    {
      "epoch": 2.332543875630048,
      "grad_norm": 9.00413703918457,
      "learning_rate": 1.1724361140034745e-05,
      "loss": 3.6454,
      "step": 38410
    },
    {
      "epoch": 2.3331511507864215,
      "grad_norm": 8.324052810668945,
      "learning_rate": 1.1703910127973606e-05,
      "loss": 3.517,
      "step": 38420
    },
    {
      "epoch": 2.333758425942795,
      "grad_norm": 10.81627368927002,
      "learning_rate": 1.1683474603461125e-05,
      "loss": 3.2207,
      "step": 38430
    },
    {
      "epoch": 2.334365701099168,
      "grad_norm": 7.744399070739746,
      "learning_rate": 1.1663054574761761e-05,
      "loss": 3.2757,
      "step": 38440
    },
    {
      "epoch": 2.3349729762555413,
      "grad_norm": 7.826662540435791,
      "learning_rate": 1.1642650050133703e-05,
      "loss": 3.3541,
      "step": 38450
    },
    {
      "epoch": 2.3355802514119146,
      "grad_norm": 6.067100524902344,
      "learning_rate": 1.1622261037828853e-05,
      "loss": 3.0724,
      "step": 38460
    },
    {
      "epoch": 2.3361875265682883,
      "grad_norm": 6.023543357849121,
      "learning_rate": 1.160188754609286e-05,
      "loss": 3.1456,
      "step": 38470
    },
    {
      "epoch": 2.3367948017246616,
      "grad_norm": 7.236881256103516,
      "learning_rate": 1.1581529583165067e-05,
      "loss": 3.3325,
      "step": 38480
    },
    {
      "epoch": 2.337402076881035,
      "grad_norm": 6.382696151733398,
      "learning_rate": 1.1561187157278602e-05,
      "loss": 3.3723,
      "step": 38490
    },
    {
      "epoch": 2.338009352037408,
      "grad_norm": 4.9043097496032715,
      "learning_rate": 1.1540860276660236e-05,
      "loss": 3.4176,
      "step": 38500
    },
    {
      "epoch": 2.3386166271937814,
      "grad_norm": 5.884983539581299,
      "learning_rate": 1.1520548949530507e-05,
      "loss": 3.4494,
      "step": 38510
    },
    {
      "epoch": 2.339223902350155,
      "grad_norm": 6.698777198791504,
      "learning_rate": 1.1500253184103592e-05,
      "loss": 3.2888,
      "step": 38520
    },
    {
      "epoch": 2.3398311775065284,
      "grad_norm": 6.4762468338012695,
      "learning_rate": 1.1479972988587467e-05,
      "loss": 3.4127,
      "step": 38530
    },
    {
      "epoch": 2.3404384526629016,
      "grad_norm": 8.679746627807617,
      "learning_rate": 1.1459708371183752e-05,
      "loss": 3.5581,
      "step": 38540
    },
    {
      "epoch": 2.341045727819275,
      "grad_norm": 7.3214874267578125,
      "learning_rate": 1.1439459340087776e-05,
      "loss": 3.1011,
      "step": 38550
    },
    {
      "epoch": 2.341653002975648,
      "grad_norm": 7.508699893951416,
      "learning_rate": 1.1419225903488561e-05,
      "loss": 3.6538,
      "step": 38560
    },
    {
      "epoch": 2.342260278132022,
      "grad_norm": 7.462082386016846,
      "learning_rate": 1.1399008069568873e-05,
      "loss": 3.6442,
      "step": 38570
    },
    {
      "epoch": 2.342867553288395,
      "grad_norm": 6.270242691040039,
      "learning_rate": 1.137880584650508e-05,
      "loss": 3.5723,
      "step": 38580
    },
    {
      "epoch": 2.3434748284447684,
      "grad_norm": 5.637196063995361,
      "learning_rate": 1.1358619242467305e-05,
      "loss": 3.1985,
      "step": 38590
    },
    {
      "epoch": 2.3440821036011417,
      "grad_norm": 4.670695781707764,
      "learning_rate": 1.1338448265619306e-05,
      "loss": 3.3127,
      "step": 38600
    },
    {
      "epoch": 2.344689378757515,
      "grad_norm": 6.318118572235107,
      "learning_rate": 1.1318292924118584e-05,
      "loss": 3.3381,
      "step": 38610
    },
    {
      "epoch": 2.345296653913888,
      "grad_norm": 4.843393325805664,
      "learning_rate": 1.1298153226116265e-05,
      "loss": 3.1351,
      "step": 38620
    },
    {
      "epoch": 2.345903929070262,
      "grad_norm": 5.467980861663818,
      "learning_rate": 1.127802917975716e-05,
      "loss": 3.0747,
      "step": 38630
    },
    {
      "epoch": 2.346511204226635,
      "grad_norm": 3.9031057357788086,
      "learning_rate": 1.125792079317976e-05,
      "loss": 3.1382,
      "step": 38640
    },
    {
      "epoch": 2.3471184793830084,
      "grad_norm": 6.29188346862793,
      "learning_rate": 1.123782807451621e-05,
      "loss": 3.0854,
      "step": 38650
    },
    {
      "epoch": 2.3477257545393817,
      "grad_norm": 6.901821136474609,
      "learning_rate": 1.1217751031892326e-05,
      "loss": 3.3595,
      "step": 38660
    },
    {
      "epoch": 2.348333029695755,
      "grad_norm": 5.7325263023376465,
      "learning_rate": 1.1197689673427581e-05,
      "loss": 3.3389,
      "step": 38670
    },
    {
      "epoch": 2.3489403048521287,
      "grad_norm": 7.191955089569092,
      "learning_rate": 1.1177644007235116e-05,
      "loss": 3.2019,
      "step": 38680
    },
    {
      "epoch": 2.349547580008502,
      "grad_norm": 8.978702545166016,
      "learning_rate": 1.1157614041421722e-05,
      "loss": 3.3128,
      "step": 38690
    },
    {
      "epoch": 2.350154855164875,
      "grad_norm": 8.94212532043457,
      "learning_rate": 1.1137599784087827e-05,
      "loss": 3.707,
      "step": 38700
    },
    {
      "epoch": 2.3507621303212485,
      "grad_norm": 8.429515838623047,
      "learning_rate": 1.1117601243327518e-05,
      "loss": 3.5031,
      "step": 38710
    },
    {
      "epoch": 2.3513694054776217,
      "grad_norm": 7.230405807495117,
      "learning_rate": 1.1097618427228524e-05,
      "loss": 3.1534,
      "step": 38720
    },
    {
      "epoch": 2.3519766806339955,
      "grad_norm": 6.662740230560303,
      "learning_rate": 1.1077651343872214e-05,
      "loss": 3.7322,
      "step": 38730
    },
    {
      "epoch": 2.3525839557903687,
      "grad_norm": 6.647398948669434,
      "learning_rate": 1.1057700001333588e-05,
      "loss": 3.7285,
      "step": 38740
    },
    {
      "epoch": 2.353191230946742,
      "grad_norm": 8.086933135986328,
      "learning_rate": 1.1037764407681284e-05,
      "loss": 3.5525,
      "step": 38750
    },
    {
      "epoch": 2.3537985061031153,
      "grad_norm": 9.043636322021484,
      "learning_rate": 1.1017844570977592e-05,
      "loss": 3.4843,
      "step": 38760
    },
    {
      "epoch": 2.3544057812594885,
      "grad_norm": 7.400579452514648,
      "learning_rate": 1.0997940499278403e-05,
      "loss": 3.1885,
      "step": 38770
    },
    {
      "epoch": 2.355013056415862,
      "grad_norm": 7.923943519592285,
      "learning_rate": 1.0978052200633238e-05,
      "loss": 3.7088,
      "step": 38780
    },
    {
      "epoch": 2.3556203315722355,
      "grad_norm": 6.0839667320251465,
      "learning_rate": 1.095817968308524e-05,
      "loss": 3.532,
      "step": 38790
    },
    {
      "epoch": 2.3562276067286088,
      "grad_norm": 7.396910190582275,
      "learning_rate": 1.0938322954671177e-05,
      "loss": 3.6286,
      "step": 38800
    },
    {
      "epoch": 2.356834881884982,
      "grad_norm": 6.383071422576904,
      "learning_rate": 1.0918482023421416e-05,
      "loss": 3.196,
      "step": 38810
    },
    {
      "epoch": 2.3574421570413553,
      "grad_norm": 6.690852642059326,
      "learning_rate": 1.089865689735996e-05,
      "loss": 3.2394,
      "step": 38820
    },
    {
      "epoch": 2.3580494321977286,
      "grad_norm": 7.0781779289245605,
      "learning_rate": 1.087884758450438e-05,
      "loss": 3.2192,
      "step": 38830
    },
    {
      "epoch": 2.3586567073541023,
      "grad_norm": 6.749350547790527,
      "learning_rate": 1.0859054092865917e-05,
      "loss": 2.7247,
      "step": 38840
    },
    {
      "epoch": 2.3592639825104755,
      "grad_norm": 4.299288749694824,
      "learning_rate": 1.0839276430449358e-05,
      "loss": 2.6141,
      "step": 38850
    },
    {
      "epoch": 2.359871257666849,
      "grad_norm": 6.243557929992676,
      "learning_rate": 1.0819514605253095e-05,
      "loss": 2.7909,
      "step": 38860
    },
    {
      "epoch": 2.360478532823222,
      "grad_norm": 6.279702663421631,
      "learning_rate": 1.0799768625269107e-05,
      "loss": 2.9638,
      "step": 38870
    },
    {
      "epoch": 2.3610858079795953,
      "grad_norm": 6.015127182006836,
      "learning_rate": 1.0780038498483025e-05,
      "loss": 3.2783,
      "step": 38880
    },
    {
      "epoch": 2.361693083135969,
      "grad_norm": 5.485583305358887,
      "learning_rate": 1.0760324232874002e-05,
      "loss": 3.6332,
      "step": 38890
    },
    {
      "epoch": 2.3623003582923423,
      "grad_norm": 7.4647603034973145,
      "learning_rate": 1.0740625836414797e-05,
      "loss": 3.5782,
      "step": 38900
    },
    {
      "epoch": 2.3629076334487156,
      "grad_norm": 9.480755805969238,
      "learning_rate": 1.072094331707178e-05,
      "loss": 3.2309,
      "step": 38910
    },
    {
      "epoch": 2.363514908605089,
      "grad_norm": 9.539761543273926,
      "learning_rate": 1.0701276682804868e-05,
      "loss": 3.3533,
      "step": 38920
    },
    {
      "epoch": 2.364122183761462,
      "grad_norm": 8.907629013061523,
      "learning_rate": 1.0681625941567541e-05,
      "loss": 3.2189,
      "step": 38930
    },
    {
      "epoch": 2.364729458917836,
      "grad_norm": 5.325688362121582,
      "learning_rate": 1.0661991101306868e-05,
      "loss": 3.4725,
      "step": 38940
    },
    {
      "epoch": 2.365336734074209,
      "grad_norm": 4.789928436279297,
      "learning_rate": 1.0642372169963522e-05,
      "loss": 3.3233,
      "step": 38950
    },
    {
      "epoch": 2.3659440092305823,
      "grad_norm": 5.541765213012695,
      "learning_rate": 1.0622769155471696e-05,
      "loss": 3.1509,
      "step": 38960
    },
    {
      "epoch": 2.3665512843869556,
      "grad_norm": 6.293309688568115,
      "learning_rate": 1.060318206575916e-05,
      "loss": 3.2281,
      "step": 38970
    },
    {
      "epoch": 2.367158559543329,
      "grad_norm": 6.331206321716309,
      "learning_rate": 1.058361090874725e-05,
      "loss": 3.4287,
      "step": 38980
    },
    {
      "epoch": 2.3677658346997026,
      "grad_norm": 7.7340922355651855,
      "learning_rate": 1.0564055692350844e-05,
      "loss": 3.2887,
      "step": 38990
    },
    {
      "epoch": 2.368373109856076,
      "grad_norm": 6.805657386779785,
      "learning_rate": 1.054451642447839e-05,
      "loss": 3.1924,
      "step": 39000
    },
    {
      "epoch": 2.368980385012449,
      "grad_norm": 7.702133655548096,
      "learning_rate": 1.0524993113031877e-05,
      "loss": 3.2489,
      "step": 39010
    },
    {
      "epoch": 2.3695876601688224,
      "grad_norm": 6.598385334014893,
      "learning_rate": 1.0505485765906836e-05,
      "loss": 3.3071,
      "step": 39020
    },
    {
      "epoch": 2.3701949353251957,
      "grad_norm": 9.251301765441895,
      "learning_rate": 1.0485994390992365e-05,
      "loss": 3.3755,
      "step": 39030
    },
    {
      "epoch": 2.3708022104815694,
      "grad_norm": 8.627951622009277,
      "learning_rate": 1.0466518996171082e-05,
      "loss": 3.5954,
      "step": 39040
    },
    {
      "epoch": 2.3714094856379426,
      "grad_norm": 7.6185808181762695,
      "learning_rate": 1.0447059589319136e-05,
      "loss": 3.6321,
      "step": 39050
    },
    {
      "epoch": 2.372016760794316,
      "grad_norm": 6.515222072601318,
      "learning_rate": 1.0427616178306233e-05,
      "loss": 3.8103,
      "step": 39060
    },
    {
      "epoch": 2.372624035950689,
      "grad_norm": 7.044720649719238,
      "learning_rate": 1.0408188770995591e-05,
      "loss": 3.8659,
      "step": 39070
    },
    {
      "epoch": 2.3732313111070624,
      "grad_norm": 9.349793434143066,
      "learning_rate": 1.038877737524397e-05,
      "loss": 3.973,
      "step": 39080
    },
    {
      "epoch": 2.373838586263436,
      "grad_norm": 8.368269920349121,
      "learning_rate": 1.0369381998901639e-05,
      "loss": 3.5702,
      "step": 39090
    },
    {
      "epoch": 2.3744458614198094,
      "grad_norm": 8.707915306091309,
      "learning_rate": 1.0350002649812384e-05,
      "loss": 3.5544,
      "step": 39100
    },
    {
      "epoch": 2.3750531365761827,
      "grad_norm": 10.57526683807373,
      "learning_rate": 1.0330639335813558e-05,
      "loss": 3.3986,
      "step": 39110
    },
    {
      "epoch": 2.375660411732556,
      "grad_norm": 6.0436577796936035,
      "learning_rate": 1.0311292064735972e-05,
      "loss": 3.4903,
      "step": 39120
    },
    {
      "epoch": 2.376267686888929,
      "grad_norm": 7.659472465515137,
      "learning_rate": 1.0291960844403975e-05,
      "loss": 3.6162,
      "step": 39130
    },
    {
      "epoch": 2.376874962045303,
      "grad_norm": 7.451832294464111,
      "learning_rate": 1.0272645682635418e-05,
      "loss": 3.6821,
      "step": 39140
    },
    {
      "epoch": 2.377482237201676,
      "grad_norm": 6.806495666503906,
      "learning_rate": 1.0253346587241657e-05,
      "loss": 3.6598,
      "step": 39150
    },
    {
      "epoch": 2.3780895123580494,
      "grad_norm": 8.320223808288574,
      "learning_rate": 1.0234063566027563e-05,
      "loss": 3.5372,
      "step": 39160
    },
    {
      "epoch": 2.3786967875144227,
      "grad_norm": 6.088877201080322,
      "learning_rate": 1.0214796626791474e-05,
      "loss": 3.4027,
      "step": 39170
    },
    {
      "epoch": 2.379304062670796,
      "grad_norm": 7.665440082550049,
      "learning_rate": 1.0195545777325282e-05,
      "loss": 3.789,
      "step": 39180
    },
    {
      "epoch": 2.3799113378271697,
      "grad_norm": 7.38721227645874,
      "learning_rate": 1.0176311025414315e-05,
      "loss": 3.7036,
      "step": 39190
    },
    {
      "epoch": 2.380518612983543,
      "grad_norm": 6.715696811676025,
      "learning_rate": 1.015709237883743e-05,
      "loss": 4.2105,
      "step": 39200
    },
    {
      "epoch": 2.381125888139916,
      "grad_norm": 5.024934768676758,
      "learning_rate": 1.0137889845366916e-05,
      "loss": 3.5208,
      "step": 39210
    },
    {
      "epoch": 2.3817331632962895,
      "grad_norm": 6.706210136413574,
      "learning_rate": 1.0118703432768623e-05,
      "loss": 3.4077,
      "step": 39220
    },
    {
      "epoch": 2.3823404384526627,
      "grad_norm": 4.345231056213379,
      "learning_rate": 1.0099533148801826e-05,
      "loss": 3.3103,
      "step": 39230
    },
    {
      "epoch": 2.3829477136090365,
      "grad_norm": 4.639240264892578,
      "learning_rate": 1.0080379001219292e-05,
      "loss": 3.3711,
      "step": 39240
    },
    {
      "epoch": 2.3835549887654097,
      "grad_norm": 6.359861850738525,
      "learning_rate": 1.0061240997767257e-05,
      "loss": 3.4402,
      "step": 39250
    },
    {
      "epoch": 2.384162263921783,
      "grad_norm": 6.440340042114258,
      "learning_rate": 1.004211914618547e-05,
      "loss": 3.3255,
      "step": 39260
    },
    {
      "epoch": 2.3847695390781563,
      "grad_norm": 5.594629287719727,
      "learning_rate": 1.002301345420707e-05,
      "loss": 3.2171,
      "step": 39270
    },
    {
      "epoch": 2.3853768142345295,
      "grad_norm": 5.910820007324219,
      "learning_rate": 1.0003923929558717e-05,
      "loss": 3.3114,
      "step": 39280
    },
    {
      "epoch": 2.3859840893909032,
      "grad_norm": 5.86203145980835,
      "learning_rate": 9.984850579960508e-06,
      "loss": 3.0375,
      "step": 39290
    },
    {
      "epoch": 2.3865913645472765,
      "grad_norm": 6.048990726470947,
      "learning_rate": 9.965793413126035e-06,
      "loss": 3.2882,
      "step": 39300
    },
    {
      "epoch": 2.3871986397036498,
      "grad_norm": 8.204237937927246,
      "learning_rate": 9.946752436762309e-06,
      "loss": 3.4577,
      "step": 39310
    },
    {
      "epoch": 2.387805914860023,
      "grad_norm": 6.640039443969727,
      "learning_rate": 9.927727658569796e-06,
      "loss": 3.2506,
      "step": 39320
    },
    {
      "epoch": 2.3884131900163963,
      "grad_norm": 7.283810615539551,
      "learning_rate": 9.908719086242429e-06,
      "loss": 3.5623,
      "step": 39330
    },
    {
      "epoch": 2.38902046517277,
      "grad_norm": 7.854767799377441,
      "learning_rate": 9.889726727467574e-06,
      "loss": 3.7209,
      "step": 39340
    },
    {
      "epoch": 2.3896277403291433,
      "grad_norm": 7.915143013000488,
      "learning_rate": 9.870750589926042e-06,
      "loss": 3.4163,
      "step": 39350
    },
    {
      "epoch": 2.3902350154855165,
      "grad_norm": 6.678798198699951,
      "learning_rate": 9.851790681292073e-06,
      "loss": 3.2342,
      "step": 39360
    },
    {
      "epoch": 2.39084229064189,
      "grad_norm": 6.472805023193359,
      "learning_rate": 9.832847009233381e-06,
      "loss": 3.9989,
      "step": 39370
    },
    {
      "epoch": 2.391449565798263,
      "grad_norm": 5.342866897583008,
      "learning_rate": 9.813919581411074e-06,
      "loss": 3.5468,
      "step": 39380
    },
    {
      "epoch": 2.3920568409546368,
      "grad_norm": 4.672701358795166,
      "learning_rate": 9.79500840547971e-06,
      "loss": 3.5297,
      "step": 39390
    },
    {
      "epoch": 2.39266411611101,
      "grad_norm": 6.014372825622559,
      "learning_rate": 9.776113489087274e-06,
      "loss": 3.2024,
      "step": 39400
    },
    {
      "epoch": 2.3932713912673833,
      "grad_norm": 4.664095401763916,
      "learning_rate": 9.757234839875156e-06,
      "loss": 3.5208,
      "step": 39410
    },
    {
      "epoch": 2.3938786664237566,
      "grad_norm": 3.3975648880004883,
      "learning_rate": 9.738372465478196e-06,
      "loss": 3.3261,
      "step": 39420
    },
    {
      "epoch": 2.39448594158013,
      "grad_norm": 6.070305824279785,
      "learning_rate": 9.719526373524628e-06,
      "loss": 3.3038,
      "step": 39430
    },
    {
      "epoch": 2.3950932167365035,
      "grad_norm": 4.88860559463501,
      "learning_rate": 9.700696571636108e-06,
      "loss": 3.36,
      "step": 39440
    },
    {
      "epoch": 2.395700491892877,
      "grad_norm": 5.111954212188721,
      "learning_rate": 9.681883067427732e-06,
      "loss": 3.5326,
      "step": 39450
    },
    {
      "epoch": 2.39630776704925,
      "grad_norm": 5.942185878753662,
      "learning_rate": 9.663085868507965e-06,
      "loss": 3.0283,
      "step": 39460
    },
    {
      "epoch": 2.3969150422056233,
      "grad_norm": 6.426772594451904,
      "learning_rate": 9.644304982478697e-06,
      "loss": 3.2309,
      "step": 39470
    },
    {
      "epoch": 2.3975223173619966,
      "grad_norm": 8.45239543914795,
      "learning_rate": 9.625540416935213e-06,
      "loss": 3.5366,
      "step": 39480
    },
    {
      "epoch": 2.3981295925183703,
      "grad_norm": 6.066720008850098,
      "learning_rate": 9.6067921794662e-06,
      "loss": 3.4649,
      "step": 39490
    },
    {
      "epoch": 2.3987368676747436,
      "grad_norm": 3.988680601119995,
      "learning_rate": 9.588060277653754e-06,
      "loss": 3.4773,
      "step": 39500
    },
    {
      "epoch": 2.399344142831117,
      "grad_norm": 4.705049991607666,
      "learning_rate": 9.569344719073347e-06,
      "loss": 3.3787,
      "step": 39510
    },
    {
      "epoch": 2.39995141798749,
      "grad_norm": 5.424040794372559,
      "learning_rate": 9.550645511293832e-06,
      "loss": 3.4673,
      "step": 39520
    },
    {
      "epoch": 2.4005586931438634,
      "grad_norm": 8.014299392700195,
      "learning_rate": 9.531962661877502e-06,
      "loss": 3.5345,
      "step": 39530
    },
    {
      "epoch": 2.4011659683002367,
      "grad_norm": 8.02953052520752,
      "learning_rate": 9.513296178379999e-06,
      "loss": 3.2346,
      "step": 39540
    },
    {
      "epoch": 2.4017732434566104,
      "grad_norm": 6.842041015625,
      "learning_rate": 9.49464606835031e-06,
      "loss": 3.741,
      "step": 39550
    },
    {
      "epoch": 2.4023805186129836,
      "grad_norm": 6.150273323059082,
      "learning_rate": 9.476012339330841e-06,
      "loss": 3.5986,
      "step": 39560
    },
    {
      "epoch": 2.402987793769357,
      "grad_norm": 8.242259979248047,
      "learning_rate": 9.457394998857405e-06,
      "loss": 3.7975,
      "step": 39570
    },
    {
      "epoch": 2.40359506892573,
      "grad_norm": 6.307490348815918,
      "learning_rate": 9.438794054459121e-06,
      "loss": 3.4367,
      "step": 39580
    },
    {
      "epoch": 2.4042023440821034,
      "grad_norm": 5.51177978515625,
      "learning_rate": 9.420209513658507e-06,
      "loss": 3.6419,
      "step": 39590
    },
    {
      "epoch": 2.404809619238477,
      "grad_norm": 6.6482672691345215,
      "learning_rate": 9.401641383971477e-06,
      "loss": 3.2913,
      "step": 39600
    },
    {
      "epoch": 2.4054168943948504,
      "grad_norm": 7.143806457519531,
      "learning_rate": 9.383089672907247e-06,
      "loss": 3.0984,
      "step": 39610
    },
    {
      "epoch": 2.4060241695512237,
      "grad_norm": 7.856669902801514,
      "learning_rate": 9.364554387968433e-06,
      "loss": 3.2746,
      "step": 39620
    },
    {
      "epoch": 2.406631444707597,
      "grad_norm": 7.406698703765869,
      "learning_rate": 9.346035536650987e-06,
      "loss": 3.7686,
      "step": 39630
    },
    {
      "epoch": 2.40723871986397,
      "grad_norm": 7.831009387969971,
      "learning_rate": 9.32753312644426e-06,
      "loss": 3.7511,
      "step": 39640
    },
    {
      "epoch": 2.407845995020344,
      "grad_norm": 7.178554534912109,
      "learning_rate": 9.309047164830898e-06,
      "loss": 3.5375,
      "step": 39650
    },
    {
      "epoch": 2.408453270176717,
      "grad_norm": 7.474491596221924,
      "learning_rate": 9.290577659286925e-06,
      "loss": 3.2345,
      "step": 39660
    },
    {
      "epoch": 2.4090605453330904,
      "grad_norm": 6.476565361022949,
      "learning_rate": 9.272124617281697e-06,
      "loss": 3.4928,
      "step": 39670
    },
    {
      "epoch": 2.4096678204894637,
      "grad_norm": 5.714512825012207,
      "learning_rate": 9.253688046277926e-06,
      "loss": 3.628,
      "step": 39680
    },
    {
      "epoch": 2.410275095645837,
      "grad_norm": 5.428339958190918,
      "learning_rate": 9.235267953731652e-06,
      "loss": 3.4529,
      "step": 39690
    },
    {
      "epoch": 2.4108823708022102,
      "grad_norm": 5.893852233886719,
      "learning_rate": 9.216864347092246e-06,
      "loss": 3.244,
      "step": 39700
    },
    {
      "epoch": 2.411489645958584,
      "grad_norm": 8.329655647277832,
      "learning_rate": 9.198477233802422e-06,
      "loss": 3.1389,
      "step": 39710
    },
    {
      "epoch": 2.412096921114957,
      "grad_norm": 6.992143630981445,
      "learning_rate": 9.180106621298235e-06,
      "loss": 3.0295,
      "step": 39720
    },
    {
      "epoch": 2.4127041962713305,
      "grad_norm": 4.926278114318848,
      "learning_rate": 9.161752517009048e-06,
      "loss": 3.1047,
      "step": 39730
    },
    {
      "epoch": 2.4133114714277037,
      "grad_norm": 6.018010139465332,
      "learning_rate": 9.14341492835754e-06,
      "loss": 3.0839,
      "step": 39740
    },
    {
      "epoch": 2.413918746584077,
      "grad_norm": 6.957597732543945,
      "learning_rate": 9.125093862759743e-06,
      "loss": 3.4803,
      "step": 39750
    },
    {
      "epoch": 2.4145260217404507,
      "grad_norm": 7.911545276641846,
      "learning_rate": 9.106789327624975e-06,
      "loss": 3.2795,
      "step": 39760
    },
    {
      "epoch": 2.415133296896824,
      "grad_norm": 6.154027462005615,
      "learning_rate": 9.088501330355886e-06,
      "loss": 3.3452,
      "step": 39770
    },
    {
      "epoch": 2.4157405720531973,
      "grad_norm": 5.890086650848389,
      "learning_rate": 9.070229878348429e-06,
      "loss": 3.6555,
      "step": 39780
    },
    {
      "epoch": 2.4163478472095705,
      "grad_norm": 5.274627208709717,
      "learning_rate": 9.051974978991862e-06,
      "loss": 3.4588,
      "step": 39790
    },
    {
      "epoch": 2.416955122365944,
      "grad_norm": 5.329318523406982,
      "learning_rate": 9.033736639668777e-06,
      "loss": 3.3805,
      "step": 39800
    },
    {
      "epoch": 2.4175623975223175,
      "grad_norm": 6.82871675491333,
      "learning_rate": 9.015514867755043e-06,
      "loss": 3.4646,
      "step": 39810
    },
    {
      "epoch": 2.4181696726786908,
      "grad_norm": 7.881264686584473,
      "learning_rate": 8.997309670619836e-06,
      "loss": 3.473,
      "step": 39820
    },
    {
      "epoch": 2.418776947835064,
      "grad_norm": 8.196819305419922,
      "learning_rate": 8.979121055625617e-06,
      "loss": 3.6189,
      "step": 39830
    },
    {
      "epoch": 2.4193842229914373,
      "grad_norm": 8.96748161315918,
      "learning_rate": 8.960949030128162e-06,
      "loss": 3.7778,
      "step": 39840
    },
    {
      "epoch": 2.4199914981478106,
      "grad_norm": 8.00624942779541,
      "learning_rate": 8.942793601476518e-06,
      "loss": 3.8921,
      "step": 39850
    },
    {
      "epoch": 2.4205987733041843,
      "grad_norm": 6.526425361633301,
      "learning_rate": 8.92465477701303e-06,
      "loss": 3.5794,
      "step": 39860
    },
    {
      "epoch": 2.4212060484605575,
      "grad_norm": 7.628190040588379,
      "learning_rate": 8.906532564073339e-06,
      "loss": 3.9061,
      "step": 39870
    },
    {
      "epoch": 2.421813323616931,
      "grad_norm": 8.341195106506348,
      "learning_rate": 8.888426969986368e-06,
      "loss": 3.3788,
      "step": 39880
    },
    {
      "epoch": 2.422420598773304,
      "grad_norm": 7.690824031829834,
      "learning_rate": 8.870338002074275e-06,
      "loss": 3.4492,
      "step": 39890
    },
    {
      "epoch": 2.4230278739296773,
      "grad_norm": 7.081217288970947,
      "learning_rate": 8.852265667652525e-06,
      "loss": 3.599,
      "step": 39900
    },
    {
      "epoch": 2.423635149086051,
      "grad_norm": 6.342838287353516,
      "learning_rate": 8.834209974029888e-06,
      "loss": 3.2533,
      "step": 39910
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 6.173468589782715,
      "learning_rate": 8.816170928508365e-06,
      "loss": 3.6716,
      "step": 39920
    },
    {
      "epoch": 2.4248496993987976,
      "grad_norm": 6.9872307777404785,
      "learning_rate": 8.798148538383228e-06,
      "loss": 3.1939,
      "step": 39930
    },
    {
      "epoch": 2.425456974555171,
      "grad_norm": 9.730040550231934,
      "learning_rate": 8.780142810943004e-06,
      "loss": 3.3475,
      "step": 39940
    },
    {
      "epoch": 2.426064249711544,
      "grad_norm": 6.055309772491455,
      "learning_rate": 8.762153753469537e-06,
      "loss": 3.317,
      "step": 39950
    },
    {
      "epoch": 2.426671524867918,
      "grad_norm": 7.169532775878906,
      "learning_rate": 8.744181373237848e-06,
      "loss": 3.3879,
      "step": 39960
    },
    {
      "epoch": 2.427278800024291,
      "grad_norm": 7.739787578582764,
      "learning_rate": 8.72622567751627e-06,
      "loss": 3.3993,
      "step": 39970
    },
    {
      "epoch": 2.4278860751806643,
      "grad_norm": 6.229726791381836,
      "learning_rate": 8.708286673566357e-06,
      "loss": 3.571,
      "step": 39980
    },
    {
      "epoch": 2.4284933503370376,
      "grad_norm": 6.394381046295166,
      "learning_rate": 8.690364368642956e-06,
      "loss": 3.2725,
      "step": 39990
    },
    {
      "epoch": 2.429100625493411,
      "grad_norm": 5.143957614898682,
      "learning_rate": 8.672458769994119e-06,
      "loss": 3.5042,
      "step": 40000
    },
    {
      "epoch": 2.429100625493411,
      "eval_loss": 4.297228813171387,
      "eval_runtime": 2119.215,
      "eval_samples_per_second": 7.77,
      "eval_steps_per_second": 1.943,
      "step": 40000
    },
    {
      "epoch": 2.4297079006497846,
      "grad_norm": 5.606076240539551,
      "learning_rate": 8.654569884861153e-06,
      "loss": 4.2776,
      "step": 40010
    },
    {
      "epoch": 2.430315175806158,
      "grad_norm": 5.606778621673584,
      "learning_rate": 8.63669772047861e-06,
      "loss": 3.9432,
      "step": 40020
    },
    {
      "epoch": 2.430922450962531,
      "grad_norm": 5.857976913452148,
      "learning_rate": 8.618842284074274e-06,
      "loss": 3.6819,
      "step": 40030
    },
    {
      "epoch": 2.4315297261189044,
      "grad_norm": 8.122824668884277,
      "learning_rate": 8.601003582869178e-06,
      "loss": 3.6921,
      "step": 40040
    },
    {
      "epoch": 2.4321370012752777,
      "grad_norm": 7.396851539611816,
      "learning_rate": 8.583181624077552e-06,
      "loss": 3.3446,
      "step": 40050
    },
    {
      "epoch": 2.4327442764316514,
      "grad_norm": 8.333771705627441,
      "learning_rate": 8.565376414906911e-06,
      "loss": 3.3339,
      "step": 40060
    },
    {
      "epoch": 2.4333515515880246,
      "grad_norm": 8.74606990814209,
      "learning_rate": 8.547587962557952e-06,
      "loss": 3.4508,
      "step": 40070
    },
    {
      "epoch": 2.433958826744398,
      "grad_norm": 8.051946640014648,
      "learning_rate": 8.52981627422461e-06,
      "loss": 3.2077,
      "step": 40080
    },
    {
      "epoch": 2.434566101900771,
      "grad_norm": 7.0661301612854,
      "learning_rate": 8.512061357094036e-06,
      "loss": 2.9094,
      "step": 40090
    },
    {
      "epoch": 2.4351733770571444,
      "grad_norm": 7.937939167022705,
      "learning_rate": 8.494323218346605e-06,
      "loss": 3.2321,
      "step": 40100
    },
    {
      "epoch": 2.435780652213518,
      "grad_norm": 8.871305465698242,
      "learning_rate": 8.476601865155897e-06,
      "loss": 3.5644,
      "step": 40110
    },
    {
      "epoch": 2.4363879273698914,
      "grad_norm": 7.951601505279541,
      "learning_rate": 8.458897304688718e-06,
      "loss": 3.6745,
      "step": 40120
    },
    {
      "epoch": 2.4369952025262647,
      "grad_norm": 8.940550804138184,
      "learning_rate": 8.441209544105055e-06,
      "loss": 3.5618,
      "step": 40130
    },
    {
      "epoch": 2.437602477682638,
      "grad_norm": 8.096866607666016,
      "learning_rate": 8.423538590558144e-06,
      "loss": 3.4874,
      "step": 40140
    },
    {
      "epoch": 2.438209752839011,
      "grad_norm": 9.213115692138672,
      "learning_rate": 8.405884451194396e-06,
      "loss": 3.466,
      "step": 40150
    },
    {
      "epoch": 2.438817027995385,
      "grad_norm": 7.117950439453125,
      "learning_rate": 8.388247133153426e-06,
      "loss": 3.5411,
      "step": 40160
    },
    {
      "epoch": 2.439424303151758,
      "grad_norm": 5.603477478027344,
      "learning_rate": 8.370626643568019e-06,
      "loss": 3.1845,
      "step": 40170
    },
    {
      "epoch": 2.4400315783081314,
      "grad_norm": 8.167908668518066,
      "learning_rate": 8.35302298956422e-06,
      "loss": 3.7739,
      "step": 40180
    },
    {
      "epoch": 2.4406388534645047,
      "grad_norm": 8.612595558166504,
      "learning_rate": 8.335436178261203e-06,
      "loss": 3.7333,
      "step": 40190
    },
    {
      "epoch": 2.441246128620878,
      "grad_norm": 7.207149982452393,
      "learning_rate": 8.317866216771363e-06,
      "loss": 3.8415,
      "step": 40200
    },
    {
      "epoch": 2.4418534037772517,
      "grad_norm": 8.294702529907227,
      "learning_rate": 8.300313112200264e-06,
      "loss": 3.9011,
      "step": 40210
    },
    {
      "epoch": 2.442460678933625,
      "grad_norm": 6.413236618041992,
      "learning_rate": 8.282776871646675e-06,
      "loss": 3.4906,
      "step": 40220
    },
    {
      "epoch": 2.443067954089998,
      "grad_norm": 6.3943867683410645,
      "learning_rate": 8.265257502202539e-06,
      "loss": 3.4028,
      "step": 40230
    },
    {
      "epoch": 2.4436752292463715,
      "grad_norm": 5.336348533630371,
      "learning_rate": 8.24775501095294e-06,
      "loss": 3.785,
      "step": 40240
    },
    {
      "epoch": 2.4442825044027447,
      "grad_norm": 7.85324764251709,
      "learning_rate": 8.230269404976171e-06,
      "loss": 3.8199,
      "step": 40250
    },
    {
      "epoch": 2.4448897795591185,
      "grad_norm": 6.926254749298096,
      "learning_rate": 8.212800691343703e-06,
      "loss": 3.8908,
      "step": 40260
    },
    {
      "epoch": 2.4454970547154917,
      "grad_norm": 9.274457931518555,
      "learning_rate": 8.195348877120156e-06,
      "loss": 3.7193,
      "step": 40270
    },
    {
      "epoch": 2.446104329871865,
      "grad_norm": 6.344516277313232,
      "learning_rate": 8.17791396936331e-06,
      "loss": 3.6362,
      "step": 40280
    },
    {
      "epoch": 2.4467116050282383,
      "grad_norm": 5.883072853088379,
      "learning_rate": 8.16049597512415e-06,
      "loss": 3.4379,
      "step": 40290
    },
    {
      "epoch": 2.4473188801846115,
      "grad_norm": 15.719355583190918,
      "learning_rate": 8.143094901446762e-06,
      "loss": 3.7858,
      "step": 40300
    },
    {
      "epoch": 2.4479261553409852,
      "grad_norm": 8.194222450256348,
      "learning_rate": 8.125710755368426e-06,
      "loss": 3.3525,
      "step": 40310
    },
    {
      "epoch": 2.4485334304973585,
      "grad_norm": 9.511641502380371,
      "learning_rate": 8.10834354391955e-06,
      "loss": 3.277,
      "step": 40320
    },
    {
      "epoch": 2.4491407056537318,
      "grad_norm": 7.787377834320068,
      "learning_rate": 8.090993274123748e-06,
      "loss": 3.3607,
      "step": 40330
    },
    {
      "epoch": 2.449747980810105,
      "grad_norm": 7.570393085479736,
      "learning_rate": 8.073659952997719e-06,
      "loss": 3.3671,
      "step": 40340
    },
    {
      "epoch": 2.4503552559664783,
      "grad_norm": 7.822370529174805,
      "learning_rate": 8.056343587551347e-06,
      "loss": 3.4229,
      "step": 40350
    },
    {
      "epoch": 2.450962531122852,
      "grad_norm": 5.525564670562744,
      "learning_rate": 8.039044184787631e-06,
      "loss": 3.443,
      "step": 40360
    },
    {
      "epoch": 2.4515698062792253,
      "grad_norm": 8.734257698059082,
      "learning_rate": 8.021761751702738e-06,
      "loss": 3.711,
      "step": 40370
    },
    {
      "epoch": 2.4521770814355985,
      "grad_norm": 7.652473449707031,
      "learning_rate": 8.004496295285957e-06,
      "loss": 3.447,
      "step": 40380
    },
    {
      "epoch": 2.452784356591972,
      "grad_norm": 9.064750671386719,
      "learning_rate": 7.987247822519711e-06,
      "loss": 3.24,
      "step": 40390
    },
    {
      "epoch": 2.453391631748345,
      "grad_norm": 8.874897956848145,
      "learning_rate": 7.970016340379543e-06,
      "loss": 3.5117,
      "step": 40400
    },
    {
      "epoch": 2.4539989069047183,
      "grad_norm": 9.631631851196289,
      "learning_rate": 7.952801855834164e-06,
      "loss": 3.552,
      "step": 40410
    },
    {
      "epoch": 2.454606182061092,
      "grad_norm": 7.541869163513184,
      "learning_rate": 7.93560437584538e-06,
      "loss": 3.5479,
      "step": 40420
    },
    {
      "epoch": 2.4552134572174653,
      "grad_norm": 6.873699188232422,
      "learning_rate": 7.918423907368117e-06,
      "loss": 3.9442,
      "step": 40430
    },
    {
      "epoch": 2.4558207323738386,
      "grad_norm": 5.974618911743164,
      "learning_rate": 7.901260457350435e-06,
      "loss": 3.777,
      "step": 40440
    },
    {
      "epoch": 2.456428007530212,
      "grad_norm": 5.289093017578125,
      "learning_rate": 7.884114032733508e-06,
      "loss": 3.471,
      "step": 40450
    },
    {
      "epoch": 2.457035282686585,
      "grad_norm": 8.145233154296875,
      "learning_rate": 7.866984640451614e-06,
      "loss": 3.6526,
      "step": 40460
    },
    {
      "epoch": 2.457642557842959,
      "grad_norm": 4.017132759094238,
      "learning_rate": 7.849872287432165e-06,
      "loss": 3.7201,
      "step": 40470
    },
    {
      "epoch": 2.458249832999332,
      "grad_norm": 7.100436210632324,
      "learning_rate": 7.832776980595647e-06,
      "loss": 3.8061,
      "step": 40480
    },
    {
      "epoch": 2.4588571081557054,
      "grad_norm": 8.501044273376465,
      "learning_rate": 7.815698726855703e-06,
      "loss": 3.8127,
      "step": 40490
    },
    {
      "epoch": 2.4594643833120786,
      "grad_norm": 7.860132217407227,
      "learning_rate": 7.79863753311904e-06,
      "loss": 3.7199,
      "step": 40500
    },
    {
      "epoch": 2.460071658468452,
      "grad_norm": 6.571009635925293,
      "learning_rate": 7.78159340628547e-06,
      "loss": 3.4912,
      "step": 40510
    },
    {
      "epoch": 2.4606789336248256,
      "grad_norm": 8.471123695373535,
      "learning_rate": 7.76456635324792e-06,
      "loss": 3.6058,
      "step": 40520
    },
    {
      "epoch": 2.461286208781199,
      "grad_norm": 5.920582294464111,
      "learning_rate": 7.74755638089239e-06,
      "loss": 3.6935,
      "step": 40530
    },
    {
      "epoch": 2.461893483937572,
      "grad_norm": 4.166106700897217,
      "learning_rate": 7.730563496097992e-06,
      "loss": 3.2923,
      "step": 40540
    },
    {
      "epoch": 2.4625007590939454,
      "grad_norm": 9.512449264526367,
      "learning_rate": 7.713587705736902e-06,
      "loss": 3.3636,
      "step": 40550
    },
    {
      "epoch": 2.4631080342503187,
      "grad_norm": 10.252364158630371,
      "learning_rate": 7.696629016674427e-06,
      "loss": 3.5371,
      "step": 40560
    },
    {
      "epoch": 2.4637153094066924,
      "grad_norm": 6.3951873779296875,
      "learning_rate": 7.679687435768929e-06,
      "loss": 3.4853,
      "step": 40570
    },
    {
      "epoch": 2.4643225845630656,
      "grad_norm": 6.7184038162231445,
      "learning_rate": 7.662762969871834e-06,
      "loss": 3.8482,
      "step": 40580
    },
    {
      "epoch": 2.464929859719439,
      "grad_norm": 6.088123798370361,
      "learning_rate": 7.645855625827658e-06,
      "loss": 3.384,
      "step": 40590
    },
    {
      "epoch": 2.465537134875812,
      "grad_norm": 6.965751647949219,
      "learning_rate": 7.628965410474026e-06,
      "loss": 3.4251,
      "step": 40600
    },
    {
      "epoch": 2.4661444100321854,
      "grad_norm": 7.6985249519348145,
      "learning_rate": 7.6120923306416015e-06,
      "loss": 3.3608,
      "step": 40610
    },
    {
      "epoch": 2.4667516851885587,
      "grad_norm": 8.01001262664795,
      "learning_rate": 7.595236393154131e-06,
      "loss": 3.5488,
      "step": 40620
    },
    {
      "epoch": 2.4673589603449324,
      "grad_norm": 6.928506374359131,
      "learning_rate": 7.578397604828413e-06,
      "loss": 3.6956,
      "step": 40630
    },
    {
      "epoch": 2.4679662355013057,
      "grad_norm": 5.664020538330078,
      "learning_rate": 7.561575972474328e-06,
      "loss": 3.6799,
      "step": 40640
    },
    {
      "epoch": 2.468573510657679,
      "grad_norm": 6.082873344421387,
      "learning_rate": 7.544771502894809e-06,
      "loss": 3.6971,
      "step": 40650
    },
    {
      "epoch": 2.469180785814052,
      "grad_norm": 4.957890510559082,
      "learning_rate": 7.527984202885857e-06,
      "loss": 3.6472,
      "step": 40660
    },
    {
      "epoch": 2.4697880609704255,
      "grad_norm": 5.456209659576416,
      "learning_rate": 7.511214079236506e-06,
      "loss": 3.3455,
      "step": 40670
    },
    {
      "epoch": 2.470395336126799,
      "grad_norm": 5.614343166351318,
      "learning_rate": 7.4944611387288865e-06,
      "loss": 3.4573,
      "step": 40680
    },
    {
      "epoch": 2.4710026112831724,
      "grad_norm": 7.502503395080566,
      "learning_rate": 7.477725388138141e-06,
      "loss": 3.6375,
      "step": 40690
    },
    {
      "epoch": 2.4716098864395457,
      "grad_norm": 8.483842849731445,
      "learning_rate": 7.461006834232476e-06,
      "loss": 3.5995,
      "step": 40700
    },
    {
      "epoch": 2.472217161595919,
      "grad_norm": 8.819951057434082,
      "learning_rate": 7.444305483773145e-06,
      "loss": 3.3735,
      "step": 40710
    },
    {
      "epoch": 2.4728244367522922,
      "grad_norm": 8.24169921875,
      "learning_rate": 7.427621343514429e-06,
      "loss": 3.739,
      "step": 40720
    },
    {
      "epoch": 2.473431711908666,
      "grad_norm": 10.669027328491211,
      "learning_rate": 7.410954420203669e-06,
      "loss": 3.6306,
      "step": 40730
    },
    {
      "epoch": 2.474038987065039,
      "grad_norm": 7.053476810455322,
      "learning_rate": 7.394304720581219e-06,
      "loss": 3.5134,
      "step": 40740
    },
    {
      "epoch": 2.4746462622214125,
      "grad_norm": 7.156007289886475,
      "learning_rate": 7.3776722513805075e-06,
      "loss": 3.6951,
      "step": 40750
    },
    {
      "epoch": 2.4752535373777858,
      "grad_norm": 8.292184829711914,
      "learning_rate": 7.361057019327961e-06,
      "loss": 3.551,
      "step": 40760
    },
    {
      "epoch": 2.475860812534159,
      "grad_norm": 6.786746501922607,
      "learning_rate": 7.344459031143036e-06,
      "loss": 3.606,
      "step": 40770
    },
    {
      "epoch": 2.4764680876905327,
      "grad_norm": 6.47135591506958,
      "learning_rate": 7.3278782935382315e-06,
      "loss": 3.7222,
      "step": 40780
    },
    {
      "epoch": 2.477075362846906,
      "grad_norm": 7.534141540527344,
      "learning_rate": 7.3113148132190555e-06,
      "loss": 3.782,
      "step": 40790
    },
    {
      "epoch": 2.4776826380032793,
      "grad_norm": 7.126948833465576,
      "learning_rate": 7.294768596884049e-06,
      "loss": 3.6698,
      "step": 40800
    },
    {
      "epoch": 2.4782899131596525,
      "grad_norm": 5.170383930206299,
      "learning_rate": 7.278239651224761e-06,
      "loss": 3.6552,
      "step": 40810
    },
    {
      "epoch": 2.478897188316026,
      "grad_norm": 8.836197853088379,
      "learning_rate": 7.261727982925748e-06,
      "loss": 3.7505,
      "step": 40820
    },
    {
      "epoch": 2.4795044634723995,
      "grad_norm": 7.144367218017578,
      "learning_rate": 7.245233598664619e-06,
      "loss": 3.4645,
      "step": 40830
    },
    {
      "epoch": 2.4801117386287728,
      "grad_norm": 8.11091136932373,
      "learning_rate": 7.228756505111955e-06,
      "loss": 3.4995,
      "step": 40840
    },
    {
      "epoch": 2.480719013785146,
      "grad_norm": 6.511067867279053,
      "learning_rate": 7.21229670893136e-06,
      "loss": 3.6717,
      "step": 40850
    },
    {
      "epoch": 2.4813262889415193,
      "grad_norm": 5.944953918457031,
      "learning_rate": 7.1958542167794075e-06,
      "loss": 3.8557,
      "step": 40860
    },
    {
      "epoch": 2.4819335640978926,
      "grad_norm": 6.550126075744629,
      "learning_rate": 7.1794290353057406e-06,
      "loss": 4.0556,
      "step": 40870
    },
    {
      "epoch": 2.4825408392542663,
      "grad_norm": 7.057715892791748,
      "learning_rate": 7.163021171152945e-06,
      "loss": 3.5889,
      "step": 40880
    },
    {
      "epoch": 2.4831481144106395,
      "grad_norm": 5.9430060386657715,
      "learning_rate": 7.14663063095663e-06,
      "loss": 3.6723,
      "step": 40890
    },
    {
      "epoch": 2.483755389567013,
      "grad_norm": 7.213380336761475,
      "learning_rate": 7.130257421345371e-06,
      "loss": 3.6738,
      "step": 40900
    },
    {
      "epoch": 2.484362664723386,
      "grad_norm": 5.585146903991699,
      "learning_rate": 7.113901548940799e-06,
      "loss": 3.4068,
      "step": 40910
    },
    {
      "epoch": 2.4849699398797593,
      "grad_norm": 5.490096092224121,
      "learning_rate": 7.09756302035745e-06,
      "loss": 3.6305,
      "step": 40920
    },
    {
      "epoch": 2.485577215036133,
      "grad_norm": 6.752452850341797,
      "learning_rate": 7.081241842202896e-06,
      "loss": 3.8815,
      "step": 40930
    },
    {
      "epoch": 2.4861844901925063,
      "grad_norm": 6.662947177886963,
      "learning_rate": 7.064938021077672e-06,
      "loss": 3.6912,
      "step": 40940
    },
    {
      "epoch": 2.4867917653488796,
      "grad_norm": 5.6132426261901855,
      "learning_rate": 7.048651563575326e-06,
      "loss": 3.7617,
      "step": 40950
    },
    {
      "epoch": 2.487399040505253,
      "grad_norm": 5.389719009399414,
      "learning_rate": 7.032382476282345e-06,
      "loss": 3.752,
      "step": 40960
    },
    {
      "epoch": 2.488006315661626,
      "grad_norm": 6.595432281494141,
      "learning_rate": 7.016130765778201e-06,
      "loss": 3.7289,
      "step": 40970
    },
    {
      "epoch": 2.488613590818,
      "grad_norm": 7.9622015953063965,
      "learning_rate": 6.999896438635373e-06,
      "loss": 3.9149,
      "step": 40980
    },
    {
      "epoch": 2.489220865974373,
      "grad_norm": 7.326808452606201,
      "learning_rate": 6.983679501419249e-06,
      "loss": 3.4889,
      "step": 40990
    },
    {
      "epoch": 2.4898281411307464,
      "grad_norm": 5.464046001434326,
      "learning_rate": 6.967479960688234e-06,
      "loss": 3.403,
      "step": 41000
    },
    {
      "epoch": 2.4904354162871196,
      "grad_norm": 7.380500316619873,
      "learning_rate": 6.9512978229936634e-06,
      "loss": 3.511,
      "step": 41010
    },
    {
      "epoch": 2.491042691443493,
      "grad_norm": 7.001899242401123,
      "learning_rate": 6.935133094879876e-06,
      "loss": 3.5878,
      "step": 41020
    },
    {
      "epoch": 2.4916499665998666,
      "grad_norm": 6.498128890991211,
      "learning_rate": 6.918985782884135e-06,
      "loss": 3.636,
      "step": 41030
    },
    {
      "epoch": 2.49225724175624,
      "grad_norm": 7.009227275848389,
      "learning_rate": 6.9028558935366695e-06,
      "loss": 3.5884,
      "step": 41040
    },
    {
      "epoch": 2.492864516912613,
      "grad_norm": 8.320123672485352,
      "learning_rate": 6.88674343336066e-06,
      "loss": 3.682,
      "step": 41050
    },
    {
      "epoch": 2.4934717920689864,
      "grad_norm": 9.831170082092285,
      "learning_rate": 6.870648408872243e-06,
      "loss": 3.4404,
      "step": 41060
    },
    {
      "epoch": 2.4940790672253597,
      "grad_norm": 9.422088623046875,
      "learning_rate": 6.854570826580508e-06,
      "loss": 3.583,
      "step": 41070
    },
    {
      "epoch": 2.4946863423817334,
      "grad_norm": 7.114957332611084,
      "learning_rate": 6.838510692987482e-06,
      "loss": 3.4073,
      "step": 41080
    },
    {
      "epoch": 2.4952936175381066,
      "grad_norm": 6.5429768562316895,
      "learning_rate": 6.822468014588129e-06,
      "loss": 3.9521,
      "step": 41090
    },
    {
      "epoch": 2.49590089269448,
      "grad_norm": 6.740923881530762,
      "learning_rate": 6.806442797870383e-06,
      "loss": 3.454,
      "step": 41100
    },
    {
      "epoch": 2.496508167850853,
      "grad_norm": 7.867502212524414,
      "learning_rate": 6.790435049315086e-06,
      "loss": 3.5903,
      "step": 41110
    },
    {
      "epoch": 2.4971154430072264,
      "grad_norm": 6.849205017089844,
      "learning_rate": 6.774444775396027e-06,
      "loss": 3.6977,
      "step": 41120
    },
    {
      "epoch": 2.4977227181636,
      "grad_norm": 8.282538414001465,
      "learning_rate": 6.758471982579934e-06,
      "loss": 3.6851,
      "step": 41130
    },
    {
      "epoch": 2.4983299933199734,
      "grad_norm": 9.700836181640625,
      "learning_rate": 6.7425166773264495e-06,
      "loss": 3.4359,
      "step": 41140
    },
    {
      "epoch": 2.4989372684763467,
      "grad_norm": 7.60175085067749,
      "learning_rate": 6.726578866088157e-06,
      "loss": 3.4754,
      "step": 41150
    },
    {
      "epoch": 2.49954454363272,
      "grad_norm": 10.248394012451172,
      "learning_rate": 6.710658555310556e-06,
      "loss": 3.5665,
      "step": 41160
    },
    {
      "epoch": 2.500151818789093,
      "grad_norm": 8.687663078308105,
      "learning_rate": 6.694755751432075e-06,
      "loss": 3.7092,
      "step": 41170
    },
    {
      "epoch": 2.500759093945467,
      "grad_norm": 8.866936683654785,
      "learning_rate": 6.678870460884073e-06,
      "loss": 3.7084,
      "step": 41180
    },
    {
      "epoch": 2.50136636910184,
      "grad_norm": 9.107325553894043,
      "learning_rate": 6.663002690090814e-06,
      "loss": 3.8188,
      "step": 41190
    },
    {
      "epoch": 2.5019736442582134,
      "grad_norm": 7.413517475128174,
      "learning_rate": 6.647152445469451e-06,
      "loss": 3.6155,
      "step": 41200
    },
    {
      "epoch": 2.5025809194145867,
      "grad_norm": 9.478874206542969,
      "learning_rate": 6.631319733430108e-06,
      "loss": 3.9147,
      "step": 41210
    },
    {
      "epoch": 2.50318819457096,
      "grad_norm": 10.88866901397705,
      "learning_rate": 6.615504560375768e-06,
      "loss": 3.924,
      "step": 41220
    },
    {
      "epoch": 2.5037954697273337,
      "grad_norm": 11.106185913085938,
      "learning_rate": 6.5997069327023435e-06,
      "loss": 4.1951,
      "step": 41230
    },
    {
      "epoch": 2.504402744883707,
      "grad_norm": 11.427855491638184,
      "learning_rate": 6.583926856798633e-06,
      "loss": 3.8818,
      "step": 41240
    },
    {
      "epoch": 2.50501002004008,
      "grad_norm": 12.860428810119629,
      "learning_rate": 6.568164339046373e-06,
      "loss": 3.9828,
      "step": 41250
    },
    {
      "epoch": 2.5056172951964535,
      "grad_norm": 12.018080711364746,
      "learning_rate": 6.55241938582018e-06,
      "loss": 3.463,
      "step": 41260
    },
    {
      "epoch": 2.5062245703528268,
      "grad_norm": 9.062466621398926,
      "learning_rate": 6.536692003487543e-06,
      "loss": 3.6828,
      "step": 41270
    },
    {
      "epoch": 2.5068318455092005,
      "grad_norm": 9.421553611755371,
      "learning_rate": 6.520982198408859e-06,
      "loss": 3.9841,
      "step": 41280
    },
    {
      "epoch": 2.5074391206655737,
      "grad_norm": 11.140461921691895,
      "learning_rate": 6.505289976937451e-06,
      "loss": 3.8259,
      "step": 41290
    },
    {
      "epoch": 2.508046395821947,
      "grad_norm": 9.101447105407715,
      "learning_rate": 6.489615345419492e-06,
      "loss": 3.688,
      "step": 41300
    },
    {
      "epoch": 2.5086536709783203,
      "grad_norm": 9.118483543395996,
      "learning_rate": 6.473958310194045e-06,
      "loss": 3.385,
      "step": 41310
    },
    {
      "epoch": 2.5092609461346935,
      "grad_norm": 7.3987932205200195,
      "learning_rate": 6.458318877593078e-06,
      "loss": 3.2863,
      "step": 41320
    },
    {
      "epoch": 2.5098682212910672,
      "grad_norm": 4.939738750457764,
      "learning_rate": 6.442697053941416e-06,
      "loss": 3.3995,
      "step": 41330
    },
    {
      "epoch": 2.51047549644744,
      "grad_norm": 10.45053768157959,
      "learning_rate": 6.427092845556776e-06,
      "loss": 3.9866,
      "step": 41340
    },
    {
      "epoch": 2.5110827716038138,
      "grad_norm": 7.086581707000732,
      "learning_rate": 6.4115062587497495e-06,
      "loss": 3.6398,
      "step": 41350
    },
    {
      "epoch": 2.511690046760187,
      "grad_norm": 8.116800308227539,
      "learning_rate": 6.395937299823784e-06,
      "loss": 3.5397,
      "step": 41360
    },
    {
      "epoch": 2.5122973219165603,
      "grad_norm": 7.079566478729248,
      "learning_rate": 6.380385975075248e-06,
      "loss": 3.49,
      "step": 41370
    },
    {
      "epoch": 2.512904597072934,
      "grad_norm": 5.5104827880859375,
      "learning_rate": 6.364852290793322e-06,
      "loss": 3.6507,
      "step": 41380
    },
    {
      "epoch": 2.513511872229307,
      "grad_norm": 7.608292579650879,
      "learning_rate": 6.349336253260085e-06,
      "loss": 3.8897,
      "step": 41390
    },
    {
      "epoch": 2.5141191473856805,
      "grad_norm": 7.484237194061279,
      "learning_rate": 6.333837868750464e-06,
      "loss": 3.8218,
      "step": 41400
    },
    {
      "epoch": 2.514726422542054,
      "grad_norm": 6.834229469299316,
      "learning_rate": 6.31835714353225e-06,
      "loss": 3.6859,
      "step": 41410
    },
    {
      "epoch": 2.515333697698427,
      "grad_norm": 6.948515892028809,
      "learning_rate": 6.302894083866101e-06,
      "loss": 3.8156,
      "step": 41420
    },
    {
      "epoch": 2.515940972854801,
      "grad_norm": 7.6748247146606445,
      "learning_rate": 6.28744869600551e-06,
      "loss": 3.7083,
      "step": 41430
    },
    {
      "epoch": 2.5165482480111736,
      "grad_norm": 7.042078971862793,
      "learning_rate": 6.272020986196864e-06,
      "loss": 3.7381,
      "step": 41440
    },
    {
      "epoch": 2.5171555231675473,
      "grad_norm": 7.159420967102051,
      "learning_rate": 6.25661096067936e-06,
      "loss": 3.7948,
      "step": 41450
    },
    {
      "epoch": 2.5177627983239206,
      "grad_norm": 7.525523662567139,
      "learning_rate": 6.241218625685052e-06,
      "loss": 3.651,
      "step": 41460
    },
    {
      "epoch": 2.518370073480294,
      "grad_norm": 8.843524932861328,
      "learning_rate": 6.2258439874388615e-06,
      "loss": 3.8268,
      "step": 41470
    },
    {
      "epoch": 2.518977348636667,
      "grad_norm": 9.827567100524902,
      "learning_rate": 6.21048705215852e-06,
      "loss": 3.9457,
      "step": 41480
    },
    {
      "epoch": 2.5195846237930404,
      "grad_norm": 6.320642471313477,
      "learning_rate": 6.195147826054626e-06,
      "loss": 3.5634,
      "step": 41490
    },
    {
      "epoch": 2.520191898949414,
      "grad_norm": 8.174952507019043,
      "learning_rate": 6.179826315330606e-06,
      "loss": 3.8467,
      "step": 41500
    },
    {
      "epoch": 2.5207991741057874,
      "grad_norm": 9.824455261230469,
      "learning_rate": 6.164522526182709e-06,
      "loss": 3.3049,
      "step": 41510
    },
    {
      "epoch": 2.5214064492621606,
      "grad_norm": 10.492103576660156,
      "learning_rate": 6.149236464800051e-06,
      "loss": 3.506,
      "step": 41520
    },
    {
      "epoch": 2.522013724418534,
      "grad_norm": 11.034570693969727,
      "learning_rate": 6.13396813736456e-06,
      "loss": 3.6765,
      "step": 41530
    },
    {
      "epoch": 2.522620999574907,
      "grad_norm": 8.446971893310547,
      "learning_rate": 6.1187175500509845e-06,
      "loss": 3.8732,
      "step": 41540
    },
    {
      "epoch": 2.523228274731281,
      "grad_norm": 6.709682464599609,
      "learning_rate": 6.103484709026885e-06,
      "loss": 3.8024,
      "step": 41550
    },
    {
      "epoch": 2.523835549887654,
      "grad_norm": 7.286640167236328,
      "learning_rate": 6.088269620452697e-06,
      "loss": 3.6218,
      "step": 41560
    },
    {
      "epoch": 2.5244428250440274,
      "grad_norm": 8.53380298614502,
      "learning_rate": 6.073072290481624e-06,
      "loss": 3.6165,
      "step": 41570
    },
    {
      "epoch": 2.5250501002004007,
      "grad_norm": 7.235139846801758,
      "learning_rate": 6.057892725259717e-06,
      "loss": 3.4649,
      "step": 41580
    },
    {
      "epoch": 2.525657375356774,
      "grad_norm": 8.186863899230957,
      "learning_rate": 6.042730930925821e-06,
      "loss": 3.7883,
      "step": 41590
    },
    {
      "epoch": 2.5262646505131476,
      "grad_norm": 8.024341583251953,
      "learning_rate": 6.027586913611638e-06,
      "loss": 4.0399,
      "step": 41600
    },
    {
      "epoch": 2.526871925669521,
      "grad_norm": 6.4062604904174805,
      "learning_rate": 6.0124606794416225e-06,
      "loss": 3.6392,
      "step": 41610
    },
    {
      "epoch": 2.527479200825894,
      "grad_norm": 8.699980735778809,
      "learning_rate": 5.9973522345330655e-06,
      "loss": 3.5811,
      "step": 41620
    },
    {
      "epoch": 2.5280864759822674,
      "grad_norm": 7.867906093597412,
      "learning_rate": 5.982261584996063e-06,
      "loss": 3.9599,
      "step": 41630
    },
    {
      "epoch": 2.5286937511386407,
      "grad_norm": 9.015737533569336,
      "learning_rate": 5.967188736933527e-06,
      "loss": 4.011,
      "step": 41640
    },
    {
      "epoch": 2.5293010262950144,
      "grad_norm": 8.669193267822266,
      "learning_rate": 5.952133696441153e-06,
      "loss": 4.2088,
      "step": 41650
    },
    {
      "epoch": 2.5299083014513877,
      "grad_norm": 7.607169151306152,
      "learning_rate": 5.937096469607434e-06,
      "loss": 3.6089,
      "step": 41660
    },
    {
      "epoch": 2.530515576607761,
      "grad_norm": 6.258040904998779,
      "learning_rate": 5.922077062513665e-06,
      "loss": 3.8826,
      "step": 41670
    },
    {
      "epoch": 2.531122851764134,
      "grad_norm": 6.5434675216674805,
      "learning_rate": 5.907075481233937e-06,
      "loss": 3.982,
      "step": 41680
    },
    {
      "epoch": 2.5317301269205075,
      "grad_norm": 6.877368450164795,
      "learning_rate": 5.892091731835125e-06,
      "loss": 3.7149,
      "step": 41690
    },
    {
      "epoch": 2.532337402076881,
      "grad_norm": 7.142702102661133,
      "learning_rate": 5.877125820376888e-06,
      "loss": 3.5904,
      "step": 41700
    },
    {
      "epoch": 2.5329446772332544,
      "grad_norm": 6.278517246246338,
      "learning_rate": 5.8621777529116964e-06,
      "loss": 3.8453,
      "step": 41710
    },
    {
      "epoch": 2.5335519523896277,
      "grad_norm": 7.426680564880371,
      "learning_rate": 5.847247535484779e-06,
      "loss": 3.5762,
      "step": 41720
    },
    {
      "epoch": 2.534159227546001,
      "grad_norm": 7.025753021240234,
      "learning_rate": 5.832335174134151e-06,
      "loss": 3.5545,
      "step": 41730
    },
    {
      "epoch": 2.5347665027023742,
      "grad_norm": 7.5927534103393555,
      "learning_rate": 5.8174406748906115e-06,
      "loss": 3.4909,
      "step": 41740
    },
    {
      "epoch": 2.535373777858748,
      "grad_norm": 8.72828197479248,
      "learning_rate": 5.802564043777731e-06,
      "loss": 3.709,
      "step": 41750
    },
    {
      "epoch": 2.5359810530151212,
      "grad_norm": 8.388964653015137,
      "learning_rate": 5.787705286811856e-06,
      "loss": 3.6736,
      "step": 41760
    },
    {
      "epoch": 2.5365883281714945,
      "grad_norm": 7.520598411560059,
      "learning_rate": 5.7728644100021105e-06,
      "loss": 3.5711,
      "step": 41770
    },
    {
      "epoch": 2.5371956033278678,
      "grad_norm": 8.314980506896973,
      "learning_rate": 5.758041419350363e-06,
      "loss": 3.558,
      "step": 41780
    },
    {
      "epoch": 2.537802878484241,
      "grad_norm": 6.678313255310059,
      "learning_rate": 5.743236320851292e-06,
      "loss": 3.7876,
      "step": 41790
    },
    {
      "epoch": 2.5384101536406147,
      "grad_norm": 4.7839155197143555,
      "learning_rate": 5.728449120492302e-06,
      "loss": 3.7592,
      "step": 41800
    },
    {
      "epoch": 2.539017428796988,
      "grad_norm": 5.450376987457275,
      "learning_rate": 5.713679824253576e-06,
      "loss": 3.6251,
      "step": 41810
    },
    {
      "epoch": 2.5396247039533613,
      "grad_norm": 4.474588871002197,
      "learning_rate": 5.698928438108047e-06,
      "loss": 3.5194,
      "step": 41820
    },
    {
      "epoch": 2.5402319791097345,
      "grad_norm": 5.971132278442383,
      "learning_rate": 5.684194968021422e-06,
      "loss": 3.2129,
      "step": 41830
    },
    {
      "epoch": 2.540839254266108,
      "grad_norm": 4.8401570320129395,
      "learning_rate": 5.669479419952139e-06,
      "loss": 3.1569,
      "step": 41840
    },
    {
      "epoch": 2.5414465294224815,
      "grad_norm": 5.3808135986328125,
      "learning_rate": 5.6547817998514e-06,
      "loss": 3.401,
      "step": 41850
    },
    {
      "epoch": 2.5420538045788548,
      "grad_norm": 7.799161911010742,
      "learning_rate": 5.640102113663154e-06,
      "loss": 3.3725,
      "step": 41860
    },
    {
      "epoch": 2.542661079735228,
      "grad_norm": 8.224835395812988,
      "learning_rate": 5.62544036732412e-06,
      "loss": 3.4932,
      "step": 41870
    },
    {
      "epoch": 2.5432683548916013,
      "grad_norm": 7.534143924713135,
      "learning_rate": 5.610796566763738e-06,
      "loss": 3.3518,
      "step": 41880
    },
    {
      "epoch": 2.5438756300479746,
      "grad_norm": 6.908139228820801,
      "learning_rate": 5.596170717904159e-06,
      "loss": 3.434,
      "step": 41890
    },
    {
      "epoch": 2.5444829052043483,
      "grad_norm": 5.438641548156738,
      "learning_rate": 5.581562826660347e-06,
      "loss": 3.0161,
      "step": 41900
    },
    {
      "epoch": 2.5450901803607215,
      "grad_norm": 8.731348037719727,
      "learning_rate": 5.566972898939954e-06,
      "loss": 3.1505,
      "step": 41910
    },
    {
      "epoch": 2.545697455517095,
      "grad_norm": 8.306672096252441,
      "learning_rate": 5.552400940643382e-06,
      "loss": 3.4526,
      "step": 41920
    },
    {
      "epoch": 2.546304730673468,
      "grad_norm": 5.965444087982178,
      "learning_rate": 5.537846957663745e-06,
      "loss": 3.5307,
      "step": 41930
    },
    {
      "epoch": 2.5469120058298413,
      "grad_norm": 6.875627040863037,
      "learning_rate": 5.523310955886945e-06,
      "loss": 3.4849,
      "step": 41940
    },
    {
      "epoch": 2.547519280986215,
      "grad_norm": 9.144190788269043,
      "learning_rate": 5.508792941191537e-06,
      "loss": 3.652,
      "step": 41950
    },
    {
      "epoch": 2.5481265561425883,
      "grad_norm": 6.393211841583252,
      "learning_rate": 5.494292919448846e-06,
      "loss": 3.2686,
      "step": 41960
    },
    {
      "epoch": 2.5487338312989616,
      "grad_norm": 8.971954345703125,
      "learning_rate": 5.479810896522902e-06,
      "loss": 3.6895,
      "step": 41970
    },
    {
      "epoch": 2.549341106455335,
      "grad_norm": 8.023975372314453,
      "learning_rate": 5.465346878270494e-06,
      "loss": 3.5943,
      "step": 41980
    },
    {
      "epoch": 2.549948381611708,
      "grad_norm": 6.56473970413208,
      "learning_rate": 5.450900870541081e-06,
      "loss": 3.2951,
      "step": 41990
    },
    {
      "epoch": 2.550555656768082,
      "grad_norm": 8.11553955078125,
      "learning_rate": 5.436472879176863e-06,
      "loss": 3.6866,
      "step": 42000
    },
    {
      "epoch": 2.551162931924455,
      "grad_norm": 8.217304229736328,
      "learning_rate": 5.422062910012754e-06,
      "loss": 3.8218,
      "step": 42010
    },
    {
      "epoch": 2.5517702070808284,
      "grad_norm": 8.393756866455078,
      "learning_rate": 5.407670968876366e-06,
      "loss": 3.3326,
      "step": 42020
    },
    {
      "epoch": 2.5523774822372016,
      "grad_norm": 8.022753715515137,
      "learning_rate": 5.393297061588032e-06,
      "loss": 3.4383,
      "step": 42030
    },
    {
      "epoch": 2.552984757393575,
      "grad_norm": 6.4228997230529785,
      "learning_rate": 5.3789411939607904e-06,
      "loss": 3.1264,
      "step": 42040
    },
    {
      "epoch": 2.5535920325499486,
      "grad_norm": 9.758872985839844,
      "learning_rate": 5.364603371800375e-06,
      "loss": 3.5155,
      "step": 42050
    },
    {
      "epoch": 2.554199307706322,
      "grad_norm": 9.297613143920898,
      "learning_rate": 5.350283600905248e-06,
      "loss": 3.8333,
      "step": 42060
    },
    {
      "epoch": 2.554806582862695,
      "grad_norm": 8.191040992736816,
      "learning_rate": 5.335981887066538e-06,
      "loss": 3.8946,
      "step": 42070
    },
    {
      "epoch": 2.5554138580190684,
      "grad_norm": 6.954245567321777,
      "learning_rate": 5.321698236068096e-06,
      "loss": 3.7715,
      "step": 42080
    },
    {
      "epoch": 2.5560211331754417,
      "grad_norm": 6.550708293914795,
      "learning_rate": 5.307432653686445e-06,
      "loss": 3.807,
      "step": 42090
    },
    {
      "epoch": 2.5566284083318154,
      "grad_norm": 8.253494262695312,
      "learning_rate": 5.293185145690827e-06,
      "loss": 3.7408,
      "step": 42100
    },
    {
      "epoch": 2.5572356834881886,
      "grad_norm": 7.906269073486328,
      "learning_rate": 5.2789557178431535e-06,
      "loss": 3.8427,
      "step": 42110
    },
    {
      "epoch": 2.557842958644562,
      "grad_norm": 5.382870197296143,
      "learning_rate": 5.264744375898018e-06,
      "loss": 3.5943,
      "step": 42120
    },
    {
      "epoch": 2.558450233800935,
      "grad_norm": 6.1665496826171875,
      "learning_rate": 5.2505511256027425e-06,
      "loss": 3.5528,
      "step": 42130
    },
    {
      "epoch": 2.5590575089573084,
      "grad_norm": 8.215380668640137,
      "learning_rate": 5.2363759726972886e-06,
      "loss": 3.9013,
      "step": 42140
    },
    {
      "epoch": 2.559664784113682,
      "grad_norm": 6.529289722442627,
      "learning_rate": 5.222218922914312e-06,
      "loss": 3.8047,
      "step": 42150
    },
    {
      "epoch": 2.5602720592700554,
      "grad_norm": 4.323862075805664,
      "learning_rate": 5.2080799819791585e-06,
      "loss": 3.6247,
      "step": 42160
    },
    {
      "epoch": 2.5608793344264287,
      "grad_norm": 6.371924877166748,
      "learning_rate": 5.193959155609829e-06,
      "loss": 3.7375,
      "step": 42170
    },
    {
      "epoch": 2.561486609582802,
      "grad_norm": 5.571949481964111,
      "learning_rate": 5.179856449517018e-06,
      "loss": 3.6184,
      "step": 42180
    },
    {
      "epoch": 2.562093884739175,
      "grad_norm": 6.131163120269775,
      "learning_rate": 5.165771869404079e-06,
      "loss": 3.502,
      "step": 42190
    },
    {
      "epoch": 2.562701159895549,
      "grad_norm": 5.573098659515381,
      "learning_rate": 5.1517054209670405e-06,
      "loss": 3.5639,
      "step": 42200
    },
    {
      "epoch": 2.563308435051922,
      "grad_norm": 5.991480350494385,
      "learning_rate": 5.137657109894606e-06,
      "loss": 3.486,
      "step": 42210
    },
    {
      "epoch": 2.5639157102082955,
      "grad_norm": 7.098677635192871,
      "learning_rate": 5.1236269418681424e-06,
      "loss": 3.9485,
      "step": 42220
    },
    {
      "epoch": 2.5645229853646687,
      "grad_norm": 6.3480048179626465,
      "learning_rate": 5.1096149225616505e-06,
      "loss": 3.9246,
      "step": 42230
    },
    {
      "epoch": 2.565130260521042,
      "grad_norm": 8.695943832397461,
      "learning_rate": 5.095621057641825e-06,
      "loss": 3.999,
      "step": 42240
    },
    {
      "epoch": 2.5657375356774157,
      "grad_norm": 5.939596652984619,
      "learning_rate": 5.081645352768011e-06,
      "loss": 3.6735,
      "step": 42250
    },
    {
      "epoch": 2.5663448108337885,
      "grad_norm": 8.291768074035645,
      "learning_rate": 5.0676878135922034e-06,
      "loss": 3.6765,
      "step": 42260
    },
    {
      "epoch": 2.5669520859901622,
      "grad_norm": 9.108102798461914,
      "learning_rate": 5.05374844575906e-06,
      "loss": 3.4685,
      "step": 42270
    },
    {
      "epoch": 2.5675593611465355,
      "grad_norm": 11.23572063446045,
      "learning_rate": 5.03982725490586e-06,
      "loss": 3.6728,
      "step": 42280
    },
    {
      "epoch": 2.5681666363029088,
      "grad_norm": 8.961091041564941,
      "learning_rate": 5.0259242466626e-06,
      "loss": 4.1603,
      "step": 42290
    },
    {
      "epoch": 2.5687739114592825,
      "grad_norm": 7.867033958435059,
      "learning_rate": 5.012039426651833e-06,
      "loss": 3.8888,
      "step": 42300
    },
    {
      "epoch": 2.5693811866156553,
      "grad_norm": 7.75423526763916,
      "learning_rate": 4.9981728004888205e-06,
      "loss": 3.6998,
      "step": 42310
    },
    {
      "epoch": 2.569988461772029,
      "grad_norm": 11.509535789489746,
      "learning_rate": 4.9843243737814346e-06,
      "loss": 3.8005,
      "step": 42320
    },
    {
      "epoch": 2.5705957369284023,
      "grad_norm": 8.505875587463379,
      "learning_rate": 4.970494152130217e-06,
      "loss": 3.8337,
      "step": 42330
    },
    {
      "epoch": 2.5712030120847755,
      "grad_norm": 8.138344764709473,
      "learning_rate": 4.956682141128327e-06,
      "loss": 3.924,
      "step": 42340
    },
    {
      "epoch": 2.5718102872411492,
      "grad_norm": 16.1044979095459,
      "learning_rate": 4.942888346361557e-06,
      "loss": 3.8327,
      "step": 42350
    },
    {
      "epoch": 2.572417562397522,
      "grad_norm": 5.572579383850098,
      "learning_rate": 4.929112773408334e-06,
      "loss": 3.7605,
      "step": 42360
    },
    {
      "epoch": 2.5730248375538958,
      "grad_norm": 6.8320746421813965,
      "learning_rate": 4.915355427839724e-06,
      "loss": 3.8233,
      "step": 42370
    },
    {
      "epoch": 2.573632112710269,
      "grad_norm": 9.802935600280762,
      "learning_rate": 4.901616315219415e-06,
      "loss": 3.7365,
      "step": 42380
    },
    {
      "epoch": 2.5742393878666423,
      "grad_norm": 9.827689170837402,
      "learning_rate": 4.8878954411037125e-06,
      "loss": 3.4977,
      "step": 42390
    },
    {
      "epoch": 2.5748466630230156,
      "grad_norm": 9.861544609069824,
      "learning_rate": 4.874192811041578e-06,
      "loss": 3.4547,
      "step": 42400
    },
    {
      "epoch": 2.575453938179389,
      "grad_norm": 6.9252214431762695,
      "learning_rate": 4.860508430574557e-06,
      "loss": 3.9183,
      "step": 42410
    },
    {
      "epoch": 2.5760612133357625,
      "grad_norm": 10.458857536315918,
      "learning_rate": 4.846842305236843e-06,
      "loss": 3.9207,
      "step": 42420
    },
    {
      "epoch": 2.576668488492136,
      "grad_norm": 10.050243377685547,
      "learning_rate": 4.8331944405552275e-06,
      "loss": 4.122,
      "step": 42430
    },
    {
      "epoch": 2.577275763648509,
      "grad_norm": 6.9961018562316895,
      "learning_rate": 4.819564842049118e-06,
      "loss": 3.695,
      "step": 42440
    },
    {
      "epoch": 2.5778830388048823,
      "grad_norm": 7.428250312805176,
      "learning_rate": 4.805953515230554e-06,
      "loss": 3.7185,
      "step": 42450
    },
    {
      "epoch": 2.5784903139612556,
      "grad_norm": 6.840975761413574,
      "learning_rate": 4.7923604656041675e-06,
      "loss": 3.9772,
      "step": 42460
    },
    {
      "epoch": 2.5790975891176293,
      "grad_norm": 6.458374977111816,
      "learning_rate": 4.778785698667193e-06,
      "loss": 3.872,
      "step": 42470
    },
    {
      "epoch": 2.5797048642740026,
      "grad_norm": 5.819318771362305,
      "learning_rate": 4.765229219909501e-06,
      "loss": 3.6137,
      "step": 42480
    },
    {
      "epoch": 2.580312139430376,
      "grad_norm": 6.4409918785095215,
      "learning_rate": 4.751691034813544e-06,
      "loss": 3.6087,
      "step": 42490
    },
    {
      "epoch": 2.580919414586749,
      "grad_norm": 7.221935272216797,
      "learning_rate": 4.7381711488543816e-06,
      "loss": 3.6064,
      "step": 42500
    },
    {
      "epoch": 2.5815266897431224,
      "grad_norm": 5.661789894104004,
      "learning_rate": 4.72466956749964e-06,
      "loss": 3.8621,
      "step": 42510
    },
    {
      "epoch": 2.582133964899496,
      "grad_norm": 5.53878927230835,
      "learning_rate": 4.711186296209613e-06,
      "loss": 3.7239,
      "step": 42520
    },
    {
      "epoch": 2.5827412400558694,
      "grad_norm": 7.1363701820373535,
      "learning_rate": 4.697721340437139e-06,
      "loss": 3.9451,
      "step": 42530
    },
    {
      "epoch": 2.5833485152122426,
      "grad_norm": 7.024799346923828,
      "learning_rate": 4.68427470562765e-06,
      "loss": 3.8784,
      "step": 42540
    },
    {
      "epoch": 2.583955790368616,
      "grad_norm": 6.179656505584717,
      "learning_rate": 4.670846397219187e-06,
      "loss": 4.005,
      "step": 42550
    },
    {
      "epoch": 2.584563065524989,
      "grad_norm": 5.160618305206299,
      "learning_rate": 4.6574364206423695e-06,
      "loss": 3.7367,
      "step": 42560
    },
    {
      "epoch": 2.585170340681363,
      "grad_norm": 5.1602983474731445,
      "learning_rate": 4.644044781320422e-06,
      "loss": 3.8464,
      "step": 42570
    },
    {
      "epoch": 2.585777615837736,
      "grad_norm": 5.173073768615723,
      "learning_rate": 4.6306714846691025e-06,
      "loss": 3.9525,
      "step": 42580
    },
    {
      "epoch": 2.5863848909941094,
      "grad_norm": 8.345770835876465,
      "learning_rate": 4.617316536096816e-06,
      "loss": 4.2182,
      "step": 42590
    },
    {
      "epoch": 2.5869921661504827,
      "grad_norm": 5.700467109680176,
      "learning_rate": 4.603979941004505e-06,
      "loss": 3.6774,
      "step": 42600
    },
    {
      "epoch": 2.587599441306856,
      "grad_norm": 5.873910903930664,
      "learning_rate": 4.590661704785698e-06,
      "loss": 3.6226,
      "step": 42610
    },
    {
      "epoch": 2.5882067164632296,
      "grad_norm": 6.2193193435668945,
      "learning_rate": 4.577361832826499e-06,
      "loss": 3.7728,
      "step": 42620
    },
    {
      "epoch": 2.588813991619603,
      "grad_norm": 6.613841533660889,
      "learning_rate": 4.56408033050561e-06,
      "loss": 3.4525,
      "step": 42630
    },
    {
      "epoch": 2.589421266775976,
      "grad_norm": 7.872379302978516,
      "learning_rate": 4.550817203194257e-06,
      "loss": 3.5138,
      "step": 42640
    },
    {
      "epoch": 2.5900285419323494,
      "grad_norm": 7.412203311920166,
      "learning_rate": 4.537572456256267e-06,
      "loss": 3.6941,
      "step": 42650
    },
    {
      "epoch": 2.5906358170887227,
      "grad_norm": 5.928974151611328,
      "learning_rate": 4.524346095048021e-06,
      "loss": 3.6604,
      "step": 42660
    },
    {
      "epoch": 2.5912430922450964,
      "grad_norm": 5.998292922973633,
      "learning_rate": 4.51113812491849e-06,
      "loss": 3.4872,
      "step": 42670
    },
    {
      "epoch": 2.5918503674014697,
      "grad_norm": 7.31710147857666,
      "learning_rate": 4.497948551209169e-06,
      "loss": 3.5973,
      "step": 42680
    },
    {
      "epoch": 2.592457642557843,
      "grad_norm": 8.896223068237305,
      "learning_rate": 4.48477737925414e-06,
      "loss": 3.9428,
      "step": 42690
    },
    {
      "epoch": 2.593064917714216,
      "grad_norm": 9.476240158081055,
      "learning_rate": 4.471624614380038e-06,
      "loss": 4.0044,
      "step": 42700
    },
    {
      "epoch": 2.5936721928705895,
      "grad_norm": 7.982412338256836,
      "learning_rate": 4.45849026190604e-06,
      "loss": 3.5757,
      "step": 42710
    },
    {
      "epoch": 2.594279468026963,
      "grad_norm": 9.40401554107666,
      "learning_rate": 4.445374327143892e-06,
      "loss": 3.6027,
      "step": 42720
    },
    {
      "epoch": 2.5948867431833365,
      "grad_norm": 6.601041316986084,
      "learning_rate": 4.43227681539789e-06,
      "loss": 3.7716,
      "step": 42730
    },
    {
      "epoch": 2.5954940183397097,
      "grad_norm": 9.497100830078125,
      "learning_rate": 4.419197731964858e-06,
      "loss": 3.65,
      "step": 42740
    },
    {
      "epoch": 2.596101293496083,
      "grad_norm": 6.908984661102295,
      "learning_rate": 4.4061370821342205e-06,
      "loss": 3.5951,
      "step": 42750
    },
    {
      "epoch": 2.5967085686524563,
      "grad_norm": 7.7683868408203125,
      "learning_rate": 4.393094871187886e-06,
      "loss": 3.761,
      "step": 42760
    },
    {
      "epoch": 2.59731584380883,
      "grad_norm": 6.784564971923828,
      "learning_rate": 4.380071104400341e-06,
      "loss": 3.7791,
      "step": 42770
    },
    {
      "epoch": 2.5979231189652032,
      "grad_norm": 9.059990882873535,
      "learning_rate": 4.367065787038604e-06,
      "loss": 3.7398,
      "step": 42780
    },
    {
      "epoch": 2.5985303941215765,
      "grad_norm": 6.1960320472717285,
      "learning_rate": 4.354078924362226e-06,
      "loss": 4.0356,
      "step": 42790
    },
    {
      "epoch": 2.5991376692779498,
      "grad_norm": 8.431634902954102,
      "learning_rate": 4.34111052162331e-06,
      "loss": 4.0097,
      "step": 42800
    },
    {
      "epoch": 2.599744944434323,
      "grad_norm": 4.937150001525879,
      "learning_rate": 4.3281605840664706e-06,
      "loss": 4.1682,
      "step": 42810
    },
    {
      "epoch": 2.6003522195906967,
      "grad_norm": 6.489490032196045,
      "learning_rate": 4.315229116928893e-06,
      "loss": 4.1162,
      "step": 42820
    },
    {
      "epoch": 2.60095949474707,
      "grad_norm": 6.604197025299072,
      "learning_rate": 4.302316125440248e-06,
      "loss": 3.9722,
      "step": 42830
    },
    {
      "epoch": 2.6015667699034433,
      "grad_norm": 7.37163782119751,
      "learning_rate": 4.2894216148227636e-06,
      "loss": 4.0033,
      "step": 42840
    },
    {
      "epoch": 2.6021740450598165,
      "grad_norm": 7.784292221069336,
      "learning_rate": 4.276545590291186e-06,
      "loss": 3.9686,
      "step": 42850
    },
    {
      "epoch": 2.60278132021619,
      "grad_norm": 8.532334327697754,
      "learning_rate": 4.263688057052778e-06,
      "loss": 3.7255,
      "step": 42860
    },
    {
      "epoch": 2.6033885953725635,
      "grad_norm": 8.178911209106445,
      "learning_rate": 4.2508490203073344e-06,
      "loss": 3.8574,
      "step": 42870
    },
    {
      "epoch": 2.6039958705289368,
      "grad_norm": 7.671751976013184,
      "learning_rate": 4.23802848524717e-06,
      "loss": 3.8664,
      "step": 42880
    },
    {
      "epoch": 2.60460314568531,
      "grad_norm": 7.102042198181152,
      "learning_rate": 4.225226457057096e-06,
      "loss": 3.744,
      "step": 42890
    },
    {
      "epoch": 2.6052104208416833,
      "grad_norm": 7.327637195587158,
      "learning_rate": 4.212442940914485e-06,
      "loss": 3.7563,
      "step": 42900
    },
    {
      "epoch": 2.6058176959980566,
      "grad_norm": 9.396851539611816,
      "learning_rate": 4.199677941989183e-06,
      "loss": 4.0129,
      "step": 42910
    },
    {
      "epoch": 2.6064249711544303,
      "grad_norm": 9.79915714263916,
      "learning_rate": 4.18693146544355e-06,
      "loss": 4.0144,
      "step": 42920
    },
    {
      "epoch": 2.6070322463108035,
      "grad_norm": 10.117754936218262,
      "learning_rate": 4.174203516432456e-06,
      "loss": 3.7721,
      "step": 42930
    },
    {
      "epoch": 2.607639521467177,
      "grad_norm": 9.142223358154297,
      "learning_rate": 4.1614941001033135e-06,
      "loss": 3.7398,
      "step": 42940
    },
    {
      "epoch": 2.60824679662355,
      "grad_norm": 9.46265697479248,
      "learning_rate": 4.148803221595998e-06,
      "loss": 3.5529,
      "step": 42950
    },
    {
      "epoch": 2.6088540717799233,
      "grad_norm": 6.1261820793151855,
      "learning_rate": 4.136130886042905e-06,
      "loss": 3.9551,
      "step": 42960
    },
    {
      "epoch": 2.609461346936297,
      "grad_norm": 7.420252799987793,
      "learning_rate": 4.1234770985689284e-06,
      "loss": 4.0075,
      "step": 42970
    },
    {
      "epoch": 2.6100686220926703,
      "grad_norm": 7.867307186126709,
      "learning_rate": 4.110841864291459e-06,
      "loss": 3.9934,
      "step": 42980
    },
    {
      "epoch": 2.6106758972490436,
      "grad_norm": 7.453133583068848,
      "learning_rate": 4.098225188320387e-06,
      "loss": 3.9997,
      "step": 42990
    },
    {
      "epoch": 2.611283172405417,
      "grad_norm": 6.258919715881348,
      "learning_rate": 4.0856270757581025e-06,
      "loss": 3.6915,
      "step": 43000
    },
    {
      "epoch": 2.61189044756179,
      "grad_norm": 6.12811279296875,
      "learning_rate": 4.07304753169947e-06,
      "loss": 3.6913,
      "step": 43010
    },
    {
      "epoch": 2.612497722718164,
      "grad_norm": 8.4098539352417,
      "learning_rate": 4.060486561231875e-06,
      "loss": 4.0719,
      "step": 43020
    },
    {
      "epoch": 2.613104997874537,
      "grad_norm": 9.32599925994873,
      "learning_rate": 4.0479441694351634e-06,
      "loss": 3.9046,
      "step": 43030
    },
    {
      "epoch": 2.6137122730309104,
      "grad_norm": 6.61264705657959,
      "learning_rate": 4.035420361381681e-06,
      "loss": 4.0858,
      "step": 43040
    },
    {
      "epoch": 2.6143195481872836,
      "grad_norm": 7.932532787322998,
      "learning_rate": 4.022915142136257e-06,
      "loss": 3.8779,
      "step": 43050
    },
    {
      "epoch": 2.614926823343657,
      "grad_norm": 9.35326862335205,
      "learning_rate": 4.010428516756198e-06,
      "loss": 3.9346,
      "step": 43060
    },
    {
      "epoch": 2.6155340985000306,
      "grad_norm": 7.394750118255615,
      "learning_rate": 3.997960490291297e-06,
      "loss": 3.9819,
      "step": 43070
    },
    {
      "epoch": 2.616141373656404,
      "grad_norm": 6.341104984283447,
      "learning_rate": 3.985511067783809e-06,
      "loss": 4.016,
      "step": 43080
    },
    {
      "epoch": 2.616748648812777,
      "grad_norm": 9.409540176391602,
      "learning_rate": 3.973080254268502e-06,
      "loss": 4.0014,
      "step": 43090
    },
    {
      "epoch": 2.6173559239691504,
      "grad_norm": 6.2230000495910645,
      "learning_rate": 3.960668054772576e-06,
      "loss": 3.7687,
      "step": 43100
    },
    {
      "epoch": 2.6179631991255237,
      "grad_norm": 7.104257106781006,
      "learning_rate": 3.948274474315739e-06,
      "loss": 3.7257,
      "step": 43110
    },
    {
      "epoch": 2.6185704742818974,
      "grad_norm": 6.336688995361328,
      "learning_rate": 3.935899517910136e-06,
      "loss": 3.7441,
      "step": 43120
    },
    {
      "epoch": 2.6191777494382706,
      "grad_norm": 7.6010613441467285,
      "learning_rate": 3.923543190560408e-06,
      "loss": 3.8782,
      "step": 43130
    },
    {
      "epoch": 2.619785024594644,
      "grad_norm": 8.744675636291504,
      "learning_rate": 3.911205497263642e-06,
      "loss": 3.9638,
      "step": 43140
    },
    {
      "epoch": 2.620392299751017,
      "grad_norm": 8.055829048156738,
      "learning_rate": 3.898886443009403e-06,
      "loss": 3.6041,
      "step": 43150
    },
    {
      "epoch": 2.6209995749073904,
      "grad_norm": 8.255582809448242,
      "learning_rate": 3.886586032779704e-06,
      "loss": 3.5764,
      "step": 43160
    },
    {
      "epoch": 2.621606850063764,
      "grad_norm": 6.683435440063477,
      "learning_rate": 3.874304271549039e-06,
      "loss": 3.7149,
      "step": 43170
    },
    {
      "epoch": 2.622214125220137,
      "grad_norm": 7.010900020599365,
      "learning_rate": 3.862041164284347e-06,
      "loss": 3.7022,
      "step": 43180
    },
    {
      "epoch": 2.6228214003765107,
      "grad_norm": 7.609306335449219,
      "learning_rate": 3.849796715945025e-06,
      "loss": 3.8696,
      "step": 43190
    },
    {
      "epoch": 2.623428675532884,
      "grad_norm": 7.428755760192871,
      "learning_rate": 3.837570931482904e-06,
      "loss": 3.9967,
      "step": 43200
    },
    {
      "epoch": 2.624035950689257,
      "grad_norm": 8.979872703552246,
      "learning_rate": 3.825363815842309e-06,
      "loss": 4.0397,
      "step": 43210
    },
    {
      "epoch": 2.624643225845631,
      "grad_norm": 8.325953483581543,
      "learning_rate": 3.8131753739599764e-06,
      "loss": 4.0957,
      "step": 43220
    },
    {
      "epoch": 2.6252505010020037,
      "grad_norm": 7.959798812866211,
      "learning_rate": 3.801005610765118e-06,
      "loss": 3.7383,
      "step": 43230
    },
    {
      "epoch": 2.6258577761583775,
      "grad_norm": 9.64794635772705,
      "learning_rate": 3.7888545311793645e-06,
      "loss": 4.0895,
      "step": 43240
    },
    {
      "epoch": 2.6264650513147507,
      "grad_norm": 9.852066993713379,
      "learning_rate": 3.7767221401168283e-06,
      "loss": 3.684,
      "step": 43250
    },
    {
      "epoch": 2.627072326471124,
      "grad_norm": 9.612275123596191,
      "learning_rate": 3.7646084424840245e-06,
      "loss": 3.6888,
      "step": 43260
    },
    {
      "epoch": 2.6276796016274977,
      "grad_norm": 11.305070877075195,
      "learning_rate": 3.7525134431799223e-06,
      "loss": 3.8682,
      "step": 43270
    },
    {
      "epoch": 2.6282868767838705,
      "grad_norm": 7.546562671661377,
      "learning_rate": 3.740437147095949e-06,
      "loss": 3.6463,
      "step": 43280
    },
    {
      "epoch": 2.6288941519402442,
      "grad_norm": 8.239426612854004,
      "learning_rate": 3.7283795591159353e-06,
      "loss": 3.7327,
      "step": 43290
    },
    {
      "epoch": 2.6295014270966175,
      "grad_norm": 9.505099296569824,
      "learning_rate": 3.716340684116176e-06,
      "loss": 3.7897,
      "step": 43300
    },
    {
      "epoch": 2.6301087022529908,
      "grad_norm": 12.035463333129883,
      "learning_rate": 3.7043205269653605e-06,
      "loss": 3.8693,
      "step": 43310
    },
    {
      "epoch": 2.630715977409364,
      "grad_norm": 6.645949840545654,
      "learning_rate": 3.692319092524671e-06,
      "loss": 3.9565,
      "step": 43320
    },
    {
      "epoch": 2.6313232525657373,
      "grad_norm": 7.096540451049805,
      "learning_rate": 3.6803363856476426e-06,
      "loss": 4.2295,
      "step": 43330
    },
    {
      "epoch": 2.631930527722111,
      "grad_norm": 6.146286964416504,
      "learning_rate": 3.6683724111802874e-06,
      "loss": 3.8762,
      "step": 43340
    },
    {
      "epoch": 2.6325378028784843,
      "grad_norm": 5.249961853027344,
      "learning_rate": 3.6564271739610136e-06,
      "loss": 3.9547,
      "step": 43350
    },
    {
      "epoch": 2.6331450780348575,
      "grad_norm": 4.868539810180664,
      "learning_rate": 3.644500678820689e-06,
      "loss": 3.8375,
      "step": 43360
    },
    {
      "epoch": 2.633752353191231,
      "grad_norm": 5.776801109313965,
      "learning_rate": 3.6325929305825623e-06,
      "loss": 3.899,
      "step": 43370
    },
    {
      "epoch": 2.634359628347604,
      "grad_norm": 7.022653102874756,
      "learning_rate": 3.620703934062325e-06,
      "loss": 3.9343,
      "step": 43380
    },
    {
      "epoch": 2.6349669035039778,
      "grad_norm": 5.498514652252197,
      "learning_rate": 3.6088336940680702e-06,
      "loss": 3.8841,
      "step": 43390
    },
    {
      "epoch": 2.635574178660351,
      "grad_norm": 4.356518268585205,
      "learning_rate": 3.596982215400313e-06,
      "loss": 3.7708,
      "step": 43400
    },
    {
      "epoch": 2.6361814538167243,
      "grad_norm": 5.442893981933594,
      "learning_rate": 3.585149502851981e-06,
      "loss": 3.8668,
      "step": 43410
    },
    {
      "epoch": 2.6367887289730976,
      "grad_norm": 5.2284722328186035,
      "learning_rate": 3.5733355612084128e-06,
      "loss": 3.8358,
      "step": 43420
    },
    {
      "epoch": 2.637396004129471,
      "grad_norm": 5.955626010894775,
      "learning_rate": 3.5615403952473423e-06,
      "loss": 3.7559,
      "step": 43430
    },
    {
      "epoch": 2.6380032792858445,
      "grad_norm": 6.632540225982666,
      "learning_rate": 3.5497640097389427e-06,
      "loss": 4.0387,
      "step": 43440
    },
    {
      "epoch": 2.638610554442218,
      "grad_norm": 5.921104431152344,
      "learning_rate": 3.53800640944576e-06,
      "loss": 3.774,
      "step": 43450
    },
    {
      "epoch": 2.639217829598591,
      "grad_norm": 5.433637619018555,
      "learning_rate": 3.526267599122762e-06,
      "loss": 3.5933,
      "step": 43460
    },
    {
      "epoch": 2.6398251047549643,
      "grad_norm": 6.3824663162231445,
      "learning_rate": 3.514547583517308e-06,
      "loss": 3.8328,
      "step": 43470
    },
    {
      "epoch": 2.6404323799113376,
      "grad_norm": 5.528193473815918,
      "learning_rate": 3.5028463673691504e-06,
      "loss": 3.7143,
      "step": 43480
    },
    {
      "epoch": 2.6410396550677113,
      "grad_norm": 5.406456470489502,
      "learning_rate": 3.491163955410459e-06,
      "loss": 3.6615,
      "step": 43490
    },
    {
      "epoch": 2.6416469302240846,
      "grad_norm": 4.870041847229004,
      "learning_rate": 3.479500352365783e-06,
      "loss": 3.4018,
      "step": 43500
    },
    {
      "epoch": 2.642254205380458,
      "grad_norm": 5.344148635864258,
      "learning_rate": 3.4678555629520547e-06,
      "loss": 3.7675,
      "step": 43510
    },
    {
      "epoch": 2.642861480536831,
      "grad_norm": 5.921706676483154,
      "learning_rate": 3.4562295918786403e-06,
      "loss": 3.5081,
      "step": 43520
    },
    {
      "epoch": 2.6434687556932044,
      "grad_norm": 6.007984161376953,
      "learning_rate": 3.4446224438472564e-06,
      "loss": 3.491,
      "step": 43530
    },
    {
      "epoch": 2.644076030849578,
      "grad_norm": 7.198062896728516,
      "learning_rate": 3.4330341235520035e-06,
      "loss": 4.3002,
      "step": 43540
    },
    {
      "epoch": 2.6446833060059514,
      "grad_norm": 5.820985794067383,
      "learning_rate": 3.4214646356794055e-06,
      "loss": 3.7799,
      "step": 43550
    },
    {
      "epoch": 2.6452905811623246,
      "grad_norm": 5.225235462188721,
      "learning_rate": 3.4099139849083307e-06,
      "loss": 3.863,
      "step": 43560
    },
    {
      "epoch": 2.645897856318698,
      "grad_norm": 5.440532207489014,
      "learning_rate": 3.3983821759100587e-06,
      "loss": 3.8705,
      "step": 43570
    },
    {
      "epoch": 2.646505131475071,
      "grad_norm": 7.315376281738281,
      "learning_rate": 3.3868692133482205e-06,
      "loss": 3.8282,
      "step": 43580
    },
    {
      "epoch": 2.647112406631445,
      "grad_norm": 7.459756374359131,
      "learning_rate": 3.375375101878858e-06,
      "loss": 4.0133,
      "step": 43590
    },
    {
      "epoch": 2.647719681787818,
      "grad_norm": 10.352322578430176,
      "learning_rate": 3.3638998461503756e-06,
      "loss": 3.6842,
      "step": 43600
    },
    {
      "epoch": 2.6483269569441914,
      "grad_norm": 6.811849117279053,
      "learning_rate": 3.352443450803533e-06,
      "loss": 3.7259,
      "step": 43610
    },
    {
      "epoch": 2.6489342321005647,
      "grad_norm": 6.549104690551758,
      "learning_rate": 3.3410059204714795e-06,
      "loss": 3.7368,
      "step": 43620
    },
    {
      "epoch": 2.649541507256938,
      "grad_norm": 5.306596755981445,
      "learning_rate": 3.3295872597797484e-06,
      "loss": 3.8426,
      "step": 43630
    },
    {
      "epoch": 2.6501487824133116,
      "grad_norm": 8.167237281799316,
      "learning_rate": 3.318187473346224e-06,
      "loss": 3.7469,
      "step": 43640
    },
    {
      "epoch": 2.650756057569685,
      "grad_norm": 6.951498031616211,
      "learning_rate": 3.3068065657811565e-06,
      "loss": 3.9206,
      "step": 43650
    },
    {
      "epoch": 2.651363332726058,
      "grad_norm": 6.919229984283447,
      "learning_rate": 3.2954445416871647e-06,
      "loss": 3.7082,
      "step": 43660
    },
    {
      "epoch": 2.6519706078824314,
      "grad_norm": 6.1209259033203125,
      "learning_rate": 3.28410140565924e-06,
      "loss": 3.4716,
      "step": 43670
    },
    {
      "epoch": 2.6525778830388047,
      "grad_norm": 6.216311454772949,
      "learning_rate": 3.2727771622847235e-06,
      "loss": 3.7316,
      "step": 43680
    },
    {
      "epoch": 2.6531851581951784,
      "grad_norm": 5.473995685577393,
      "learning_rate": 3.2614718161433187e-06,
      "loss": 3.867,
      "step": 43690
    },
    {
      "epoch": 2.6537924333515517,
      "grad_norm": 6.472269535064697,
      "learning_rate": 3.250185371807074e-06,
      "loss": 3.7318,
      "step": 43700
    },
    {
      "epoch": 2.654399708507925,
      "grad_norm": 7.423208236694336,
      "learning_rate": 3.238917833840438e-06,
      "loss": 3.8728,
      "step": 43710
    },
    {
      "epoch": 2.655006983664298,
      "grad_norm": 6.319300651550293,
      "learning_rate": 3.22766920680016e-06,
      "loss": 3.9161,
      "step": 43720
    },
    {
      "epoch": 2.6556142588206715,
      "grad_norm": 6.917909622192383,
      "learning_rate": 3.216439495235368e-06,
      "loss": 3.9636,
      "step": 43730
    },
    {
      "epoch": 2.656221533977045,
      "grad_norm": 6.611261367797852,
      "learning_rate": 3.2052287036875407e-06,
      "loss": 3.7109,
      "step": 43740
    },
    {
      "epoch": 2.6568288091334185,
      "grad_norm": 5.756270885467529,
      "learning_rate": 3.1940368366905005e-06,
      "loss": 3.6797,
      "step": 43750
    },
    {
      "epoch": 2.6574360842897917,
      "grad_norm": 7.213639259338379,
      "learning_rate": 3.1828638987704105e-06,
      "loss": 3.781,
      "step": 43760
    },
    {
      "epoch": 2.658043359446165,
      "grad_norm": 9.885762214660645,
      "learning_rate": 3.1717098944457847e-06,
      "loss": 3.6118,
      "step": 43770
    },
    {
      "epoch": 2.6586506346025383,
      "grad_norm": 6.509820938110352,
      "learning_rate": 3.160574828227486e-06,
      "loss": 3.7845,
      "step": 43780
    },
    {
      "epoch": 2.659257909758912,
      "grad_norm": 7.496149063110352,
      "learning_rate": 3.149458704618713e-06,
      "loss": 3.9781,
      "step": 43790
    },
    {
      "epoch": 2.6598651849152852,
      "grad_norm": 7.427824020385742,
      "learning_rate": 3.138361528114997e-06,
      "loss": 4.101,
      "step": 43800
    },
    {
      "epoch": 2.6604724600716585,
      "grad_norm": 7.0301594734191895,
      "learning_rate": 3.1272833032042203e-06,
      "loss": 3.9476,
      "step": 43810
    },
    {
      "epoch": 2.6610797352280318,
      "grad_norm": 8.172950744628906,
      "learning_rate": 3.1162240343665883e-06,
      "loss": 4.154,
      "step": 43820
    },
    {
      "epoch": 2.661687010384405,
      "grad_norm": 5.941287994384766,
      "learning_rate": 3.105183726074645e-06,
      "loss": 3.7763,
      "step": 43830
    },
    {
      "epoch": 2.6622942855407787,
      "grad_norm": 7.942244052886963,
      "learning_rate": 3.0941623827932697e-06,
      "loss": 4.2078,
      "step": 43840
    },
    {
      "epoch": 2.662901560697152,
      "grad_norm": 8.085511207580566,
      "learning_rate": 3.0831600089796632e-06,
      "loss": 4.0898,
      "step": 43850
    },
    {
      "epoch": 2.6635088358535253,
      "grad_norm": 6.66991662979126,
      "learning_rate": 3.0721766090833715e-06,
      "loss": 3.8655,
      "step": 43860
    },
    {
      "epoch": 2.6641161110098985,
      "grad_norm": 5.328240871429443,
      "learning_rate": 3.0612121875462476e-06,
      "loss": 4.0803,
      "step": 43870
    },
    {
      "epoch": 2.664723386166272,
      "grad_norm": 6.094661712646484,
      "learning_rate": 3.0502667488024882e-06,
      "loss": 3.8897,
      "step": 43880
    },
    {
      "epoch": 2.6653306613226455,
      "grad_norm": 5.708083629608154,
      "learning_rate": 3.0393402972785813e-06,
      "loss": 3.9483,
      "step": 43890
    },
    {
      "epoch": 2.6659379364790188,
      "grad_norm": 6.220958709716797,
      "learning_rate": 3.0284328373933855e-06,
      "loss": 4.0299,
      "step": 43900
    },
    {
      "epoch": 2.666545211635392,
      "grad_norm": 6.951364994049072,
      "learning_rate": 3.0175443735580345e-06,
      "loss": 3.687,
      "step": 43910
    },
    {
      "epoch": 2.6671524867917653,
      "grad_norm": 7.985813140869141,
      "learning_rate": 3.0066749101759992e-06,
      "loss": 3.7676,
      "step": 43920
    },
    {
      "epoch": 2.6677597619481386,
      "grad_norm": 6.058926105499268,
      "learning_rate": 2.9958244516430535e-06,
      "loss": 3.9514,
      "step": 43930
    },
    {
      "epoch": 2.6683670371045123,
      "grad_norm": 4.192285060882568,
      "learning_rate": 2.984993002347325e-06,
      "loss": 3.9076,
      "step": 43940
    },
    {
      "epoch": 2.6689743122608856,
      "grad_norm": 5.284934997558594,
      "learning_rate": 2.9741805666692046e-06,
      "loss": 3.9614,
      "step": 43950
    },
    {
      "epoch": 2.669581587417259,
      "grad_norm": 6.65277099609375,
      "learning_rate": 2.9633871489814057e-06,
      "loss": 4.1237,
      "step": 43960
    },
    {
      "epoch": 2.670188862573632,
      "grad_norm": 6.152524948120117,
      "learning_rate": 2.9526127536489644e-06,
      "loss": 3.9326,
      "step": 43970
    },
    {
      "epoch": 2.6707961377300053,
      "grad_norm": 7.840274810791016,
      "learning_rate": 2.9418573850292286e-06,
      "loss": 4.1934,
      "step": 43980
    },
    {
      "epoch": 2.671403412886379,
      "grad_norm": 8.165939331054688,
      "learning_rate": 2.9311210474718355e-06,
      "loss": 3.827,
      "step": 43990
    },
    {
      "epoch": 2.6720106880427523,
      "grad_norm": 7.246501922607422,
      "learning_rate": 2.9204037453187283e-06,
      "loss": 3.9268,
      "step": 44000
    },
    {
      "epoch": 2.6726179631991256,
      "grad_norm": 6.664638996124268,
      "learning_rate": 2.9097054829041514e-06,
      "loss": 3.8917,
      "step": 44010
    },
    {
      "epoch": 2.673225238355499,
      "grad_norm": 5.7581610679626465,
      "learning_rate": 2.899026264554655e-06,
      "loss": 3.8405,
      "step": 44020
    },
    {
      "epoch": 2.673832513511872,
      "grad_norm": 5.269912242889404,
      "learning_rate": 2.88836609458909e-06,
      "loss": 3.8552,
      "step": 44030
    },
    {
      "epoch": 2.674439788668246,
      "grad_norm": 5.709246635437012,
      "learning_rate": 2.8777249773185855e-06,
      "loss": 3.8718,
      "step": 44040
    },
    {
      "epoch": 2.675047063824619,
      "grad_norm": 5.694291591644287,
      "learning_rate": 2.867102917046599e-06,
      "loss": 3.8991,
      "step": 44050
    },
    {
      "epoch": 2.6756543389809924,
      "grad_norm": 5.015069484710693,
      "learning_rate": 2.8564999180688447e-06,
      "loss": 4.0507,
      "step": 44060
    },
    {
      "epoch": 2.6762616141373656,
      "grad_norm": 5.640233993530273,
      "learning_rate": 2.8459159846733528e-06,
      "loss": 3.9589,
      "step": 44070
    },
    {
      "epoch": 2.676868889293739,
      "grad_norm": 5.008426189422607,
      "learning_rate": 2.835351121140434e-06,
      "loss": 3.7543,
      "step": 44080
    },
    {
      "epoch": 2.6774761644501126,
      "grad_norm": 6.2327094078063965,
      "learning_rate": 2.82480533174268e-06,
      "loss": 3.9266,
      "step": 44090
    },
    {
      "epoch": 2.6780834396064854,
      "grad_norm": 6.268536567687988,
      "learning_rate": 2.814278620744987e-06,
      "loss": 3.8833,
      "step": 44100
    },
    {
      "epoch": 2.678690714762859,
      "grad_norm": 6.125851154327393,
      "learning_rate": 2.803770992404514e-06,
      "loss": 3.9792,
      "step": 44110
    },
    {
      "epoch": 2.6792979899192324,
      "grad_norm": 6.385724067687988,
      "learning_rate": 2.793282450970708e-06,
      "loss": 4.1145,
      "step": 44120
    },
    {
      "epoch": 2.6799052650756057,
      "grad_norm": 5.720548629760742,
      "learning_rate": 2.7828130006853246e-06,
      "loss": 3.935,
      "step": 44130
    },
    {
      "epoch": 2.6805125402319794,
      "grad_norm": 5.945166110992432,
      "learning_rate": 2.7723626457823614e-06,
      "loss": 4.1014,
      "step": 44140
    },
    {
      "epoch": 2.681119815388352,
      "grad_norm": 7.4112162590026855,
      "learning_rate": 2.7619313904881094e-06,
      "loss": 3.9503,
      "step": 44150
    },
    {
      "epoch": 2.681727090544726,
      "grad_norm": 5.954587459564209,
      "learning_rate": 2.751519239021139e-06,
      "loss": 3.7999,
      "step": 44160
    },
    {
      "epoch": 2.682334365701099,
      "grad_norm": 6.60278844833374,
      "learning_rate": 2.7411261955922807e-06,
      "loss": 3.9161,
      "step": 44170
    },
    {
      "epoch": 2.6829416408574724,
      "grad_norm": 6.264689922332764,
      "learning_rate": 2.7307522644046567e-06,
      "loss": 3.6954,
      "step": 44180
    },
    {
      "epoch": 2.6835489160138457,
      "grad_norm": 5.9031453132629395,
      "learning_rate": 2.7203974496536445e-06,
      "loss": 3.8479,
      "step": 44190
    },
    {
      "epoch": 2.684156191170219,
      "grad_norm": 6.488574981689453,
      "learning_rate": 2.710061755526894e-06,
      "loss": 3.9148,
      "step": 44200
    },
    {
      "epoch": 2.6847634663265927,
      "grad_norm": 6.139149188995361,
      "learning_rate": 2.6997451862043276e-06,
      "loss": 3.6367,
      "step": 44210
    },
    {
      "epoch": 2.685370741482966,
      "grad_norm": 4.207306861877441,
      "learning_rate": 2.6894477458581425e-06,
      "loss": 3.6173,
      "step": 44220
    },
    {
      "epoch": 2.685978016639339,
      "grad_norm": 6.612926006317139,
      "learning_rate": 2.679169438652762e-06,
      "loss": 3.8212,
      "step": 44230
    },
    {
      "epoch": 2.6865852917957125,
      "grad_norm": 5.3190412521362305,
      "learning_rate": 2.6689102687449176e-06,
      "loss": 3.6356,
      "step": 44240
    },
    {
      "epoch": 2.6871925669520857,
      "grad_norm": 7.623624801635742,
      "learning_rate": 2.6586702402835728e-06,
      "loss": 3.6021,
      "step": 44250
    },
    {
      "epoch": 2.6877998421084595,
      "grad_norm": 8.067666053771973,
      "learning_rate": 2.6484493574099658e-06,
      "loss": 3.6832,
      "step": 44260
    },
    {
      "epoch": 2.6884071172648327,
      "grad_norm": 7.0963263511657715,
      "learning_rate": 2.6382476242575615e-06,
      "loss": 3.838,
      "step": 44270
    },
    {
      "epoch": 2.689014392421206,
      "grad_norm": 5.764078140258789,
      "learning_rate": 2.628065044952144e-06,
      "loss": 3.7158,
      "step": 44280
    },
    {
      "epoch": 2.6896216675775793,
      "grad_norm": 5.007065296173096,
      "learning_rate": 2.617901623611674e-06,
      "loss": 4.0305,
      "step": 44290
    },
    {
      "epoch": 2.6902289427339525,
      "grad_norm": 5.632218837738037,
      "learning_rate": 2.6077573643464138e-06,
      "loss": 4.1796,
      "step": 44300
    },
    {
      "epoch": 2.6908362178903262,
      "grad_norm": 4.232178688049316,
      "learning_rate": 2.5976322712588543e-06,
      "loss": 3.6403,
      "step": 44310
    },
    {
      "epoch": 2.6914434930466995,
      "grad_norm": 5.4206438064575195,
      "learning_rate": 2.587526348443764e-06,
      "loss": 3.7044,
      "step": 44320
    },
    {
      "epoch": 2.6920507682030728,
      "grad_norm": 6.454549312591553,
      "learning_rate": 2.5774395999881294e-06,
      "loss": 3.7979,
      "step": 44330
    },
    {
      "epoch": 2.692658043359446,
      "grad_norm": 5.575981140136719,
      "learning_rate": 2.567372029971188e-06,
      "loss": 3.6714,
      "step": 44340
    },
    {
      "epoch": 2.6932653185158193,
      "grad_norm": 4.210122585296631,
      "learning_rate": 2.557323642464432e-06,
      "loss": 3.7806,
      "step": 44350
    },
    {
      "epoch": 2.693872593672193,
      "grad_norm": 4.242825508117676,
      "learning_rate": 2.5472944415315837e-06,
      "loss": 3.7534,
      "step": 44360
    },
    {
      "epoch": 2.6944798688285663,
      "grad_norm": 4.874639511108398,
      "learning_rate": 2.537284431228615e-06,
      "loss": 4.0373,
      "step": 44370
    },
    {
      "epoch": 2.6950871439849395,
      "grad_norm": 6.003957271575928,
      "learning_rate": 2.527293615603732e-06,
      "loss": 3.8992,
      "step": 44380
    },
    {
      "epoch": 2.695694419141313,
      "grad_norm": 5.269852638244629,
      "learning_rate": 2.517321998697375e-06,
      "loss": 3.7537,
      "step": 44390
    },
    {
      "epoch": 2.696301694297686,
      "grad_norm": 5.045388698577881,
      "learning_rate": 2.5073695845422295e-06,
      "loss": 3.8788,
      "step": 44400
    },
    {
      "epoch": 2.69690896945406,
      "grad_norm": 6.878175258636475,
      "learning_rate": 2.4974363771632204e-06,
      "loss": 3.7662,
      "step": 44410
    },
    {
      "epoch": 2.697516244610433,
      "grad_norm": 6.348892688751221,
      "learning_rate": 2.4875223805774785e-06,
      "loss": 3.6212,
      "step": 44420
    },
    {
      "epoch": 2.6981235197668063,
      "grad_norm": 5.7255072593688965,
      "learning_rate": 2.477627598794391e-06,
      "loss": 3.7567,
      "step": 44430
    },
    {
      "epoch": 2.6987307949231796,
      "grad_norm": 4.930418014526367,
      "learning_rate": 2.467752035815563e-06,
      "loss": 4.0157,
      "step": 44440
    },
    {
      "epoch": 2.699338070079553,
      "grad_norm": 5.245756149291992,
      "learning_rate": 2.457895695634832e-06,
      "loss": 4.1009,
      "step": 44450
    },
    {
      "epoch": 2.6999453452359266,
      "grad_norm": 5.060605525970459,
      "learning_rate": 2.448058582238244e-06,
      "loss": 3.8372,
      "step": 44460
    },
    {
      "epoch": 2.7005526203923,
      "grad_norm": 4.435391426086426,
      "learning_rate": 2.4382406996041053e-06,
      "loss": 3.7768,
      "step": 44470
    },
    {
      "epoch": 2.701159895548673,
      "grad_norm": 5.541075706481934,
      "learning_rate": 2.428442051702917e-06,
      "loss": 3.803,
      "step": 44480
    },
    {
      "epoch": 2.7017671707050464,
      "grad_norm": 5.424156188964844,
      "learning_rate": 2.4186626424974045e-06,
      "loss": 4.0731,
      "step": 44490
    },
    {
      "epoch": 2.7023744458614196,
      "grad_norm": 4.089211940765381,
      "learning_rate": 2.4089024759425204e-06,
      "loss": 3.6607,
      "step": 44500
    },
    {
      "epoch": 2.7029817210177933,
      "grad_norm": 3.7627437114715576,
      "learning_rate": 2.39916155598543e-06,
      "loss": 3.6073,
      "step": 44510
    },
    {
      "epoch": 2.7035889961741666,
      "grad_norm": 4.452617168426514,
      "learning_rate": 2.3894398865655143e-06,
      "loss": 4.0063,
      "step": 44520
    },
    {
      "epoch": 2.70419627133054,
      "grad_norm": 6.745008945465088,
      "learning_rate": 2.3797374716143737e-06,
      "loss": 4.2645,
      "step": 44530
    },
    {
      "epoch": 2.704803546486913,
      "grad_norm": 5.179974555969238,
      "learning_rate": 2.370054315055814e-06,
      "loss": 4.1693,
      "step": 44540
    },
    {
      "epoch": 2.7054108216432864,
      "grad_norm": 5.249565124511719,
      "learning_rate": 2.360390420805869e-06,
      "loss": 4.3115,
      "step": 44550
    },
    {
      "epoch": 2.70601809679966,
      "grad_norm": 5.602412223815918,
      "learning_rate": 2.350745792772774e-06,
      "loss": 3.7255,
      "step": 44560
    },
    {
      "epoch": 2.7066253719560334,
      "grad_norm": 4.846261978149414,
      "learning_rate": 2.3411204348569527e-06,
      "loss": 4.0512,
      "step": 44570
    },
    {
      "epoch": 2.7072326471124066,
      "grad_norm": 5.1106367111206055,
      "learning_rate": 2.331514350951053e-06,
      "loss": 3.9731,
      "step": 44580
    },
    {
      "epoch": 2.70783992226878,
      "grad_norm": 5.253548622131348,
      "learning_rate": 2.3219275449399445e-06,
      "loss": 3.8371,
      "step": 44590
    },
    {
      "epoch": 2.708447197425153,
      "grad_norm": 5.703530788421631,
      "learning_rate": 2.3123600207006825e-06,
      "loss": 3.8381,
      "step": 44600
    },
    {
      "epoch": 2.709054472581527,
      "grad_norm": 4.053709506988525,
      "learning_rate": 2.302811782102515e-06,
      "loss": 3.9617,
      "step": 44610
    },
    {
      "epoch": 2.7096617477379,
      "grad_norm": 4.345765113830566,
      "learning_rate": 2.293282833006899e-06,
      "loss": 3.6582,
      "step": 44620
    },
    {
      "epoch": 2.7102690228942734,
      "grad_norm": 5.59677791595459,
      "learning_rate": 2.2837731772675176e-06,
      "loss": 3.5971,
      "step": 44630
    },
    {
      "epoch": 2.7108762980506467,
      "grad_norm": 4.918282985687256,
      "learning_rate": 2.2742828187302056e-06,
      "loss": 4.0237,
      "step": 44640
    },
    {
      "epoch": 2.71148357320702,
      "grad_norm": 3.682277202606201,
      "learning_rate": 2.264811761233021e-06,
      "loss": 3.9339,
      "step": 44650
    },
    {
      "epoch": 2.7120908483633936,
      "grad_norm": 4.278617858886719,
      "learning_rate": 2.2553600086061997e-06,
      "loss": 4.055,
      "step": 44660
    },
    {
      "epoch": 2.712698123519767,
      "grad_norm": 5.129177570343018,
      "learning_rate": 2.245927564672201e-06,
      "loss": 3.8395,
      "step": 44670
    },
    {
      "epoch": 2.71330539867614,
      "grad_norm": 4.2088398933410645,
      "learning_rate": 2.2365144332456457e-06,
      "loss": 4.0239,
      "step": 44680
    },
    {
      "epoch": 2.7139126738325134,
      "grad_norm": 6.6603264808654785,
      "learning_rate": 2.227120618133349e-06,
      "loss": 3.8584,
      "step": 44690
    },
    {
      "epoch": 2.7145199489888867,
      "grad_norm": 4.653142929077148,
      "learning_rate": 2.217746123134329e-06,
      "loss": 3.9653,
      "step": 44700
    },
    {
      "epoch": 2.7151272241452604,
      "grad_norm": 3.723043918609619,
      "learning_rate": 2.208390952039774e-06,
      "loss": 4.1154,
      "step": 44710
    },
    {
      "epoch": 2.7157344993016337,
      "grad_norm": 4.185772895812988,
      "learning_rate": 2.199055108633069e-06,
      "loss": 3.9541,
      "step": 44720
    },
    {
      "epoch": 2.716341774458007,
      "grad_norm": 6.476805686950684,
      "learning_rate": 2.1897385966897667e-06,
      "loss": 4.2475,
      "step": 44730
    },
    {
      "epoch": 2.71694904961438,
      "grad_norm": 4.657016277313232,
      "learning_rate": 2.18044141997763e-06,
      "loss": 3.7999,
      "step": 44740
    },
    {
      "epoch": 2.7175563247707535,
      "grad_norm": 4.9046430587768555,
      "learning_rate": 2.1711635822565746e-06,
      "loss": 3.822,
      "step": 44750
    },
    {
      "epoch": 2.718163599927127,
      "grad_norm": 4.680359840393066,
      "learning_rate": 2.161905087278704e-06,
      "loss": 3.434,
      "step": 44760
    },
    {
      "epoch": 2.7187708750835005,
      "grad_norm": 5.82283878326416,
      "learning_rate": 2.152665938788312e-06,
      "loss": 4.0881,
      "step": 44770
    },
    {
      "epoch": 2.7193781502398737,
      "grad_norm": 4.7882914543151855,
      "learning_rate": 2.143446140521843e-06,
      "loss": 4.2137,
      "step": 44780
    },
    {
      "epoch": 2.719985425396247,
      "grad_norm": 5.767758846282959,
      "learning_rate": 2.1342456962079428e-06,
      "loss": 3.7871,
      "step": 44790
    },
    {
      "epoch": 2.7205927005526203,
      "grad_norm": 5.980600357055664,
      "learning_rate": 2.125064609567412e-06,
      "loss": 4.0054,
      "step": 44800
    },
    {
      "epoch": 2.721199975708994,
      "grad_norm": 5.481406211853027,
      "learning_rate": 2.11590288431322e-06,
      "loss": 3.9836,
      "step": 44810
    },
    {
      "epoch": 2.7218072508653672,
      "grad_norm": 5.858946800231934,
      "learning_rate": 2.1067605241505252e-06,
      "loss": 3.8611,
      "step": 44820
    },
    {
      "epoch": 2.7224145260217405,
      "grad_norm": 5.465233325958252,
      "learning_rate": 2.0976375327766473e-06,
      "loss": 4.1315,
      "step": 44830
    },
    {
      "epoch": 2.7230218011781138,
      "grad_norm": 5.184696197509766,
      "learning_rate": 2.088533913881069e-06,
      "loss": 4.0744,
      "step": 44840
    },
    {
      "epoch": 2.723629076334487,
      "grad_norm": 4.767087936401367,
      "learning_rate": 2.079449671145417e-06,
      "loss": 3.8853,
      "step": 44850
    },
    {
      "epoch": 2.7242363514908607,
      "grad_norm": 5.345233917236328,
      "learning_rate": 2.0703848082435305e-06,
      "loss": 4.2033,
      "step": 44860
    },
    {
      "epoch": 2.724843626647234,
      "grad_norm": 3.8828444480895996,
      "learning_rate": 2.061339328841372e-06,
      "loss": 3.8425,
      "step": 44870
    },
    {
      "epoch": 2.7254509018036073,
      "grad_norm": 5.506280422210693,
      "learning_rate": 2.0523132365970756e-06,
      "loss": 4.0757,
      "step": 44880
    },
    {
      "epoch": 2.7260581769599805,
      "grad_norm": 5.381812572479248,
      "learning_rate": 2.043306535160938e-06,
      "loss": 3.7546,
      "step": 44890
    },
    {
      "epoch": 2.726665452116354,
      "grad_norm": 6.031261444091797,
      "learning_rate": 2.0343192281754244e-06,
      "loss": 3.6797,
      "step": 44900
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 7.039952754974365,
      "learning_rate": 2.0253513192751373e-06,
      "loss": 3.9492,
      "step": 44910
    },
    {
      "epoch": 2.727880002429101,
      "grad_norm": 6.32716178894043,
      "learning_rate": 2.0164028120868263e-06,
      "loss": 3.9548,
      "step": 44920
    },
    {
      "epoch": 2.728487277585474,
      "grad_norm": 5.023057460784912,
      "learning_rate": 2.007473710229435e-06,
      "loss": 4.0149,
      "step": 44930
    },
    {
      "epoch": 2.7290945527418473,
      "grad_norm": 5.101616382598877,
      "learning_rate": 1.9985640173140197e-06,
      "loss": 3.8141,
      "step": 44940
    },
    {
      "epoch": 2.7297018278982206,
      "grad_norm": 6.194119453430176,
      "learning_rate": 1.989673736943809e-06,
      "loss": 3.8465,
      "step": 44950
    },
    {
      "epoch": 2.7303091030545943,
      "grad_norm": 4.634875297546387,
      "learning_rate": 1.98080287271416e-06,
      "loss": 3.9831,
      "step": 44960
    },
    {
      "epoch": 2.7309163782109676,
      "grad_norm": 6.2330546379089355,
      "learning_rate": 1.971951428212615e-06,
      "loss": 4.0234,
      "step": 44970
    },
    {
      "epoch": 2.731523653367341,
      "grad_norm": 5.221096992492676,
      "learning_rate": 1.9631194070188262e-06,
      "loss": 3.7667,
      "step": 44980
    },
    {
      "epoch": 2.732130928523714,
      "grad_norm": 6.42039680480957,
      "learning_rate": 1.9543068127045982e-06,
      "loss": 4.0047,
      "step": 44990
    },
    {
      "epoch": 2.7327382036800874,
      "grad_norm": 4.270226955413818,
      "learning_rate": 1.9455136488338855e-06,
      "loss": 3.6221,
      "step": 45000
    },
    {
      "epoch": 2.7327382036800874,
      "eval_loss": 4.160186290740967,
      "eval_runtime": 2174.9965,
      "eval_samples_per_second": 7.571,
      "eval_steps_per_second": 1.893,
      "step": 45000
    }
  ],
  "logging_steps": 10,
  "max_steps": 49401,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "total_flos": 9.406513152e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
